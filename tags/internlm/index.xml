<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>InternLM on 奔跑的蜗牛</title>
        <link>https://ntopic.cn/tags/internlm/</link>
        <description>Recent content in InternLM on 奔跑的蜗牛</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>zh-cn</language>
        <lastBuildDate>Fri, 23 Aug 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://ntopic.cn/tags/internlm/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>InternLM 2.5 书生·浦语 开源大模型本地部署体验</title>
        <link>https://ntopic.cn/p/2024082301/</link>
        <pubDate>Fri, 23 Aug 2024 00:00:00 +0000</pubDate>
        
        <guid>https://ntopic.cn/p/2024082301/</guid>
        <description>&lt;img src="https://ntopic.cn/p/2024082301/InternLM.jpg" alt="Featured image of post InternLM 2.5 书生·浦语 开源大模型本地部署体验" /&gt;&lt;p&gt;老牛同学之前偶尔刷到过&lt;strong&gt;InternLM&lt;/strong&gt;大模型相关的介绍文章，因为在老牛同学心中，&lt;strong&gt;Qwen2&lt;/strong&gt;千问才是国内开源模型中最适合自己的大模型，原因是自己在本地部署和应用&lt;strong&gt;Qwen2&lt;/strong&gt;都非常满意，所以没有在意&lt;strong&gt;InternLM&lt;/strong&gt;大模型，也就没有动力去了解它。&lt;/p&gt;
&lt;p&gt;今天老牛同学又刷到&lt;strong&gt;InternLM&lt;/strong&gt;大模型发布&lt;strong&gt;1.8B&lt;/strong&gt;新开源版本的文章，同时还知道了&lt;strong&gt;书生·浦语&lt;/strong&gt;是它的中文名。因老牛同学在上海生活了十几年了，当看到&lt;strong&gt;浦&lt;/strong&gt;字时有点敏感，猜测想是不是代表&lt;strong&gt;上海浦东&lt;/strong&gt;的意思？所以特意去查了一下，官网介绍：书生·浦语（InternLM）大语言模型由上海人工智能实验室联合多家机构共同推出。官网并没有解释&lt;strong&gt;浦&lt;/strong&gt;字的含义，因此老牛同学就算自己的猜测是对的了。&lt;/p&gt;
&lt;p&gt;既然是自己生活的城市发布的大语音模型，那就没有理由不去了解一下了，顺便部署体验一翻：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;InternLM&lt;/strong&gt; 大模型的简单介绍，顺便介绍一下官网的评测数据，方便大家阅读&lt;/li&gt;
&lt;li&gt;通过 Ollama 本地部署 &lt;strong&gt;InternLM&lt;/strong&gt; 大模型，同时通过不同方式进行推理调用，包括 API 调用、WebUI 等&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;书生浦语internlm介绍&#34;&gt;书生·浦语（InternLM）介绍&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;书生·浦语&lt;/strong&gt;系列大模型主页：&lt;a class=&#34;link&#34; href=&#34;https://internlm.intern-ai.org.cn&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://internlm.intern-ai.org.cn&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;书生·浦语（InternLM）&lt;/strong&gt; 由上海人工智能实验室（上海 AI 实验室）联合推出，上海 AI 实验室是我国人工智能领域的新型科研机构，它的研究方向包括：人工智能基础理论、人工智能开放平台、人工智能基础软件和基础硬件系统、人工智能应用、人工智能核心技术和人工智能伦理与政策。感觉就是个政府机构，老牛同学生活了这么多年竟然都不知道！&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;书生·浦语（InternLM）&lt;/strong&gt; 自 2023 年 6 月 7 日发布第 1 个大模型，到本月 8 月 4 号，开源发布&lt;strong&gt;InternLM 2.5 1.8B&lt;/strong&gt;小尺寸模型，目前&lt;strong&gt;InternLM 2.5&lt;/strong&gt;有 3 个不同尺寸：&lt;strong&gt;1.8B&lt;/strong&gt;、&lt;strong&gt;7B&lt;/strong&gt;和&lt;strong&gt;20B&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;同时，针对&lt;strong&gt;20B&lt;/strong&gt;参数量版本，官网提供了一些评测数据：&lt;a class=&#34;link&#34; href=&#34;https://github.com/InternLM/InternLM&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/InternLM/InternLM&lt;/a&gt;。&lt;strong&gt;InternLM-20B&lt;/strong&gt;显著领先主流的 13B 量级开源模型，在语言、知识学科综合评测上都超越 Llama2-70B，在推理能力评测上和 Llama2-70B 持平，而知识方面则仍有一定差距。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ntopic.cn/p/2024082301/01.jpg&#34;
	width=&#34;1356&#34;
	height=&#34;1303&#34;
	srcset=&#34;https://ntopic.cn/p/2024082301/01_hub98e112e57706562f9bab17915cbe0e3_195356_480x0_resize_q75_box.jpg 480w, https://ntopic.cn/p/2024082301/01_hub98e112e57706562f9bab17915cbe0e3_195356_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;InternLM能力评测&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;104&#34;
		data-flex-basis=&#34;249px&#34;
	
&gt;&lt;/p&gt;
&lt;h1 id=&#34;本地部署-internlm-25-大模型&#34;&gt;本地部署 InternLM 2.5 大模型&lt;/h1&gt;
&lt;p&gt;目前 Ollama 已经支持&lt;strong&gt;InternLM 2.5&lt;/strong&gt;大模型了：&lt;a class=&#34;link&#34; href=&#34;https://ollama.com/internlm/internlm2.5:1.8b-chat&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://ollama.com/internlm/internlm2.5:1.8b-chat&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ntopic.cn/p/2024082301/02.jpg&#34;
	width=&#34;1218&#34;
	height=&#34;898&#34;
	srcset=&#34;https://ntopic.cn/p/2024082301/02_hu48f049dcd9eff203995955877df01ba5_92835_480x0_resize_q75_box.jpg 480w, https://ntopic.cn/p/2024082301/02_hu48f049dcd9eff203995955877df01ba5_92835_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Ollama选择不同版本&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;135&#34;
		data-flex-basis=&#34;325px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;关于 Ollama 详细介绍，老牛同学之前有专门的文章，本文不在累赘：&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/majDONtuAUzN2SAaYWxH1Q&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Ollama 完整教程：本地 LLM 管理、WebUI 对话、Python/Java 客户端 API 应用&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;我们可以根据自己的需要选择不同的参数版本，老牛同学选择的最新发布的&lt;strong&gt;1.8B&lt;/strong&gt;参数量版本。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;下载模型权重文件&lt;/strong&gt;：&lt;code&gt;ollama run internlm/internlm2.5:1.8b-chat&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;下载完成之后，其实我们已经有了个控制台的对话界面了：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ntopic.cn/p/2024082301/03.jpg&#34;
	width=&#34;1537&#34;
	height=&#34;1288&#34;
	srcset=&#34;https://ntopic.cn/p/2024082301/03_hu7c82a5cb93d9808dc9af60bbb269dad0_191392_480x0_resize_q75_box.jpg 480w, https://ntopic.cn/p/2024082301/03_hu7c82a5cb93d9808dc9af60bbb269dad0_191392_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Ollama对话界面&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;119&#34;
		data-flex-basis=&#34;286px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;接下来，我们就可以通过多种方式使用推理服务了，包括：HTTP、Python 客户端、Java 客户端、WebUI 等，老牛同学简单介绍以下 WebUI 方式：&lt;/p&gt;
&lt;p&gt;Ollama 自带控制台对话界面体验总归是不太好，接下来部署 Web 可视化聊天界面：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;下载并安装 Node.js 工具：&lt;a class=&#34;link&#34; href=&#34;https://nodejs.org/zh-cn&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://nodejs.org/zh-cn&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;下载&lt;code&gt;ollama-webui&lt;/code&gt;工程代码：&lt;code&gt;git clone https://github.com/ollama-webui/ollama-webui-lite ollama-webui&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;切换&lt;code&gt;ollama-webui&lt;/code&gt;代码的目录：&lt;code&gt;cd ollama-webui&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;设置 Node.js 工具包镜像源（下载提速）：&lt;code&gt;npm config set registry http://mirrors.cloud.tencent.com/npm/&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;安装 Node.js 依赖的工具包：&lt;code&gt;npm install&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;最后，启动 Web 可视化界面：&lt;code&gt;npm run dev&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;11
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;12
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&amp;gt;npm run dev
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&amp;gt; ollama-webui-lite@0.0.1 dev
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&amp;gt; vite dev --host --port &lt;span class=&#34;m&#34;&gt;3000&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  VITE v4.5.2  ready in &lt;span class=&#34;m&#34;&gt;16023&lt;/span&gt; ms
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  ➜  Local:   http://localhost:3000/
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  ➜  Network: http://192.168.101.35:3000/
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  ➜  Network: http://172.27.112.1:3000/
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  ➜  Network: http://172.25.64.1:3000/
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  ➜  press h to show &lt;span class=&#34;nb&#34;&gt;help&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;如果看到以上输出，代表 Web 可视化界面已经成功了！&lt;/p&gt;
&lt;p&gt;浏览器打开 Web 可视化界面：&lt;a class=&#34;link&#34; href=&#34;http://localhost:3000&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;http://localhost:3000/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ntopic.cn/p/2024082301/04.jpg&#34;
	width=&#34;1261&#34;
	height=&#34;1351&#34;
	srcset=&#34;https://ntopic.cn/p/2024082301/04_hu375d01e32970dcf6165092ccfae7d2b4_173055_480x0_resize_q75_box.jpg 480w, https://ntopic.cn/p/2024082301/04_hu375d01e32970dcf6165092ccfae7d2b4_173055_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Ollam WebUI对话界面&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;93&#34;
		data-flex-basis=&#34;224px&#34;
	
&gt;&lt;/p&gt;
&lt;h1 id=&#34;总结internlm-其他能力&#34;&gt;总结：InternLM 其他能力&lt;/h1&gt;
&lt;p&gt;以上是老牛同学介绍如何部署和推理&lt;strong&gt;书生·浦语（InternLM）&lt;/strong&gt; 大模型，并进行最简单的对话推理，&lt;strong&gt;InternLM&lt;/strong&gt;的其他能力相关介绍，我们在官网都可以查到，包括：复杂的多步推理、多轮对话意图理解、对输出格式的控制和操作和复杂指令的理解。大家可以在本地进行体验。&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/KM-Z6FtVfaySewRTmvEc6w&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;vLLM CPU 和 GPU 模式署和推理 Qwen2 等大语言模型详细教程&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/x2RKTvFeKgRvi982X5cymA&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;MiniCPM-V 2.6 面壁“小钢炮”，多图、视频理解多模态模型，部署和推理实战教程&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/PpY3k3kReKfQdeOJyrB6aw&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;基于 Qwen2/Lllama3 等大模型，部署团队私有化 RAG 知识库系统的详细教程（Docker+AnythingLLM）&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/vt1EXVWtwm6ltZVYtB4-Tg&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;使用 Llama3/Qwen2 等开源大模型，部署团队私有化 Code Copilot 和使用教程&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/eq6K8_s9uX459OeUcRPEug&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;基于 Qwen2 大模型微调技术详细教程（LoRA 参数高效微调和 SwanLab 可视化监控）&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ntopic.cn/WX-21.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;微信公众号：老牛同学&#34;
	
	
&gt;&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>
