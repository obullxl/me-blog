<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Ollama on 奔跑的蜗牛</title>
        <link>https://ntopic.cn/tags/ollama/</link>
        <description>Recent content in Ollama on 奔跑的蜗牛</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>zh-cn</language>
        <lastBuildDate>Sat, 07 Dec 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://ntopic.cn/tags/ollama/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>Bolt.new 用一句话快速构建全栈应用：本地部署与应用实战（Ollama/Qwen2.5 等）</title>
        <link>https://ntopic.cn/p/2024120701/</link>
        <pubDate>Sat, 07 Dec 2024 00:00:00 +0000</pubDate>
        
        <guid>https://ntopic.cn/p/2024120701/</guid>
        <description>&lt;img src="https://ntopic.cn/p/2024120701/00.jpg" alt="Featured image of post Bolt.new 用一句话快速构建全栈应用：本地部署与应用实战（Ollama/Qwen2.5 等）" /&gt;&lt;p&gt;随着 AI 编程工具的迅猛发展，从早期的 Code Copilot（代码辅助）到如今备受瞩目的 Cursor、v0、Windsurf 和 &lt;strong&gt;Bolt.new&lt;/strong&gt; 等全栈开发平台。这些创新工具旨在加速项目开发、简化工作流程并提高研发效率。然而，访问这些工具通常依赖于“通畅的网络”和海外 LLM 模型，在某些情况下可能成为使用这些工具的障碍。&lt;/p&gt;
&lt;p&gt;作为一位大模型的爱好者和学习者，老牛同学今天分享一条不同的路径——&lt;strong&gt;如何利用本地 Ollama 和国内的大模型 API，在本地部署和使用 Bolt.new？&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;以下是老牛同学录制的本地部署和使用 &lt;strong&gt;Bolt.new&lt;/strong&gt; 的视频：通过一句话，即可自动完成整个小项目的代码编写和部署预览。&lt;/p&gt;
&lt;p&gt;【微信公众号视频链接】&lt;/p&gt;
&lt;h1 id=&#34;1-boltnew-概览&#34;&gt;1. Bolt.new 概览&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Bolt.new&lt;/strong&gt;是由 StackBlitz 推出的一款革新性的 AI 驱动全栈开发平台，它以几个关键特性脱颖而出：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;即时全栈环境&lt;/strong&gt;：借助 WebContainer 技术，Bolt.new 能够在浏览器中直接运行真实的 Node.js 环境，支持 npm 包安装、服务器配置及第三方 API 交互，为开发者提供了前所未有的便捷性。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;智能 AI 助手&lt;/strong&gt;：内置的强大 AI 功能可以理解并执行复杂的指令，无论是创建文件、编辑代码还是解决问题，都能显著提高工作效率。特别是其一键修复错误的功能，能够自动处理编译或运行时出现的问题，极大地节省了时间。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;简易部署流程&lt;/strong&gt;：集成的聊天界面让用户可以直接上传代码至云端，并选择合适的托管服务（如 Vercel）进行部署。生成的应用程序可以通过 URL 轻松分享，促进团队协作和成果展示。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;尽管 Bolt.new 带来了诸多便利，但也存在一些局限：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;缺乏版本控制&lt;/strong&gt;：代码调整可能导致原有版本被覆盖，增加了数据丢失的风险。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;频繁重新生成和部署&lt;/strong&gt;：每次修改需求时，Bolt.new 会重新生成整个代码库并部署，需要较长时间。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;对于快速原型设计和全栈功能开发，Bolt.new 凭借其完整的开发环境、智能化的辅助工具和简便的协作机制，是一个不错的选择。&lt;/p&gt;
&lt;h1 id=&#34;2-本地部署-boltnew&#34;&gt;2. 本地部署 Bolt.new&lt;/h1&gt;
&lt;h2 id=&#34;准备本地大模型&#34;&gt;准备本地大模型&lt;/h2&gt;
&lt;p&gt;Bolt.new 底层依赖 LLM，我们先准备 2 个 LLM 选项：本地运行 Ollama，和 API 调用的远程 LLM 服务（非必须）&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;本地 Ollama&lt;/strong&gt;：关于 Ollama 详细使用教程，请参考之前文章（&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/majDONtuAUzN2SAaYWxH1Q&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Ollama 完整教程&lt;/a&gt;），建议下载和启动&lt;strong&gt;Qwen2.5-Coder-7B&lt;/strong&gt;模型：&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;ollama run qwen2.5-coder:7b
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;LLM 服务 API&lt;/strong&gt;：Ollama 依赖电脑硬件配置，如果电脑硬件条件有限，我们还可以直接用户大模型服务 API，只需要兼容 OpenAPI 接口标准即可（老牛同学用的是百炼平台 Qwen2.5-Coder-32B 大模型）。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;下载和配置-boltnew&#34;&gt;下载和配置 Bolt.new&lt;/h2&gt;
&lt;p&gt;官方提供的 Bolt.new 并不直接支持本地 LLM 或自定义 API 设置。幸运的是，社区牛人&lt;strong&gt;coleam00&lt;/strong&gt;基于官方版本开发了一个增强版——&lt;a class=&#34;link&#34; href=&#34;https://github.com/stackblitz/bolt.new&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;bolt.new-any-llm&lt;/a&gt;，该版本不仅兼容多种 LLM，还能灵活配置 API 接口。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;克隆项目仓库&lt;/strong&gt;：&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;git clone https://github.com/coleam00/bolt.new-any-llm bolt.new-any-LLM
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;cd&lt;/span&gt; bolt.new-any-LLM
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;&lt;strong&gt;配置环境变量&lt;/strong&gt;：复制&lt;code&gt;.env.example&lt;/code&gt;为&lt;code&gt;.env&lt;/code&gt;，然后根据实际情况编辑&lt;code&gt;.env&lt;/code&gt;配置文件中的 API 地址和密钥。例如，Ollama 需要设置&lt;code&gt;OLLAMA_API_BASE_URL&lt;/code&gt;参数，国内模型 API 服务，则需要设置&lt;code&gt;OPENAI_LIKE_API_BASE_URL&lt;/code&gt;和&lt;code&gt;OPENAI_LIKE_API_KEY&lt;/code&gt;这 2 个参数。&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 复制配置文件&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;cp .env.example .env
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;然后，打开&lt;code&gt;.env&lt;/code&gt;配置文件，可以看到支持的模型列表，包括 GROQ、HuggingFace、Open AI 等，根据需要进行内容修改：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-plaintext&#34; data-lang=&#34;plaintext&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;# Ollama配置
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;OLLAMA_API_BASE_URL=http://localhost:11434
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;# 【可选】 老牛同学使用的是百炼平台
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;OPENAI_LIKE_API_BASE_URL=https://dashscope.aliyuncs.com/compatible-mode/v1
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;OPENAI_LIKE_API_KEY=真实Key内容
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;&lt;strong&gt;说明&lt;/strong&gt;：&lt;code&gt;OPENAI_LIKE_API_BASE_URL&lt;/code&gt;和&lt;code&gt;OPENAI_LIKE_API_KEY&lt;/code&gt;意思就是兼容 OpenAI 接口标准的大模型地址和 API Key，目前国内厂商基本都支持 OpenAPI 接口标准。&lt;/p&gt;
&lt;h2 id=&#34;boltnew-项目部署&#34;&gt;Bolt.new 项目部署&lt;/h2&gt;
&lt;p&gt;为了加快 Node.js 包下载速度，我们可以设置一下镜像源（老牛同学使用的是淘宝镜像）：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;npm config &lt;span class=&#34;nb&#34;&gt;set&lt;/span&gt; registry https://registry.npmmirror.com
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;其他镜像源如下列表，请按需选择：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;8
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-plaintext&#34; data-lang=&#34;plaintext&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;NPM官方: https://registry.npmjs.org
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;淘宝镜像: http://registry.npmmirror.com
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;阿里云镜像: https://npm.aliyun.com
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;腾讯云: https://mirrors.cloud.tencent.com/npm
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;华为云: https://mirrors.huaweicloud.com/repository/npm
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;网易: https://mirrors.163.com/npm
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;中科大: http://mirrors.ustc.edu.cn
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;清华: https://mirrors.tuna.tsinghua.edu.cn
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;然后，我们执行以下命令来安装依赖并启动 Bolt.new：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;8
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 安装pnpm包管理工具&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;npm install -g pnpm
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 安装项目依赖包&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;pnpm install
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 启动Bolt.new&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;pnpm run dev
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;启动成功后，我们可以看到如下输出信息：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;8
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&amp;gt;pnpm run dev
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&amp;gt; bolt@ dev D:&lt;span class=&#34;se&#34;&gt;\C&lt;/span&gt;odeSpace&lt;span class=&#34;se&#34;&gt;\b&lt;/span&gt;olt.new
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&amp;gt; remix vite:dev
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  ➜  Local:   http://localhost:5173/
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  ➜  Network: use --host to expose
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  ➜  press h + enter to show &lt;span class=&#34;nb&#34;&gt;help&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;接下来，我们开始体验本地化的 Bolt.new！&lt;/p&gt;
&lt;h1 id=&#34;3-使用-boltnew-进行开发&#34;&gt;3. 使用 Bolt.new 进行开发&lt;/h1&gt;
&lt;p&gt;通过浏览器打开 Bolt.new 本地地址：&lt;a class=&#34;link&#34; href=&#34;http://localhost:5173&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;http://localhost:5173&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;首先可以看到如下页面，与官方相比，多了一个&lt;strong&gt;Model Settings&lt;/strong&gt;的选项，在这里我们可以选择自己的模型：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ntopic.cn/p/2024120701/31.jpg&#34;
	width=&#34;1020&#34;
	height=&#34;988&#34;
	srcset=&#34;https://ntopic.cn/p/2024120701/31_hu9477948d34bd4d2c15bb2dbe15809e56_85332_480x0_resize_q75_box.jpg 480w, https://ntopic.cn/p/2024120701/31_hu9477948d34bd4d2c15bb2dbe15809e56_85332_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Bolt.new设置模型&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;103&#34;
		data-flex-basis=&#34;247px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;我们可以选择 Ollama 模型（如上图），也可以选择配置过&lt;strong&gt;OPENAI_LIKE_API&lt;/strong&gt;尝试模型（如老牛同学百炼平台 API 模型）：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ntopic.cn/p/2024120701/32.jpg&#34;
	width=&#34;996&#34;
	height=&#34;825&#34;
	srcset=&#34;https://ntopic.cn/p/2024120701/32_hu802f84a813b980fdab9c8d74c9bb9dce_62633_480x0_resize_q75_box.jpg 480w, https://ntopic.cn/p/2024120701/32_hu802f84a813b980fdab9c8d74c9bb9dce_62633_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;OpenAI接口模型&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;120&#34;
		data-flex-basis=&#34;289px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;选择完模型，我可以输入我们的需求：&lt;code&gt;写一个计算器页面&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;接下来的过程，就是老牛同学上面录制的视频所示了。&lt;/p&gt;
&lt;p&gt;Bolt.new 可以根据我们的一句话内容，自动拆分成不同的小步骤：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ntopic.cn/p/2024120701/91.jpg&#34;
	width=&#34;985&#34;
	height=&#34;1006&#34;
	srcset=&#34;https://ntopic.cn/p/2024120701/91_hua37bb5ea70b40a2d99acb7b32878cf30_97340_480x0_resize_q75_box.jpg 480w, https://ntopic.cn/p/2024120701/91_hua37bb5ea70b40a2d99acb7b32878cf30_97340_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;拆分实现步骤&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;97&#34;
		data-flex-basis=&#34;234px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;然后，自动生成完整的项目结构和执行步骤，包括文件名等：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ntopic.cn/p/2024120701/92.jpg&#34;
	width=&#34;822&#34;
	height=&#34;798&#34;
	srcset=&#34;https://ntopic.cn/p/2024120701/92_hufae3bc8dbabe05cf8b7de8d85444b594_42833_480x0_resize_q75_box.jpg 480w, https://ntopic.cn/p/2024120701/92_hufae3bc8dbabe05cf8b7de8d85444b594_42833_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;项目结构和步骤&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;103&#34;
		data-flex-basis=&#34;247px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;在右侧，显示源文件列表和动态展示每个文件生成过程：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ntopic.cn/p/2024120701/93.jpg&#34;
	width=&#34;1290&#34;
	height=&#34;1326&#34;
	srcset=&#34;https://ntopic.cn/p/2024120701/93_hu13709eb150c5a69a13b107a5dec56fc6_108379_480x0_resize_q75_box.jpg 480w, https://ntopic.cn/p/2024120701/93_hu13709eb150c5a69a13b107a5dec56fc6_108379_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;源文件列表和内容&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;97&#34;
		data-flex-basis=&#34;233px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;最终，所有源代码研发完成，自动部署整个和提供预览：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ntopic.cn/p/2024120701/94.jpg&#34;
	width=&#34;1282&#34;
	height=&#34;1327&#34;
	srcset=&#34;https://ntopic.cn/p/2024120701/94_hu612bbd794967ab483ed00619009bf697_79497_480x0_resize_q75_box.jpg 480w, https://ntopic.cn/p/2024120701/94_hu612bbd794967ab483ed00619009bf697_79497_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;项目部署和预览&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;96&#34;
		data-flex-basis=&#34;231px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;接下来，如果我们觉得哪里需要修改、或者有什么报错，直接提问，Bolt.new 会自动进行修改并部署和预览！&lt;/p&gt;
&lt;h1 id=&#34;4-总结&#34;&gt;4. 总结&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Bolt.new&lt;/strong&gt;只需通过自然语音，就能实现全栈研发和自动部署的能力，对于追求高效开发和快速交付的团队而言，这是一个值得尝试的工具。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;友情提示&lt;/strong&gt;：对于保密性较高、或数据安全要求较高的项目，通过调用外部大模型 API 服务使用 Bolt.new 工具时，请注意数据安全问题！&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Pipeline 任务：&lt;/p&gt;
&lt;p&gt;&lt;small&gt;&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/FR4384AZV2FE2xtweSh9bA&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Transformers 框架任务概览：从零开始掌握 Pipeline（管道）与 Task（任务）&lt;/a&gt;&lt;/small&gt;&lt;/p&gt;
&lt;p&gt;&lt;small&gt;&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/uN2BFIOxDFEh4T-W7tsPbg&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Transformers 框架 Pipeline 任务详解：文本转音频（text-to-audio 或 text-to-speech）&lt;/a&gt;&lt;/small&gt;&lt;/p&gt;
&lt;p&gt;&lt;small&gt;&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/9ccEDNfeGNf_Q9pO0Usg2w&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Transformers 框架 Pipeline 任务详解：文本分类（text-classification）&lt;/a&gt;&lt;/small&gt;&lt;/p&gt;
&lt;p&gt;往期推荐文章：&lt;/p&gt;
&lt;p&gt;&lt;small&gt;&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/lAAIfl0YJRNrppp5-Vuusw&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;深入解析 Transformers 框架（一）：包和对象加载中的设计巧思与实用技巧&lt;/a&gt;&lt;/small&gt;&lt;/p&gt;
&lt;p&gt;&lt;small&gt;&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/WIbbrkf1HjVC1CtBNcU8Ow&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;深入解析 Transformers 框架（二）：AutoModel 初始化及 Qwen2.5 模型加载全流程&lt;/a&gt;&lt;/small&gt;&lt;/p&gt;
&lt;p&gt;&lt;small&gt;&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/Shg30uUFByM0tKTi0rETfg&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;深入解析 Transformers 框架（三）：Qwen2.5 大模型的 AutoTokenizer 技术细节&lt;/a&gt;&lt;/small&gt;&lt;/p&gt;
&lt;p&gt;&lt;small&gt;&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/GnoHXsIYKYFU1Xo4u5sE1w&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;深入解析 Transformers 框架（四）：Qwen2.5/GPT 分词流程与 BPE 分词算法技术细节详解&lt;/a&gt;&lt;/small&gt;&lt;/p&gt;
&lt;p&gt;&lt;small&gt;&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/8f3xna9TRmxMDaY_cQhy8Q&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;基于 Qwen2.5-Coder 模型和 CrewAI 多智能体框架，实现智能编程系统的实战教程&lt;/a&gt;&lt;/small&gt;&lt;/p&gt;
&lt;p&gt;&lt;small&gt;&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/KM-Z6FtVfaySewRTmvEc6w&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;vLLM CPU 和 GPU 模式署和推理 Qwen2 等大语言模型详细教程&lt;/a&gt;&lt;/small&gt;&lt;/p&gt;
&lt;p&gt;&lt;small&gt;&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/PpY3k3kReKfQdeOJyrB6aw&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;基于 Qwen2/Lllama3 等大模型，部署团队私有化 RAG 知识库系统的详细教程（Docker+AnythingLLM）&lt;/a&gt;&lt;/small&gt;&lt;/p&gt;
&lt;p&gt;&lt;small&gt;&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/vt1EXVWtwm6ltZVYtB4-Tg&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;使用 Llama3/Qwen2 等开源大模型，部署团队私有化 Code Copilot 和使用教程&lt;/a&gt;&lt;/small&gt;&lt;/p&gt;
&lt;p&gt;&lt;small&gt;&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/eq6K8_s9uX459OeUcRPEug&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;基于 Qwen2 大模型微调技术详细教程（LoRA 参数高效微调和 SwanLab 可视化监控）&lt;/a&gt;&lt;/small&gt;&lt;/p&gt;
&lt;p&gt;&lt;small&gt;&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/9ldLuh3YLvx8oWvwnrSGUA&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;ChatTTS 长音频合成和本地部署 2 种方式，让你的“儿童绘本”发声的实战教程&lt;/a&gt;&lt;/small&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ntopic.cn/WX-21.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;微信公众号：老牛同学&#34;
	
	
&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title>InternLM 2.5 书生·浦语 开源大模型本地部署体验</title>
        <link>https://ntopic.cn/p/2024082301/</link>
        <pubDate>Fri, 23 Aug 2024 00:00:00 +0000</pubDate>
        
        <guid>https://ntopic.cn/p/2024082301/</guid>
        <description>&lt;img src="https://ntopic.cn/p/2024082301/InternLM.jpg" alt="Featured image of post InternLM 2.5 书生·浦语 开源大模型本地部署体验" /&gt;&lt;p&gt;老牛同学之前偶尔刷到过&lt;strong&gt;InternLM&lt;/strong&gt;大模型相关的介绍文章，因为在老牛同学心中，&lt;strong&gt;Qwen2&lt;/strong&gt;千问才是国内开源模型中最适合自己的大模型，原因是自己在本地部署和应用&lt;strong&gt;Qwen2&lt;/strong&gt;都非常满意，所以没有在意&lt;strong&gt;InternLM&lt;/strong&gt;大模型，也就没有动力去了解它。&lt;/p&gt;
&lt;p&gt;今天老牛同学又刷到&lt;strong&gt;InternLM&lt;/strong&gt;大模型发布&lt;strong&gt;1.8B&lt;/strong&gt;新开源版本的文章，同时还知道了&lt;strong&gt;书生·浦语&lt;/strong&gt;是它的中文名。因老牛同学在上海生活了十几年了，当看到&lt;strong&gt;浦&lt;/strong&gt;字时有点敏感，猜测想是不是代表&lt;strong&gt;上海浦东&lt;/strong&gt;的意思？所以特意去查了一下，官网介绍：书生·浦语（InternLM）大语言模型由上海人工智能实验室联合多家机构共同推出。官网并没有解释&lt;strong&gt;浦&lt;/strong&gt;字的含义，因此老牛同学就算自己的猜测是对的了。&lt;/p&gt;
&lt;p&gt;既然是自己生活的城市发布的大语音模型，那就没有理由不去了解一下了，顺便部署体验一翻：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;InternLM&lt;/strong&gt; 大模型的简单介绍，顺便介绍一下官网的评测数据，方便大家阅读&lt;/li&gt;
&lt;li&gt;通过 Ollama 本地部署 &lt;strong&gt;InternLM&lt;/strong&gt; 大模型，同时通过不同方式进行推理调用，包括 API 调用、WebUI 等&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;书生浦语internlm介绍&#34;&gt;书生·浦语（InternLM）介绍&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;书生·浦语&lt;/strong&gt;系列大模型主页：&lt;a class=&#34;link&#34; href=&#34;https://internlm.intern-ai.org.cn&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://internlm.intern-ai.org.cn&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;书生·浦语（InternLM）&lt;/strong&gt; 由上海人工智能实验室（上海 AI 实验室）联合推出，上海 AI 实验室是我国人工智能领域的新型科研机构，它的研究方向包括：人工智能基础理论、人工智能开放平台、人工智能基础软件和基础硬件系统、人工智能应用、人工智能核心技术和人工智能伦理与政策。感觉就是个政府机构，老牛同学生活了这么多年竟然都不知道！&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;书生·浦语（InternLM）&lt;/strong&gt; 自 2023 年 6 月 7 日发布第 1 个大模型，到本月 8 月 4 号，开源发布&lt;strong&gt;InternLM 2.5 1.8B&lt;/strong&gt;小尺寸模型，目前&lt;strong&gt;InternLM 2.5&lt;/strong&gt;有 3 个不同尺寸：&lt;strong&gt;1.8B&lt;/strong&gt;、&lt;strong&gt;7B&lt;/strong&gt;和&lt;strong&gt;20B&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;同时，针对&lt;strong&gt;20B&lt;/strong&gt;参数量版本，官网提供了一些评测数据：&lt;a class=&#34;link&#34; href=&#34;https://github.com/InternLM/InternLM&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/InternLM/InternLM&lt;/a&gt;。&lt;strong&gt;InternLM-20B&lt;/strong&gt;显著领先主流的 13B 量级开源模型，在语言、知识学科综合评测上都超越 Llama2-70B，在推理能力评测上和 Llama2-70B 持平，而知识方面则仍有一定差距。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ntopic.cn/p/2024082301/01.jpg&#34;
	width=&#34;1356&#34;
	height=&#34;1303&#34;
	srcset=&#34;https://ntopic.cn/p/2024082301/01_hub98e112e57706562f9bab17915cbe0e3_195356_480x0_resize_q75_box.jpg 480w, https://ntopic.cn/p/2024082301/01_hub98e112e57706562f9bab17915cbe0e3_195356_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;InternLM能力评测&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;104&#34;
		data-flex-basis=&#34;249px&#34;
	
&gt;&lt;/p&gt;
&lt;h1 id=&#34;本地部署-internlm-25-大模型&#34;&gt;本地部署 InternLM 2.5 大模型&lt;/h1&gt;
&lt;p&gt;目前 Ollama 已经支持&lt;strong&gt;InternLM 2.5&lt;/strong&gt;大模型了：&lt;a class=&#34;link&#34; href=&#34;https://ollama.com/internlm/internlm2.5:1.8b-chat&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://ollama.com/internlm/internlm2.5:1.8b-chat&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ntopic.cn/p/2024082301/02.jpg&#34;
	width=&#34;1218&#34;
	height=&#34;898&#34;
	srcset=&#34;https://ntopic.cn/p/2024082301/02_hu48f049dcd9eff203995955877df01ba5_92835_480x0_resize_q75_box.jpg 480w, https://ntopic.cn/p/2024082301/02_hu48f049dcd9eff203995955877df01ba5_92835_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Ollama选择不同版本&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;135&#34;
		data-flex-basis=&#34;325px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;关于 Ollama 详细介绍，老牛同学之前有专门的文章，本文不在累赘：&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/majDONtuAUzN2SAaYWxH1Q&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Ollama 完整教程：本地 LLM 管理、WebUI 对话、Python/Java 客户端 API 应用&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;我们可以根据自己的需要选择不同的参数版本，老牛同学选择的最新发布的&lt;strong&gt;1.8B&lt;/strong&gt;参数量版本。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;下载模型权重文件&lt;/strong&gt;：&lt;code&gt;ollama run internlm/internlm2.5:1.8b-chat&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;下载完成之后，其实我们已经有了个控制台的对话界面了：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ntopic.cn/p/2024082301/03.jpg&#34;
	width=&#34;1537&#34;
	height=&#34;1288&#34;
	srcset=&#34;https://ntopic.cn/p/2024082301/03_hu7c82a5cb93d9808dc9af60bbb269dad0_191392_480x0_resize_q75_box.jpg 480w, https://ntopic.cn/p/2024082301/03_hu7c82a5cb93d9808dc9af60bbb269dad0_191392_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Ollama对话界面&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;119&#34;
		data-flex-basis=&#34;286px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;接下来，我们就可以通过多种方式使用推理服务了，包括：HTTP、Python 客户端、Java 客户端、WebUI 等，老牛同学简单介绍以下 WebUI 方式：&lt;/p&gt;
&lt;p&gt;Ollama 自带控制台对话界面体验总归是不太好，接下来部署 Web 可视化聊天界面：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;下载并安装 Node.js 工具：&lt;a class=&#34;link&#34; href=&#34;https://nodejs.org/zh-cn&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://nodejs.org/zh-cn&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;下载&lt;code&gt;ollama-webui&lt;/code&gt;工程代码：&lt;code&gt;git clone https://github.com/ollama-webui/ollama-webui-lite ollama-webui&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;切换&lt;code&gt;ollama-webui&lt;/code&gt;代码的目录：&lt;code&gt;cd ollama-webui&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;设置 Node.js 工具包镜像源（下载提速）：&lt;code&gt;npm config set registry http://mirrors.cloud.tencent.com/npm/&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;安装 Node.js 依赖的工具包：&lt;code&gt;npm install&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;最后，启动 Web 可视化界面：&lt;code&gt;npm run dev&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;11
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;12
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&amp;gt;npm run dev
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&amp;gt; ollama-webui-lite@0.0.1 dev
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&amp;gt; vite dev --host --port &lt;span class=&#34;m&#34;&gt;3000&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  VITE v4.5.2  ready in &lt;span class=&#34;m&#34;&gt;16023&lt;/span&gt; ms
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  ➜  Local:   http://localhost:3000/
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  ➜  Network: http://192.168.101.35:3000/
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  ➜  Network: http://172.27.112.1:3000/
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  ➜  Network: http://172.25.64.1:3000/
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  ➜  press h to show &lt;span class=&#34;nb&#34;&gt;help&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;如果看到以上输出，代表 Web 可视化界面已经成功了！&lt;/p&gt;
&lt;p&gt;浏览器打开 Web 可视化界面：&lt;a class=&#34;link&#34; href=&#34;http://localhost:3000&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;http://localhost:3000/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ntopic.cn/p/2024082301/04.jpg&#34;
	width=&#34;1261&#34;
	height=&#34;1351&#34;
	srcset=&#34;https://ntopic.cn/p/2024082301/04_hu375d01e32970dcf6165092ccfae7d2b4_173055_480x0_resize_q75_box.jpg 480w, https://ntopic.cn/p/2024082301/04_hu375d01e32970dcf6165092ccfae7d2b4_173055_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Ollam WebUI对话界面&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;93&#34;
		data-flex-basis=&#34;224px&#34;
	
&gt;&lt;/p&gt;
&lt;h1 id=&#34;总结internlm-其他能力&#34;&gt;总结：InternLM 其他能力&lt;/h1&gt;
&lt;p&gt;以上是老牛同学介绍如何部署和推理&lt;strong&gt;书生·浦语（InternLM）&lt;/strong&gt; 大模型，并进行最简单的对话推理，&lt;strong&gt;InternLM&lt;/strong&gt;的其他能力相关介绍，我们在官网都可以查到，包括：复杂的多步推理、多轮对话意图理解、对输出格式的控制和操作和复杂指令的理解。大家可以在本地进行体验。&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/KM-Z6FtVfaySewRTmvEc6w&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;vLLM CPU 和 GPU 模式署和推理 Qwen2 等大语言模型详细教程&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/x2RKTvFeKgRvi982X5cymA&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;MiniCPM-V 2.6 面壁“小钢炮”，多图、视频理解多模态模型，部署和推理实战教程&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/PpY3k3kReKfQdeOJyrB6aw&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;基于 Qwen2/Lllama3 等大模型，部署团队私有化 RAG 知识库系统的详细教程（Docker+AnythingLLM）&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/vt1EXVWtwm6ltZVYtB4-Tg&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;使用 Llama3/Qwen2 等开源大模型，部署团队私有化 Code Copilot 和使用教程&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/eq6K8_s9uX459OeUcRPEug&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;基于 Qwen2 大模型微调技术详细教程（LoRA 参数高效微调和 SwanLab 可视化监控）&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ntopic.cn/WX-21.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;微信公众号：老牛同学&#34;
	
	
&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Ollama完整教程：本地LLM管理、WebUI对话、Python/Java客户端API应用</title>
        <link>https://ntopic.cn/p/2024071001/</link>
        <pubDate>Wed, 10 Jul 2024 00:00:00 +0000</pubDate>
        
        <guid>https://ntopic.cn/p/2024071001/</guid>
        <description>&lt;img src="https://ntopic.cn/p/2024071001/00.png" alt="Featured image of post Ollama完整教程：本地LLM管理、WebUI对话、Python/Java客户端API应用" /&gt;&lt;p&gt;老牛同学在前面有关大模型应用的文章中，多次使用了&lt;strong&gt;Ollama&lt;/strong&gt;来管理和部署本地大模型（包括：&lt;strong&gt;Qwen2&lt;/strong&gt;、&lt;strong&gt;Llama3&lt;/strong&gt;、&lt;strong&gt;Phi3&lt;/strong&gt;、&lt;strong&gt;Gemma2&lt;/strong&gt;等），但对&lt;strong&gt;Ollama&lt;/strong&gt;这个非常方便管理本地大模型的软件的介绍却很少。&lt;/p&gt;
&lt;p&gt;目前，清华和智谱 AI 联合发布开源的&lt;strong&gt;GLM4-9B&lt;/strong&gt;大模型也能支持&lt;strong&gt;Ollama&lt;/strong&gt;进行本地部署了（&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/g7lDfnRRGdrHqN7WGMSkAg&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;本地部署 GLM-4-9B 清华智谱开源大模型方法和对话效果体验&lt;/a&gt;），&lt;strong&gt;Ollama&lt;/strong&gt;支持的大模型越多越普及，对于的应用也就越多。为了降低大家查阅资料等学习时间，老牛同学今天尝试着对 Ollama 进行一次详细完整介绍。毕竟老牛同学也在不断学习中，若有疏漏或者错误之处，还请各位朋友多多指正，谢谢大家。&lt;/p&gt;
&lt;p&gt;本文将分为以下章节对 Ollama 进行介绍：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Ollama 基本介绍，它的作用是什么&lt;/li&gt;
&lt;li&gt;Ollama 软件安装、一些常用的系统参数设置&lt;/li&gt;
&lt;li&gt;Ollama 管理本地已有大模型（包括终端对话界面）&lt;/li&gt;
&lt;li&gt;Ollama 导入模型到本地的三种方式：直接从 Ollama 远程仓库拉取、通过 GGUF 模型权重文件导入到本地、通过 safetensors 模型权限文件导入到本地&lt;/li&gt;
&lt;li&gt;基于 WebUI 部署 Ollama 可视化对话界面&lt;/li&gt;
&lt;li&gt;Ollama 客户端 API 应用，包括 Python API 和 Java API 接口应用&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;ollama-是什么它与-llama-有什么关系&#34;&gt;Ollama 是什么，它与 Llama 有什么关系？&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Ollama&lt;/strong&gt;官网：&lt;a class=&#34;link&#34; href=&#34;https://ollama.com&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://ollama.com/&lt;/a&gt;，官方网站的介绍就一句话：&lt;strong&gt;Get up and running with large language models.&lt;/strong&gt; （开始使用大语言模型。）&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Ollama&lt;/strong&gt;是一个开源的 LLM（大型语言模型）服务工具，用于简化在本地运行大语言模型、降低使用大语言模型的门槛，使得大模型的开发者、研究人员和爱好者能够在本地环境快速实验、管理和部署最新大语言模型，包括如&lt;strong&gt;Qwen2&lt;/strong&gt;、&lt;strong&gt;Llama3&lt;/strong&gt;、&lt;strong&gt;Phi3&lt;/strong&gt;、&lt;strong&gt;Gemma2&lt;/strong&gt;等开源的大型语言模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Ollama&lt;/strong&gt;支持的大语言模型列表，可通过搜索模型名称查看：&lt;a class=&#34;link&#34; href=&#34;https://ollama.com/library&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://ollama.com/library&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Ollama&lt;/strong&gt;官方 GitHub 源代码仓库：&lt;a class=&#34;link&#34; href=&#34;https://github.com/ollama/ollama&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/ollama/ollama/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Llama&lt;/strong&gt;是 Meta 公司开源的备受欢迎的一个通用大语言模型，和其他大模型一样，&lt;strong&gt;Llama&lt;/strong&gt;可以通过&lt;strong&gt;Ollama&lt;/strong&gt;进行管理部署和推理等。&lt;/p&gt;
&lt;p&gt;因此，&lt;code&gt;Ollama&lt;/code&gt;与&lt;code&gt;Llama&lt;/code&gt;的关系：&lt;code&gt;Llama&lt;/code&gt;是大语言模型，而&lt;code&gt;Ollama&lt;/code&gt;是大语言模型（不限于&lt;code&gt;Llama&lt;/code&gt;模型）便捷的管理和运维工具，它们只是名字后面部分恰巧相同而已！&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ntopic.cn/p/2024071001/01.png&#34;
	width=&#34;1184&#34;
	height=&#34;868&#34;
	srcset=&#34;https://ntopic.cn/p/2024071001/01_hufd706277049935fc4199d22c78f03807_115448_480x0_resize_box_3.png 480w, https://ntopic.cn/p/2024071001/01_hufd706277049935fc4199d22c78f03807_115448_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Ollama官网&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;136&#34;
		data-flex-basis=&#34;327px&#34;
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;ollama-安装和常用系统参数设置&#34;&gt;Ollama 安装和常用系统参数设置&lt;/h2&gt;
&lt;p&gt;在官网首页，我们可以直接下载&lt;strong&gt;Ollama&lt;/strong&gt;安装程序（支持 Windows/MacOS/Linux）：&lt;a class=&#34;link&#34; href=&#34;https://ollama.com&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://ollama.com/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Ollama&lt;/strong&gt;的安装过程，与安装其他普通软件并没有什么两样，安装完成之后，有几个常用的系统&lt;strong&gt;环境变量&lt;/strong&gt;参数建议进行设置：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;OLLAMA_MODELS&lt;/strong&gt;：模型文件存放目录，默认目录为当前用户目录（Windows 目录：&lt;code&gt;C:\Users%username%.ollama\models&lt;/code&gt;，MacOS 目录：&lt;code&gt;~/.ollama/models&lt;/code&gt;，Linux 目录：&lt;code&gt;/usr/share/ollama/.ollama/models&lt;/code&gt;），如果是 Windows 系统&lt;strong&gt;建议修改&lt;/strong&gt;（如：D:\OllamaModels），避免 C 盘空间吃紧&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;OLLAMA_HOST&lt;/strong&gt;：Ollama 服务监听的网络地址，默认为&lt;strong&gt;127.0.0.1&lt;/strong&gt;，如果允许其他电脑访问 Ollama（如：局域网中的其他电脑），&lt;strong&gt;建议设置&lt;/strong&gt;成&lt;strong&gt;0.0.0.0&lt;/strong&gt;，从而允许其他网络访问&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;OLLAMA_PORT&lt;/strong&gt;：Ollama 服务监听的默认端口，默认为&lt;strong&gt;11434&lt;/strong&gt;，如果端口有冲突，可以修改设置成其他端口（如：&lt;strong&gt;8080&lt;/strong&gt;等）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;OLLAMA_ORIGINS&lt;/strong&gt;：HTTP 客户端请求来源，半角逗号分隔列表，若本地使用无严格要求，可以设置成星号，代表不受限制&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;OLLAMA_KEEP_ALIVE&lt;/strong&gt;：大模型加载到内存中后的存活时间，默认为&lt;strong&gt;5m&lt;/strong&gt;即 5 分钟（如：纯数字如 300 代表 300 秒，0 代表处理请求响应后立即卸载模型，任何负数则表示一直存活）；我们可设置成&lt;strong&gt;24h&lt;/strong&gt;，即模型在内存中保持 24 小时，提高访问速度&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;OLLAMA_NUM_PARALLEL&lt;/strong&gt;：请求处理并发数量，默认为&lt;strong&gt;1&lt;/strong&gt;，即单并发串行处理请求，可根据实际情况进行调整&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;OLLAMA_MAX_QUEUE&lt;/strong&gt;：请求队列长度，默认值为&lt;strong&gt;512&lt;/strong&gt;，可以根据情况设置，超过队列长度请求被抛弃&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;OLLAMA_DEBUG&lt;/strong&gt;：输出 Debug 日志标识，应用研发阶段可以设置成&lt;strong&gt;1&lt;/strong&gt;，即输出详细日志信息，便于排查问题&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;OLLAMA_MAX_LOADED_MODELS&lt;/strong&gt;：最多同时加载到内存中模型的数量，默认为&lt;strong&gt;1&lt;/strong&gt;，即只能有 1 个模型在内存中&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;ollama-管理本地已有大模型&#34;&gt;Ollama 管理本地已有大模型&lt;/h2&gt;
&lt;p&gt;【展示本地大模型列表：&lt;code&gt;ollama list&lt;/code&gt;】&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&amp;gt;ollama list
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;NAME            ID              SIZE    MODIFIED
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;gemma2:9b       c19987e1e6e2    5.4 GB  &lt;span class=&#34;m&#34;&gt;7&lt;/span&gt; days ago
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;qwen2:7b        e0d4e1163c58    4.4 GB  &lt;span class=&#34;m&#34;&gt;10&lt;/span&gt; days ago
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;可以看到，老牛同学本地有 2 个大模型，它们的名称（&lt;strong&gt;NAME&lt;/strong&gt;）分别为&lt;strong&gt;gemma2:9b&lt;/strong&gt;和&lt;strong&gt;qwen2:7b&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;【删除单个本地大模型：&lt;code&gt;ollama rm 本地模型名称&lt;/code&gt;】&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&amp;gt;ollama rm gemma2:9b
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;deleted &lt;span class=&#34;s1&#34;&gt;&amp;#39;gemma2:9b&amp;#39;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&amp;gt;ollama list
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;NAME            ID              SIZE    MODIFIED
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;qwen2:7b        e0d4e1163c58    4.4 GB  &lt;span class=&#34;m&#34;&gt;10&lt;/span&gt; days ago
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;老牛同学通过&lt;code&gt;rm&lt;/code&gt;命令删除了&lt;strong&gt;gemma2:9b&lt;/strong&gt;大模型之后，再次通过&lt;code&gt;list&lt;/code&gt;命令查看，本地只有&lt;strong&gt;qwen2:7b&lt;/strong&gt;一个大模型了。&lt;/p&gt;
&lt;p&gt;【启动本地模型：&lt;code&gt;ollama run 本地模型名&lt;/code&gt;】&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&amp;gt;ollama run qwen2:0.5b
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&amp;gt;&amp;gt;&amp;gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;启动成功之后，就可以通过终端对话界面进行对话了（本命令下面也会讲到，其他详细暂且忽略）&lt;/p&gt;
&lt;p&gt;【查看本地运行中模型列表：&lt;code&gt;ollama ps&lt;/code&gt;】&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&amp;gt;ollama ps
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;NAME            ID              SIZE    PROCESSOR       UNTIL
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;qwen2:0.5b      6f48b936a09f    &lt;span class=&#34;m&#34;&gt;693&lt;/span&gt; MB  100% CPU        &lt;span class=&#34;m&#34;&gt;4&lt;/span&gt; minutes from now
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;通过&lt;code&gt;ps&lt;/code&gt;命名可以看到，老牛同学本地&lt;strong&gt;qwen2:0.5b&lt;/strong&gt;大模型正在运行中。&lt;/p&gt;
&lt;p&gt;【复制本地大模型：&lt;code&gt;ollama cp 本地存在的模型名 新复制模型名&lt;/code&gt;】&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;8
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&amp;gt;ollama cp qwen2:0.5b Qwen2-0.5B
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;copied &lt;span class=&#34;s1&#34;&gt;&amp;#39;qwen2:0.5b&amp;#39;&lt;/span&gt; to &lt;span class=&#34;s1&#34;&gt;&amp;#39;Qwen2-0.5B&amp;#39;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&amp;gt;ollama list
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;NAME                    ID              SIZE    MODIFIED
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Qwen2-0.5B:latest       6f48b936a09f    &lt;span class=&#34;m&#34;&gt;352&lt;/span&gt; MB  &lt;span class=&#34;m&#34;&gt;4&lt;/span&gt; seconds ago
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;qwen2:0.5b              6f48b936a09f    &lt;span class=&#34;m&#34;&gt;352&lt;/span&gt; MB  &lt;span class=&#34;m&#34;&gt;29&lt;/span&gt; minutes ago
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;qwen2:7b                e0d4e1163c58    4.4 GB  &lt;span class=&#34;m&#34;&gt;10&lt;/span&gt; days ago
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;上面&lt;code&gt;cp&lt;/code&gt;命令，老牛同学把本地&lt;strong&gt;qwen2:0.5b&lt;/strong&gt;复制了一份，新模型名为&lt;strong&gt;Qwen2-0.5B&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;下面老牛同学介绍三种通过 Ollama 下载到本地大模型方式：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;方式一：直接通过 Ollama 远程仓库下载，这是最直接的方式，也是最推荐、最常用的方式&lt;/li&gt;
&lt;li&gt;方式二：如果已经有 GGUF 模型权重文件了，不想重新下载，也可以通过 Ollama 把该文件直接导入到本地（不推荐、不常用）&lt;/li&gt;
&lt;li&gt;方式三：如果已经有 safetensors 模型权重文件，也不想重新下载，也可以通过 Ollama 把该文件直接导入到本地（不推荐、不常用）&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;方式一ollama-从远程仓库下载大模型到本地&#34;&gt;方式一：Ollama 从远程仓库下载大模型到本地&lt;/h2&gt;
&lt;p&gt;【下载或者更新本地大模型：&lt;code&gt;ollama pull 本地/远程仓库模型名称&lt;/code&gt;】&lt;/p&gt;
&lt;p&gt;本&lt;code&gt;pull&lt;/code&gt;命令从 Ollama 远程仓库完整下载或增量更新模型文件，模型名称&lt;strong&gt;格式&lt;/strong&gt;为：&lt;strong&gt;模型名称:参数规格&lt;/strong&gt;；如&lt;code&gt;ollama pull qwen2:0.5b&lt;/code&gt; 则代表从 Ollama 仓库下载&lt;strong&gt;qwen2&lt;/strong&gt;大模型的&lt;strong&gt;0.5b&lt;/strong&gt;参数规格大模型文件到本地磁盘：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ntopic.cn/p/2024071001/02.png&#34;
	width=&#34;1263&#34;
	height=&#34;843&#34;
	srcset=&#34;https://ntopic.cn/p/2024071001/02_hu02ca4010d74396d6714efc9541980158_62927_480x0_resize_box_3.png 480w, https://ntopic.cn/p/2024071001/02_hu02ca4010d74396d6714efc9541980158_62927_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Qwen2模型列表&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;149&#34;
		data-flex-basis=&#34;359px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;如果参数规格标记为&lt;code&gt;latest&lt;/code&gt;则代表为默认参数规格，下载时&lt;strong&gt;可以&lt;/strong&gt;不用指定，如&lt;strong&gt;Qwen2&lt;/strong&gt;的&lt;strong&gt;7b&lt;/strong&gt;被标记为&lt;code&gt;latest&lt;/code&gt;，则&lt;code&gt;ollama pull qwen2&lt;/code&gt;和&lt;code&gt;ollama pull qwen2:7b&lt;/code&gt;这 2 个命令的意义是一样的，都下载的为&lt;strong&gt;7b&lt;/strong&gt;参数规格模型。为了保证后续维护方便、避免误操作等，老牛同学&lt;strong&gt;建议&lt;/strong&gt;不管是否为默认参数规格，我们下载命令中均明确参数规格。&lt;/p&gt;
&lt;p&gt;值得一提的是，今天开始&lt;strong&gt;GLM4&lt;/strong&gt;支持 Ollama 部署和推理，老牛同学特意列出它的下载命令：&lt;code&gt;ollama pull glm4:9b&lt;/code&gt;（和其他模型相比，其实并没有特殊支出）。需要注意的是：Ollama 最低版本为&lt;strong&gt;0.2.0&lt;/strong&gt;才能支持&lt;strong&gt;GLM4&lt;/strong&gt;大模型！&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ntopic.cn/p/2024071001/03.png&#34;
	width=&#34;1243&#34;
	height=&#34;828&#34;
	srcset=&#34;https://ntopic.cn/p/2024071001/03_hu118c2da1f699e89313971e2fd074c126_74513_480x0_resize_box_3.png 480w, https://ntopic.cn/p/2024071001/03_hu118c2da1f699e89313971e2fd074c126_74513_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;GLM4模型列表&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;150&#34;
		data-flex-basis=&#34;360px&#34;
	
&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;11
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;12
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;13
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;14
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;15
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;16
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;17
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;18
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;19
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;20
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&amp;gt;ollama pull qwen2:0.5b
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;pulling manifest
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;pulling manifest
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;pulling manifest
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;pulling manifest
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;pulling manifest
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;pulling 8de95da68dc4... 100% ▕████████████████████████▏ &lt;span class=&#34;m&#34;&gt;352&lt;/span&gt; MB
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;pulling 62fbfd9ed093... 100% ▕████████████████████████▏  &lt;span class=&#34;m&#34;&gt;182&lt;/span&gt; B
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;pulling c156170b718e... 100% ▕████████████████████████▏  &lt;span class=&#34;m&#34;&gt;11&lt;/span&gt; KB
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;pulling f02dd72bb242... 100% ▕████████████████████████▏   &lt;span class=&#34;m&#34;&gt;59&lt;/span&gt; B
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;pulling 2184ab82477b... 100% ▕████████████████████████▏  &lt;span class=&#34;m&#34;&gt;488&lt;/span&gt; B
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;verifying sha256 digest
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;writing manifest
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;removing any unused layers
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;success
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&amp;gt;ollama list
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;NAME            ID              SIZE    MODIFIED
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;qwen2:0.5b      6f48b936a09f    &lt;span class=&#34;m&#34;&gt;352&lt;/span&gt; MB  &lt;span class=&#34;m&#34;&gt;9&lt;/span&gt; minutes ago
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;qwen2:7b        e0d4e1163c58    4.4 GB  &lt;span class=&#34;m&#34;&gt;10&lt;/span&gt; days ago
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;若本地不存在大模型，则&lt;strong&gt;下载&lt;/strong&gt;完整模型文件到本地磁盘；若本地磁盘存在该大模型，则&lt;strong&gt;增量&lt;/strong&gt;下载大模型更新文件到本地磁盘。&lt;/p&gt;
&lt;p&gt;从上面最后的&lt;code&gt;list&lt;/code&gt;命令结果可以看到，老牛同学本地存在了&lt;strong&gt;qwen2:0.5b&lt;/strong&gt;这个名称的大模型。&lt;/p&gt;
&lt;p&gt;【下载且运行本地大模型：&lt;code&gt;ollama run 本地/远程仓库模型名称&lt;/code&gt;】&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&amp;gt;ollama run qwen2:0.5b
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&amp;gt;&amp;gt;&amp;gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;若本地不存在大模型，则&lt;strong&gt;下载&lt;/strong&gt;完整模型文件到本地磁盘（类似于&lt;code&gt;pull&lt;/code&gt;命令），然后&lt;strong&gt;启动&lt;/strong&gt;大模型；若本地存在大模型，则直接启动（不进行更新）。&lt;/p&gt;
&lt;p&gt;启动成功后，默认为终端对客界面：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ntopic.cn/p/2024071001/04.png&#34;
	width=&#34;1207&#34;
	height=&#34;216&#34;
	srcset=&#34;https://ntopic.cn/p/2024071001/04_huf67a4b71ed958ca54d4b3d68cae80c4d_33165_480x0_resize_box_3.png 480w, https://ntopic.cn/p/2024071001/04_huf67a4b71ed958ca54d4b3d68cae80c4d_33165_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Ollama终端对话界面&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;558&#34;
		data-flex-basis=&#34;1341px&#34;
	
&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;若需要输入多行文本，需要用&lt;strong&gt;三引号&lt;/strong&gt;包裹，如：&lt;code&gt;&amp;quot;&amp;quot;&amp;quot;这里是多行文本&amp;quot;&amp;quot;&amp;quot;&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;/clear&lt;/code&gt;清除对话上下文信息&lt;/li&gt;
&lt;li&gt;&lt;code&gt;/bye&lt;/code&gt;则退出对话窗口&lt;/li&gt;
&lt;li&gt;&lt;code&gt;/set parameter num_ctx 4096&lt;/code&gt;可设置窗口大小为 4096 个 Token，也可以通过请求设置，如：&lt;code&gt;curl &amp;lt;http://localhost:11434/api/generate&amp;gt; -d &#39;{ &amp;quot;model&amp;quot;: &amp;quot;qwen2:7b&amp;quot;, &amp;quot;prompt&amp;quot;: &amp;quot;Why is the sky blue?&amp;quot;, &amp;quot;options&amp;quot;: { &amp;quot;num_ctx&amp;quot;: 4096 }}&#39;&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;/show info&lt;/code&gt;可以查看当前模型详情：
，&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;11
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;12
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;13
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;14
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;15
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-plaintext&#34; data-lang=&#34;plaintext&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&amp;gt;&amp;gt;&amp;gt; /show info
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  Model
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        arch                    qwen2
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        parameters              494.03M
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        quantization            Q4_0
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        context length          32768
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        embedding length        896
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  Parameters
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        stop    &amp;#34;&amp;lt;|im_start|&amp;gt;&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        stop    &amp;#34;&amp;lt;|im_end|&amp;gt;&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  License
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        Apache License
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        Version 2.0, January 2004
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h2 id=&#34;方式二ollama-导入-gguf-模型文件到本地磁盘&#34;&gt;方式二：Ollama 导入 GGUF 模型文件到本地磁盘&lt;/h2&gt;
&lt;p&gt;若我们已经从 HF 或者 ModeScope 下载了 GGUF 文件（文件名为：&lt;strong&gt;Meta-Llama-3-8B-Instruct.Q4_K_M.gguf&lt;/strong&gt;），在我们存放&lt;code&gt;Llama3-8B&lt;/code&gt;的 GGUF 模型文件目录中，创建一个文件名为&lt;code&gt;Modelfile&lt;/code&gt;的文件，该文件的内容如下：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;FROM ./Meta-Llama-3-8B-Instruct.Q4_K_M.gguf
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;然后，打开终端，执行命令导入模型文件：&lt;code&gt;ollama create 模型名称 -f ./Modelfile&lt;/code&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&amp;gt;ollama create Llama-3-8B -f ./Modelfile
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;transferring model data
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;using existing layer sha256:647a2b64cbcdbe670432d0502ebb2592b36dd364d51a9ef7a1387b7a4365781f
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;creating new layer sha256:459d7c837b2bd7f895a15b0a5213846912693beedaf0257fbba2a508bc1c88d9
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;writing manifest
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;success
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;导入成功之后，我们就可以通过&lt;code&gt;list&lt;/code&gt;命名，看到名为&lt;strong&gt;Llama-3-8B&lt;/strong&gt;的本地模型了，后续可以和其他模型一样进行管理了。&lt;/p&gt;
&lt;h2 id=&#34;方式三ollama-导入-safetensors-模型文件到到本地磁盘&#34;&gt;方式三：Ollama 导入 safetensors 模型文件到到本地磁盘&lt;/h2&gt;
&lt;p&gt;官方操作文档：&lt;a class=&#34;link&#34; href=&#34;https://ollama.fan/getting-started/import/#importing-pytorch-safetensors&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://ollama.fan/getting-started/import/#importing-pytorch-safetensors&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;若我们已经从 HF 或者 ModeScope 下载了 safetensors 文件（文件目录为：&lt;strong&gt;Mistral-7B&lt;/strong&gt;），&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;git lfs install
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;git clone https://www.modelscope.cn/rubraAI/Mistral-7B-Instruct-v0.3.git Mistral-7B
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;然后，我们转换模型（结果：&lt;code&gt;Mistral-7B-v0.3.bin&lt;/code&gt;）：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;python llm/llama.cpp/convert.py ./Mistral-7B --outtype f16 --outfile Mistral-7B-v0.3.bin
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;接下来，进行量化量化：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;llm/llama.cpp/quantize Mistral-7B-v0.3.bin Mistral-7B-v0.3_Q4.bin q4_0
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;最后，通过 Ollama 导入到本地磁盘，创建&lt;code&gt;Modelfile&lt;/code&gt;模型文件：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;FROM Mistral-7B-v0.3_Q4.bin
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;执行导入命令，导入模型文件：&lt;code&gt;ollama create 模型名称 -f ./Modelfile&lt;/code&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&amp;gt;ollama create Mistral-7B-v0.3 -f ./Modelfile
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;transferring model data
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;using existing layer sha256:647a2b64cbcdbe670432d0502ebb2592b36dd364d51a9ef7a1387b7a4365781f
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;creating new layer sha256:459d7c837b2bd7f895a15b0a5213846912693beedaf0257fbba2a508bc1c88d9
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;writing manifest
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;success
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;导入成功之后，我们就可以通过&lt;code&gt;list&lt;/code&gt;命名，看到名为&lt;strong&gt;Mistral-7B-v0.3&lt;/strong&gt;的本地模型了，后续可以和其他模型一样进行管理了。&lt;/p&gt;
&lt;h2 id=&#34;基于-webui-部署-ollama-可视化对话界面&#34;&gt;基于 WebUI 部署 Ollama 可视化对话界面&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Ollama&lt;/strong&gt;自带控制台对话界面体验总归是不太好，接下来部署 Web 可视化聊天界面：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;下载并安装 Node.js 工具：&lt;a class=&#34;link&#34; href=&#34;https://nodejs.org/zh-cn&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://nodejs.org/zh-cn&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;下载&lt;code&gt;ollama-webui&lt;/code&gt;工程代码：&lt;code&gt;git clone https://github.com/ollama-webui/ollama-webui-lite ollama-webui&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;切换&lt;code&gt;ollama-webui&lt;/code&gt;代码的目录：&lt;code&gt;cd ollama-webui&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;设置 Node.js 工具包镜像源（下载提速）：&lt;code&gt;npm config set registry http://mirrors.cloud.tencent.com/npm/&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;安装 Node.js 依赖的工具包：&lt;code&gt;npm install&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;最后，启动 Web 可视化界面：&lt;code&gt;npm run dev&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;https://ntopic.cn/p/2024071001/05.jpg&#34;
	width=&#34;824&#34;
	height=&#34;270&#34;
	srcset=&#34;https://ntopic.cn/p/2024071001/05_hu40881898ab4c7102699947df68ce27e5_49721_480x0_resize_q75_box.jpg 480w, https://ntopic.cn/p/2024071001/05_hu40881898ab4c7102699947df68ce27e5_49721_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Ollam WebUI启动成功&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;305&#34;
		data-flex-basis=&#34;732px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;如果看到以上输出，代表 Web 可视化界面已经成功了！&lt;/p&gt;
&lt;p&gt;浏览器打开 Web 可视化界面：&lt;a class=&#34;link&#34; href=&#34;http://localhost:3000&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;http://localhost:3000/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ntopic.cn/p/2024071001/06.png&#34;
	width=&#34;1834&#34;
	height=&#34;1441&#34;
	srcset=&#34;https://ntopic.cn/p/2024071001/06_hu608dd0b0c5cb0d5d28aae5fe81670e22_224513_480x0_resize_box_3.png 480w, https://ntopic.cn/p/2024071001/06_hu608dd0b0c5cb0d5d28aae5fe81670e22_224513_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Ollam WebUI对话界面&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;127&#34;
		data-flex-basis=&#34;305px&#34;
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;ollama-客户端http-访问服务&#34;&gt;Ollama 客户端：HTTP 访问服务&lt;/h2&gt;
&lt;p&gt;Ollama 默认提供了&lt;code&gt;generate&lt;/code&gt;和&lt;code&gt;chat&lt;/code&gt;这 2 个原始的 API 接口，使用方式如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;code&gt;generate&lt;/code&gt;接口的使用样例：&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;curl http://localhost:11434/api/generate -d &lt;span class=&#34;s2&#34;&gt;&amp;#34;{
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;  &amp;#39;model&amp;#39;: &amp;#39;qwen:0.5b&amp;#39;,
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;  &amp;#39;prompt&amp;#39;: &amp;#39;为什么天空是蓝色的？&amp;#39;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;}&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;&lt;code&gt;chat&lt;/code&gt;接口的使用样例：&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;curl http://localhost:11434/api/chat -d &lt;span class=&#34;s1&#34;&gt;&amp;#39;{
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s1&#34;&gt;  &amp;#34;model&amp;#34;: &amp;#34;qwen:7b&amp;#34;,
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s1&#34;&gt;  &amp;#34;messages&amp;#34;: [
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s1&#34;&gt;    { &amp;#34;role&amp;#34;: &amp;#34;user&amp;#34;, &amp;#34;content&amp;#34;: &amp;#34;为什么天空是蓝色的？&amp;#34; }
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s1&#34;&gt;  ]
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s1&#34;&gt;}&amp;#39;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;接下来的&lt;strong&gt;Python&lt;/strong&gt;和&lt;strong&gt;Java&lt;/strong&gt;客户端应用，都是对这 2 个接口的封装。&lt;/p&gt;
&lt;h2 id=&#34;ollama-客户端python-api-应用&#34;&gt;Ollama 客户端：Python API 应用&lt;/h2&gt;
&lt;p&gt;我们把 Ollama 集成到 Python 应用中，只需要以下简单 2 步即可：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;第一步&lt;/strong&gt;，安装 Python 依赖包：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;pip install ollama
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;&lt;strong&gt;第二步&lt;/strong&gt;，使用 Ollama 接口，&lt;code&gt;stream=True&lt;/code&gt;代表按照流式输出：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;11
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;12
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;13
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;14
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;15
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;16
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;17
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;18
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;19
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;20
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;21
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;22
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;23
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;24
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;25
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;26
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;27
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;28
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;29
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;30
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;ollama&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 流式输出&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;api_generate&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;text&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;提问：&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;text&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;n&#34;&gt;stream&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ollama&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;generate&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;stream&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;model&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;qwen:7b&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;prompt&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;text&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;-----------------------------------------&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;chunk&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;stream&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;not&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;chunk&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;done&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;chunk&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;response&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;end&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;flush&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;else&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;\n&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;-----------------------------------------&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;总耗时：&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;chunk&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;total_duration&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;-----------------------------------------&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;vm&#34;&gt;__name__&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;==&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;__main__&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;c1&#34;&gt;# 流式输出&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;n&#34;&gt;api_generate&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;text&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;天空为什么是蓝色的？&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;c1&#34;&gt;# 非流式输出&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;n&#34;&gt;content&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ollama&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;generate&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;model&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;qwen:0.5b&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;prompt&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;天空为什么是蓝色的？&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;content&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h2 id=&#34;ollama-客户端java-api-应用springboot-应用&#34;&gt;Ollama 客户端：Java API 应用（SpringBoot 应用）&lt;/h2&gt;
&lt;p&gt;我们也可以把 Ollama 集成到 SpringBoot 应用中，只需要以下简单 3 步即可：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;第一步&lt;/strong&gt;，在总&lt;code&gt;pom.xml&lt;/code&gt;中新增 SpringBoot Starter 依赖：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-xml&#34; data-lang=&#34;xml&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nt&#34;&gt;&amp;lt;dependency&amp;gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;nt&#34;&gt;&amp;lt;groupId&amp;gt;&lt;/span&gt;io.springboot.ai&lt;span class=&#34;nt&#34;&gt;&amp;lt;/groupId&amp;gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;nt&#34;&gt;&amp;lt;artifactId&amp;gt;&lt;/span&gt;spring-ai-ollama-spring-boot-starter&lt;span class=&#34;nt&#34;&gt;&amp;lt;/artifactId&amp;gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;nt&#34;&gt;&amp;lt;version&amp;gt;&lt;/span&gt;1.0.0&lt;span class=&#34;nt&#34;&gt;&amp;lt;/version&amp;gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nt&#34;&gt;&amp;lt;/dependency&amp;gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;&lt;strong&gt;第二步&lt;/strong&gt;，在 SpringBoot 配置文件&lt;code&gt;application.properties&lt;/code&gt;中增加 Ollama 配置信息：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-properties&#34; data-lang=&#34;properties&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;na&#34;&gt;server.port&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;8088&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;na&#34;&gt;spring.application.name&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;NTopicBootX&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;na&#34;&gt;spring.ai.ollama.base-url&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;http://localhost:11434&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;na&#34;&gt;spring.ai.ollama.chat.options.model&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;qwen:0.5b&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;配置文件指定了 Ollama API 地址和端口，同时指定了默认模型&lt;strong&gt;qwen:0.5b&lt;/strong&gt;（注意：模型需要在本地已经存在）&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;第三步&lt;/strong&gt;，使用&lt;code&gt;OllamaChatClient&lt;/code&gt;进行文字生成或者对话：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;11
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;12
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;13
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;14
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;15
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;16
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;17
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;18
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;19
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;20
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;21
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;22
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;23
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;24
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;25
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;26
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;27
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;28
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;29
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;30
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;31
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;32
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;33
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;34
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;35
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;36
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;37
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;38
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;39
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;40
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;41
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;42
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;43
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;44
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;45
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;46
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;47
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;48
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;49
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-java&#34; data-lang=&#34;java&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nn&#34;&gt;org.springframework.ai.chat.ChatResponse&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nn&#34;&gt;org.springframework.ai.chat.prompt.Prompt&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nn&#34;&gt;org.springframework.ai.ollama.OllamaChatClient&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nn&#34;&gt;org.springframework.ai.ollama.api.OllamaOptions&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nn&#34;&gt;org.springframework.beans.factory.annotation.Autowired&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nn&#34;&gt;org.springframework.beans.factory.annotation.Qualifier&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nn&#34;&gt;org.springframework.web.bind.annotation.GetMapping&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nn&#34;&gt;org.springframework.web.bind.annotation.RequestParam&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nn&#34;&gt;org.springframework.web.bind.annotation.RestController&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nd&#34;&gt;@RestController&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;kd&#34;&gt;public&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;kd&#34;&gt;class&lt;/span&gt; &lt;span class=&#34;nc&#34;&gt;OllamaClientController&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nd&#34;&gt;@Autowired&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nd&#34;&gt;@Qualifier&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;ollamaChatClient&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;kd&#34;&gt;private&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;OllamaChatClient&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ollamaChatClient&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;cm&#34;&gt;/**
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;cm&#34;&gt;     * http://localhost:8088/ollama/chat/v1?msg=天空为什么是蓝色的？
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;cm&#34;&gt;     */&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nd&#34;&gt;@GetMapping&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;/ollama/chat/v1&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;kd&#34;&gt;public&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;String&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nf&#34;&gt;ollamaChat&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nd&#34;&gt;@RequestParam&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;String&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;msg&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;k&#34;&gt;return&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;k&#34;&gt;this&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;na&#34;&gt;ollamaChatClient&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;na&#34;&gt;call&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;msg&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;cm&#34;&gt;/**
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;cm&#34;&gt;     * http://localhost:8088/ollama/chat/v2?msg=人为什么要不断的追求卓越？
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;cm&#34;&gt;     */&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nd&#34;&gt;@GetMapping&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;/ollama/chat/v2&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;kd&#34;&gt;public&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Object&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nf&#34;&gt;ollamaChatV2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nd&#34;&gt;@RequestParam&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;String&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;msg&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Prompt&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;prompt&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;k&#34;&gt;new&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Prompt&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;msg&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ChatResponse&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;chatResponse&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ollamaChatClient&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;na&#34;&gt;call&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;prompt&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;k&#34;&gt;return&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;chatResponse&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;cm&#34;&gt;/**
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;cm&#34;&gt;     * http://localhost:8088/ollama/chat/v3?msg=你认为老牛同学的文章如何？
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;cm&#34;&gt;     */&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nd&#34;&gt;@GetMapping&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;/ollama/chat/v3&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;kd&#34;&gt;public&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Object&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nf&#34;&gt;ollamaChatV3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nd&#34;&gt;@RequestParam&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;String&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;msg&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Prompt&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;prompt&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;k&#34;&gt;new&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Prompt&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;                &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;msg&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;                &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;OllamaOptions&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;na&#34;&gt;create&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;                        &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;na&#34;&gt;withModel&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;qwen:0.5b&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;                        &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;na&#34;&gt;withTemperature&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;na&#34;&gt;4F&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;));&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ChatResponse&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;chatResponse&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ollamaChatClient&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;na&#34;&gt;call&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;prompt&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;k&#34;&gt;return&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;chatResponse&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;na&#34;&gt;getResult&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;().&lt;/span&gt;&lt;span class=&#34;na&#34;&gt;getOutput&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;().&lt;/span&gt;&lt;span class=&#34;na&#34;&gt;getContent&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;();&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;以上是 Java 客户端的简单样例，我们可以通过&lt;code&gt;OllamaChatClient&lt;/code&gt;访问 Ollama 接口，既可以使用默认大模型，也可以在参数指定模型名称！&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/PpY3k3kReKfQdeOJyrB6aw&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;基于 Qwen2/Lllama3 等大模型，部署团队私有化 RAG 知识库系统的详细教程（Docker+AnythingLLM）&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/vt1EXVWtwm6ltZVYtB4-Tg&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;使用 Llama3/Qwen2 等开源大模型，部署团队私有化 Code Copilot 和使用教程&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/g7lDfnRRGdrHqN7WGMSkAg&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;本地部署 GLM-4-9B 清华智谱开源大模型方法和对话效果体验&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/MekCUJDhKzuUnoykkGoH2g&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;玩转 AI，笔记本电脑安装属于自己的 Llama 3 8B 大模型和对话客户端&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/rL3vyJ_xEj7GGoKaxUh8_A&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;ChatTTS 开源文本转语音模型本地部署、API 使用和搭建 WebUI 界面&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ntopic.cn/WX-21.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;微信公众号：老牛同学&#34;
	
	
&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Google 发布了最新的开源大模型 Gemma 2，本地快速部署和体验</title>
        <link>https://ntopic.cn/p/2024070201/</link>
        <pubDate>Tue, 02 Jul 2024 00:00:00 +0000</pubDate>
        
        <guid>https://ntopic.cn/p/2024070201/</guid>
        <description>&lt;img src="https://ntopic.cn/p/2024070201/00.png" alt="Featured image of post Google 发布了最新的开源大模型 Gemma 2，本地快速部署和体验" /&gt;&lt;p&gt;Gemma 2 是 Google 最新发布的开源大语言模型。它有两种规模：&lt;strong&gt;90 亿&lt;/strong&gt;（9B）参数和 &lt;strong&gt;270 亿&lt;/strong&gt;（27B）参数，分别具有&lt;strong&gt;基础&lt;/strong&gt;（预训练）和&lt;strong&gt;指令调优&lt;/strong&gt;版本，拥有 8K Tokens 的上下文长度：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Gemma-2-9b：&lt;/strong&gt; 90 亿参数基础模型版本&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Gemma-2-9b-it：&lt;/strong&gt; 90 亿参数基础模型的指令调优版本&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Gemma-2-27B：&lt;/strong&gt; 270 亿参数基础模型版本&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Gemma-2-27B-it：&lt;/strong&gt; 270 亿参数基础模型的指令调优版本&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://ntopic.cn/p/2024070201/01.png&#34;
	width=&#34;1689&#34;
	height=&#34;702&#34;
	srcset=&#34;https://ntopic.cn/p/2024070201/01_huc0c4e1950e8ea56d5272b8c68d576173_1058031_480x0_resize_box_3.png 480w, https://ntopic.cn/p/2024070201/01_huc0c4e1950e8ea56d5272b8c68d576173_1058031_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Gemma 2大模型&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;240&#34;
		data-flex-basis=&#34;577px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;Gemma 2 模型的训练数据量约为其第一代的两倍，总计 13 万亿 Tokens（270 亿模型）和 8 万亿 Tokens（90 亿模型）的网页数据（主要是英语）、代码和数学数据。同时，相比较第一代，Gemma 2 的推理性能更高、效率更高，并在安全性方面取得了重大进步。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;许可协议：&lt;/strong&gt; Gemma 2 与第一代使用相同的许可证，这是一个允许再分发、微调、商业用途和衍生作品的宽松许可证。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;性能优异：&lt;/strong&gt; Gemma 2 27B 版本在同规模级别中性能最佳，甚至比两倍于其尺寸的机型更具竞争力。9B 版本的性能在同类产品中也处于领先地位，超过了 Llama 3 8B 和其他同规模的开放模型。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ntopic.cn/p/2024070201/02.png&#34;
	width=&#34;1000&#34;
	height=&#34;562&#34;
	srcset=&#34;https://ntopic.cn/p/2024070201/02_hu8f408a2d11d5e81665e92e21d0e62dbd_111352_480x0_resize_box_3.png 480w, https://ntopic.cn/p/2024070201/02_hu8f408a2d11d5e81665e92e21d0e62dbd_111352_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Gemma 2评测对比&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;177&#34;
		data-flex-basis=&#34;427px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;其他关于 Gemma 2 的介绍信息，可以参见 Google 官方博客：&lt;a class=&#34;link&#34; href=&#34;https://blog.google/technology/developers/google-gemma-2/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://blog.google/technology/developers/google-gemma-2/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Google 重磅发布产品，我们肯定需要体验以下。今天，老牛同学就和大家一起，分别通过 2 种方式在个人笔记本电脑本地部署和体验 &lt;strong&gt;Gemma2-9B&lt;/strong&gt; 大模型。&lt;/p&gt;
&lt;h2 id=&#34;方式一通过-ollama-部署大模型&#34;&gt;方式一：通过 Ollama 部署大模型&lt;/h2&gt;
&lt;p&gt;关于 Ollama 是什么以及它的使用方式，老牛同学前面的博文中有介绍，本文不在赘述，感兴趣的朋友可以看一下之前的博文。&lt;/p&gt;
&lt;p&gt;Ollama 管理和维护 Gemma 2 比较简单，主要流程如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;下载并安装 Ollama 软件（Windows/Linux/MacOS 均支持）：&lt;a class=&#34;link&#34; href=&#34;https://ollama.com/download&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://ollama.com/download&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;通过 Ollama 下载并启动 Gemma 2 大模型：&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;ollama run gemma2:9b
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;模型文件大小为 &lt;strong&gt;5.4GB&lt;/strong&gt; 左右，需要耐心等待模型下载完成。下载完成之后，Ollama 自动启动模型，就可以通过 Ollama 进行对话了：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ntopic.cn/p/2024070201/03.png&#34;
	width=&#34;1639&#34;
	height=&#34;700&#34;
	srcset=&#34;https://ntopic.cn/p/2024070201/03_hu95d5fa022cb76a43452306ec30e4292f_72955_480x0_resize_box_3.png 480w, https://ntopic.cn/p/2024070201/03_hu95d5fa022cb76a43452306ec30e4292f_72955_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Gemma 2对话界面&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;234&#34;
		data-flex-basis=&#34;561px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;如果觉得通过控制台的方式对话体验不好，可以部署 WebUI 的方式与模型对话。WebUI 的部署方式，可以参见老牛同学之前的博文：&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/P_ufvz4MWVSqv_VM-rJp9w&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://mp.weixin.qq.com/s/P_ufvz4MWVSqv_VM-rJp9w&lt;/a&gt;，主要部署步骤：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;下载并安装 Node.js 工具：&lt;a class=&#34;link&#34; href=&#34;https://nodejs.org/zh-cn&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://nodejs.org/zh-cn&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;下载&lt;code&gt;ollama-webui&lt;/code&gt;工程代码：&lt;code&gt;git clone https://github.com/ollama-webui/ollama-webui-lite ollama-webui&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;切换&lt;code&gt;ollama-webui&lt;/code&gt;代码的目录：&lt;code&gt;cd ollama-webui&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;设置 Node.js 工具包镜像源（下载提速）：&lt;code&gt;npm config set registry http://mirrors.cloud.tencent.com/npm/&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;安装 Node.js 依赖的工具包：&lt;code&gt;npm install&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;最后，启动 Web 可视化界面：&lt;code&gt;npm run dev&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;然后，通过浏览器打开 WebUI 对话界面了：&lt;a class=&#34;link&#34; href=&#34;http://localhost:3000/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;http://localhost:3000/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ntopic.cn/p/2024070201/04.png&#34;
	width=&#34;1849&#34;
	height=&#34;1431&#34;
	srcset=&#34;https://ntopic.cn/p/2024070201/04_huc01418b7783981310b3740fccc9ba825_206579_480x0_resize_box_3.png 480w, https://ntopic.cn/p/2024070201/04_huc01418b7783981310b3740fccc9ba825_206579_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;WebUI对话界面示例&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;129&#34;
		data-flex-basis=&#34;310px&#34;
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;方式二通过-gguf-部署大模型&#34;&gt;方式二：通过 GGUF 部署大模型&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;GGUF&lt;/strong&gt;模型文件格式是为了快速推理和优化内存使用而设计的，支持更复杂的令牌化过程和特殊令牌处理，能更好地应对多样化的语言模型需求。&lt;strong&gt;GGUF&lt;/strong&gt;就一个文件，也简化了模型交换和部署的过程，它对促进模型的普及和应用有着积极作用。&lt;/p&gt;
&lt;p&gt;GGUF 模型文件列表：&lt;a class=&#34;link&#34; href=&#34;https://modelscope.cn/models/LLM-Research/gemma-2-9b-it-GGUF/files&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://modelscope.cn/models/LLM-Research/gemma-2-9b-it-GGUF/files&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ntopic.cn/p/2024070201/05.png&#34;
	width=&#34;705&#34;
	height=&#34;1131&#34;
	srcset=&#34;https://ntopic.cn/p/2024070201/05_hu7d700f8c9f468421316f1872bf9d2f39_104635_480x0_resize_box_3.png 480w, https://ntopic.cn/p/2024070201/05_hu7d700f8c9f468421316f1872bf9d2f39_104635_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;GGUF 模型文件列表&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;62&#34;
		data-flex-basis=&#34;149px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;GGUF 模型文件名称格式，如&lt;code&gt;gemma-2-9b-it-Q5_K_M.gguf&lt;/code&gt;等：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;it&lt;/strong&gt;代表本模型是对基线模型进行了微调，用于更好地理解和生成遵循指令（instruction-following）的文本，以提供符合要求的响应&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Q4/Q5 等&lt;/strong&gt;代表模型权重的量化位数（其中&lt;strong&gt;Q&lt;/strong&gt;是&lt;strong&gt;Quantization&lt;/strong&gt;的缩小，即量化），是一种模型压缩技术，用于减少模型大小，同时降低对计算资源的需求（特别是内存），但又尽量保持模型的性能；数字&lt;strong&gt;4&lt;/strong&gt;或&lt;strong&gt;5&lt;/strong&gt;则代表量化精度的位数（Q4 是 4 位，Q5 是 5 位等），精度越高模型体积和内存使用也会越大，但仍然远小于未量化的基线模型&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;K_M/K_S&lt;/strong&gt;代表了与注意力机制相关的特定配置，&lt;strong&gt;K_M&lt;/strong&gt; 可能是指 Key 的 Mask，即用来屏蔽某些位置的键值对，防止它们在注意力计算中被考虑；而 &lt;strong&gt;K_S&lt;/strong&gt; 可能是指 Key 的 Scale 或 Size，涉及到键向量缩放，这是在多头注意力机制中常见的操作，以稳定梯度&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;点击&lt;strong&gt;下载&lt;/strong&gt;图标即可下载，由于文件较大，浏览器的下载容易过程容易终端，重试可继续下载（假设下载本地的文件名为：&lt;code&gt;Gemma-2-9B-it-Q5_K_M.gguf&lt;/code&gt;）：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;打开一个终端窗口，切换到 GGUF 文件所在目录：&lt;code&gt;cd Gemma2&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;切换 Python 虚拟环境：&lt;code&gt;conda activate PY3.12&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;安装 Python 依赖包：&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;8
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;pip install llama-cpp-python
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;pip install openai
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;pip install uvicorn
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;pip install starlette
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;pip install fastapi
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;pip install sse_starlette
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;pip install starlette_context
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;pip install pydantic_settings
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;或者，我们也可以一把进行安装：&lt;code&gt;pip install -r requirements.txt&lt;/code&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;9
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-plaintext&#34; data-lang=&#34;plaintext&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;# requirements.txt
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;llama-cpp-python
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;openai
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;uvicorn
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;starlette
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;fastapi
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;sse_starlette
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;starlette_context
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;pydantic_settings
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;最后，启动大模型：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 启动Llama大模型&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;python -m llama_cpp.server --host 0.0.0.0 --model ./Gemma-2-9B-it-Q5_K_M.gguf --n_ctx &lt;span class=&#34;m&#34;&gt;2048&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;模型启动命令中，&lt;code&gt;n_ctx 2048&lt;/code&gt;代表单次回话最大 Token 数量。&lt;/p&gt;
&lt;p&gt;启动成功，我们应该看到类似如下的信息：&lt;code&gt;INFO: Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ntopic.cn/06.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;Gemma 启动成功&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;最后一步：&lt;/strong&gt; 我们使用 &lt;strong&gt;openai&lt;/strong&gt; 库在个人电脑上快速搭建客户端。Python 客户端代码（&lt;code&gt;Client.py&lt;/code&gt;）如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;我们使用&lt;strong&gt;OpenAI&lt;/strong&gt;接口来与 Gemma 交互，上面启动模型的最后，我们看到服务端 IP 是本地，端口是&lt;strong&gt;8000&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;接着，我们使用 2 条信息对历史记录进行初始化：第一个条是&lt;strong&gt;系统信息&lt;/strong&gt;，第二个条是要求模型自我介绍的&lt;strong&gt;用户提示&lt;/strong&gt;，为了避免长篇大论，我这里限制了回答的长度和字数&lt;/li&gt;
&lt;li&gt;接下来，通过&lt;code&gt;&amp;gt;&lt;/code&gt;提示符等待用户（即我们）输入，输入&lt;code&gt;bye&lt;/code&gt;、&lt;code&gt;quit&lt;/code&gt;和&lt;code&gt;exit&lt;/code&gt;任意一个即代表退出客户端&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;11
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;12
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;13
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;14
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;15
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;16
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;17
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;18
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;19
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;20
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;21
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;22
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;23
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;24
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;25
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;26
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;27
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;28
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;29
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;30
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;31
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;32
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;33
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;34
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;35
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;36
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;37
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;38
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;39
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# Client.py&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;openai&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;OpenAI&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 注意服务端端口，因为是本地，所以不需要api_key&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;client&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;OpenAI&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;base_url&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;http://localhost:8000/v1&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;api_key&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;EMPTY&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 对话历史：设定系统角色是一个只能助理，同时提交“自我介绍”问题&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;history&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;role&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;system&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;content&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;你是一个智能助理，你的回答总是正确的、有用的和内容非常精简.&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;},&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;role&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;user&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;content&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;请用中文进行自我介绍，要求不能超过5句话，总字数不超过100个字。&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;},&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;\033&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;[92;1m&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 首次自我介绍完毕，接下来是等代码我们的提示&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;while&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;completion&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;client&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;chat&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;completions&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;create&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;model&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;local-model&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;messages&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;history&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;temperature&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;0.7&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;stream&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;new_message&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;role&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;assistant&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;content&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;chunk&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;completion&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;chunk&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;choices&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;delta&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;content&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;chunk&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;choices&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;delta&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;content&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;end&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;flush&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;n&#34;&gt;new_message&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;content&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;chunk&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;choices&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;delta&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;content&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;history&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;append&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;new_message&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;\033&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;[91;1m&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;userinput&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;input&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&amp;gt; &amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;userinput&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;lower&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;bye&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;quit&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;exit&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]:&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# 我们输入bye/quit/exit等均退出客户端&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;\033&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;[0mBYE BYE!&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;break&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;history&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;append&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;({&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;role&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;user&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;content&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;userinput&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;})&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;\033&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;[92;1m&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;我们打开一个 Terminal 终端，运行客户端：&lt;code&gt;python Client.py&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ntopic.cn/p/2024070201/07.png&#34;
	width=&#34;1720&#34;
	height=&#34;441&#34;
	srcset=&#34;https://ntopic.cn/p/2024070201/07_hu614b48ac6240ec91d965e84d307b0bed_47523_480x0_resize_box_3.png 480w, https://ntopic.cn/p/2024070201/07_hu614b48ac6240ec91d965e84d307b0bed_47523_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Gemma 对话&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;390&#34;
		data-flex-basis=&#34;936px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;恭喜你，第二种方式也部署成功了，我们可以愉快地与大模型进行对话了，包括把大模型作为我们 &lt;strong&gt;Code Copilot&lt;/strong&gt; 的底层模型，部署我们团队私有化的 &lt;strong&gt;Code Copilot&lt;/strong&gt; 的底层模型，部署我们团队私有化的了：&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/vt1EXVWtwm6ltZVYtB4-Tg&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;个人或团队私有化 Code Copilot 部署和使用教程&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;
&lt;p&gt;以上是老牛同学和大家一起采用 2 种方式快速部署 Gemma 2 大模型，这 2 种方式是同样的方式，同样适用于其他大模型。&lt;/p&gt;
&lt;p&gt;相对来说，Ollama 部署配置比较简单，目前常见的大模型均支持 Ollama 推理协议（包括：Qwen/Lllama/Phi 等大模型），推荐使用；同时，GGUF 部署方式仅需要依赖一个模型文件，使用 Llama.cpp 框架进行推理，依赖也少部署也很方便，同样推荐使用。如何抉择，就看我们自己喜好了！&lt;/p&gt;
&lt;p&gt;Gemma 2 在内最近发布的开源大模型，可以看出当前大模型研究的趋势，即探索用更轻量级、更实用的模型来实现更强的性能，并确保易部署，以更好地满足不同用户的需求。老牛同学觉得未来低成本、定制化的垂直场景小模型将会越来越多，也会越来越受欢迎！&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;关注本公众号，我们共同学习交流进步 👇🏻👇🏻👇🏻&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ntopic.cn/WX-21.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;微信公众号：老牛同学&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Phi-3 开源大模型&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/bNxHM3B7HOLNvJtjwvt8iw&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Phi-3 模型手机部署教程（微软发布的可与 GPT-3.5 媲美的小模型）&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Qwen2-7B 开源大模型&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/u_Uw88dpQRgbtfI4_1OOwQ&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Qwen2 阿里最强开源大模型（Qwen2-7B）本地部署、API 调用和 WebUI 对话机器人&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Llama-3-8B 开源大模型&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/MekCUJDhKzuUnoykkGoH2g&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;玩转 AI，笔记本电脑安装属于自己的 Llama 3 8B 大模型和对话客户端&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/2DVYO75h0o5EHN_K_GF4Eg&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;一文彻底整明白，基于 Ollama 工具的 LLM 大语言模型 Web 可视化对话机器人部署指南&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/idcdIr8mMWDQ_iZU5r_UEQ&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;基于 Llama 3 搭建中文版（Llama3-Chinese-Chat）大模型对话聊天机器人&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;GLM-4-9B 开源大模型&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/g7lDfnRRGdrHqN7WGMSkAg&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;本地部署 GLM-4-9B 清华智谱开源大模型方法和对话效果体验&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;ChatTTS 文本转语音模型&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/rL3vyJ_xEj7GGoKaxUh8_A&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;ChatTTS 开源文本转语音模型本地部署、API 使用和搭建 WebUI 界面&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Stable Diffusion 3 文生图模型&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/Sax4z2k8Dvn82h15jf51Hw&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Stable Diffusion 3 文生图“开源英雄”大模型本地部署和使用教程，轻松实现 AI 绘图自由&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;大模型应用实战&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/vt1EXVWtwm6ltZVYtB4-Tg&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;使用 Llama3/Qwen2 等开源大模型，部署团队私有化 Code Copilot 和使用教程&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/P_ufvz4MWVSqv_VM-rJp9w&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;大模型应用研发基础环境配置（Miniconda、Python、Jupyter Lab、Ollama 等）&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/m_O2OSoXWLL0PJurLCdzng&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;借助 AI 大模型，三分钟原创一部儿童故事短视频（附完整操作步骤）&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/gaLw3yP-oANvQyjRSkVjyw&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;高效编写大模型 Prompt 提示词，解锁 AI 无限创意潜能&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Python 小游戏&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/hv2tE-yot_H04HCezxQWXg&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;AI 已来，我与 AI 一起用 Python 编写了一个消消乐小游戏&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/tkTlt4rbFKQ73zudluPO1A&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Python 游戏编程：一步步用 Python 打造经典贪吃蛇小游戏&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
</description>
        </item>
        <item>
        <title>使用Llama3/Qwen2等开源大模型，部署团队私有化Code Copilot和使用教程</title>
        <link>https://ntopic.cn/p/2024062901/</link>
        <pubDate>Sat, 29 Jun 2024 00:00:00 +0000</pubDate>
        
        <guid>https://ntopic.cn/p/2024062901/</guid>
        <description>&lt;img src="https://ntopic.cn/p/2024062901/02.png" alt="Featured image of post 使用Llama3/Qwen2等开源大模型，部署团队私有化Code Copilot和使用教程" /&gt;&lt;p&gt;目前市面上有不少基于大模型的 Code Copilot 产品，部分产品对于个人开发者来说可免费使用，比如阿里的&lt;strong&gt;通义灵码&lt;/strong&gt;、百度的&lt;strong&gt;文心快码&lt;/strong&gt;等。这些免费的产品均通过 API 的方式提供服务，因此调用时均必须&lt;strong&gt;联网&lt;/strong&gt;、同时需要把&lt;strong&gt;代码&lt;/strong&gt;、&lt;strong&gt;提示词&lt;/strong&gt;等内容作为 API 的入参在网络中传输和 API 服务器中进行处理，这里就涉及到一个比较重要的问题：&lt;strong&gt;隐私安全&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ntopic.cn/p/2024062901/01.png&#34;
	width=&#34;1330&#34;
	height=&#34;505&#34;
	srcset=&#34;https://ntopic.cn/p/2024062901/01_hu8dd8e27267ebf087ef85497c46878453_47149_480x0_resize_box_3.png 480w, https://ntopic.cn/p/2024062901/01_hu8dd8e27267ebf087ef85497c46878453_47149_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Code Copilot的公网API调用&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;263&#34;
		data-flex-basis=&#34;632px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;针对小团队私有保密项目、创新团队孵化新产品，隐私安全问题就显得格外重要。因此，团队内部部署私有的&lt;strong&gt;Code Copilot&lt;/strong&gt;方案就应运而出了：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ntopic.cn/p/2024062901/02.png&#34;
	width=&#34;1206&#34;
	height=&#34;525&#34;
	srcset=&#34;https://ntopic.cn/p/2024062901/02_hu1fa0aae459ee14b2d05b63839e018423_44026_480x0_resize_box_3.png 480w, https://ntopic.cn/p/2024062901/02_hu1fa0aae459ee14b2d05b63839e018423_44026_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Code Copilot的内部API服务&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;229&#34;
		data-flex-basis=&#34;551px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;今天老牛同学和大家一起来使用&lt;strong&gt;Qwen2-7&lt;/strong&gt;构建个人或者团队专属的私有化 &lt;strong&gt;Code Copilot&lt;/strong&gt;，除了&lt;strong&gt;代码生成&lt;/strong&gt;之外，还可以是支持 &lt;strong&gt;AI 问答&lt;/strong&gt;、&lt;strong&gt;代码解释&lt;/strong&gt;、&lt;strong&gt;语言转换&lt;/strong&gt;、生成单元&lt;strong&gt;测试用例&lt;/strong&gt;等功能。不但可以提高我们的研发效率，还可以保护我们的代码隐私。&lt;/p&gt;
&lt;h2 id=&#34;第一步部署私有的大模型qwen2-7b&#34;&gt;第一步：部署私有的大模型（Qwen2-7B）&lt;/h2&gt;
&lt;p&gt;前面老牛同学介绍了本地部署大模型和使用教程，包括&lt;strong&gt;Llama3-8B&lt;/strong&gt;、&lt;strong&gt;Qwen2-7B&lt;/strong&gt;、&lt;strong&gt;GLM4-9B&lt;/strong&gt;、和&lt;strong&gt;Phi3&lt;/strong&gt;等。大家可以看一下之前的部署教程，老牛同学今天使用&lt;strong&gt;Qwen2-7B&lt;/strong&gt;作为我们 Copilot 底层大模型，对于本文&lt;strong&gt;Code Copilot&lt;/strong&gt;的部署和使用，其他大模型都是一样的，看大家的喜好，没有特殊要求。&lt;/p&gt;
&lt;p&gt;老牛同学这里简单列一下部署教程：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;环境安装：&lt;/strong&gt; 主要是&lt;strong&gt;Miniconda&lt;/strong&gt;包管理安装，参见教程，注意只需要安装&lt;strong&gt;Miniconda&lt;/strong&gt;和&lt;strong&gt;Python&lt;/strong&gt;即可，其他非必须：大模型应用研发基础环境配置（Miniconda、Python、Jupyter Lab、Ollama 等）：&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/P_ufvz4MWVSqv_VM-rJp9w&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://mp.weixin.qq.com/s/P_ufvz4MWVSqv_VM-rJp9w&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Code Copilot&lt;/strong&gt;底层大模型的部署方式，老牛同学验证主要以下 2 种，任选一种即可：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方式一&lt;/strong&gt; 基于 Ollama 部署：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;下载并安装&lt;strong&gt;Ollama&lt;/strong&gt;本地模型管理软件（Windows/Mac/Linux 均支持）：&lt;a class=&#34;link&#34; href=&#34;https://ollama.com/download&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://ollama.com/download&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;通过 Ollama 下载并启动大模型：&lt;code&gt;ollama run qwen2:7b&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;通过以上 2 步，即完成了大模型的部署。特别注意的是：&lt;strong&gt;GLM&lt;/strong&gt;系列大模型还不支持 Ollama 管理！因此如果你希望用 GLM 系列（如：&lt;strong&gt;GLM4-9B&lt;/strong&gt;）大模型，那么请看方式二。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方式二&lt;/strong&gt; 通过 Python 部署：通用方法，所有模型都支持，以下是老牛同学验证过的部署教程&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Phi3&lt;/strong&gt;：&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/bNxHM3B7HOLNvJtjwvt8iw&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Phi-3 模型手机部署教程（微软发布的可与 GPT-3.5 媲美的小模型）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Qwen2-7B&lt;/strong&gt;：&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/u_Uw88dpQRgbtfI4_1OOwQ&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Qwen2 阿里最强开源大模型（Qwen2-7B）本地部署、API 调用和 WebUI 对话机器人&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Llama3-8B&lt;/strong&gt;：&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/MekCUJDhKzuUnoykkGoH2g&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;玩转 AI，笔记本电脑安装属于自己的 Llama 3 8B 大模型和对话客户端&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;GLM4-9B&lt;/strong&gt;：&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/g7lDfnRRGdrHqN7WGMSkAg&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;本地部署 GLM-4-9B 清华智谱开源大模型方法和对话效果体验&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;第二步ide-安装和配置-continue-插件&#34;&gt;第二步：IDE 安装和配置 Continue 插件&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Code Copilot Continue&lt;/strong&gt;是目前最受欢迎的插件之一，它插件的形式使用，目前支持&lt;strong&gt;VS Code&lt;/strong&gt;和&lt;strong&gt;JetBrains 全家桶&lt;/strong&gt;这 2 个主流 IDE 的扩展，可以在对应的插件市场中直接搜索&lt;strong&gt;Continue&lt;/strong&gt;下载安装。&lt;strong&gt;Continue&lt;/strong&gt;插件的一个最大优势在于它可以指定连接本地或者局域网内的大模型，所以对于信息安全高的项目，我们完全可以在本地或者局域网内部署大模型，然后用&lt;strong&gt;Continue&lt;/strong&gt;插件连接使用，在提高开发生产效率同时保护了研发代码的隐私。目前&lt;strong&gt;Continue&lt;/strong&gt;插件的社区活跃，GitHub 仓库已达&lt;strong&gt;12K&lt;/strong&gt;星，插件版本更新频繁，越来越成熟。&lt;/p&gt;
&lt;p&gt;因为老牛同学日常使用&lt;strong&gt;VS Code&lt;/strong&gt;较多，因此下面的安装和配置，基于&lt;strong&gt;VS Code&lt;/strong&gt;完成，&lt;strong&gt;JetBrains&lt;/strong&gt;（如：PyCharm 等）的方式类同。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;安装 Continue 插件：&lt;/strong&gt; 插件管理器中，搜索&lt;strong&gt;Continue&lt;/strong&gt;，点击安装，安装成功之后重启 IDE 即可。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ntopic.cn/p/2024062901/03.png&#34;
	width=&#34;1150&#34;
	height=&#34;229&#34;
	srcset=&#34;https://ntopic.cn/p/2024062901/03_hu8cc20be909358df9055e7c51ec9bc751_47783_480x0_resize_box_3.png 480w, https://ntopic.cn/p/2024062901/03_hu8cc20be909358df9055e7c51ec9bc751_47783_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;安装Continue插件&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;502&#34;
		data-flex-basis=&#34;1205px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Continue 插件状态：&lt;/strong&gt; 安装成功并重启之后，可以看到&lt;strong&gt;VS Code&lt;/strong&gt;的左侧多了一个&lt;strong&gt;Continue&lt;/strong&gt;按钮。同时，右下角也多了一个&lt;strong&gt;Continue&lt;/strong&gt;状态图标：&lt;strong&gt;√ Continue&lt;/strong&gt;则代表&lt;strong&gt;Continue&lt;/strong&gt;在 IDE 中生效了，&lt;strong&gt;🚫Continue&lt;/strong&gt;则代表&lt;strong&gt;Continue&lt;/strong&gt;在 IDE 未启用。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ntopic.cn/p/2024062901/04.png&#34;
	width=&#34;1477&#34;
	height=&#34;928&#34;
	srcset=&#34;https://ntopic.cn/p/2024062901/04_huc87a0c471c6d34f1cf27f2e24755b5ff_191124_480x0_resize_box_3.png 480w, https://ntopic.cn/p/2024062901/04_huc87a0c471c6d34f1cf27f2e24755b5ff_191124_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Continue插件状态&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;159&#34;
		data-flex-basis=&#34;381px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;配置 Continue 插件：&lt;/strong&gt; 点击左下角启用&lt;strong&gt;Continue&lt;/strong&gt;插件，然后点击左侧的&lt;strong&gt;Continue&lt;/strong&gt;按钮，可以看到一个对话界面，点击左下角&lt;strong&gt;齿轮&lt;/strong&gt;形状的&lt;strong&gt;配置&lt;/strong&gt;图标，则打开了一个 JSON 格式的配置文件，在这里可以配置&lt;strong&gt;Continue&lt;/strong&gt;底层使用的大模型信息：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ntopic.cn/p/2024062901/05.png&#34;
	width=&#34;1477&#34;
	height=&#34;933&#34;
	srcset=&#34;https://ntopic.cn/p/2024062901/05_hu6f91a15c2ce200f1e8bbb3ef90337f69_131417_480x0_resize_box_3.png 480w, https://ntopic.cn/p/2024062901/05_hu6f91a15c2ce200f1e8bbb3ef90337f69_131417_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Continue插件配置&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;158&#34;
		data-flex-basis=&#34;379px&#34;
	
&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;11
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;12
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;13
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;14
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;15
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;16
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;17
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;18
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;19
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;20
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;21
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;22
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;23
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;24
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;25
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;26
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;27
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;28
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;29
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;30
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;31
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;32
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;33
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;34
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;35
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;36
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;37
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;38
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;39
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;40
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;41
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;42
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;43
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;44
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-json&#34; data-lang=&#34;json&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;nt&#34;&gt;&amp;#34;models&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &lt;span class=&#34;nt&#34;&gt;&amp;#34;title&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;Qwen2:7B&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &lt;span class=&#34;nt&#34;&gt;&amp;#34;provider&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;ollama&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &lt;span class=&#34;nt&#34;&gt;&amp;#34;model&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;qwen2:7b&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;p&#34;&gt;},&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &lt;span class=&#34;nt&#34;&gt;&amp;#34;title&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;Llama3-8B&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &lt;span class=&#34;nt&#34;&gt;&amp;#34;provider&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;openai&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &lt;span class=&#34;nt&#34;&gt;&amp;#34;model&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;Local-Model&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &lt;span class=&#34;nt&#34;&gt;&amp;#34;apiBase&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;http://127.0.0.1:8000/v1&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &lt;span class=&#34;nt&#34;&gt;&amp;#34;apiKey&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;EMPTY&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &lt;span class=&#34;nt&#34;&gt;&amp;#34;completionOptions&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;nt&#34;&gt;&amp;#34;stop&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;          &lt;span class=&#34;s2&#34;&gt;&amp;#34;&amp;lt;|eot_id|&amp;gt;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;p&#34;&gt;],&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;nt&#34;&gt;&amp;#34;customCommands&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &lt;span class=&#34;nt&#34;&gt;&amp;#34;name&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;test&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &lt;span class=&#34;nt&#34;&gt;&amp;#34;prompt&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;{{{ input }}}\n\nWrite a comprehensive set of unit tests for the selected code. It should setup, run tests that check for correctness including important edge cases, and teardown. Ensure that the tests are complete and sophisticated. Give the tests just as chat output, don&amp;#39;t edit any file.&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &lt;span class=&#34;nt&#34;&gt;&amp;#34;description&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;Write unit tests for highlighted code&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;p&#34;&gt;],&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;c1&#34;&gt;// &amp;#34;tabAutocompleteModel&amp;#34;: {
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;//   &amp;#34;title&amp;#34;: &amp;#34;Qwen2:7B&amp;#34;,
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;//   &amp;#34;provider&amp;#34;: &amp;#34;ollama&amp;#34;,
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;//   &amp;#34;model&amp;#34;: &amp;#34;qwen2:7b&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;// },
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;  &lt;span class=&#34;nt&#34;&gt;&amp;#34;tabAutocompleteModel&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nt&#34;&gt;&amp;#34;title&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;Llama3-8B&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nt&#34;&gt;&amp;#34;model&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;Local-Model&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nt&#34;&gt;&amp;#34;apiBase&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;http://127.0.0.1:8000/v1&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nt&#34;&gt;&amp;#34;provider&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;openai&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nt&#34;&gt;&amp;#34;apiKey&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;EMPTY&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;p&#34;&gt;},&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;nt&#34;&gt;&amp;#34;allowAnonymousTelemetry&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;true&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;nt&#34;&gt;&amp;#34;embeddingsProvider&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nt&#34;&gt;&amp;#34;provider&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;transformers.js&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;&lt;strong&gt;Continue&lt;/strong&gt;插件的配置信息主要分为 3 块：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;models&lt;/strong&gt; 代表所有的模型列表：上面样例配置文件，老牛同学分别配置了基于&lt;strong&gt;Ollama&lt;/strong&gt;部署的&lt;strong&gt;Qwen2-7B&lt;/strong&gt;大模型和基于&lt;strong&gt;OpenAI&lt;/strong&gt;客户端部署的&lt;strong&gt;Llama3-7B&lt;/strong&gt;大模型（也就是涵盖了上面大模型的 2 种不同部署方式的不同配置方式）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;customCommands&lt;/strong&gt; 自定义给大模型的提示词格式，&lt;strong&gt;input&lt;/strong&gt;代表输入内容（代码或者文本等），可以按照实际需求进行调整&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;tabAutocompleteModel&lt;/strong&gt; 代码编写过程中，当按下代码生成快捷键时，&lt;strong&gt;Continue&lt;/strong&gt;生成代码的大模型，从&lt;strong&gt;models&lt;/strong&gt;列表里面指定一个即可，当然也可以设置一个其他的大模型（&lt;strong&gt;注意：&lt;/strong&gt; 只能选择一个模型，上面样例配置注释掉了一个，在实际使用时需要删除掉，因为 JSON 不支持注释！）&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;第三步使用-code-copilot-continue-插件&#34;&gt;第三步：使用 Code Copilot Continue 插件&lt;/h2&gt;
&lt;p&gt;首先，启动大模型（如：&lt;strong&gt;Qwen2-7B&lt;/strong&gt;）；其次，点击 IDE 左下角图标，以启用&lt;strong&gt;Continue&lt;/strong&gt;插件；然后，选择一个对话大模型（默认选中第 1 个大模型）：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ntopic.cn/p/2024062901/06.png&#34;
	width=&#34;1477&#34;
	height=&#34;933&#34;
	srcset=&#34;https://ntopic.cn/p/2024062901/06_hu32bbbaf1d8b6c886e87e2f972712a733_110476_480x0_resize_box_3.png 480w, https://ntopic.cn/p/2024062901/06_hu32bbbaf1d8b6c886e87e2f972712a733_110476_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;启动Continue插件&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;158&#34;
		data-flex-basis=&#34;379px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;使用方式一：&lt;/strong&gt; 我们可以选择不同的大模型进行对话，相当于&lt;strong&gt;Continue&lt;/strong&gt;插件提供了一个可视化对话客户端&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;使用方式二：&lt;/strong&gt; 代码生成，也是本文的重点，下面老牛同学以&lt;strong&gt;快速排序算法&lt;/strong&gt;为例，来如何利用&lt;strong&gt;Continue&lt;/strong&gt;完成代码编写：&lt;/p&gt;
&lt;p&gt;【&lt;strong&gt;场景一：AI 对话代码同步&lt;/strong&gt;】&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;使用大模型对话能力，写出快速排序算法代码，Prompt 提示词：&lt;strong&gt;请提供一下功能代码：Python 实现快速排序算法函数&lt;/strong&gt;，大模型很好的实现了算法逻辑，并且给了很多的说明和解释&lt;/li&gt;
&lt;li&gt;选中快速排序算法函数的代码，然后按下&lt;code&gt;CTRL+Shift+L&lt;/code&gt;快捷键，或者点击代码框右上角的&lt;strong&gt;Apply to current file&lt;/strong&gt;按钮，Python 自动同步到了编辑器中了：&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;https://ntopic.cn/p/2024062901/07.png&#34;
	width=&#34;2124&#34;
	height=&#34;1510&#34;
	srcset=&#34;https://ntopic.cn/p/2024062901/07_hufc4e9c4011f804afba2d1ba127ac905a_306886_480x0_resize_box_3.png 480w, https://ntopic.cn/p/2024062901/07_hufc4e9c4011f804afba2d1ba127ac905a_306886_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;AI对话代码同步&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;140&#34;
		data-flex-basis=&#34;337px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;【&lt;strong&gt;场景二：代码自动补全&lt;/strong&gt;】&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;在编辑器中，只需要输入注释内容（不要按回车键）：&lt;code&gt;# Python实现快速排序算法函数&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;然后按下&lt;code&gt;CTRL+Shift+L&lt;/code&gt;快捷键，然后在按下&lt;strong&gt;回车键&lt;/strong&gt;，可以看到代码已经生成，可以按&lt;code&gt;Tab键&lt;/code&gt;使用代码&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;https://ntopic.cn/p/2024062901/08.png&#34;
	width=&#34;1228&#34;
	height=&#34;414&#34;
	srcset=&#34;https://ntopic.cn/p/2024062901/08_hu70348972b6750d9e206a140f2874073a_45269_480x0_resize_box_3.png 480w, https://ntopic.cn/p/2024062901/08_hu70348972b6750d9e206a140f2874073a_45269_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;代码自动补全&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;296&#34;
		data-flex-basis=&#34;711px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;【&lt;strong&gt;场景三：增加单元测试代码&lt;/strong&gt;】&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;在编辑器中，选中一段代码函数，然后按下&lt;code&gt;CTRL+I&lt;/code&gt;快捷键&lt;/li&gt;
&lt;li&gt;然后，在弹出的&lt;strong&gt;Ctrl+I&lt;/strong&gt;指令框中输入：&lt;code&gt;增加代码测试代码&lt;/code&gt;，然后按下&lt;strong&gt;回车键&lt;/strong&gt;，可以看到生成了测试用例代码&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;https://ntopic.cn/p/2024062901/09.png&#34;
	width=&#34;2107&#34;
	height=&#34;456&#34;
	srcset=&#34;https://ntopic.cn/p/2024062901/09_hu93aa817444527a6f197704379570d9db_106042_480x0_resize_box_3.png 480w, https://ntopic.cn/p/2024062901/09_hu93aa817444527a6f197704379570d9db_106042_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;输入选中代码指令&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;462&#34;
		data-flex-basis=&#34;1108px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ntopic.cn/p/2024062901/10.png&#34;
	width=&#34;1222&#34;
	height=&#34;1209&#34;
	srcset=&#34;https://ntopic.cn/p/2024062901/10_huf962071a436f95d62b7df4759f420cea_160720_480x0_resize_box_3.png 480w, https://ntopic.cn/p/2024062901/10_huf962071a436f95d62b7df4759f420cea_160720_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;增加单元测试用例代码&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;101&#34;
		data-flex-basis=&#34;242px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;【&lt;strong&gt;场景四：代码自动填充对话框&lt;/strong&gt;】&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;在编辑器中，选中一段代码，然后按下&lt;code&gt;Ctrl+L&lt;/code&gt;快捷键，代码&lt;strong&gt;自动填充&lt;/strong&gt;到了左侧的对话框中&lt;/li&gt;
&lt;li&gt;可以在对话框中继续输入指令，如：&lt;code&gt;请解释一下这段代码&lt;/code&gt;，然后按&lt;strong&gt;回车键&lt;/strong&gt;提交大模型：&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;https://ntopic.cn/p/2024062901/11.png&#34;
	width=&#34;2101&#34;
	height=&#34;1404&#34;
	srcset=&#34;https://ntopic.cn/p/2024062901/11_huffed8460599e222dc21ace10c23a4f2e_353205_480x0_resize_box_3.png 480w, https://ntopic.cn/p/2024062901/11_huffed8460599e222dc21ace10c23a4f2e_353205_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;代码填充对话框和解释&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;149&#34;
		data-flex-basis=&#34;359px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;【&lt;strong&gt;其他场景&lt;/strong&gt;】&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;增加注释&lt;/li&gt;
&lt;li&gt;代码异常堆栈分析&lt;/li&gt;
&lt;li&gt;…… 其他请大家补充&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;最后&#34;&gt;最后：&lt;/h2&gt;
&lt;p&gt;使用开源大模型作为团队的 Code Copilot 工具，可以提高团队的开发效率，同时也可以保护团队的代码隐私。虽然目前开源的 LLM 相比闭源商用的 LLM 还有一些差距，但是随着开源 LLM 的不断发展，相信两者的差距以后会越来越小。以上就是今天介绍的内容，希望对大家有所帮助。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;关注本公众号，我们共同学习交流进步 👇🏻👇🏻👇🏻&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ntopic.cn/WX-21.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;微信公众号：老牛同学&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Phi-3 开源大模型&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/bNxHM3B7HOLNvJtjwvt8iw&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Phi-3 模型手机部署教程（微软发布的可与 GPT-3.5 媲美的小模型）&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Qwen2-7B 开源大模型&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/u_Uw88dpQRgbtfI4_1OOwQ&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Qwen2 阿里最强开源大模型（Qwen2-7B）本地部署、API 调用和 WebUI 对话机器人&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Llama-3-8B 开源大模型&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/MekCUJDhKzuUnoykkGoH2g&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;玩转 AI，笔记本电脑安装属于自己的 Llama 3 8B 大模型和对话客户端&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/2DVYO75h0o5EHN_K_GF4Eg&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;一文彻底整明白，基于 Ollama 工具的 LLM 大语言模型 Web 可视化对话机器人部署指南&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/idcdIr8mMWDQ_iZU5r_UEQ&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;基于 Llama 3 搭建中文版（Llama3-Chinese-Chat）大模型对话聊天机器人&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;GLM-4-9B 开源大模型&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/g7lDfnRRGdrHqN7WGMSkAg&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;本地部署 GLM-4-9B 清华智谱开源大模型方法和对话效果体验&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;ChatTTS 文本转语音模型&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/rL3vyJ_xEj7GGoKaxUh8_A&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;ChatTTS 开源文本转语音模型本地部署、API 使用和搭建 WebUI 界面&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Stable Diffusion 3 文生图模型&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/Sax4z2k8Dvn82h15jf51Hw&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Stable Diffusion 3 文生图“开源英雄”大模型本地部署和使用教程，轻松实现 AI 绘图自由&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;大模型应用案例&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/P_ufvz4MWVSqv_VM-rJp9w&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;大模型应用研发基础环境配置（Miniconda、Python、Jupyter Lab、Ollama 等）&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/m_O2OSoXWLL0PJurLCdzng&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;借助 AI 大模型，三分钟原创一部儿童故事短视频（附完整操作步骤）&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/gaLw3yP-oANvQyjRSkVjyw&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;高效编写大模型 Prompt 提示词，解锁 AI 无限创意潜能&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Python 小游戏&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/hv2tE-yot_H04HCezxQWXg&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;AI 已来，我与 AI 一起用 Python 编写了一个消消乐小游戏&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/tkTlt4rbFKQ73zudluPO1A&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Python 游戏编程：一步步用 Python 打造经典贪吃蛇小游戏&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
</description>
        </item>
        <item>
        <title>大模型应用研发基础环境配置（Miniconda、Python、Jupyter Lab、Ollama等）</title>
        <link>https://ntopic.cn/p/2024062501/</link>
        <pubDate>Tue, 25 Jun 2024 00:00:00 +0000</pubDate>
        
        <guid>https://ntopic.cn/p/2024062501/</guid>
        <description>&lt;img src="https://ntopic.cn/p/2024062501/00.jpg" alt="Featured image of post 大模型应用研发基础环境配置（Miniconda、Python、Jupyter Lab、Ollama等）" /&gt;&lt;p&gt;老牛同学之前使用的&lt;strong&gt;MacBook Pro&lt;/strong&gt;电脑配置有点旧（2015 年生产），跑大模型感觉有点吃力，操作起来有点卡顿，因此不得已捡起了尘封了快两年的&lt;strong&gt;MateBook Pro&lt;/strong&gt;电脑（老牛同学其实不太喜欢用 Windows 电脑做研发工作）。此文注意是记录配置新电脑的内容，一来给老牛同学留个备忘，同时也特别希望能给其他朋友一些帮助。&lt;/p&gt;
&lt;p&gt;配置一台方便用于大模型应用研发的新电脑，最基础的需要包括以下配置内容：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Miniconda&lt;/strong&gt;包管理工具的安装和配置（兼容&lt;strong&gt;pip&lt;/strong&gt;）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Python&lt;/strong&gt;虚拟环境配置（指定 Python 版本且无需单独下载 Python 安装）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Jupyter Lab&lt;/strong&gt; Python 研发 WebIDE 配置&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Ollama&lt;/strong&gt;本地大模型管理软件的配置和应用&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Ollama&lt;/strong&gt;大模型 Web 界面对话客户端配置和使用&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;miniconda-安装和配置&#34;&gt;Miniconda 安装和配置&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Miniconda&lt;/strong&gt;和&lt;strong&gt;pip&lt;/strong&gt;都是 Python 生态中的两个不同的包管理工具，它们都用于安装和管理 Python 包。但是在大模型相关的应用研发中，老牛同学推荐使用&lt;strong&gt;Miniconda&lt;/strong&gt;的原因：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;包范围：&lt;/strong&gt; Miniconda 通过 Conda 可以管理 Python 以及非 Python 包，而 pip 只管理 Python 包。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;环境管理：&lt;/strong&gt; Miniconda 内置了环境管理功能，而 pip 需要与其他工具（如 virtualenv 或 venv）搭配使用以创建隔离的环境。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;包源：&lt;/strong&gt; pip 通常从 PyPI 下载包，而 Conda 从 Anaconda Repository 或其他自定义设置的通道下载。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;包格式：&lt;/strong&gt; Conda 使用自己的包格式(.conda 或.tar.bz2)，而 pip 使用 wheel 或源码形式。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;依赖解决：&lt;/strong&gt; Conda 在安装包时会考虑到系统级别的依赖和包之间的依赖关系，而 pip 主要解决 Python 级别的依赖。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;在实际使用中，由于&lt;strong&gt;Minionda&lt;/strong&gt;可以很好地处理复杂的依赖关系和环境管理，它通常是首选工具。但是，如果只需要安装纯 Python 包，使用&lt;strong&gt;pip&lt;/strong&gt;可能会更加简单直接。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;第一步：&lt;/strong&gt; 下载地址：&lt;a class=&#34;link&#34; href=&#34;https://docs.anaconda.com/miniconda&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://docs.anaconda.com/miniconda/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;根据操作系统，选择安装包，支持包括：Windows、MacOS 和 Linux 系统&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;第二步：&lt;/strong&gt; 安装和配置：安装过程和普通软件没有两样，安装完成之后，我们配置 Python 环境：&lt;/p&gt;
&lt;p&gt;老牛同学的安装目录是：&lt;code&gt;D:\Software\miniconda3&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;因后面需要执行&lt;code&gt;conda&lt;/code&gt;命令，因此提前把以下目录添加到&lt;strong&gt;系统环境变量&lt;/strong&gt;中（变量名：&lt;code&gt;Path&lt;/code&gt;）：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;安装目录：&lt;code&gt;D:\Software\miniconda3&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;脚本目录：&lt;code&gt;D:\Software\miniconda3\Scripts&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;依赖库目录：&lt;code&gt;D:\Software\miniconda3\Library\bin&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;第三步：&lt;/strong&gt; 设置 Conda 虚拟环境目录（&lt;strong&gt;可选&lt;/strong&gt;）：默认情况下，虚拟环境内容在&lt;code&gt;C:\&lt;/code&gt;盘，老牛同学 C 盘比较较小，就把它设置到&lt;code&gt;D:\&lt;/code&gt;盘&lt;/p&gt;
&lt;p&gt;打开 Termianl 终端，查看&lt;strong&gt;Conda&lt;/strong&gt;基本信息：&lt;code&gt;conda info&lt;/code&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;11
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;12
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;13
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;14
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;15
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;16
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;17
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;18
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;19
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;20
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;21
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;22
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;23
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;24
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;25
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;26
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;27
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;28
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;29
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;30
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;31
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;32
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&amp;gt; conda info
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;     active environment : None
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;       user config file : C:&lt;span class=&#34;se&#34;&gt;\U&lt;/span&gt;sers&lt;span class=&#34;se&#34;&gt;\o&lt;/span&gt;bull&lt;span class=&#34;se&#34;&gt;\.&lt;/span&gt;condarc
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; populated config files :
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;          conda version : 24.4.0
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    conda-build version : not installed
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;         python version : 3.12.3.final.0
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                 solver : libmamba &lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;default&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;       virtual packages : &lt;span class=&#34;nv&#34;&gt;__archspec&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;x86_64_v3
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                          &lt;span class=&#34;nv&#34;&gt;__conda&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;24.4.0&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                          &lt;span class=&#34;nv&#34;&gt;__win&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;       base environment : D:&lt;span class=&#34;se&#34;&gt;\S&lt;/span&gt;oftware&lt;span class=&#34;se&#34;&gt;\m&lt;/span&gt;iniconda3  &lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;writable&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      conda av data dir : D:&lt;span class=&#34;se&#34;&gt;\S&lt;/span&gt;oftware&lt;span class=&#34;se&#34;&gt;\m&lt;/span&gt;iniconda3&lt;span class=&#34;se&#34;&gt;\e&lt;/span&gt;tc&lt;span class=&#34;se&#34;&gt;\c&lt;/span&gt;onda
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  conda av metadata url : None
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;           channel URLs : https://repo.anaconda.com/pkgs/main/win-64
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                          https://repo.anaconda.com/pkgs/main/noarch
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                          https://repo.anaconda.com/pkgs/r/win-64
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                          https://repo.anaconda.com/pkgs/r/noarch
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                          https://repo.anaconda.com/pkgs/msys2/win-64
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                          https://repo.anaconda.com/pkgs/msys2/noarch
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;          package cache : D:&lt;span class=&#34;se&#34;&gt;\S&lt;/span&gt;oftware&lt;span class=&#34;se&#34;&gt;\m&lt;/span&gt;iniconda3&lt;span class=&#34;se&#34;&gt;\p&lt;/span&gt;kgs
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                          C:&lt;span class=&#34;se&#34;&gt;\U&lt;/span&gt;sers&lt;span class=&#34;se&#34;&gt;\o&lt;/span&gt;bull&lt;span class=&#34;se&#34;&gt;\.&lt;/span&gt;conda&lt;span class=&#34;se&#34;&gt;\p&lt;/span&gt;kgs
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                          C:&lt;span class=&#34;se&#34;&gt;\U&lt;/span&gt;sers&lt;span class=&#34;se&#34;&gt;\o&lt;/span&gt;bull&lt;span class=&#34;se&#34;&gt;\A&lt;/span&gt;ppData&lt;span class=&#34;se&#34;&gt;\L&lt;/span&gt;ocal&lt;span class=&#34;se&#34;&gt;\c&lt;/span&gt;onda&lt;span class=&#34;se&#34;&gt;\c&lt;/span&gt;onda&lt;span class=&#34;se&#34;&gt;\p&lt;/span&gt;kgs
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;       envs directories : D:&lt;span class=&#34;se&#34;&gt;\S&lt;/span&gt;oftware&lt;span class=&#34;se&#34;&gt;\m&lt;/span&gt;iniconda3&lt;span class=&#34;se&#34;&gt;\e&lt;/span&gt;nvs
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                          C:&lt;span class=&#34;se&#34;&gt;\U&lt;/span&gt;sers&lt;span class=&#34;se&#34;&gt;\o&lt;/span&gt;bull&lt;span class=&#34;se&#34;&gt;\.&lt;/span&gt;conda&lt;span class=&#34;se&#34;&gt;\e&lt;/span&gt;nvs
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                          C:&lt;span class=&#34;se&#34;&gt;\U&lt;/span&gt;sers&lt;span class=&#34;se&#34;&gt;\o&lt;/span&gt;bull&lt;span class=&#34;se&#34;&gt;\A&lt;/span&gt;ppData&lt;span class=&#34;se&#34;&gt;\L&lt;/span&gt;ocal&lt;span class=&#34;se&#34;&gt;\c&lt;/span&gt;onda&lt;span class=&#34;se&#34;&gt;\c&lt;/span&gt;onda&lt;span class=&#34;se&#34;&gt;\e&lt;/span&gt;nvs
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;               platform : win-64
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;             user-agent : conda/24.4.0 requests/2.31.0 CPython/3.12.3 Windows/11 Windows/10.0.22621 solver/libmamba conda-libmamba-solver/24.1.0 libmambapy/1.5.8 aau/0.4.4 c/. s/. e/.
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;          administrator : False
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;             netrc file : None
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;           offline mode : False
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;可以看到几个重要信息：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Conda&lt;/strong&gt;配置文件：&lt;code&gt;C:\Users\obull\.condarc&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Conda&lt;/strong&gt;包下载渠道：&lt;strong&gt;channel URLs&lt;/strong&gt;列表几个地址&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Conda&lt;/strong&gt;包缓存目录：&lt;code&gt;D:\Software\miniconda3\pkgs&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Conda&lt;/strong&gt;虚拟环境目录：&lt;code&gt;D:\Software\miniconda3\envs&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;以上配置都是默认配置，其中包环境和虚拟环境目录比较占磁盘空间，可以设置为其他目录，同时下载渠道可以使用国内镜像以提升包的下载速度：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;打开配置文件：&lt;code&gt;C:\Users\obull\.condarc&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;若&lt;code&gt;.condarc&lt;/code&gt;配置文件不存在，可以执行命令自动生成一个默认文件：&lt;code&gt;conda config --set show_channel_urls yes&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;打开&lt;code&gt;.condarc&lt;/code&gt;配置，设置虚拟环境目录和：&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;11
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;12
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;13
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;14
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;15
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;16
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;17
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;18
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;19
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;20
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-plaintext&#34; data-lang=&#34;plaintext&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;envs_dirs:
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  - D:/Software/miniconda3/pkgs
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;pkgs_dirs:
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  - D:/Software/miniconda3/envs
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;show_channel_urls: true
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;channels:
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/msys2/
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge/
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  - defaults
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;custom_channels:
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  conda-forge: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  msys2: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  bioconda: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  menpo: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  pytorch: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  pytorch-lts: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  simpleitk: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;保存配置文件之后，我们可以看到&lt;strong&gt;Conda&lt;/strong&gt;信息的变化：&lt;code&gt;conda info&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Conda&lt;/strong&gt;命令还有一些其他用法，比较常用的命令如下列表：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;查看&lt;strong&gt;Conda&lt;/strong&gt;版本：&lt;code&gt;conda --version&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;更新&lt;strong&gt;Conda&lt;/strong&gt;版本：&lt;code&gt;conda update conda&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;安装 Python 包（如安装&lt;code&gt;numpy&lt;/code&gt;包）：&lt;code&gt;conda install numpy&lt;/code&gt; 通过首选渠道下载包文件&lt;/li&gt;
&lt;li&gt;指定渠道安装 Python 包：&lt;code&gt;conda install conda-forge::numpy&lt;/code&gt; 通过&lt;strong&gt;conda-forge&lt;/strong&gt;渠道下载包文件&lt;/li&gt;
&lt;li&gt;安装 Python 包到指定的虚拟环境：&lt;code&gt;conda install --name PY2.7 matplotlib&lt;/code&gt; 安装&lt;code&gt;matplotlib&lt;/code&gt;包到指定的&lt;code&gt;PY2.7&lt;/code&gt;虚拟环境&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;python-虚拟环境配置&#34;&gt;Python 虚拟环境配置&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;特别注意：&lt;/strong&gt; 有了&lt;strong&gt;Miniconda&lt;/strong&gt;包管理工具，我们无需单独下载和安装 Python，可直接通过 Conda 安装对应的版本即可。&lt;/p&gt;
&lt;p&gt;老牛同学创建一个名称为&lt;strong&gt;PY3.12&lt;/strong&gt;的虚拟环境，使用 Python 版本为&lt;strong&gt;3.12.3&lt;/strong&gt;：&lt;code&gt;conda create --name PY3.12 python=3.12.3&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;新建虚拟环境需要初始化基础包（包括 Python SDK 等），基础包下载完成之后，可以通过命令查看虚拟环境列表：&lt;code&gt;conda info --envs&lt;/code&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&amp;gt; conda info --envs
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;base                     D:&lt;span class=&#34;se&#34;&gt;\S&lt;/span&gt;oftware&lt;span class=&#34;se&#34;&gt;\m&lt;/span&gt;iniconda3
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;PY3.12                   D:&lt;span class=&#34;se&#34;&gt;\S&lt;/span&gt;oftware&lt;span class=&#34;se&#34;&gt;\m&lt;/span&gt;iniconda3&lt;span class=&#34;se&#34;&gt;\p&lt;/span&gt;kgs&lt;span class=&#34;se&#34;&gt;\P&lt;/span&gt;Y3.12
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;其中，&lt;code&gt;base&lt;/code&gt;是&lt;strong&gt;Conda&lt;/strong&gt;默认的虚拟环境，我们刚创建的&lt;code&gt;PY3.12&lt;/code&gt;虚拟环境已经存在了！&lt;/p&gt;
&lt;h2 id=&#34;python-虚拟环境使用&#34;&gt;Python 虚拟环境使用&lt;/h2&gt;
&lt;p&gt;默认虚拟环境是&lt;code&gt;base&lt;/code&gt;，我们可以激活和取消虚拟环境。若是首次使用，则需要执行&lt;code&gt;conda init&lt;/code&gt;命令进行初始化：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;激活虚拟环境：&lt;code&gt;conda activate PY3.12&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;取消虚拟环境：&lt;code&gt;conda deactivate&lt;/code&gt;（无需指定环境名）&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;7
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;C:&lt;span class=&#34;se&#34;&gt;\U&lt;/span&gt;sers&lt;span class=&#34;se&#34;&gt;\o&lt;/span&gt;bull&amp;gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;C:&lt;span class=&#34;se&#34;&gt;\U&lt;/span&gt;sers&lt;span class=&#34;se&#34;&gt;\o&lt;/span&gt;bull&amp;gt;conda activate PY3.12
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;D:&lt;span class=&#34;se&#34;&gt;\S&lt;/span&gt;oftware&lt;span class=&#34;se&#34;&gt;\m&lt;/span&gt;iniconda3&lt;span class=&#34;se&#34;&gt;\p&lt;/span&gt;kgs&lt;span class=&#34;se&#34;&gt;\P&lt;/span&gt;Y3.12&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt; C:&lt;span class=&#34;se&#34;&gt;\U&lt;/span&gt;sers&lt;span class=&#34;se&#34;&gt;\o&lt;/span&gt;bull&amp;gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;D:&lt;span class=&#34;se&#34;&gt;\S&lt;/span&gt;oftware&lt;span class=&#34;se&#34;&gt;\m&lt;/span&gt;iniconda3&lt;span class=&#34;se&#34;&gt;\p&lt;/span&gt;kgs&lt;span class=&#34;se&#34;&gt;\P&lt;/span&gt;Y3.12&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt; C:&lt;span class=&#34;se&#34;&gt;\U&lt;/span&gt;sers&lt;span class=&#34;se&#34;&gt;\o&lt;/span&gt;bull&amp;gt;conda deactivate
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;C:&lt;span class=&#34;se&#34;&gt;\U&lt;/span&gt;sers&lt;span class=&#34;se&#34;&gt;\o&lt;/span&gt;bull&amp;gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h2 id=&#34;jupyter-lab-安装和配置可选&#34;&gt;Jupyter Lab 安装和配置（可选）&lt;/h2&gt;
&lt;p&gt;在 Python 研发 IDE 选择上，老牛同学推荐推荐使用&lt;strong&gt;Jupyter Lab&lt;/strong&gt;，当然如果有&lt;strong&gt;PyCharm&lt;/strong&gt;等 Python 开发工具，也是一个不错的选择：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Jupyter Lab&lt;/strong&gt; 主要是为了数据科学、科学计算和教育而设计的。它支持交互式计算和数据可视化，非常适合探索性数据分析、机器学习、数值模拟等。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Jupyter Lab&lt;/strong&gt; 提供了一个基于 Web 的用户界面，支持在浏览器中直接编写代码、运行分析，并查看结果。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Jupyter Lab&lt;/strong&gt; 强调的是交互式编程和数据可视化。它允许用户逐段运行代码并即时查看输出，支持 Markdown 和富媒体，非常适合制作和展示研究结果。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Jupyter Lab&lt;/strong&gt; 作为一个轻量级的 Web 应用，其启动速度快，但在处理大型数据集时，性能可能会受到浏览器和硬件资源的限制。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Jupyter Lab&lt;/strong&gt; 特别适合做数据分析、数据科学教育、探索性研究和创建可分享的交互式报告。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Jupyter Lab&lt;/strong&gt; IDE 可以通过&lt;strong&gt;Conda&lt;/strong&gt;安装，其安装命令如下：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 激活Python虚拟环境&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;conda activate PY3.12
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 安装Jupyter Lab（指定下载源）&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;conda install -c conda-forge jupyterlab
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;conda install -c conda-forge ipywidgets
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;&lt;strong&gt;Jupyter Lab&lt;/strong&gt; IDE 成功安装之后，可以通过以下命令打开：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;8
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 激活Python虚拟环境&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;conda activate PY3.12
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 切换到Jupyter目录（我们以后代码存放的目录）&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;cd&lt;/span&gt; ~/JupyterLab
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 启动Jupyter WebIDE&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;jupyter-lab .
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;浏览器自动打开了 Web IDE，或者自己打开：&lt;a class=&#34;link&#34; href=&#34;http://localhost:8888/lab&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;http://localhost:8888/lab&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ntopic.cn/p/2024062501/11.jpg&#34;
	width=&#34;1630&#34;
	height=&#34;1062&#34;
	srcset=&#34;https://ntopic.cn/p/2024062501/11_hu5d70941486d04791f921b0884192345b_61983_480x0_resize_q75_box.jpg 480w, https://ntopic.cn/p/2024062501/11_hu5d70941486d04791f921b0884192345b_61983_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Jupyter Lab界面&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;153&#34;
		data-flex-basis=&#34;368px&#34;
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;ollama-安装和使用&#34;&gt;Ollama 安装和使用&lt;/h2&gt;
&lt;p&gt;使用&lt;strong&gt;Ollama&lt;/strong&gt;可以非常方便的管理本地大模型，目前主流大模型都支持&lt;strong&gt;Ollama&lt;/strong&gt;，包括&lt;strong&gt;Phi&lt;/strong&gt;、&lt;strong&gt;Qwen&lt;/strong&gt;、&lt;strong&gt;Llama&lt;/strong&gt;等，因此使用&lt;strong&gt;Ollama&lt;/strong&gt;可以提升我们管理和使用大模型效率：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;下载并安装&lt;strong&gt;Ollama&lt;/strong&gt;：&lt;a class=&#34;link&#34; href=&#34;https://ollama.com/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Ollama 官网&lt;/a&gt;（支持：Windows、Mac 和 Linux 系统）&lt;/li&gt;
&lt;li&gt;设置模型数据文件路径（可选）：默认情况下，模型文件存放在&lt;strong&gt;C:盘&lt;/strong&gt;用户目录。我们可以通过&lt;strong&gt;系统环境变量&lt;/strong&gt;设置成其他目录。&lt;strong&gt;系统环境变量&lt;/strong&gt;名为：&lt;code&gt;OLLAMA_MODELS&lt;/code&gt;，&lt;strong&gt;系统环境变量&lt;/strong&gt;的值为新的目录（如老牛同学设置为：&lt;code&gt;D:\ModelSpace\Ollama&lt;/code&gt;）&lt;/li&gt;
&lt;li&gt;下载并启动大模型，老牛同学以阿里的&lt;code&gt;Qwen2-7B&lt;/code&gt;为例：&lt;code&gt;ollama run qwen:7B&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;模型文件下载完成之后，自动就有了对话客户端：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;7
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;D:&lt;span class=&#34;se&#34;&gt;\&amp;gt;&lt;/span&gt;conda activate PY3.12
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;D:&lt;span class=&#34;se&#34;&gt;\S&lt;/span&gt;oftware&lt;span class=&#34;se&#34;&gt;\m&lt;/span&gt;iniconda3&lt;span class=&#34;se&#34;&gt;\p&lt;/span&gt;kgs&lt;span class=&#34;se&#34;&gt;\P&lt;/span&gt;Y3.12&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt; D:&lt;span class=&#34;se&#34;&gt;\&amp;gt;&lt;/span&gt;ollama list
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;NAME    ID              SIZE    MODIFIED
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;qwen:7b 2091ee8c8d8f    4.5 GB  &lt;span class=&#34;m&#34;&gt;3&lt;/span&gt; hours ago
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;D:&lt;span class=&#34;se&#34;&gt;\S&lt;/span&gt;oftware&lt;span class=&#34;se&#34;&gt;\m&lt;/span&gt;iniconda3&lt;span class=&#34;se&#34;&gt;\p&lt;/span&gt;kgs&lt;span class=&#34;se&#34;&gt;\P&lt;/span&gt;Y3.12&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt; D:&lt;span class=&#34;se&#34;&gt;\&amp;gt;&lt;/span&gt;ollama run qwen:7b
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://ntopic.cn/p/2024062501/12.jpg&#34;
	width=&#34;2511&#34;
	height=&#34;769&#34;
	srcset=&#34;https://ntopic.cn/p/2024062501/12_huabb101567fffc4fda39f34836a7651dd_307705_480x0_resize_q75_box.jpg 480w, https://ntopic.cn/p/2024062501/12_huabb101567fffc4fda39f34836a7651dd_307705_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Ollama对话客户端&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;326&#34;
		data-flex-basis=&#34;783px&#34;
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;ollama-web-界面对话客户端&#34;&gt;Ollama Web 界面对话客户端&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Ollama&lt;/strong&gt;自带控制台聊天对话界面体验总归是不太好，接下来部署 Web 可视化聊天界面：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;下载并安装 Node.js 工具：&lt;a class=&#34;link&#34; href=&#34;https://nodejs.org/zh-cn&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://nodejs.org/zh-cn&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;下载&lt;code&gt;ollama-webui&lt;/code&gt;工程代码：&lt;code&gt;git clone https://github.com/ollama-webui/ollama-webui-lite ollama-webui&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;切换&lt;code&gt;ollama-webui&lt;/code&gt;代码的目录：&lt;code&gt;cd ollama-webui&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;设置 Node.js 工具包镜像源（下载提速）：&lt;code&gt;npm config set registry http://mirrors.cloud.tencent.com/npm/&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;安装 Node.js 依赖的工具包：&lt;code&gt;npm install&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;最后，启动 Web 可视化界面：&lt;code&gt;npm run dev&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;https://ntopic.cn/p/2024062501/13.jpg&#34;
	width=&#34;1834&#34;
	height=&#34;1441&#34;
	srcset=&#34;https://ntopic.cn/p/2024062501/13_hu608dd0b0c5cb0d5d28aae5fe81670e22_224513_480x0_resize_q75_box.jpg 480w, https://ntopic.cn/p/2024062501/13_hu608dd0b0c5cb0d5d28aae5fe81670e22_224513_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Ollam WebUI对话&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;127&#34;
		data-flex-basis=&#34;305px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;如果看到以上输出，代表 Web 可视化界面已经成功了！&lt;/p&gt;
&lt;p&gt;浏览器打开 Web 可视化界面：&lt;a class=&#34;link&#34; href=&#34;http://localhost:3000&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;http://localhost:3000/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Ollama&lt;/strong&gt;其他的命令工具：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;11
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 查看当前Ollama的模型&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;ollama list
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 增量更新当前部署的模型&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;ollama pull qwen:7b
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 删除一个模型文件&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;ollama rm qwen:7b
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 复制一个模型&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;ollama cp qwen:7b Qwen-7B
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;&lt;strong&gt;Ollama&lt;/strong&gt;API 结果返回&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;curl http://localhost:11434/api/generate -d &lt;span class=&#34;s1&#34;&gt;&amp;#39;{
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s1&#34;&gt;  &amp;#34;model&amp;#34;: &amp;#34;qwen:7b&amp;#34;,
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s1&#34;&gt;  &amp;#34;prompt&amp;#34;:&amp;#34;为什么天空是蓝色的？&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s1&#34;&gt;}&amp;#39;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;&lt;strong&gt;Ollama API&lt;/strong&gt;聊天对话&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;curl http://localhost:11434/api/chat -d &lt;span class=&#34;s1&#34;&gt;&amp;#39;{
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s1&#34;&gt;  &amp;#34;model&amp;#34;: &amp;#34;qwen:7b&amp;#34;,
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s1&#34;&gt;  &amp;#34;messages&amp;#34;: [
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s1&#34;&gt;    { &amp;#34;role&amp;#34;: &amp;#34;user&amp;#34;, &amp;#34;content&amp;#34;: &amp;#34;为什么天空是蓝色的？&amp;#34; }
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s1&#34;&gt;  ]
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s1&#34;&gt;}&amp;#39;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;hr&gt;
&lt;p&gt;关注本公众号，我们共同学习进步 👇🏻👇🏻👇🏻&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ntopic.cn/WX-21.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;微信公众号：老牛同学&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Phi-3 开源大模型&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/bNxHM3B7HOLNvJtjwvt8iw&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Phi-3 模型手机部署教程（微软发布的可与 GPT-3.5 媲美的小模型）&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Qwen2-7B 开源大模型&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/u_Uw88dpQRgbtfI4_1OOwQ&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Qwen2 阿里最强开源大模型（Qwen2-7B）本地部署、API 调用和 WebUI 对话机器人&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Llama-3-8B 开源大模型&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/MekCUJDhKzuUnoykkGoH2g&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;玩转 AI，笔记本电脑安装属于自己的 Llama 3 8B 大模型和对话客户端&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/2DVYO75h0o5EHN_K_GF4Eg&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;一文彻底整明白，基于 Ollama 工具的 LLM 大语言模型 Web 可视化对话机器人部署指南&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/idcdIr8mMWDQ_iZU5r_UEQ&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;基于 Llama 3 搭建中文版（Llama3-Chinese-Chat）大模型对话聊天机器人&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;GLM-4-9B 开源大模型&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/g7lDfnRRGdrHqN7WGMSkAg&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;本地部署 GLM-4-9B 清华智谱开源大模型方法和对话效果体验&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;ChatTTS 文本转语音模型&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/rL3vyJ_xEj7GGoKaxUh8_A&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;ChatTTS 开源文本转语音模型本地部署、API 使用和搭建 WebUI 界面&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Stable Diffusion 3 文生图模型&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/Sax4z2k8Dvn82h15jf51Hw&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Stable Diffusion 3 文生图“开源英雄”大模型本地部署和使用教程，轻松实现 AI 绘图自由&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;大模型应用案例&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/m_O2OSoXWLL0PJurLCdzng&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;借助 AI 大模型，三分钟原创一部儿童故事短视频（附完整操作步骤）&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/gaLw3yP-oANvQyjRSkVjyw&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;高效编写大模型 Prompt 提示词，解锁 AI 无限创意潜能&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Python 小游戏&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/hv2tE-yot_H04HCezxQWXg&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;AI 已来，我与 AI 一起用 Python 编写了一个消消乐小游戏&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/tkTlt4rbFKQ73zudluPO1A&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Python 游戏编程：一步步用 Python 打造经典贪吃蛇小游戏&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
</description>
        </item>
        <item>
        <title>Phi-3 模型手机部署教程（微软发布的可与GPT-3.5媲美的小模型）</title>
        <link>https://ntopic.cn/p/2024062301/</link>
        <pubDate>Sun, 23 Jun 2024 00:00:00 +0000</pubDate>
        
        <guid>https://ntopic.cn/p/2024062301/</guid>
        <description>&lt;img src="https://ntopic.cn/p/2024062301/00.jpg" alt="Featured image of post Phi-3 模型手机部署教程（微软发布的可与GPT-3.5媲美的小模型）" /&gt;&lt;p&gt;前面几篇博文，老牛同学和大家一起在个人电脑部署了&lt;strong&gt;Qwen2&lt;/strong&gt;、&lt;strong&gt;GLM4&lt;/strong&gt;、&lt;strong&gt;Llama3&lt;/strong&gt;、&lt;strong&gt;ChatTTS&lt;/strong&gt;和&lt;strong&gt;Stable Diffusion&lt;/strong&gt;等 LLM 大模型，也通过 API 和 WebUI 的方式完成了体验。&lt;/p&gt;
&lt;p&gt;但是这些大模型因为部署在个人电脑本地，不能够随时携带。如果能在手机上部署大模型的话，老牛同学感觉很有意义，手机与我们的生活更为密切相关，并且手机上也有大量的个人数据，与大模型交互起来也更加方便。同时，在手机上跑个大模型，还是很酷！&lt;/p&gt;
&lt;p&gt;老牛同学期望能通过本文，和大家一起完成这项很酷且有意义的事情。老牛同学用的是&lt;strong&gt;小米 10 Pro&lt;/strong&gt;手机，其配置参数如下：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ntopic.cn/p/2024062301/01.jpg&#34;
	width=&#34;1080&#34;
	height=&#34;2340&#34;
	srcset=&#34;https://ntopic.cn/p/2024062301/01_huf9de8c070f651996b22bfe488baf7e14_274937_480x0_resize_q75_box.jpg 480w, https://ntopic.cn/p/2024062301/01_huf9de8c070f651996b22bfe488baf7e14_274937_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;老牛同学手机配置&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;46&#34;
		data-flex-basis=&#34;110px&#34;
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;选择-phi-3-微软开源模型&#34;&gt;选择 Phi-3 微软开源模型&lt;/h2&gt;
&lt;p&gt;受限于手机 CPU 和内存等硬件配置，我们要选择小语言模型（SLM）。其中，阿里开源了&lt;strong&gt;Qwen2-0.5B&lt;/strong&gt;和&lt;strong&gt;Qwen2-1.5B&lt;/strong&gt;两款小尺寸模型，微软了开源&lt;strong&gt;Phi-3 Mini&lt;/strong&gt;（&lt;strong&gt;3.8B&lt;/strong&gt;）和&lt;strong&gt;Phi-3 medium&lt;/strong&gt;（&lt;strong&gt;14B&lt;/strong&gt;）两款尺寸模型。&lt;/p&gt;
&lt;p&gt;由于之前我们在笔记本部署了&lt;strong&gt;Qwen2-7B&lt;/strong&gt;大模型，本次我们就在手机部署&lt;strong&gt;Phi-3 Mini&lt;/strong&gt;模型，顺便也体验一下不同科技公司的大模型产品，其效果可以媲美&lt;strong&gt;GPT-3.5&lt;/strong&gt;大模型：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ntopic.cn/p/2024062301/11.jpg&#34;
	width=&#34;1080&#34;
	height=&#34;1136&#34;
	srcset=&#34;https://ntopic.cn/p/2024062301/11_hufca9c8e53812d6de8d893ed0b898ab7f_336127_480x0_resize_q75_box.jpg 480w, https://ntopic.cn/p/2024062301/11_hufca9c8e53812d6de8d893ed0b898ab7f_336127_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Phi-3性能评测报告&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;95&#34;
		data-flex-basis=&#34;228px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Qwen2-7B&lt;/strong&gt;本地部署：&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/u_Uw88dpQRgbtfI4_1OOwQ&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Qwen2 阿里最强开源大模型（Qwen2-7B）本地部署、API 调用和 WebUI 对话机器人&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;手机配置-linux-环境termux-应用&#34;&gt;手机配置 Linux 环境（Termux 应用）&lt;/h2&gt;
&lt;p&gt;小米等安卓手机的基于 Linux 内核的操作系统，但是我们无法像在 Linux 那样执行 Linux 命令，因此我首先得配置一下 Linux 环境。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Termux&lt;/strong&gt;是一个 Android 的终端模拟器，可以在 Android 设备上运行 Linux 命令和工具。&lt;strong&gt;Termux&lt;/strong&gt;的 Android APP 可通过官网下载并安装：&lt;a class=&#34;link&#34; href=&#34;https://github.com/termux/termux-app/releases&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/termux/termux-app/releases&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;当前官网的最新稳定版本：&lt;code&gt;v0.118.1 - 2024-06-18 00.05&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ntopic.cn/p/2024062301/02.jpg&#34;
	width=&#34;1792&#34;
	height=&#34;1368&#34;
	srcset=&#34;https://ntopic.cn/p/2024062301/02_hu8929c3d33f0597ba0060fb44fae62288_262616_480x0_resize_q75_box.jpg 480w, https://ntopic.cn/p/2024062301/02_hu8929c3d33f0597ba0060fb44fae62288_262616_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Termux安装文件&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;130&#34;
		data-flex-basis=&#34;314px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;请根据手机情况，下载对应的 apk 文件。老牛同学下载的 apk 文件：&lt;strong&gt;termux-app_v0.118.1+github-debug_universal.apk&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;apk 安装成功后，打开&lt;strong&gt;Termux&lt;/strong&gt;应用后，默认展示如下，就可以开始输入 Linux 命令了：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ntopic.cn/p/2024062301/03.jpg&#34;
	width=&#34;1080&#34;
	height=&#34;2340&#34;
	srcset=&#34;https://ntopic.cn/p/2024062301/03_hu38b15bc0bda6dc964e79f04dbde22382_304569_480x0_resize_q75_box.jpg 480w, https://ntopic.cn/p/2024062301/03_hu38b15bc0bda6dc964e79f04dbde22382_304569_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Termux应用界面&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;46&#34;
		data-flex-basis=&#34;110px&#34;
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;手机上安装-linux-操作系统&#34;&gt;手机上安装 Linux 操作系统&lt;/h2&gt;
&lt;p&gt;首先，我们安装&lt;strong&gt;proot-distro&lt;/strong&gt;系统管理工具，&lt;strong&gt;proot-distro&lt;/strong&gt;可以非常方便在 Termux 中&lt;strong&gt;安装&lt;/strong&gt;、&lt;strong&gt;卸载&lt;/strong&gt;和&lt;strong&gt;运行&lt;/strong&gt;Linux 的发行版本（包括：Ubuntu、Debian、Arch Linux 等）：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;友情提示：&lt;/strong&gt; 在手机中输入以下命令效率比较低，我们可以把命令发到微信，然后一条一条复制粘贴！&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;pkg install proot-distro
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;&lt;strong&gt;proot-distro&lt;/strong&gt; 安装成功之后，我们安装&lt;strong&gt;Debian&lt;/strong&gt;操作系统：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;proot-distro install debian
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://ntopic.cn/p/2024062301/04.jpg&#34;
	width=&#34;1080&#34;
	height=&#34;2340&#34;
	srcset=&#34;https://ntopic.cn/p/2024062301/04_huea1eb21800f85aa4f475e6e988978d89_1188837_480x0_resize_q75_box.jpg 480w, https://ntopic.cn/p/2024062301/04_huea1eb21800f85aa4f475e6e988978d89_1188837_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Debian安装成功&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;46&#34;
		data-flex-basis=&#34;110px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;最后，登录新安装的&lt;strong&gt;Debian&lt;/strong&gt;操作系统：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;proot-distro login debian
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;登录之后，自动启动了&lt;strong&gt;Shell&lt;/strong&gt;命令行终端：&lt;code&gt;root@localhost:~#&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;可以执行相关的 Linux 命令了：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ntopic.cn/p/2024062301/05.jpg&#34;
	width=&#34;2340&#34;
	height=&#34;1080&#34;
	srcset=&#34;https://ntopic.cn/p/2024062301/05_hu503792de75c28e8bf910255688f0d100_334989_480x0_resize_q75_box.jpg 480w, https://ntopic.cn/p/2024062301/05_hu503792de75c28e8bf910255688f0d100_334989_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Debian系统命令&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;216&#34;
		data-flex-basis=&#34;520px&#34;
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;通过-termux-安装-phi-3-模型&#34;&gt;通过 Termux 安装 Phi-3 模型&lt;/h2&gt;
&lt;p&gt;通过上面的操作，我们已经在手机上安装好了&lt;strong&gt;Debian&lt;/strong&gt;操作系统，接下来在&lt;strong&gt;Debian&lt;/strong&gt;操作系统中安装&lt;strong&gt;Phi-3 Mini&lt;/strong&gt;模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;第一步：&lt;/strong&gt; 在&lt;strong&gt;Debian&lt;/strong&gt;系统中安装&lt;strong&gt;Ollama&lt;/strong&gt;软件，没错，就是之前在个人电脑部署&lt;strong&gt;Llama3&lt;/strong&gt;、&lt;strong&gt;Qwen2&lt;/strong&gt;等大模型时，用于管理本地大模型的&lt;strong&gt;Ollama&lt;/strong&gt;软件。由于我们在手机上安装了&lt;strong&gt;Debian&lt;/strong&gt;系统，那么和电脑一样，&lt;strong&gt;Ollama&lt;/strong&gt;也可以管理&lt;strong&gt;Debian&lt;/strong&gt;系统本地部署的大模型：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 安装Ollama软件&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;curl -fsSL https://ollama.com/install.sh &lt;span class=&#34;p&#34;&gt;|&lt;/span&gt; sh
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;&lt;strong&gt;Ollama&lt;/strong&gt;安装成功输出信息如下：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ntopic.cn/p/2024062301/06.jpg&#34;
	width=&#34;1080&#34;
	height=&#34;2340&#34;
	srcset=&#34;https://ntopic.cn/p/2024062301/06_hu7a830a0b321e84a755d8d7c217322a1c_2010366_480x0_resize_q75_box.jpg 480w, https://ntopic.cn/p/2024062301/06_hu7a830a0b321e84a755d8d7c217322a1c_2010366_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Ollama安装成功&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;46&#34;
		data-flex-basis=&#34;110px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;第二步：&lt;/strong&gt; 通过&lt;strong&gt;后台&lt;/strong&gt;启动&lt;strong&gt;Ollama&lt;/strong&gt;服务：&lt;code&gt;nohup ollama serve &amp;amp;&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;我们用&lt;strong&gt;Ollama&lt;/strong&gt;命令，查看 Ollama 信息，如：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;查看版本：&lt;code&gt;ollama -v&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;查看本地模型列表：&lt;code&gt;ollama list&lt;/code&gt;（目前还没有部署模型，因此结果列表为&lt;strong&gt;空&lt;/strong&gt;）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;第三步：&lt;/strong&gt; 通过&lt;strong&gt;Ollama&lt;/strong&gt;安装并启动&lt;strong&gt;Phi-3 Mini&lt;/strong&gt;模型：&lt;code&gt;ollama run phi3:mini&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Phi-3 Mini&lt;/strong&gt;模型文件总大小为&lt;strong&gt;2.4GB&lt;/strong&gt;左右，因此下载需要一点时间：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ntopic.cn/p/2024062301/07.jpg&#34;
	width=&#34;1080&#34;
	height=&#34;2340&#34;
	srcset=&#34;https://ntopic.cn/p/2024062301/07_hu3cf50d426911112bcdb0ac26d4ee1b8e_565757_480x0_resize_q75_box.jpg 480w, https://ntopic.cn/p/2024062301/07_hu3cf50d426911112bcdb0ac26d4ee1b8e_565757_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Phi-3 Mini安装成功&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;46&#34;
		data-flex-basis=&#34;110px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;到此，&lt;strong&gt;Phi-3 Mini&lt;/strong&gt;模型部署成功，我们可以体验手机上的大模型，比如：&lt;strong&gt;请用 100 个汉字解释一下，天空为什么是蓝色的？&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ntopic.cn/p/2024062301/08.jpg&#34;
	width=&#34;1144&#34;
	height=&#34;1160&#34;
	srcset=&#34;https://ntopic.cn/p/2024062301/08_hua04b28422bbb4c6a12cc5f607adbb1af_300229_480x0_resize_q75_box.jpg 480w, https://ntopic.cn/p/2024062301/08_hua04b28422bbb4c6a12cc5f607adbb1af_300229_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Phi-3 Mini模型推理&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;98&#34;
		data-flex-basis=&#34;236px&#34;
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;
&lt;p&gt;相比电脑端，手机的算力弱太多，就算老牛同学用的是最小尺寸的模型（&lt;strong&gt;Phi-3 Mini&lt;/strong&gt;），其推理的速度还是慢得多。其内容的输出速度，比我手机打字都要慢。盯着屏幕，看着模型一个字一个字的输出，感觉也挺有趣 😁&lt;/p&gt;
&lt;p&gt;目前 AI 是大热门，各大公司推出的大模型参数一个比一个大，能力一个比一个厉害。但大模型训练和推理成本均比较高昂，在很大程度上限制了其发展，因此大模型 AI 应用相对较少，或者说对我们生活影响还很小，因此其还有很大的发展空间。&lt;/p&gt;
&lt;p&gt;而反观针对特定业务场景定制的小模型（比如&lt;strong&gt;Phi&lt;/strong&gt;定制等），其成本就低得多，就能更有效地应用于各种垂直场景。&lt;/p&gt;
&lt;p&gt;老牛同学觉得这种“&lt;strong&gt;小而美&lt;/strong&gt;”的 AI 模型将会越来越多，也会越来越受欢迎！&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;关注本公众号，我们共同学习进步 👇🏻👇🏻👇🏻&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ntopic.cn/WX-21.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;微信公众号：老牛同学&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Qwen2-7B 开源大模型&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/u_Uw88dpQRgbtfI4_1OOwQ&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Qwen2 阿里最强开源大模型（Qwen2-7B）本地部署、API 调用和 WebUI 对话机器人&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Llama-3-8B 开源大模型&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/MekCUJDhKzuUnoykkGoH2g&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;玩转 AI，笔记本电脑安装属于自己的 Llama 3 8B 大模型和对话客户端&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/2DVYO75h0o5EHN_K_GF4Eg&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;一文彻底整明白，基于 Ollama 工具的 LLM 大语言模型 Web 可视化对话机器人部署指南&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/idcdIr8mMWDQ_iZU5r_UEQ&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;基于 Llama 3 搭建中文版（Llama3-Chinese-Chat）大模型对话聊天机器人&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;GLM-4-9B 开源大模型&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/g7lDfnRRGdrHqN7WGMSkAg&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;本地部署 GLM-4-9B 清华智谱开源大模型方法和对话效果体验&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;ChatTTS 文本转语音模型&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/rL3vyJ_xEj7GGoKaxUh8_A&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;ChatTTS 开源文本转语音模型本地部署、API 使用和搭建 WebUI 界面&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Stable Diffusion 3 文生图模型&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/Sax4z2k8Dvn82h15jf51Hw&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Stable Diffusion 3 文生图“开源英雄”大模型本地部署和使用教程，轻松实现 AI 绘图自由&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;大模型应用案例&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/m_O2OSoXWLL0PJurLCdzng&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;借助 AI 大模型，三分钟原创一部儿童故事短视频（附完整操作步骤）&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/gaLw3yP-oANvQyjRSkVjyw&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;高效编写大模型 Prompt 提示词，解锁 AI 无限创意潜能&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Python 小游戏&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/hv2tE-yot_H04HCezxQWXg&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;AI 已来，我与 AI 一起用 Python 编写了一个消消乐小游戏&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/tkTlt4rbFKQ73zudluPO1A&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Python 游戏编程：一步步用 Python 打造经典贪吃蛇小游戏&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
</description>
        </item>
        <item>
        <title>基于Llama 3搭建中文版（Llama3-Chinese-Chat）大模型对话聊天机器人</title>
        <link>https://ntopic.cn/p/2024052101/</link>
        <pubDate>Tue, 21 May 2024 00:00:00 +0000</pubDate>
        
        <guid>https://ntopic.cn/p/2024052101/</guid>
        <description>&lt;img src="https://ntopic.cn/p/2024052101/02.jpg" alt="Featured image of post 基于Llama 3搭建中文版（Llama3-Chinese-Chat）大模型对话聊天机器人" /&gt;&lt;blockquote&gt;
&lt;p&gt;前面两篇博文，我们分别在个人笔记本电脑部署了&lt;strong&gt;Llama 3 8B&lt;/strong&gt;参数大模型，并使用&lt;strong&gt;Ollama&lt;/strong&gt;搭建了基于 Web 可视化对话聊天机器人，可以在自己电脑上愉快的与&lt;strong&gt;Llama&lt;/strong&gt;大模型 Web 机器人对话聊天了。但在使用过程中，笔者发现&lt;strong&gt;Llama&lt;/strong&gt;大模型经常出现&lt;strong&gt;中文问题英文回答&lt;/strong&gt;的问题，需要使用&lt;strong&gt;中文回答&lt;/strong&gt;等提示词告诉大模型用中文回答，体验还不是最好的。今天，本博文就来解决这个问题，让我们有个中文版的&lt;strong&gt;Llama 3&lt;/strong&gt;Web 对话机器人（&lt;strong&gt;Llama3-Chinese-Chat&lt;/strong&gt;）……&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;第一篇&lt;strong&gt;Llama 3 8B&lt;/strong&gt;大模型部署和 Python 版对话机器人博文：&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/MekCUJDhKzuUnoykkGoH2g&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;玩转 AI，笔记本电脑安装属于自己的 Llama 3 8B 大模型和对话客户端&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;第二篇基于&lt;strong&gt;Ollama&lt;/strong&gt;部署&lt;strong&gt;Llama 3 8B&lt;/strong&gt;大模型 Web 版本对话机器人博文：&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/2DVYO75h0o5EHN_K_GF4Eg&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;一文彻底整明白，基于 Ollama 工具的 LLM 大语言模型 Web 可视化对话机器人部署指南&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;注意：&lt;/strong&gt; 因为本博文介绍的是&lt;strong&gt;Llama 3 中文版&lt;/strong&gt;（&lt;strong&gt;Llama3-Chinese-Chat&lt;/strong&gt;）对话机器人，涉及到前面两篇博文内容，特别是第二篇 Web 版本对话机器人部署，因此建议按照前文博文部署好&lt;strong&gt;Llama 3 8B&lt;/strong&gt;大语言模型。&lt;/p&gt;
&lt;h2 id=&#34;hf-上选择排名最高的模型&#34;&gt;HF 上选择排名最高的模型&lt;/h2&gt;
&lt;p&gt;模型列表官网地址：&lt;a class=&#34;link&#34; href=&#34;https://huggingface.co/models&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://huggingface.co/models&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;模型列表国内镜像（&lt;strong&gt;推荐&lt;/strong&gt;）：&lt;a class=&#34;link&#34; href=&#34;https://hf-mirror.com/models&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://hf-mirror.com/models&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;在模型列表页面按照关键字&lt;code&gt;llama chinese&lt;/code&gt;搜索，并按照&lt;strong&gt;趋势&lt;/strong&gt;排序，可以看到中文版模型：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ntopic.cn/p/2024052101/01.jpg&#34;
	width=&#34;2530&#34;
	height=&#34;1384&#34;
	srcset=&#34;https://ntopic.cn/p/2024052101/01_huae61d300a0f7989ceb023d401e9fe593_1105009_480x0_resize_q75_box.jpg 480w, https://ntopic.cn/p/2024052101/01_huae61d300a0f7989ceb023d401e9fe593_1105009_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;LLama中文版模型&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;182&#34;
		data-flex-basis=&#34;438px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;可以看出，第一名模型的&lt;strong&gt;下载&lt;/strong&gt;数量和&lt;strong&gt;点赞&lt;/strong&gt;数量，比第二名要多好多，我们就选择&lt;strong&gt;shenzhi-wang&lt;/strong&gt;这位作者发布的模型。&lt;/p&gt;
&lt;h2 id=&#34;方式一通过-gguf-量化模型安装推荐&#34;&gt;方式一：通过 GGUF 量化模型安装（推荐）&lt;/h2&gt;
&lt;p&gt;GGUF 安装比较简单，下载单个文件即可：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ntopic.cn/p/2024052101/02.jpg&#34;
	width=&#34;1040&#34;
	height=&#34;1086&#34;
	srcset=&#34;https://ntopic.cn/p/2024052101/02_hub3da83b4b5bc32de4db9cd09399c10b7_374346_480x0_resize_q75_box.jpg 480w, https://ntopic.cn/p/2024052101/02_hub3da83b4b5bc32de4db9cd09399c10b7_374346_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;LLama中文版GGUF模型&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;95&#34;
		data-flex-basis=&#34;229px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;下载到本地之后，按照我的&lt;strong&gt;第一篇&lt;/strong&gt;博文，即可进行控制台聊天了：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;启动大模型&lt;/strong&gt;Shell 脚本：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;source&lt;/span&gt; ./venv/bin/activate
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;python -m llama_cpp.server --host 0.0.0.0 --model &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;   ./Llama3-8B-Chinese-Chat-q4_0-v2_1.gguf &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;   --n_ctx &lt;span class=&#34;m&#34;&gt;20480&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;&lt;strong&gt;Python 对话客户端&lt;/strong&gt;代码：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;11
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;12
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;13
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;14
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;15
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;16
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;17
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;18
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;19
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;20
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;21
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;22
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;23
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;24
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;25
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;26
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;27
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;28
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;29
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;30
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;31
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;32
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;33
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;34
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;35
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;36
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;37
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;38
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;39
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;openai&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;OpenAI&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 注意服务端端口，因为是本地，所以不需要api_key&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;ip&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;127.0.0.1&amp;#39;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#ip = &amp;#39;192.168.1.37&amp;#39;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;client&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;OpenAI&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;base_url&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;http://&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;:8000/v1&amp;#34;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;format&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ip&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;         &lt;span class=&#34;n&#34;&gt;api_key&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;not-needed&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 对话历史：设定系统角色是一个只能助理，同时提交“自我介绍”问题&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;history&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;role&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;system&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;content&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;你是一个智能助理，你的回答总是容易理解的、正确的、有用的和内容非常精简.&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;},&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 首次自我介绍完毕，接下来是等代码我们的提示&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;while&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;completion&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;client&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;chat&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;completions&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;create&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;model&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;local-model&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;messages&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;history&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;temperature&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;0.7&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;stream&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;new_message&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;role&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;assistant&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;content&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;chunk&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;completion&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;chunk&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;choices&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;delta&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;content&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;chunk&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;choices&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;delta&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;content&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;end&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;flush&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;n&#34;&gt;new_message&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;content&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;chunk&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;choices&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;delta&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;content&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;history&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;append&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;new_message&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;\033&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;[91;1m&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;userinput&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;input&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&amp;gt; &amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;userinput&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;lower&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;bye&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;quit&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;exit&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]:&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# 我们输入bye/quit/exit等均退出客户端&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;\033&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;[0mBYE BYE!&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;break&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;history&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;append&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;({&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;role&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;user&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;content&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;userinput&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;})&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;\033&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;[92;1m&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;&lt;strong&gt;运行 Python 客户端&lt;/strong&gt;即可：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ntopic.cn/p/2024052101/03.jpg&#34;
	width=&#34;1632&#34;
	height=&#34;246&#34;
	srcset=&#34;https://ntopic.cn/p/2024052101/03_huc7b96f9887ab9df1f5da9de1f651a003_95258_480x0_resize_q75_box.jpg 480w, https://ntopic.cn/p/2024052101/03_huc7b96f9887ab9df1f5da9de1f651a003_95258_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Python控制台对话客户端&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;663&#34;
		data-flex-basis=&#34;1592px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;按照第二篇博文，部署基于 Web 版对话机器人：&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/2DVYO75h0o5EHN_K_GF4Eg&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;一文彻底整明白，基于 Ollama 工具的 LLM 大语言模型 Web 可视化对话机器人部署指南&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;基于 GGUF 量化模型&lt;strong&gt;生成 Ollama&lt;/strong&gt;模型文件，假设文件名为&lt;code&gt;Modelfile-Chinese&lt;/code&gt;，内容如下：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;FROM ./Llama3-8B-Chinese-Chat-q4_0-v2_1.gguf
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;执行 Ollama 模型转换，&lt;code&gt;Llama-3-8B-Chinese&lt;/code&gt;为 Ollama 模型名：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ ollama create Llama-3-8B-Chinese -f ./Modelfile-Chinese
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;transferring model data
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;using existing layer sha256:242ac8dd3eabcb1e5fcd3d78912eaf904f08bb6ecfed8bac9ac9a0b7a837fcb8
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;creating new layer sha256:9f3bfa6cfc3061e49f8d5ab5fba0f93426be5f8207d8d8a9eebf638bd12b627a
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;writing manifest
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;success
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;可以通过 Ollama 查看目前的大模型列表：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ ollama list
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;NAME                      ID            SIZE    MODIFIED
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Llama-3-8B-Chinese:latest 37143cf1f51f  4.7 GB  &lt;span class=&#34;m&#34;&gt;42&lt;/span&gt; seconds ago
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Llama-3-8B:latest         74abc0712fc1  4.9 GB  &lt;span class=&#34;m&#34;&gt;3&lt;/span&gt; days ago
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;可以看到我们刚安装的大模型：&lt;strong&gt;Llama-3-8B-Chinese&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;启动&lt;strong&gt;ollama-webui-lite&lt;/strong&gt;项目，可以选择&lt;strong&gt;Llama-3-8B-Chinese&lt;/strong&gt;模型和对话聊天了：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;11
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;12
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ npm run dev
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&amp;gt; ollama-webui-lite@0.0.1 dev
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&amp;gt; vite dev --host --port &lt;span class=&#34;m&#34;&gt;3000&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  VITE v4.5.3  ready in &lt;span class=&#34;m&#34;&gt;1797&lt;/span&gt; ms
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  ➜  Local:   http://localhost:3000/
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  ➜  Network: http://192.168.101.30:3000/
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  ➜  press h to show &lt;span class=&#34;nb&#34;&gt;help&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://ntopic.cn/p/2024052101/04.jpg&#34;
	width=&#34;1550&#34;
	height=&#34;264&#34;
	srcset=&#34;https://ntopic.cn/p/2024052101/04_hu97c9217b73ef8283d49772062613e0f2_34116_480x0_resize_q75_box.jpg 480w, https://ntopic.cn/p/2024052101/04_hu97c9217b73ef8283d49772062613e0f2_34116_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;OlLama选择中文版模型&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;587&#34;
		data-flex-basis=&#34;1409px&#34;
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;方式二通过-ollama-拉取模型文件&#34;&gt;方式二：通过 Ollama 拉取模型文件&lt;/h2&gt;
&lt;p&gt;这种方式比较简单，无需下载 GGUF 模型文件，可以让 Ollama 直接拉取模型文件并完成安装：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;8
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# Llama3-8B-Chinese-Chat的4位量化版本（对机器性能要求最低）&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;ollama run wangshenzhi/llama3-8b-chinese-chat-ollama-q4
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# Llama3-8B-Chinese-Chat的8位量化版本（对机器性能要求中等）&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;ollama run wangshenzhi/llama3-8b-chinese-chat-ollama-q8
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# Llama3-8B-Chinese-Chat的f16未量化版本（对机器性能要求最高）&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;ollama run wangshenzhi/llama3-8b-chinese-chat-ollama-fp16
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;Ollama 自动下载并完成安装，之后启动&lt;strong&gt;ollama-webui-lite&lt;/strong&gt;项目，就可以使用了~&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;我的本博客原地址：&lt;a class=&#34;link&#34; href=&#34;https://ntopic.cn/p/2024052101/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://ntopic.cn/p/2024052101&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;img src=&#34;https://ntopic.cn/WX-21.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;微信公众号：老牛同学&#34;
	
	
&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title>一文彻底整明白，基于Ollama工具的LLM大语言模型Web可视化对话机器人部署指南</title>
        <link>https://ntopic.cn/p/2024051801/</link>
        <pubDate>Sat, 18 May 2024 00:00:00 +0000</pubDate>
        
        <guid>https://ntopic.cn/p/2024051801/</guid>
        <description>&lt;img src="https://ntopic.cn/p/2024051801/02.jpg" alt="Featured image of post 一文彻底整明白，基于Ollama工具的LLM大语言模型Web可视化对话机器人部署指南" /&gt;&lt;blockquote&gt;
&lt;p&gt;在上一篇博文中，我们在本地部署了&lt;strong&gt;Llama 3 8B&lt;/strong&gt;参数大模型，并用 Python 写了一个控制台对话客户端，基本能愉快的与 Llama 大模型对话聊天了。但控制台总归太技术化，体验不是很友好，我们希望能有个类似 ChatGPT 那样的 Web 聊天对话界面，本博文就安排起来……&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;上一篇&lt;strong&gt;Llama 3 8B&lt;/strong&gt;大模型部署和 Python 对话客户端博文：&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s/MekCUJDhKzuUnoykkGoH2g&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;玩转 AI，笔记本电脑安装属于自己的 Llama 3 8B 大模型和对话客户端&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;因为本博文介绍的&lt;strong&gt;Web 可视化&lt;/strong&gt;对话机器人，涉及到前文的&lt;strong&gt;Llama 3 8B&lt;/strong&gt;大模型（并不是强依赖），因此建议提取安装前文部署好&lt;strong&gt;Llama 3 8B&lt;/strong&gt;大语言模型。&lt;/p&gt;
&lt;p&gt;为了方便把我们的大模型对话机器人分享出去，聊天机器人最后是基于&lt;strong&gt;Web&lt;/strong&gt;网站，可通过浏览器访问，本文正是通过&lt;code&gt;Ollama&lt;/code&gt;和&lt;code&gt;WebUI&lt;/code&gt;在本地部署&lt;code&gt;Llama 3&lt;/code&gt;Web 版聊天机器人，本文包括如下部分：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;什么是&lt;code&gt;Ollama&lt;/code&gt;，它与&lt;code&gt;Llama&lt;/code&gt;是什么关系？&lt;/li&gt;
&lt;li&gt;安装&lt;code&gt;Ollama&lt;/code&gt;大语言模型工具&lt;/li&gt;
&lt;li&gt;安装&lt;code&gt;Node.js&lt;/code&gt;编程语言工具包（为接下来的 Web 可视化聊天界面做好准备）&lt;/li&gt;
&lt;li&gt;基于&lt;code&gt;Llama 3 8B&lt;/code&gt;GGUF 模型文件创建&lt;code&gt;Ollama&lt;/code&gt;模型文件&lt;/li&gt;
&lt;li&gt;部署&lt;code&gt;Ollama&lt;/code&gt;大模型 Web 可视化聊天界面&lt;/li&gt;
&lt;li&gt;愉快的与&lt;code&gt;Llama 3&lt;/code&gt;大模型俩天对话&lt;/li&gt;
&lt;li&gt;最后，&lt;code&gt;Ollama&lt;/code&gt;大模型工具的其他用法&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;什么是ollama它与llama是什么关系&#34;&gt;什么是&lt;code&gt;Ollama&lt;/code&gt;，它与&lt;code&gt;Llama&lt;/code&gt;是什么关系？&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;Ollama&lt;/code&gt;是一个开源的 LLM（大型语言模型）服务工具，用于简化在本地运行大语言模型，降低使用大语言模型的门槛，使得大模型的开发者、研究人员和爱好者能够在本地环境快速实验、管理和部署最新大语言模型，包括如&lt;code&gt;Llama 3&lt;/code&gt;、&lt;code&gt;Phi 3&lt;/code&gt;、&lt;code&gt;Mistral&lt;/code&gt;、&lt;code&gt;Gemma&lt;/code&gt;等开源的大型语言模型。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Ollama&lt;/code&gt;目前支持以下大语言模型：&lt;a class=&#34;link&#34; href=&#34;https://ollama.com/library&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://ollama.com/library&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ntopic.cn/p/2024051801/01.jpg&#34;
	width=&#34;1278&#34;
	height=&#34;980&#34;
	srcset=&#34;https://ntopic.cn/p/2024051801/01_hua04b28422bbb4c6a12cc5f607adbb1af_199841_480x0_resize_q75_box.jpg 480w, https://ntopic.cn/p/2024051801/01_hua04b28422bbb4c6a12cc5f607adbb1af_199841_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;130&#34;
		data-flex-basis=&#34;312px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;因此，&lt;code&gt;Ollama&lt;/code&gt;与&lt;code&gt;Llama&lt;/code&gt;的关系：&lt;code&gt;Llama&lt;/code&gt;是大语言模型，而&lt;code&gt;Ollama&lt;/code&gt;是大语言模型（不限于&lt;code&gt;Llama&lt;/code&gt;模型）便捷的管理和运维工具&lt;/p&gt;
&lt;h2 id=&#34;安装ollama大语言模型工具&#34;&gt;安装&lt;code&gt;Ollama&lt;/code&gt;大语言模型工具&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;Ollama&lt;/code&gt;提供了&lt;strong&gt;MacOS&lt;/strong&gt;、&lt;strong&gt;Linux&lt;/strong&gt;和&lt;strong&gt;Windows&lt;/strong&gt;操作系统的安装包，大家可根据自己的操作系统，下载安装即可：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ntopic.cn/p/2024051801/02.jpg&#34;
	width=&#34;1184&#34;
	height=&#34;868&#34;
	srcset=&#34;https://ntopic.cn/p/2024051801/02_hua04b28422bbb4c6a12cc5f607adbb1af_86436_480x0_resize_q75_box.jpg 480w, https://ntopic.cn/p/2024051801/02_hua04b28422bbb4c6a12cc5f607adbb1af_86436_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;136&#34;
		data-flex-basis=&#34;327px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;安装包下载之后的安装过程，和日常安装其他软件没有差别，包括点击&lt;code&gt;Next&lt;/code&gt;以及&lt;code&gt;Install&lt;/code&gt;等安装&lt;code&gt;ollama&lt;/code&gt;到命令行。安装后续步骤中，我们可无需安装任何模型（默认是&lt;code&gt;Llama 3&lt;/code&gt;），因为我们在上文中已经安装了&lt;code&gt;Llama 3 8B&lt;/code&gt;大模型，后面可以直接使用。&lt;/p&gt;
&lt;p&gt;当然，假如没有根据我的前面博文安装&lt;code&gt;Llama 3 8B&lt;/code&gt;模型，在安装&lt;code&gt;Ollama&lt;/code&gt;过程中，也可以一起进行安装。&lt;/p&gt;
&lt;h2 id=&#34;安装nodejs编程语言工具包&#34;&gt;安装&lt;code&gt;Node.js&lt;/code&gt;编程语言工具包&lt;/h2&gt;
&lt;p&gt;安装&lt;code&gt;Node.js&lt;/code&gt;编程语言工具包和安装其他软件包一样，下载安装即可：&lt;a class=&#34;link&#34; href=&#34;https://nodejs.org&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://nodejs.org&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ntopic.cn/p/2024051801/03.jpg&#34;
	width=&#34;1282&#34;
	height=&#34;1250&#34;
	srcset=&#34;https://ntopic.cn/p/2024051801/03_hua04b28422bbb4c6a12cc5f607adbb1af_208693_480x0_resize_q75_box.jpg 480w, https://ntopic.cn/p/2024051801/03_hua04b28422bbb4c6a12cc5f607adbb1af_208693_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;102&#34;
		data-flex-basis=&#34;246px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;安装完成之后，可以验证一下 Node.js 的版本，建议用目前的最新&lt;strong&gt;v20&lt;/strong&gt;版本：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;node -v
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;我安装的版本：&lt;strong&gt;v20.13.1&lt;/strong&gt;（最新版本）&lt;/p&gt;
&lt;h2 id=&#34;基于llama-3-8bgguf-模型文件创建ollama模型&#34;&gt;基于&lt;code&gt;Llama 3 8B&lt;/code&gt;GGUF 模型文件创建&lt;code&gt;Ollama&lt;/code&gt;模型&lt;/h2&gt;
&lt;p&gt;在我们存放&lt;code&gt;Llama 3 8B&lt;/code&gt;的 GGUF 模型文件目录中，创建一个文件名为&lt;code&gt;Modelfile&lt;/code&gt;的文件，该文件的内容如下：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;FROM ./Meta-Llama-3-8B-Instruct.Q4_K_M.gguf
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;然后在控制台，使用这个文件创建&lt;code&gt;Ollama&lt;/code&gt;模型，这里我把&lt;code&gt;Ollama&lt;/code&gt;的模型取名为&lt;strong&gt;Llama-3-8B&lt;/strong&gt;：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ ollama create Llama-3-8B -f ./Modelfile
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;transferring model data
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;using existing layer sha256:647a2b64cbcdbe670432d0502ebb2592b36dd364d51a9ef7a1387b7a4365781f
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;creating new layer sha256:459d7c837b2bd7f895a15b0a5213846912693beedaf0257fbba2a508bc1c88d9
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;writing manifest
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;success
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;最后，通过&lt;code&gt;Ollama&lt;/code&gt;启动我们刚创建的大语言模型：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;ollama run Llama-3-8B
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://ntopic.cn/p/2024051801/04.jpg&#34;
	width=&#34;1628&#34;
	height=&#34;738&#34;
	srcset=&#34;https://ntopic.cn/p/2024051801/04_hua04b28422bbb4c6a12cc5f607adbb1af_425764_480x0_resize_q75_box.jpg 480w, https://ntopic.cn/p/2024051801/04_hua04b28422bbb4c6a12cc5f607adbb1af_425764_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;220&#34;
		data-flex-basis=&#34;529px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;启动完毕，其实我们已经有了一个控制台聊天界面，可以通过控制台与&lt;code&gt;Llama-3-8B&lt;/code&gt;聊天了&lt;/p&gt;
&lt;p&gt;如果我们不想要这个模型了，也可以通过命令行删除模型文件：&lt;code&gt;ollama rm Llama-3-8B&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Ollama&lt;/code&gt;存放模型文件根目录：&lt;code&gt;~/.ollama&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;部署ollama大模型-web-可视化聊天界面&#34;&gt;部署&lt;code&gt;Ollama&lt;/code&gt;大模型 Web 可视化聊天界面&lt;/h2&gt;
&lt;p&gt;控制台聊天对话界面体验总归是不太好，接下来部署 Web 可视化聊天界面。&lt;/p&gt;
&lt;p&gt;首先，下载&lt;code&gt;ollama-webui&lt;/code&gt;Web 工程代码：&lt;code&gt;git clone https://github.com/ollama-webui/ollama-webui-lite&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;然后切换&lt;code&gt;ollama-webui&lt;/code&gt;代码的目录：&lt;code&gt;cd ollama-webui-lite&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;设置 Node.js 工具包镜像源，以接下来下载 Node.js 的依赖包更加快速：&lt;code&gt;npm config set registry http://mirrors.cloud.tencent.com/npm/&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;安装 Node.js 依赖的工具包：&lt;code&gt;npm install&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;最后，启动 Web 可视化界面：&lt;code&gt;npm run dev&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ntopic.cn/p/2024051801/05.jpg&#34;
	width=&#34;824&#34;
	height=&#34;270&#34;
	srcset=&#34;https://ntopic.cn/p/2024051801/05_hua04b28422bbb4c6a12cc5f607adbb1af_52060_480x0_resize_q75_box.jpg 480w, https://ntopic.cn/p/2024051801/05_hua04b28422bbb4c6a12cc5f607adbb1af_52060_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;305&#34;
		data-flex-basis=&#34;732px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;如果看到以上输出，代表 Web 可视化界面已经成功了！&lt;/p&gt;
&lt;h2 id=&#34;愉快的与llama-3大模型俩天对话&#34;&gt;愉快的与&lt;code&gt;Llama 3&lt;/code&gt;大模型俩天对话&lt;/h2&gt;
&lt;p&gt;浏览器打开 Web 可视化界面：&lt;a class=&#34;link&#34; href=&#34;http://localhost:3000&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;http://localhost:3000/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;可以看到&lt;code&gt;Ollama&lt;/code&gt;的初始化页面，默认没有模型，需要选择，我们选择刚创建并部署的&lt;code&gt;Llama-3-8B&lt;/code&gt;模型：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ntopic.cn/p/2024051801/06.jpg&#34;
	width=&#34;2116&#34;
	height=&#34;692&#34;
	srcset=&#34;https://ntopic.cn/p/2024051801/06_hua04b28422bbb4c6a12cc5f607adbb1af_73102_480x0_resize_q75_box.jpg 480w, https://ntopic.cn/p/2024051801/06_hua04b28422bbb4c6a12cc5f607adbb1af_73102_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;305&#34;
		data-flex-basis=&#34;733px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ntopic.cn/p/2024051801/07.jpg&#34;
	width=&#34;2118&#34;
	height=&#34;712&#34;
	srcset=&#34;https://ntopic.cn/p/2024051801/07_hua04b28422bbb4c6a12cc5f607adbb1af_86097_480x0_resize_q75_box.jpg 480w, https://ntopic.cn/p/2024051801/07_hua04b28422bbb4c6a12cc5f607adbb1af_86097_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;297&#34;
		data-flex-basis=&#34;713px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;底部就是聊天输入框，至此可以愉快的与&lt;code&gt;Llama 3&lt;/code&gt;聊天对话了：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ntopic.cn/p/2024051801/08.jpg&#34;
	width=&#34;1534&#34;
	height=&#34;794&#34;
	srcset=&#34;https://ntopic.cn/p/2024051801/08_hua04b28422bbb4c6a12cc5f607adbb1af_154487_480x0_resize_q75_box.jpg 480w, https://ntopic.cn/p/2024051801/08_hua04b28422bbb4c6a12cc5f607adbb1af_154487_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;193&#34;
		data-flex-basis=&#34;463px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;Web 对话聊天机器人的设置，大家可以基于 Web 网站设置，这里不在介绍，有需要的网友可以私信一起研究进步！&lt;/p&gt;
&lt;h2 id=&#34;禅定ollama工具的其他用法&#34;&gt;禅定：&lt;code&gt;Ollama&lt;/code&gt;工具的其他用法&lt;/h2&gt;
&lt;p&gt;从上文的介绍可以看到，基于&lt;code&gt;Ollama&lt;/code&gt;部署一个大模型的 Web 可视化对话机器人，还是非常方便。下面整理了部分&lt;code&gt;Ollama&lt;/code&gt;提供的用法或者。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Ollama 命令&lt;/strong&gt;工具&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;11
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 查看当前Ollama的模型&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;ollama list
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 增量更新当前部署的模型&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;ollama pull Llama-3-8B
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 删除一个模型文件&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;ollama rm Llama-3-8B
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 复制一个模型&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;ollama cp Llama-3-8B Llama-newModel
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;&lt;strong&gt;Ollama API&lt;/strong&gt;结果返回&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;curl http://localhost:11434/api/generate -d &lt;span class=&#34;s1&#34;&gt;&amp;#39;{
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s1&#34;&gt;  &amp;#34;model&amp;#34;: &amp;#34;Llama-3-8B&amp;#34;,
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s1&#34;&gt;  &amp;#34;prompt&amp;#34;:&amp;#34;为什么天空是蓝色的？&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s1&#34;&gt;}&amp;#39;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;&lt;strong&gt;Ollama API&lt;/strong&gt;聊天对话&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;curl http://localhost:11434/api/chat -d &lt;span class=&#34;s1&#34;&gt;&amp;#39;{
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s1&#34;&gt;  &amp;#34;model&amp;#34;: &amp;#34;Llama-3-8B&amp;#34;,
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s1&#34;&gt;  &amp;#34;messages&amp;#34;: [
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s1&#34;&gt;    { &amp;#34;role&amp;#34;: &amp;#34;user&amp;#34;, &amp;#34;content&amp;#34;: &amp;#34;为什么天空是蓝色的？&amp;#34; }
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s1&#34;&gt;  ]
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s1&#34;&gt;}&amp;#39;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;hr&gt;
&lt;p&gt;我的本博客原地址：&lt;a class=&#34;link&#34; href=&#34;https://ntopic.cn/p/2024051801/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://ntopic.cn/p/2024051801&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;img src=&#34;https://ntopic.cn/WX-21.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;微信公众号：老牛同学&#34;
	
	
&gt;&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>
