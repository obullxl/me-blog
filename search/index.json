[{"content":"本以为 QwQ-32B 的发布定会“炸街”，毕竟它的评测结果与 DeepSeek-R1-671B 满血版不相上下，而之前 DeepSeek-R1 横扫了整个地球，还持续扫了这么久：\n万万没有想到，冲出来的这匹强劲黑马竟然是 Manus ，号称：全球首款通用型 AI 智能体，它不仅能产出创意，更重要的是还能交付结果。\n在官方介绍视频中，Manus 在简历筛选评估、房地产信息检索和股票分析上，均表现出了惊人的能力。可惜的是，Manus 并未对所有人开放，需要有邀请码方可注册体验使用：\n幸运的是，不管是 QwQ-32B 还是 Manus，它们都是我们中国人的产品，为这些产品背后的技术大佬们致敬！\n下面是 1 个使用 Manus 的样例：https://manus.im/share/kTEHmnYgfqqubhiSAUwZR7?replay=1\n没有 Manus 邀请码，我们就无法体验到 Manus 强大能力。\n但幸运的是，OpenManus 为我们提供了一个开源的替代方案。接下来，我们就来使用 QwQ-32B 大模型体验一下 OpenManus 的能力：\nOpenManus 和 Manus 比较，源代码走读分析 OpenManus 本地部署和配置 使用与 Manus 样例中相同的提示词，体验 OpenManus 功能 1. OpenManus 源代码分析 OpenManus 源代码地址：https://github.com/mannaandpoem/OpenManus\nOpenManus 研发成员来自 MetaGPT 智能体团队。关于 MetaGPT 的使用，可以参考老牛同学之前文章：Agent(智能体)和 MetaGPT，一句话实现整个需求应用代码\n把源代码下载到本地：\n1 2 git clone https://github.com/mannaandpoem/OpenManus.git cd OpenManus 其中，main.py是主入口文件，它初始化Manus代理类并处理输入的提示词；而Manus代理使用工具集合，包括PythonExecute、BrowserUseTool、FileSaver、Terminate和GoogleSearch等。\n这些工具的功能，说明了 OpenManus 可以在本地执行 Python 代码、使用浏览器、操作本地文件、执行终端命令和搜索等。\n我们可以打开源代码目录，看到这些工具定义：./app/tool\n同时，我们通过requirements.txt文件可以看到，OpenManus 也依赖了一些组件：图像处理、文件操作、浏览器操作等。\n由此可见，OpenManus 借助其他工具，执行并完成用户的任务！\n2. OpenManus 本地配置 简单了解了 OpenManus 主要设计之后，接下来我们本地部署起来。\n首先，安装 Conda 工具，用于管理 Python 虚拟环境。可以参考文章：大模型应用研发基础环境配置\n然后，创建 Python 虚拟环境（环境名：OpenManus），并激活环境：\n1 2 3 4 5 # 创建虚拟环境 conda create -n OpenManus python=3.12 # 激活虚拟环境 conda activate OpenManus 最后，安装 OpenManus 依赖：\n1 pip install -r requirements.txt 接下来，我们配置大模型 API，我们将使用 QwQ-32B 作为 OpenManus 底层大模型。\n先复制一份配置文件：config/config.toml\n1 cp config/config.example.toml config/config.toml 然后，修改配置文件内容，把其中基础大模型配置成 QwQ-32B 基座模型，把视觉模型配置成 Qwen-Max 最新模型：\n1 2 3 4 5 6 7 8 9 10 11 12 13 # 全局模型配置 [llm] model = \u0026#34;qwen-max-latest\u0026#34; base_url = \u0026#34;https://dashscope.aliyuncs.com/compatible-mode/v1\u0026#34; api_key = \u0026#34;sk-...\u0026#34; # 替换为真实 API 密钥 max_tokens = 4096 temperature = 0.0 # 视觉模型配置 [llm.vision] model = \u0026#34;qwen-max-latest\u0026#34; base_url = \u0026#34;https://dashscope.aliyuncs.com/compatible-mode/v1\u0026#34; api_key = \u0026#34;sk-...\u0026#34; # 替换为真实 API 密钥 注意：llm.model不能使用QwQ模型（感觉有点翻车了），原因在于 QwQ 仅支持流式输出：\n1 2025-03-07 21:00:32.259 | ERROR | app.llm:ask_tool:250 - API error: Error code: 400 - {\u0026#39;error\u0026#39;: {\u0026#39;code\u0026#39;: \u0026#39;invalid_parameter_error\u0026#39;, \u0026#39;param\u0026#39;: None, \u0026#39;message\u0026#39;: \u0026#39;This model only support stream mode, please enable the stream parameter to access the model. \u0026#39;, \u0026#39;type\u0026#39;: \u0026#39;invalid_request_error\u0026#39;}, \u0026#39;id\u0026#39;: \u0026#39;chatcmpl-dbc286db-797d-9e3f-8cf1-b3adf15abc1a\u0026#39;, \u0026#39;request_id\u0026#39;: \u0026#39;dbc286db-797d-9e3f-8cf1-b3adf15abc1a\u0026#39;} 3. OpenManus 功能体验 配置完毕，就可以开始使用了：python main.py\n1 2 3 4 \u0026gt;python main.py INFO [browser_use] BrowserUse logging setup complete with level info INFO [root] Anonymized telemetry enabled. See https://docs.browser-use.com/development/telemetry for more information. Enter your prompt (or \u0026#39;exit\u0026#39; to quit): 为了和 Manus 结果进行比较，我们输入提示词：Identify 20 CRM companies in the market and gather their slogans along with their branding stories.\n从日志可以看出，总共分为30 个推理步骤：\n1 2 2025-03-07 20:58:43.891 | WARNING | __main__:main:15 - Processing your request... 2025-03-07 20:58:43.891 | INFO | app.agent.base:run:137 - Executing step 1/30 执行完成之后，看到进行了总结：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 ### Summary: We have now gathered the slogans and branding stories for the following CRM companies: 1. Salesforce 2. HubSpot 3. Zoho CRM 4. Microsoft Dynamics 365 5. SAP CRM 6. Oracle CRM 7. SugarCRM 8. Pipedrive 9. Freshsales 10. Insightly 11. Nimble 12. Agile CRM 13. Capsule CRM 14. Copper (formerly ProsperWorks) 15. Infusionsoft (by Keap) 16. Zendesk Sell (formerly Base CRM) 17. Really Simple Systems CRM 18. Apptivo CRM 19. Bitrix24 20. Vtiger CRM This information is saved in the file `crm_companies_info.txt`. 最终结果保存在本地文件：crm_companies_info.txt\n打开本地结果文件，并且和 Manus 结果进行简单对比：\n4. 总结 从最终结果看，Manus 的结果更加丰富一些，更像一份报告，包括：简介、目录，每个上市公司的品牌故事说明内容和格式也更加丰富和易于阅读；而 OpenManus 的结果就简洁多了，每个公司就包括核心的 3 个主要信息：公司名、Slogan 和品牌故事。\n目前还不好说谁更加优秀，但有一点可以肯定的是：通用智能体离我们越来越近了！\nAgent(智能体)和 MetaGPT，一句话实现整个需求应用代码\n基于 Qwen2.5-Coder 模型和 CrewAI 多智能体框架，实现智能编程系统的实战教程\n使用Llama3/Qwen2等开源大模型，部署团队私有化Code Copilot和使用教程\nBolt.new 用一句话快速构建全栈应用：本地部署与应用实战（Ollama/Qwen2.5 等）\n大模型应用研发基础环境配置（Miniconda、Python、Jupyter Lab、Ollama等）\n","date":"2025-03-07T00:00:00Z","image":"https://ntopic.cn/p/2025030701/00_hue4a04b4c167b755d1cb1f73248022c70_117178_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/2025030701/","title":"没有 Manus 邀请码？Qwen + OpenManus 开源替代方案一样能领略 AI 智能体的力量"},{"content":"新学期了，为了帮小朋友们更好的记忆和学习生字、词语和古诗词，“老牛同学”微信小程序特意上线了“爱学习”模块。但当问小朋友们“怎么样”时，得到的答复却是“不怎么样”！\n上周，同事使用豆包生成了一首歌曲，这给老牛同学带来了一点灵感：如果古诗词能以歌唱的形式呈现，小朋友们是否会更感兴趣呢？小朋友告诉我说：在古代，古诗词就是唱出来的，可以试试看！\n对啊，为何不试试看呢？接下来，老牛同学将借助各大厂商的免费大模型，创作一首唐诗《游子吟》歌曲，总体分为以下三步：\n生成音乐：把古诗制作成音乐，先给唱出来 图片制作：通过 DeepSeek/Qwen 等大模型，根据我们的需求生成图片提示词，然后使用文生图大模型生成对应的图片 制作短视频：最后一步，合成音乐和图片内容，制作短视频 【请到“老牛同学”公众号预览视频】\n1. 唱出古诗内容 文本生成音乐，目前比较好的模型是Suno：https://suno.com，它是收费的且需要通畅的网络。\n老牛同学本次使用字节的豆包：https://www.doubao.com/chat/\n点击“音乐生成” - “自定义歌词”，采用“国风”的音乐风格，表达“怀旧”的情形：\n为了能产生更好的效果，我们可以通过 (主歌)、(副歌)、(前奏)、(尾奏) 和 (间奏) 等结构组织歌词内容，老牛同学歌词内容如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 (前奏) (主歌) 慈母手中线， 游子身上衣。 临行密密缝， 意恐迟迟归。 谁言寸草心， 报得三春晖。 (间奏) (主歌) 慈母手中线， 游子身上衣。 临行密密缝， 意恐迟迟归。 (副歌) 谁言寸草心， 报得三春晖。 (尾奏) 提交之后，一首音乐很快就生成好了。试听一下，如果不满意可以多次提交生成音乐。对于满意的音乐，我们可以修改音乐名称，并下载保存：\n豆包生成音乐确实又快又好，但是有个问题：即使增加了提示词，同样的歌词每次生成的音乐效果仍可能不同！\n生成的音乐文件默认为mp4格式，为了提取纯净的音频部分，老牛同学利用 Python 编写了一个脚本将其转换为mp3格式。：\n1 2 3 4 5 6 7 8 9 10 11 12 13 # 需提前安装ffmpeg库 import ffmpeg def extract_audio_ffmpeg_lib(mp4_file, mp3_file): stream = ffmpeg.input(mp4_file) stream = ffmpeg.output(stream.audio, mp3_file, format=\u0026#39;mp3\u0026#39;, **{\u0026#39;q:a\u0026#39;: \u0026#39;0\u0026#39;}) ffmpeg.run(stream) if __name__ == \u0026#34;__main__\u0026#34;: input_mp4 = \u0026#34;游子吟-孟郊(唐).mp4\u0026#34; # MP4文件路径 output_mp3 = \u0026#34;游子吟-孟郊(唐).mp3\u0026#34; # MP3文件路径 extract_audio_ffmpeg_lib(input_mp4, output_mp3) print(f\u0026#34;音频已成功提取到 {output_mp3}\u0026#34;) 2. 制作视频图片 接下来，我需要制作几张与音乐相匹配的图片，以便更好的生成音乐 MV 视频，这里需要使用文生图大模型。\n我们首先通过元宝的 DeepSeek-R1 产出提示词：https://yuanbao.tencent.com/chat\n1 2 我需要把《游子吟》这首唐诗做成短视频，需要短视频图片素材。请为我提供腾讯元宝绘图的提示词。 要求：图片高清，比例3:2，尺寸为1024*682像素 DeepSeek-R1 把视频分成了 3 个场景，与古诗的意境一一对应。\n接下来，我们根据提示词，使用文生图大模型生成图片。老牛同学尝试了豆包和元宝的绘图功能，生成的图片都不理想，发现免费模型的图片生成效果有限。\n最后采用抖音的即梦 AI，一样的提示词，感觉效果就好多了：https://jimeng.jianying.com/ai-tool/image/generate\n把几个场景都生成一下图片，然后挑选几张感觉还不错的图片，作为接下来视频的图片：\n3. 合成古诗视频 有了以上图片和音乐素材，我们就可以剪辑成视频了。视频剪辑软件有很多，但大多数都需要付费使用，比如比较流行的“剪映”软件。\n其实我们也可以使用电脑自带的免费软件进行剪辑，比如苹果电脑有iMovie，Windows 电脑有Clipchamp都是非常好用的剪辑软件：\n导入素材，图片和音乐文件 拖到编辑器中，对于图片可以调整时长，对于音乐可以截掉头尾 根据音乐内容，增加对应字幕 最后，导出视频文件即可 导出视频，保存到本地即可！\n到此整个制作流程已经结束，总体来说比较简单，大家可以动手试下哈，如果有不清楚的地方，请留言沟通交流！\n4. 结尾 最终导出 《游子吟》 唐诗视频，我已经上传到视频号里，大家也可以搜索收看。\n本次制作的视频，图片都是静态，下次老牛同学继续优化！\n","date":"2025-03-03T00:00:00Z","image":"https://ntopic.cn/p/2025030301/00_hu42cdc8535b0a762ff23c72abe3e7986b_81971_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/2025030301/","title":"让孩子爱上古诗词，用AI把唐诗变成歌曲+视频，诗词本来就是唱的！"},{"content":"在Phi-3大模型发布之际，我们基于Termux应用，在我的小米手机部署了Phi-3模型：文章地址\nTermux应用是一个终端模拟器，它允许我们安装 Linux 操作系统，然后在 Linux 操作系统中安装 Ollama，最后基于 Ollama 下载和推理大模型，虽然方法可行，但是存在一些局限：\n操作步骤繁琐，用户体验较差：安装 Linux 操作系统、启动系统、安装和启动 Ollama 等操作，都需要通过命令行完成，体验较差。 资源消耗较大：手机内存有限，Termux 应用、Linux 操作系统、Ollama 框架等都需要消耗资源，推理速度较慢，资源吃紧 DeepSeek R1 很火爆，导致在使用 DeepSeek R1 时，老牛同学经常会遇到“服务器繁忙，请稍后重试。”的问题：\n今天老牛同学推荐一款更加先进的应用：MNN Chat，它是由阿里巴巴开源的全功能多模态模型应用。\n仅需一台手机即可运行： 完全在设备本地运行，确保数据隐私，无需将信息上传至外部服务器。 CPU 推理优化： 在安卓平台上，MNN-LLM 展现了卓越的 CPU 性能，预填充速度相较于 llama.cpp 提高了 8.6 倍，相较于 fastllm 提升了 20.5 倍，解码速度分别快了 2.3 倍和 8.9 倍。 多模态支持： 提供多种任务功能，包括文本生成文本、图像生成文本、音频转文本及文本生成图像。 广泛的模型兼容性： 支持多种领先的模型提供商，包括 Qwen、Gemma、Llama（涵盖 TinyLlama 与 MobileLLM）、Baichuan、Yi、DeepSeek、InternLM、Phi、ReaderLM 和 Smolm 等。 官网地址：https://www.mnn.zone\nGitHub 源码地址：https://github.com/alibaba/MNN\n接下来，我们在手机上部署试用一下，老牛同学的手机配置如下：\n1.安装 MNN 应用 对于 Android 手机，我们可以直接下载和安装最新版本：下载地址\n或者，我们也可以按照以下步骤，自己编译 Android 和 iOS 应用：\nAndroid 编译和使用 克隆 MNN 源代码：git clone https://github.com/alibaba/MNN.git 构建库： 1 2 3 cd project/android mkdir build_64 ../build_64.sh \u0026#34;-DMNN_LOW_MEMORY=true -DMNN_CPU_WEIGHT_DEQUANT_GEMM=true -DMNN_BUILD_LLM=true -DMNN_SUPPORT_TRANSFORMER_FUSE=true -DMNN_ARM82=true -DMNN_USE_LOGCAT=true -DMNN_OPENCL=true -DLLM_SUPPORT_VISION=true -DMNN_BUILD_OPENCV=true -DMNN_IMGCODECS=true -DLLM_SUPPORT_AUDIO=true -DMNN_BUILD_AUDIO=true -DMNN_BUILD_DIFFUSION=ON -DMNN_SEP_BUILD=ON\u0026#34; 复制到 LLM Android 应用项目： 1 2 mkdir -p ../../../apps/Android/MnnLlmChat/app/src/main/jniLibs/arm64-v8a find . -name \u0026#34;*.so\u0026#34; -exec cp {} ../../../apps/Android/MnnLlmChat/app/src/main/jniLibs/arm64-v8a \\; 构建 Android 应用项目并安装： 1 2 cd ../../../apps/Android/MnnLlmChat ./gradlew installDebug iOS 应用编译和使用 克隆 MNN 源代码：git clone https://github.com/alibaba/MNN.git 编译 MNN.framework: 1 2 3 4 5 6 7 8 9 cd MNN/ sh package_scripts/ios/buildiOS.sh \u0026#34;-DMNN_ARM82=true -DMNN_LOW_MEMORY=true -DMNN_SUPPORT_TRANSFORMER_FUSE=true -DMNN_BUILD_LLM=true -DMNN_CPU_WEIGHT_DEQUANT_GEMM=true -DMNN_METAL=ON -DMNN_BUILD_DIFFUSION=ON -DMNN_BUILD_OPENCV=ON -DMNN_IMGCODECS=ON -DMNN_OPENCL=OFF -DMNN_SEP_BUILD=OFF -DMNN_SUPPORT_TRANSFORMER_FUSE=ON\u0026#34; 拷贝 framework 到 iOS 项目中： 1 mv MNN-iOS-CPU-GPU/Static/MNN.framework /apps/iOS/MNNLLMChat/MNN.framework 确保 Link Binary With Libraried 中包含 MNN.framework，和其他三个 Framework:\n如果没有包含，可以手动添加 MNN.framework:\n修改 iOS 签名并编译项目 1 2 cd /apps/iOS/MNNLLMChat open MNNLLMiOS.xcodeproj 在 Xcode 项目属性中 Signing \u0026amp; Capabilities \u0026gt; Team 输入自己的账号和 Bundle Identifier:\n等待 Swift Package 下载完成之后，进行编译使用。\n2.下载模型文件 手机上安装好MNN Chat应用之后，我们打开应用，就可以看到它所兼容的模型列表，包括文本、音频、图像等：\n点击即可下载，老牛同学想体验一下文本和图片识别，下载了DeepSeek-R1-7B-Qwen-MNN和Qwen2-VL-2B-Instruct-MNN这 2 个大模型，等待下载完成。\n3. MNN 模型推理 先试用一下DeepSeek-R1-7B-Qwen-MNN文本模型，输入提示词：\n1 同样是一年，为什么阳历固定12个月，而阴历却有闰月？ DeepSeek R1 开始思考，分析阳历和阴历的计算方法，最终得出结论：\n从手机的推理速率来看，总体还算不错，比通过Termux应用的方式推理效率高多了。\n接下来，看看Qwen2-VL-2B-Instruct-MNN图片多模态大模型，老牛同学让它进行车牌识别：\n可以看到，识别的结果是很不错的！\n还有其他的大模型，包括音频、图片生成（Stable Diffusion）等，大家可以体验一下。\nCocos 3D 小游戏：\n01.技术选型 丨 02.研发流程 丨 03.小游戏框架丨 04.核心架构设计丨 05.分包构建发布\nTransformers 框架序列：\n01.包和对象加载中的设计巧思与实用技巧\n02.AutoModel 初始化及 Qwen2.5 模型加载全流程\n03.Qwen2.5 大模型的 AutoTokenizer 技术细节\n04.Qwen2.5/GPT 分词流程与 BPE 分词算法技术细节详解\n05.嵌入（Embedding）机制和 Word2Vec 实战\n06.位置嵌入（Positional Embedding）\nPipeline NLP 任务序列：\n零·概述 丨 01.文本转音频 丨 02.文本分类 丨 03.词元分类和命名实体识别 丨 04.问答 丨 05.表格问答 | 06.填充蒙版\n往期推荐文章：\nCline 免费插件 + Qwen2.5 大模型，零经验也能开发“对联王”微信小程序\n使用 Cursor + Qwen2.5 大模型 零经验研发微信小程序：自由构建个性化节拍器应用实战\nBolt.new 用一句话快速构建全栈应用：本地部署与应用实战（Ollama/Qwen2.5 等）\n基于 Qwen2.5-Coder 模型和 CrewAI 多智能体框架，实现智能编程系统的实战教程\nvLLM CPU 和 GPU 模式署和推理 Qwen2 等大语言模型详细教程\n基于 Qwen2/Lllama3 等大模型，部署团队私有化 RAG 知识库系统的详细教程（Docker+AnythingLLM）\n使用 Llama3/Qwen2 等开源大模型，部署团队私有化 Code Copilot 和使用教程\n基于 Qwen2 大模型微调技术详细教程（LoRA 参数高效微调和 SwanLab 可视化监控）\nChatTTS 长音频合成和本地部署 2 种方式，让你的“儿童绘本”发声的实战教程\n","date":"2025-02-21T00:00:00Z","image":"https://ntopic.cn/p/2025022101/00_hue009a34f05abb603d67a255367f6def7_210180_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/2025022101/","title":"MNN 手机本地部署 DeepSeek R1 和多模态大模型，告别服务器繁忙！"},{"content":"春节期间，老牛同学学习使用 Cocos Creator 研发了一款 3D 小游戏。开发过程中遇到了一些全新概念和用法，虽然借助大模型最终完成了小游戏功能的开发，但对这些概念却一知半解。\n其中，在需要 3D 组件旋转的功能中，“欧拉角”和“四元数”就是研发过程中遇到的一个全新的概念。如下代码是大模型帮忙实现的“左右水平滑动”功能：\n本文将对“欧拉角”和“四元数”进行尽可能详细的总结，尽可能不涉及数学证明公式，希望能给像我一样有困惑的朋友一些帮助：\n基本概念：为什么对于 3D 物体旋转，大家都采用欧拉角和四元数来表示？为什么 1 个概念还不够，还需要 2 个？他们分别解决什么主要问题？ 如何应用：在不同场景下，欧拉角和四元数如何应用旋转？它们如何应用到物体旋转？ 实际应用：最后，用一个“让立方体的对角线垂直于地面并旋转”功能，演示如何让物体旋转后并不停的旋转起来。 本文代码和小游戏源代码均可免费下载：打开“老牛同学”微信小程序-\u0026gt;点击“更多”Tab-\u0026gt;“源代码”获取下载链接。\n欧拉角和四元数存在的意义是什么？ 首先，欧拉角和四元数都是用于描述三维空间中物体旋转的常用工具，它们只是工具，核心就是三维和旋转。\n欧拉角最早由瑞士数学家莱昂哈德·欧拉（Leonhard Euler）于 18 世纪提出，他认为：三维空间中物体的任意姿态，都可以通过三个连续的旋转角度来描述（即：第一步绕某个轴旋转，第二步绕另一个轴旋转，第三步绕第三个轴旋转）。用 3 个数就可以表示旋转姿态，确实一种非常直观且易于理解的方法。\n那么，对应到游戏开发中，欧拉角的就是我们最常见的X 轴、Y 轴和Z 轴旋转属性来表示了。如下所示，把一个长方体按照 X/Y/Z 旋转后的姿态：\n注意的是，在三维空间中，我们一般用姿态或位姿来表示物体的当前旋转，它有两层含义：\n物体方向，或者朝向 物体局部坐标系相当于世界坐标系的偏移量 四元数由爱尔兰数学家**威廉·罗文·汉密尔顿（William Rowan Hamilton）**于 1843 年提出。汉密尔顿希望找到一种用于描述三维空间旋转的数系。据说，汉密尔顿有一次在散步时突然想到了四元数的基本公式，并将其刻在了一座桥上。\n四元数由一个实部和三个虚部组成，形式为：Q = W + Xi + Yj + Zk，其中W,X,Y,Z是实数，i,j,k是虚数单位。\n既然欧拉角表示旋转已经足够优秀了，为什么还需要引入四元数呢？\n原因在于，欧拉角虽然简单直观，但存在三个问题：万向锁问题（Gimbal Lock）、旋转过程中插值困难、计算效率较低的问题。\n接下来，老牛同学根据自己的理解，逐一介绍欧拉角的这几个问题，同时看看四元数是如何解决的。\n什么是旋转，什么是坐标系？ 在介绍欧拉角和四元数之前，先了解一下旋转和坐标系。\n旋转是指物体在三维空间中绕某个轴进行角度变化的运动：\n轴：在三维空间中，我们一般 X 轴、Y 轴和 Z 轴表示。 角度：通常用角度或弧度来表示。 运动方向：分为右手坐标系或者左手坐标系。Cocos 3D 使用右手坐标系，Unity 3D 使用左手坐标系。默认情况下，轴默认配置：X 轴向右为正，Y 轴 向上为正，Z 轴朝屏幕外为正。 右手坐标系旋转方向：伸出右手，大拇指指向轴的正方向（如：围绕 X 轴旋转+30°，则大拇指指向 X 轴的正方向）；弯曲四指，四指弯曲的方向为旋转正方向（如：+30°），反过来手背方向便是负方向。\n左手坐标系旋转方向：类似于右手坐标系，伸出左手即可。\n世界坐标系： 三维空间的参考框架，所有物体的参考标准，它是固定不变的。\n本地坐标系： 每个物体都有的自己的坐标系，用于描述其自身的姿态。\n一般情况下，我们所说的旋转指的是局部坐标系，当然也可以相对于全局坐标系。\n万向锁问题和解法 万向节常用在汽车动力传输上，它是汽车驱动系统关节，它可以围绕不同方向旋转：\n在解释万向锁问题之前，有三个非常重要的概念必须理解：\n无论是欧拉角还是四元数，它们表示的是物体旋转后的姿态，是一个描述，它不是一个连续的动作。比如：欧拉角(90,0,0)代表围绕 X 轴旋转+90°，它表示的是这个物体旋转+90° 后的姿态，而不是表示物体围绕 X 轴，从 0°-\u0026gt;1°-\u0026gt;2°-\u0026gt;……-\u0026gt;90° 这个过程。因此，这里的“旋转”它不是指一个连续的动作！ 物体姿态与旋转的顺序密切相关，相同的角度，不同的旋转顺序，物体的姿态是不相同的。我们可以拿个快递盒子，做个简单的实验就能明白。如：围绕 X 轴旋转 90°，Y 轴 45°，X→Y→Z旋转顺序，和Y→X→Z旋转顺序，快递盒子的姿态是不同的。 物体按照某个顺序依次旋转，前面的轴旋转时，会带动后面的轴一同旋转，而后面轴旋转却不会影响前面的轴。如：X→Y→Z 旋转顺序，当围绕 X 轴旋转时，本地坐标系 Y 轴和 Z 轴一同旋转；当围绕 Y 轴旋转时，Z 轴会一同旋转，但 X 轴是不动的；最后，当围绕 Z 轴旋转时，X 轴和 Y 轴都不会动！ 第 3 点感觉非常的诡异，老牛同学就被这个不可思议的概念折磨了好久，直到彻底理解了第 1 点：永远记住，欧拉角和四元数这些表示旋转的工具，它们只是描述旋转后的姿态，并不是旋转动作（旋转在这里当名称理解），它没有动作过程，只是最终的瞬时的姿态！\n如果大家理解了第 1 点，那么可以退一步想：X→Y→Z 旋转顺序，如果围绕 Y 轴旋转时，X 轴也跟着旋转，那么 X 轴最初的旋转不就白费了吗？\n如果第 3 点无法理解的话，那么万向锁的问题是无法理解，因为它就是万向锁产生的前提！\n至此，大家也可以到网上看看数学推导公式，在此老牛同学就不列举了！\n万向锁定义：当两个旋转轴重合时，原本三个独立的旋转轴（X、Y、Z）变成了两个，导致自由度减少的现象。\n我们先来看看，万向锁发生的场景：物体的任意旋转顺序（如：X→Y→Z，Y→X→Z 等），当围绕第 2 个轴旋转的角度为 ±90° 时，那么第 3 个轴就会和第 1 个轴就会重合。这样当物体围绕第 1 个轴旋转或者第 3 个轴旋转，姿态都是一样的，无法区分，这种现象就是“万向锁”问题。\n如上图所示：X→Y→Z 旋转顺序，(0, 90, 0)旋转角度（X 轴和 Z 轴角度可以是任意值，此处便于理解设置为 0°），当围绕 Y 轴旋转 90° 之后，X 轴和 Z 轴重合了。这个时候，物体无论围绕 X 轴还是 Z 轴旋转，姿态都是一样的。\n那么，四元数是如何解决欧拉角万向锁问题的呢：由上述公式可以看出，它没有旋转轴的概念，而是由四个数值表示旋转，表示方式为cos(θ/2)+sin(θ/2)(xi+yj+zk)；其中，θ 代表旋转的角度，(x,y,z)代表旋转轴方向。\n插值和计算效率问题 插值问题的理解：插值是指物体旋转路径上的中间状态（即平滑过渡），欧拉角使用三个角度描述旋转，容易出现冗余或等效旋转的问题（如：(0, 0, 0)和(360, 360, 360)），导致旋转路径不自然，当发生万向锁问题的时候，就会发生异常。而四元数，它支持球面线性插值方法，能够生成平滑且自然的、稳定的旋转。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 // 插值比例 const t = 0.5; // 1. 欧拉角线性插值 const startEuler = new Vec3(0, 0, 0); const endEuler = new Vec3(90, 45, 0); const interpolatedEuler = new Vec3( startEuler.x + (endEuler.x - startEuler.x) * t, startEuler.y + (endEuler.y - startEuler.y) * t, startEuler.z + (endEuler.z - startEuler.z) * t ); console.log(`欧拉角线性插值: ${interpolatedEuler}`); // 2. 四元数球面线性插值 const startQuat = new Quat(); const endQuat = new Quat(); Quat.fromEuler(startQuat, 0, 0, 0); Quat.fromEuler(endQuat, 90, 45, 0); const interpolatedQuat = new Quat(); Quat.slerp(interpolatedQuat, startQuat, endQuat, t); console.log(`四元数球面线性插值: (${interpolatedQuat.w}, ${interpolatedQuat.x}, ${interpolatedQuat.y}, ${interpolatedQuat.z})`); 计算效率问题：主要是运算复杂度高，涉及大量的三角函数运算，计算效率比较低。而四元数，可通过简单的向量点积和叉积实现计算，计算效率较高。\n欧拉角和四元数转换 在 Cocos 3D 中，欧拉角的使用简单且直观，使用Vec3对象表示：\n1 2 3 4 5 6 7 8 // 1. 获取欧拉角（单位：度） const euler: Vec3 = this.node.eulerAngles; console.log(`X: ${euler.x}°`); console.log(`Y: ${euler.y}°`); console.log(`Z: ${euler.z}°`); // 2. 设置欧拉角（X/Y/Z，单位：度） this.node.eulerAngles = new Vec3(45, 30, 15); 四元数使用Quat对象表示：\n1 2 3 4 5 6 7 // 3. 获取四元数 const quat1: Quat = this.node.getRotation(); console.log(`四元数 w: ${quat1.w}, x: ${quat1.x}, y: ${quat1.y}, z: ${quat1.z}`); // 4. 设置四元数（w, x, y, z） const quat2 = new Quat(0.707, 0, 0, 0.707); this.node.setRotation(quat2); 欧拉角和四元数，也可以相互转换：\n1 2 3 4 5 6 7 8 // 5. 欧拉角转四元数 const quat3 = new Quat(); Quat.fromEuler(quat3, euler.x, euler.y, euler.z); console.log(`欧拉角转四元数 w: ${quat3.w}, x: ${quat3.x}, y: ${quat3.y}, z: ${quat3.z}`); // 6. 四元数转欧拉角 Quat.toEuler(euler, quat2); console.log(`四元数转欧拉角: X=${euler.x}°, Y=${euler.y}°, Z=${euler.z}°`); 实战：垂直地面旋转立方体 接下来，我们来一个应用实战：旋转立方体，让对角线垂直于地面，然后围绕对角线不停地旋转！\n让立方体的对角线垂直于地面，用欧拉角很难表示。从代码中可以看到，用四元数可以轻松实现。\n首先，让立方体的对角线垂直于地面立起来：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 /** * 初始化：立起来 */ protected start(): void { // 立方体对角线顶点向量 const scale: number = this.node.scale.x; const localDiagonal = new Vec3(scale, scale, scale); // 归一化 Vec3.normalize(localDiagonal, localDiagonal); // 本地对角线旋转到Y轴的旋转 const targetDirection = new Vec3(0, 1, 0); const rotation = new Quat(); Quat.rotationTo(rotation, localDiagonal, targetDirection); // 应用旋转，立起来 this.node.setRotation(rotation); } 然后，让立方体按照指定的弧度参数，沿着世界坐标系 Y 轴进行旋转（因为第一次旋转之后，立方体的本地坐标系已经发生了变化，但是世界坐标系是固定不变）：\n1 2 3 4 5 6 7 8 /** * 帧更新：旋转 */ protected update(dt: number): void { const rotation = new Quat(); Quat.fromAxisAngle(rotation, Vec3.UNIT_Y, this.rotateSpeed); this.node.rotate(rotation, Node.NodeSpace.WORLD); } 最后，简单总结 从上面分析我们可以看到，欧拉角和四元数这 2 个表示三维空间物体旋转的工具，它们有各自的使用场景：\n欧拉角：直观，便于理解；在属性编辑器等调试场景，用欧拉角旋转；同时，对于简单的旋转操作（如：单一轴旋转），使用欧拉角就非常方便。 四元数：转换复杂，不直观；但它插值平滑，计算效率高，比较适合复杂的旋转场景。 欧拉角这么好，那如何克服“万向锁”问题：把旋转顺序的第二个轴（如：X→Y→Z 旋转顺序的 Y 轴），设置为不太容易旋转+90° 的轴。如在 Cocos 3D 等游戏研发中，Y 轴是第二个轴，它垂直于地面，因为在游戏过程中，摄像头一般都不会垂直于地面或者垂直于天空。\n提示： 以上示例源代码，老牛同学已经上传，打开“老牛同学”微信小程序-\u0026gt;点击“更多”Tab-\u0026gt;“源代码”获取下载链接。\nCocos 3D 小游戏：\n01.技术选型 丨 02.研发流程 丨 03.小游戏框架丨 04.核心架构设计丨 05.分包构建发布\nTransformers 框架序列：\n01.包和对象加载中的设计巧思与实用技巧\n02.AutoModel 初始化及 Qwen2.5 模型加载全流程\n03.Qwen2.5 大模型的 AutoTokenizer 技术细节\n04.Qwen2.5/GPT 分词流程与 BPE 分词算法技术细节详解\n05.嵌入（Embedding）机制和 Word2Vec 实战\n06.位置嵌入（Positional Embedding）\nPipeline NLP 任务序列：\n零·概述 丨 01.文本转音频 丨 02.文本分类 丨 03.词元分类和命名实体识别 丨 04.问答 丨 05.表格问答 | 06.填充蒙版\n往期推荐文章：\nCline 免费插件 + Qwen2.5 大模型，零经验也能开发“对联王”微信小程序\n使用 Cursor + Qwen2.5 大模型 零经验研发微信小程序：自由构建个性化节拍器应用实战\nBolt.new 用一句话快速构建全栈应用：本地部署与应用实战（Ollama/Qwen2.5 等）\n基于 Qwen2.5-Coder 模型和 CrewAI 多智能体框架，实现智能编程系统的实战教程\nvLLM CPU 和 GPU 模式署和推理 Qwen2 等大语言模型详细教程\n基于 Qwen2/Lllama3 等大模型，部署团队私有化 RAG 知识库系统的详细教程（Docker+AnythingLLM）\n使用 Llama3/Qwen2 等开源大模型，部署团队私有化 Code Copilot 和使用教程\n基于 Qwen2 大模型微调技术详细教程（LoRA 参数高效微调和 SwanLab 可视化监控）\nChatTTS 长音频合成和本地部署 2 种方式，让你的“儿童绘本”发声的实战教程\n","date":"2025-02-14T00:00:00Z","image":"https://ntopic.cn/p/2025021401/00_hu38a63772fa259397b2f70878a2b36caa_40924_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/2025021401/","title":"欧拉角和四元数，3D 游戏开发中“旋转”难题的通俗讲解和应用实战"},{"content":"整个春节假期，DeepSeek 与《哪吒 2》的消息接连不断，看完我总有种莫名的激动。一边是 DeepSeek 在基础技术领域的深耕细作，另一边则是《哪吒 2》对用户产品的精心打磨，两者多年来的默默努力和不懈追求，终于在今年春节实现了一飞冲天、一鸣惊人的突破！\n老牛同学作为一名长期使用 Qwen 大模型Token服务的用户，目前余额所剩无几了，对购买的服务主要就三个追求：价格、效率、效果。\n接下来，老牛同学将根据自己使用大模型的实际情况，对DeepSeek和Qwen分别进行以下几个方面的测验，哪个得分高，老牛同学后续就用哪个的 Token 服务。\n价格：说实话，现在大模型厂商都很卷，价格都很实惠。同时，Token 数量与分词算法相关，因此价格只要不超过 50%，老牛同学觉得都差多不。 效率：敢作为服务拿出来卖，老牛同学觉得效率都不会差；同时，模型产出内容不一样，这里就没有一个严格标准，因此暂不做比较。 效果，这是老牛同学最关心的点，只有满足需求的服务，才是需要的服务。编程是老牛同学使用大模型服务最多的地方，因此将通过Web 页面、Python、微信小程序和Cocos 小游戏这 4 个方面编程结果进行评测。 为了消除历史消息对上下文的影响，每次编程老牛同学都使用全新的会话。 本次编程评测编程代码结果，老牛同学全部上传共享，大家可通过打开“老牛同学”微信小程序-\u0026gt;点击“更多”Tab-\u0026gt;“源代码”获取下载链接进行复验： Qwen 和 DeepSeek 服务价格（DeepSeek 更实惠） 老牛同学将以Qwen-Max-Latest和DeepSeek-Chat进行比较（Token 数量统一为百万），以下是 2 个大模型官网公布的价格：\nQwen 价格：输入**¥2.4**，输出**¥9.6** DeepSeek 价格：输入缓存命中**¥0.5**，缓存未命中**¥2**，输出**¥8** 虽然 Qwen 和 DeepSeek 采用的分词算法可能不一样，但是考虑到都是以百万为单位，这些差异基本可以忽略。\n从上述官网价格来看：DeepSeek 更实惠！\nQwen 和 DeepSeek 编程能力 接下来，我们开始进行编程评测，分别通过Web 页面、Python、微信小程序和Cocos 小游戏这 4 个方面编程进行评测。\nWeb 页面编程（Qwen 胜出） 计算器是一个很好的 Web 程序样例，老牛同学就选择它作为考题，提示词如下：\n1 2 写一个**计算器**静态Web页面，计算器的风格参考Apple手机自带的计算器App。 对于CSS样式，请尽量复用现有的CSS框架，比如BootStrap、Tailwind CSS等等。 最终，老牛同学复制 2 个模型的输出 HTML 代码，通过浏览器打开：\n2 个模型的 Web 编程结果分析：\n风格：Qwen 更接近 Apple 手机自带的计算器。 功能：对于基本的加、减、乘、除功能，Qwen 输入框只展示了计算结果，没有显示过程算式；而 DeepSeek 点击无反应，也没有结果。 总结，老牛同学对 Web 页面编程评测结论：Qwen 胜出！\nPython 编程（Qwen 胜出） 贪吃蛇是一款经典的单机游戏，也是很多一部分学习 Python 编程技术的第一个完整程序，我们就以它作为考题，提示词如下：\n1 2 3 4 使用Python，写一个带有GUI界面的**贪吃蛇**小游戏，要求： 1. 游戏开始，有“开始”操作界面，用户点击开始游戏 2. 游戏过程中，在界面底部中间，展示当前分数 3. 游戏结束，有一个结束页面，页面展示游戏分数，并有“重新开始”操作界面 最终，老牛同学在本机分别运行这 2 个小游戏，界面如下：\n对于最终编程结果，评测分析如下：\n功能：2 个大模型的结果，均能跑起来，有开始界面，正常展示。 游戏：Qwen 蛇的速度比较适中，基本还能玩起来。而 DeepSeek 蛇速度则很快，老牛同学手速没有那么快，基本玩不起来。 结果：Qwen 能正常展示中文，而 DeepSeek 展示就是乱码了（缺少ttf字体文件）。 总结，Python 编程评测结果：Qwen 胜出！\n微信小程序编程（势均力敌） 接下来，我们进行微信小程序编程评测，老牛同学感觉比上面 2 个编程要稍微难一点：\n1 2 写一个微信小程序页面，页面样式参考**微信**手机App中“我”Tab的样式。 对于页面样式，请尽量复用现有成熟的框架，比如weui等。 对于weui框架的app.wxss文件：Qwen 给的地址文件不存在，而 DeepSeek 给的是一个样例地址。老牛同学修复这个问题之后，最终小程序页面样式如下：\n从最终小程序页面效果来看：Qwen和DeepSeek表现基本一致！\n但有个问题：Qwen 和 DeepSeek 都是直接引用weui框架样式，这样势必会增加小程序包大小170KB左右。而weui最佳使用方式是作为扩展组件库的方式引入，这样就不会增加包大小了。\nCocos 小游戏编程（Qwen 略胜） 最后，来一个 Cocos 小游戏编程。今年春节，老牛同学从 0 基础开始学习，使用 Cocos Creator 做了一款微信小游戏。过程中遇到了不少问题，我把其中一个问题拿出来作为考题：\n1 2 3 4 5 6 7 8 9 10 我正在使用Cocos Creator 3.8.5研发一款微信小游戏，在一个`ts`组件类中，我需要一个名为`prefabMap`、类型是`{[key: string]:Prefab}`映射的属性，如下TypeScript代码： @ccclass(\u0026#39;WallView\u0026#39;) export class WallView extends Component { // 预制体映射 private prefabMap: { [key: string]: Prefab } = {}; } 我希望可以通过“属性管理器”来设置**prefabMap**这个属性，包括映射`string`类型的Key和映射`Prefab`类型的Value，请帮忙实现这个代码。 从上面代码来看，Qwen和DeepSeek大模型思路基本一致：先定义一个 KeyValue 类/接口，通过“属性管理器”设置这个 KeyValue 列表，然后再组件启动start之后把 KeyValue 列表转化为映射对象。\n它们两者的编程思路没有问题，可是编写的代码却是不生效的，均存在问题：\nQwen的问题：代码已经非常接近目标，一步之遥，可 Cocos 无法识别KeyValuePair这个类。有两种办法可以解决：KeyValuePair类增加注解@ccclass('KeyValuePair')，或者增加一行代码@ccclass('KeyValuePair')(KeyValuePair)均可。 DeepSeek的问题：相比 Qwen 来说，问题就多一些了，离目标也就更远一些：PrefabKeyValue是一个接口，Cocos 根本无法识别，同时内部的key和value没有@property注解，在“属性管理器”中也无法做到嵌套配置，prefabList也就无法达成目标了。 从上面分析来看，Qwen 更接近目标，因此 Qwen 略胜一筹！\n最后，简单总结 综合上面分析和评测，我们可以看出：DeepSeek在价格上更优惠，而Qwen在编程效果上更胜一筹！\n看到这儿，我相信大家和老牛同学一样，开始犯嘀咕：当前火爆全球、如日中天、大红大紫的DeepSeek大模型，竟然在编程效果上不及Qwen大模型？\n从结果看，是的。也许是DeepSeek火爆点并不在编程效果上，也行是的提示词待优化，也许是所举样例不具备代表性，也许是需要我们再给 DeepSeek 一些时日……\n但无论如何，老牛同学目前已经明确，接下来续费大模型 Token 调用费用，该往哪个平台了。\n提示： 以上 2 个大模型辅助编程的源码，老牛同学已经上传（除微信小程序中，除修正了app.wxss样式文件的路径之外，老牛同学承诺对大模型编程结果未做任何一个字符的修改），大家可以下载复验（打开“老牛同学”微信小程序-\u0026gt;点击“更多”Tab-\u0026gt;“源代码”获取下载链接）。\nCocos 3D 小游戏：\n01.技术选型 丨 02.研发流程 丨 03.小游戏框架丨 04.核心架构设计丨 05.分包构建发布\nTransformers 框架序列：\n01.包和对象加载中的设计巧思与实用技巧\n02.AutoModel 初始化及 Qwen2.5 模型加载全流程\n03.Qwen2.5 大模型的 AutoTokenizer 技术细节\n04.Qwen2.5/GPT 分词流程与 BPE 分词算法技术细节详解\n05.嵌入（Embedding）机制和 Word2Vec 实战\n06.位置嵌入（Positional Embedding）\nPipeline NLP 任务序列：\n零·概述 丨 01.文本转音频 丨 02.文本分类 丨 03.词元分类和命名实体识别 丨 04.问答 丨 05.表格问答 | 06.填充蒙版\n往期推荐文章：\nCline 免费插件 + Qwen2.5 大模型，零经验也能开发“对联王”微信小程序\n使用 Cursor + Qwen2.5 大模型 零经验研发微信小程序：自由构建个性化节拍器应用实战\nBolt.new 用一句话快速构建全栈应用：本地部署与应用实战（Ollama/Qwen2.5 等）\n基于 Qwen2.5-Coder 模型和 CrewAI 多智能体框架，实现智能编程系统的实战教程\nvLLM CPU 和 GPU 模式署和推理 Qwen2 等大语言模型详细教程\n基于 Qwen2/Lllama3 等大模型，部署团队私有化 RAG 知识库系统的详细教程（Docker+AnythingLLM）\n使用 Llama3/Qwen2 等开源大模型，部署团队私有化 Code Copilot 和使用教程\n基于 Qwen2 大模型微调技术详细教程（LoRA 参数高效微调和 SwanLab 可视化监控）\nChatTTS 长音频合成和本地部署 2 种方式，让你的“儿童绘本”发声的实战教程\n","date":"2025-02-09T00:00:00Z","image":"https://ntopic.cn/p/2025020901/00_hu8a5434afd7142d25bc7a2119bc98b479_227024_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/2025020901/","title":"DeepSeek vs. Qwen 大模型编程能力比拼，谁更适合作为你的 AI 辅助编程助手？"},{"content":"老牛同学这几天七赶八赶，终于在假期结束前，完成了 Cocos 3D 小游戏功能研发和测试调试，节前制定的“新春小游戏计划”也算是完美收官了。\n接下来就是把在 Cocos Creator 中研发完成的小游戏上传到微信小游戏平台，可直接通过手机进行游玩体验。\n上传过程中遇到了包大小无法上传的问题（微信小游戏主包限制4MB大小），老牛同学把解决的过程（即：分包）记录一下，共网友参考。\n提示： 本游戏所有的资源和代码均可免费下载，过打开“老牛同学”微信小程序-\u0026gt;点击“更多”Tab-\u0026gt;“源代码”获取下载链接。\n【“老牛同学”微信小程序】\n源代码文件夹中有 2 个 Zip 包：1 个是 Cocos Creator 的源代码；1 个是通过 Cocos Creator 构建的微信小游戏代码，可直接导入到“微信开发者工具”中即可进行预览和上传发布。\n特别注意： 在上传发布到微信小游戏平台之前，请务必修改一下微信小游戏 AppID 内容！\n小游戏状态机设计 在分包发布之前，老牛同学补充一下这款小游戏的状态机设计图，这也是这款小游戏的最后一张设计图。之前的总体设计方案、框架设计图等，请参考前面的几篇文章。\n老牛同学把小游戏运行的状态机，抽象设计为以下 5 个：\nLAUCHED 启动状态，或者重新开始状态：这是一个入口状态，初始化游戏场景。 PLAYING 闯关状态，包括首次进入关卡或者复活进入关卡，是游戏的主场景。 SUCCESS 闯关成功状态，如果有更多关卡，则又进入“闯关状态”，可不断完成闯关。 FAILURE 闯关失败状态，可以申请复活或者重新开始游戏。 REWARD 闯关失败状态，可以申请复活或者重新开始游戏。 只有状态机还不够，因为小游戏在运行过程中，场景中需要展示或者隐藏不同组件，这就涉及到运行时数据，包括：\nstate 游戏当前状态，以上 5 个枚举值。 level 当前所在关卡 failReason 闯关失败的原因，本小游戏只有 2 个枚举值 reviveCount 当前关卡复活的失败次数 其他数据，包括：关卡的“超时时间”和“工具限额”数量等。 通过状态机和运行时数据，我们可以确保游戏整体运行按照期望执行。\n微信小游戏发布 接下来就是把小游戏发布成微信小游戏了，这样就可以通过手机进行游玩了。\n微信小游戏类目设置 首先注册个微信小程序，和注册普通小程序注册无差异。需要注意的是，小程序的类目必须设置为“小游戏”，否则它就是一个普通的小程序，是无法使用游戏引擎。\n对于小程序来说，我们只需要设置“小程序类目”即可，其他小程序的一切设置均可在后续我们有空了在操作，不影响我们上传和体验。\n如果我们需要把小程序发布上线的话，则每年都需要进行一次认证，且每次认证需要300 元的认证费用。因为这是老牛同学的第 1 个小游戏，核心目的是为了学习和试试水，所以暂时未设置小程序其他内容，包括名称等。\n设置完成小程序类目之后，我们可以复制一下“小游戏 AppID”，准备到 Cocos Creator 进行构建了：\nCocos Creator 代码分包配置 如果直接使用 Cocos Creator 项目构建出微信小游戏，然后通过微信开发者工具进行上传时，必然会报错：系统错误，错误码：80051,source size 7874KB exceed max limit 4MB\n原因是微信小游戏的主包大小超过了4MB限制。解决办法就是分包，以尽可能缩小主包大小。\n什么是主包，简单理解就是我们打开微信小游戏时，小游戏必须加载的数据，包括文件、资源等内容。如果文件很大，那么我们打开游戏就会等待。 什么是分包，简单理解就是在我们打开为小游戏之后，可以通过程序异步加载的数据。就是先让用户进入游戏，先玩起来，后面的数据内容在边玩边加载。 Cocos Creator 分包的操作如下：\n在 Cocos Creator 资源管理中，新增一个目录，暂且为subpkg，然后把目前游戏所有的资源都拖放进去。同时，在资源管理器中选择subpkg目录，把它设置为一个Bundle（即：分包）： 由于我们把目前场景设置成了 Bundle，我们需要一个场景来启动我们的游戏。新建一个场景，暂且为Start，在场景中新建一个Start空节点；同时新建一个名为Start.ts脚本，并把该脚本挂到Start空节点上： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 import { _decorator, AssetManager, assetManager, Component, director, Node } from \u0026#39;cc\u0026#39;; const { ccclass, property } = _decorator; @ccclass(\u0026#39;Start\u0026#39;) export class Start extends Component { /** * 主包启动 */ protected start(): void { assetManager.loadBundle(\u0026#39;subpkg\u0026#39;, (e, bundle: AssetManager.Bundle) =\u0026gt; { director.loadScene(\u0026#39;GameX\u0026#39;); }); } } 从代码可以看出，我们的目的最终是通过Start.ts启动我们的游戏。关于Start场景，我们可以美化以下，老牛同学就简单的放了一张图片。\n最终，Start场景和所有资源管理器内容下： 在做好了主包之后，可以根据实际情况，进一步裁剪压缩包大小：“项目” -\u0026gt; “项目设置” -\u0026gt; “功能裁剪”。我们可以根据实际情况，去掉小游戏中没有用到的功能，包括：视频、WebView、2D 物理系统、粒子系统等。 网上还有一些其他的优化手段，包括：压缩图片质量、裁剪图片大小、压缩音频文件、去掉天空盒等，这样做确实可以缩小包大小，在万不得已的情况下，老牛同学不建议这样做，因为这样会降低游戏体验。 Cocos Creator 项目构建配置 打开 Cocos Creator 项目构建设置：“项目” -\u0026gt; “构建发布”，我们可以看到Start场景为主包，原来的小游戏场景GameX为一个Bundle。在构建发布页面的底部AppID部分，填写注册的小游戏 AppID 即可开始构建（APPID也可以修改project.config.json文件的appid内容）\n微信小游戏分包上传和体验 通过以上配置，我们可以在game.json中看到分包配置：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 { \u0026#34;deviceOrientation\u0026#34;: \u0026#34;portrait\u0026#34;, \u0026#34;networkTimeout\u0026#34;: { \u0026#34;request\u0026#34;: 5000, \u0026#34;connectSocket\u0026#34;: 5000, \u0026#34;uploadFile\u0026#34;: 5000, \u0026#34;downloadFile\u0026#34;: 500000 }, \u0026#34;plugins\u0026#34;: { \u0026#34;cocos\u0026#34;: { \u0026#34;version\u0026#34;: \u0026#34;3.8.5\u0026#34;, \u0026#34;provider\u0026#34;: \u0026#34;wx0446ba2621dda60a\u0026#34; } }, \u0026#34;subpackages\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;subpkg\u0026#34;, \u0026#34;root\u0026#34;: \u0026#34;subpackages/subpkg/\u0026#34; } ] } 其中，subpackages就是我们所有的分包配置。\n通过以上配置，我们在次通过“微信开发者工具”就可以正常上传小游戏代码了。\n上传成功之后，等了微信小程序后台，把上传的版本设置为“体验版”即可：\n现在，就可以通过手机扫描，体验小游戏了~\n最后，简单总结 节前制定的“新春小游戏计划”今天算是一个重要里程碑了，接下来就是不断优化打磨。这是老牛同学研发的第一款小游戏，而且是一款 3D 小游戏。做这款小游戏的初衷是学习，就是想改一改以往春节只是吃饭、睡觉、聊天和看手机的主旋律。总体来说，老牛同学感觉今年春节是过得比较充实的一个春节。\n在“主包”的场景Start设计中，老牛同学只是放了一张图片，设计比较粗糙；同时通过director.loadScene进行场景切换比较突兀，在正式发布之前，建议进行美化。目前有几个思路，仅供参考：\n可以做成一个类似于加载页面，稍微不那么突兀 场景功能调整，拆分主包的场景，主包真正就是主场景的一部分 其他的优化，老牛同学继续公开源代码，大家可随时下载和更新：打开“老牛同学”微信小程序-\u0026gt;点击“更多”Tab-\u0026gt;“源代码”获取下载链接。\nCocos 3D 小游戏：\n01.技术选型 丨 02.研发流程 丨 03.小游戏框架丨 04.核心架构设计\nTransformers 框架序列：\n01.包和对象加载中的设计巧思与实用技巧\n02.AutoModel 初始化及 Qwen2.5 模型加载全流程\n03.Qwen2.5 大模型的 AutoTokenizer 技术细节\n04.Qwen2.5/GPT 分词流程与 BPE 分词算法技术细节详解\n05.嵌入（Embedding）机制和 Word2Vec 实战\n06.位置嵌入（Positional Embedding）\nPipeline NLP 任务序列：\n零·概述 丨 01.文本转音频 丨 02.文本分类 丨 03.词元分类和命名实体识别 丨 04.问答 丨 05.表格问答 | 06.填充蒙版\n往期推荐文章：\nCline 免费插件 + Qwen2.5 大模型，零经验也能开发“对联王”微信小程序\n使用 Cursor + Qwen2.5 大模型 零经验研发微信小程序：自由构建个性化节拍器应用实战\nBolt.new 用一句话快速构建全栈应用：本地部署与应用实战（Ollama/Qwen2.5 等）\n基于 Qwen2.5-Coder 模型和 CrewAI 多智能体框架，实现智能编程系统的实战教程\nvLLM CPU 和 GPU 模式署和推理 Qwen2 等大语言模型详细教程\n基于 Qwen2/Lllama3 等大模型，部署团队私有化 RAG 知识库系统的详细教程（Docker+AnythingLLM）\n使用 Llama3/Qwen2 等开源大模型，部署团队私有化 Code Copilot 和使用教程\n基于 Qwen2 大模型微调技术详细教程（LoRA 参数高效微调和 SwanLab 可视化监控）\nChatTTS 长音频合成和本地部署 2 种方式，让你的“儿童绘本”发声的实战教程\n","date":"2025-02-05T00:00:00Z","image":"https://ntopic.cn/p/2025020501/00_hu3d13c3dc5e08d50d569ae0711661da61_112189_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/2025020501/","title":"新春“码”启 | Cocos 3D 微信小游戏（第5天）：分包构建和上传发布（完美收官）"},{"content":"放飞自我的时光总是过得很快，一晃春节就过去 3 天了，在这几天里老牛同学玩了不少款微信小游戏，在体验这些小游戏的同时，也分析了大佬们是如何设计小游戏的。对于一个 0 基础来研发小游戏的技术来说，市面上的这些小游戏就是最好的老师。\n老牛同学的“新春小游戏计划”感觉这几天取得了一定的进展，因此本文分享给大家：\n关于游戏资源的设计，几天走了不少的弯路，有了自己心得体会，提供了几个参考建议。 针对游戏控制逻辑，老牛同学抽取了共用核心代码，包括设计思路、配置中心、消息中心等，后续游戏可直接复用。 **提示：**本游戏所有的资源和代码均可下载，打开“老牛同学”微信小程序-\u0026gt;点击“更多”Tab-\u0026gt;“源代码”获取下载链接。\n【“老牛同学”微信小程序】\n老牛同学的小游戏目前进展如下图，包括原始：UI 组件自适应，背景蓝天白云在飘动，背景音乐在播放、可点击暂停启动等：\n游戏资源的建议 老牛同学为了设计开始菜单的背景、和游戏主场景中的 3D 桥梁，浪费了近 2 天时间，几个小的建议，仅供参考。\n建议一：UI 组件满屏背景的自适应问题 游戏的开始场景是一个 UI 组件，老牛同学为了美观，花费大精力通过 AI 大模型设计了好几款背景图片，并且经过精心处理和裁剪，保证图片大小完美匹配小游戏设计的尺寸（宽：720像素，高：1280像素）。\n在 Cocos Creator 场景编辑器中，效果确实达到了预期（组件：SpriteFrame）。可是，当运行预览时，由于不同运行设备的尺寸差异较大，而背景图又无法自动拉伸，会导致背景图片无法做到满屏，没有覆盖的地方就太突兀、太难看了。\n那么最好的办法就是：不做无法确定尺寸的满屏背景图片，直接使用原生背景即可！比如就像下面的这个背景，使用默认的蓝天白云就行了。\n建议二：需要碰撞检测的组件使用原生组件 老牛同学想在游戏场景中展示一座 3D 桥梁，并且需要对桥梁增加碰撞组件（组件：Collider）。Cocos 原生的碰撞组件列表如下：\n老牛同学不会 3D 建模，于是就直接到网上买了一座 3D 桥梁（FBX格式），桥梁本身没有问题，看起来还挺不错。可接下来问题就来了：由于桥梁设计比较复杂，Cocos 自有的碰撞组件一个也用不了！\n那么最好的办法就是：对于游戏中的核心资源，使用 Cocos 自带的组件进行搭建！比如下面是老牛同学采用 Cocos 自带的圆柱体和立方体组件搭建的桥梁组件，丑是丑了一点，但是功能全部具备：\n游戏控制逻辑设计 在上一篇中，老牛同学采用director.loadScene()来进行场景控制，但遇到个问题：场景切换比较生硬，没有连贯性，体验不好！\n老牛同学这几天在玩游戏过程中，发现绝大多数的游戏设计，应该都是在同一个场景中，通过“显示/隐藏”不同区块的组件来实现控制游戏场景界面，并不是采用director.loadScene()来进行场景页面控制。\n游戏组件层级设计 也就是说，小游戏中虽然有不同的场景、需要控制不同的界面，其实在大多数情况下，只需要设计一个Scene即可。\n**注意：**这只是老牛同学理解的一般情况，并非绝对，如果游戏比较复杂，或不同的场景地图有着明显差别，建议还是设计多个Scene，以便于进行设计和管理！\n那么如何控制不同组件的显示和隐藏呢？这就是“层级管理器”发挥威力的地方了：我们可以根据区块，把子组件放到不同的父层级组件中，然后程序控制父组件的显示和隐藏即可！\n下面是老牛同学设计的层级：\n总体设计思路：游戏组件由“3D 主场景”和“UI 组件”组成。 GameRoot是一个空节点，由它控制整个游戏逻辑，包括：GameView游戏主场景、CanvasUI游戏 UI 组件。 GameView是一个空节点，由它控制游戏主场景，包括主摄像头、游戏背景、游戏节点等。 CanvasUI是一个Canvas组件，由它控制游戏 UI 组件，包括 UI 摄像头、游戏导航栏、开始菜单、游戏成功和失败界面等。 **注意：**这只是老牛同学自己根据自己经验的设计结果，并非标准！\n核心代码框架设计 游戏中存在大量组件显示/隐藏、组件实例化/销毁、动画执行、音乐和音效播放/暂停、UI 交互、碰撞检测等等大量逻辑控制。\n可能的方案：组件之间通过函数相互调用直接触发逻辑，它们相互依赖；优点是单独的 2 个组件之间的关系比较直观，缺点是整个游戏组件直接深度耦合，依赖关系可能是一张蜘蛛网，随着游戏复杂度上升，维护会越来越难。老牛同学认为这种设计不可取。\n优化的方案：也是老牛同学推荐的办法，就像“层级管理器”一样，分层管理，上层只依赖（或控制）它的直接下层，下层不能依赖上层，或同层之间不可直接依赖，但它们可以通过发送消息给顶层，有顶层来驱动游戏逻辑。\n优化的方案也是老牛同学的设计原则，下面是一张依赖关系图：\n通过上述设计框架，层级分明，职责明确，维护方便！\n核心代码：配置中心 配置中心核心职责是数据存储，包括：用户偏好设计、游戏运行状态数据等。\n配置中心为单聊模式，初始化则缓存当前数据，存储的内容就是一个 Map 对象。 当发送数据更新时，则自动完成数据存储，通过setInterval每 500 毫秒存储一次。 设计优点：\n数据缓存内存，读取效率高。 数据更新，自动存储，失败重试，调用者无需关系中间异常等。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 // 游戏存储配置ID const GAME_CONFIG_KEY: string = \u0026#39;GameConfigID\u0026#39;; /** * 存储工具类 */ export class Configuration { // 自动保存标 private autoSaveFlag = false; /** * 数据缓存 */ private data = {}; /** * 单例实例化 */ static _instance: Configuration = null; public static instance() { if (!this._instance) { this._instance = new Configuration(); this._instance.init(); } return this._instance; } /** * 构造器 */ constructor() { console.log(\u0026#39;Configuration对象初始化.\u0026#39;); } /** * 初始化 */ private init() { const storage = sys.localStorage.getItem(GAME_CONFIG_KEY); if (storage) { this.data = JSON.parse(storage); } // 缓存数据 console.log(`Configuration配置数据: ${storage}`); // 定时存储 setInterval(this.doScheduleSave.bind(this), 500); } /** * 定时存储数据 */ private doScheduleSave() { if (!this.autoSaveFlag) { return; } console.log(\u0026#39;Configuration开始存储数据.\u0026#39;); try { const content = JSON.stringify(this.data); sys.localStorage.setItem(GAME_CONFIG_KEY, content); this.autoSaveFlag = false; console.log(\u0026#39;Configuration存储数据完成.\u0026#39;); } catch (e) { console.error(`Configuration存储数据异常 | ${e}`); // 继续存储 this.autoSaveFlag = true; } } /** * 存储数据（`string`类型） */ public storeString(key: string, value: string) { this.data[key] = value; this.autoSaveFlag = true; } /** * 获取数据（`string`类型） */ public fetchString(key: string, defaultValue: string): string { const value = this.data[key]; return value || defaultValue; } /** * 存储数据（`boolean`类型） */ public storeBoolean(key: string, value: boolean) { this.data[key] = value; this.autoSaveFlag = true; } /** * 获取数据（`boolean`类型） */ public fetchBoolean(key: string, defaultValue: boolean): boolean { const value = this.data[key]; if (value == null) { return defaultValue; } return (value === true) || (value === \u0026#39;true\u0026#39;) || (value === \u0026#39;ON\u0026#39;); } /** * 存储数据（`number`类型） */ public storeNumber(key: string, value: number) { this.data[key] = value; this.autoSaveFlag = true; } /** * 获取数据（`number`类型） */ public fetchNumber(key: string, defaultValue: number): number { return this.data[key] || defaultValue; } } 核心代码：消息中心 在本小游戏中，消息中心是小游戏逻辑控制的枢纽，它的使用方式：\n游戏总控监听所有的消息事件，当子节点发生特定事件（如：点击开始游戏按钮），需要通知上游或其他节点时，他就发送消息。 游戏总控接收到消息时，开始层层驱动下游节点执行，如监听到开始游戏事件，则驱动游戏主场景显示、UI 组件隐藏开始菜单等。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 /** * 事件常量 */ export enum EventName { GAME_LAUCH = \u0026#39;GAME_LAUCH\u0026#39;, GAME_START = \u0026#39;GAME_START\u0026#39;, } /** * 游戏事件 */ interface GameEvent { // 回调函数 func: Function; // 函数对象 target: any; } /** * 事件总线 */ export class EventCenter { /** * 事件处理器（1个事件，存在多个处理器） */ private static handlers: { [key: string]: GameEvent[] } = {}; /** * 注册监听事件 * * @param eventName 事件名 * @param cb 回调函数 * @param target 函数所在对象（可空） */ public static on(eventName: string, cb: Function, target?: any): void { if (!this.handlers[eventName]) { this.handlers[eventName] = []; } this.handlers[eventName].push({ func: cb, target: target, }); console.log(`注册游戏事件：(${eventName})-\u0026gt;(${target?.name}.${cb.name})`); } /** * 取消监听事件 * * @param eventName 事件名 * @param cb 回调函数 * @param target 函数所在对象（可空） */ public static off(eventName: string, cb: Function, target?: any): void { const eventList = this.handlers[eventName]; if (!eventList || eventList.length \u0026lt;= 0) { return; } console.log(`取消游戏事件：(${eventName})-\u0026gt;(${target?.name}.${cb.name})`); for (let i = 0; i \u0026lt; eventList.length; i++) { const event = eventList[i]; if (event.func === cb \u0026amp;\u0026amp; (!target || target === event.target)) { eventList.splice(i, 1); break; } } } /** * 发送事件 * * @param eventName 事件名 * @param args 参数列表 */ public static dispatch(eventName: string, ...args: any): void { const eventList = this.handlers[eventName]; if (!eventList || eventList.length \u0026lt;= 0) { return; } console.log(`发送游戏事件：(${eventName})-\u0026gt;(${args})`); for (let i = 0; i \u0026lt; eventList.length; i++) { const event = eventList[i]; event.func.apply(event.target, args); } } } 总结 以上部分只是老牛同学挑选的部分内容，小游戏完整的资源和源代码：打开“老牛同学”微信小程序-\u0026gt;点击“更多”Tab-\u0026gt;“源代码”获取下载链接。\n【“老牛同学”微信小程序】\nCocos 0基础小游戏：\n01.技术选型 丨 02.研发流程 丨 03.小游戏框架\nTransformers 框架序列：\n01.包和对象加载中的设计巧思与实用技巧\n02.AutoModel 初始化及 Qwen2.5 模型加载全流程\n03.Qwen2.5 大模型的 AutoTokenizer 技术细节\n04.Qwen2.5/GPT 分词流程与 BPE 分词算法技术细节详解\n05.嵌入（Embedding）机制和 Word2Vec 实战\n06.位置嵌入（Positional Embedding）\nPipeline NLP 任务序列：\n零·概述 丨 01.文本转音频 丨 02.文本分类 丨 03.词元分类和命名实体识别 丨 04.问答 丨 05.表格问答 | 06.填充蒙版\n往期推荐文章：\nCline 免费插件 + Qwen2.5 大模型，零经验也能开发“对联王”微信小程序\n使用Cursor + Qwen2.5 大模型 零经验研发微信小程序：自由构建个性化节拍器应用实战\nBolt.new 用一句话快速构建全栈应用：本地部署与应用实战（Ollama/Qwen2.5 等）\n基于 Qwen2.5-Coder 模型和 CrewAI 多智能体框架，实现智能编程系统的实战教程\nvLLM CPU 和 GPU 模式署和推理 Qwen2 等大语言模型详细教程\n基于 Qwen2/Lllama3 等大模型，部署团队私有化 RAG 知识库系统的详细教程（Docker+AnythingLLM）\n使用 Llama3/Qwen2 等开源大模型，部署团队私有化 Code Copilot 和使用教程\n基于 Qwen2 大模型微调技术详细教程（LoRA 参数高效微调和 SwanLab 可视化监控）\nChatTTS 长音频合成和本地部署 2 种方式，让你的“儿童绘本”发声的实战教程\n","date":"2025-02-01T00:00:00Z","image":"https://ntopic.cn/p/2025020101/00_hu5ecad2161246203fe21cd9171930b608_109873_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/2025020101/","title":"新春“码”启 | Cocos 3D 开发微信小游戏（第4天）：游戏资源设计和框架核心源代码"},{"content":"今天是实施新春小游戏计划的第 3 天，前面 2 天主要是入门和了解，我们对微信小游戏以及 Cocos 编辑器有了初步的认识。\n第 1 天：新春“码”启 | 0 基础开发微信小游戏，Cocos 游戏引擎 + AI 辅助编程（第 1 天） 第 2 天：新春“码”启 | 0 基础开发微信小游戏，Cocos 游戏引擎 + AI 辅助编程（第 2 天） 接下来，现在我们就开始正式实战研发小游戏了：\n设计小游戏的总体方案，包括场景及其内容等方面。 开发小游戏基础框架，跑通整个游戏链路，细节部分就后续逐步完善补充。 总体设计方案 到现在，老牛同学还尚未确定小游戏的创意，这时候大模型好像没什么用了，给的想法总是达不到要求。\n虽然没有明确的创意，但是老牛同学玩了几款热门的微信小游戏，再结合自己的经验，初步设计出了这个小游戏：\n老牛同学的设计思路是这样的：\n游戏采用通关模式，最少有 1 个关卡，关卡难度由易到难递增。 明确每个关卡的通关时间限制，如果时间用完或者工具用完（像子弹、生命等），就算通关失败。 要是通关成功了，就进入下一个关卡；如果失败了，还可以选择复活，不过复活需要玩家参与互动（比如学习游戏操作手册等）。 由于这个游戏不允许有杀戮、战争等场景，所以通关的方式就不能是用子弹之类的了。\n研发基础框架 接下来，开始用 Cocos 研发基础框架，目的是串联整个游戏流程，那些不明确的部分就先跳过。例如，我们可以通过点击“成功”和“失败”这两个按钮，分别代表通关成功和失败。\n今天核心目标完成前面 3 个场景的研发：\n场景一：开始场景 开始场景主要表达角色设定、背景介绍、任务说明，还有进入游戏或者退出游戏的操作按钮。\n在“资源管理器”面板中，在assets目录右键新建目录scenes，用于存放小游戏场景内容，我们新建一个名为SceneStart的场景并双击打开它。\n由于小游戏主要在手机端运行，需要设置小游戏的宽度和高度：\n打开项目设置面板，路径：“项目” - “项目设置” 在“项目数据”配置项中，调整参数：“设计宽度”为720（像素），“设计高度”为1280（像素），同时勾选“适配屏幕宽度”和“适配屏幕高度”复选框 我们在“场景一”中，创建一个“开始游戏”按钮，点击按钮则进入“场景二”：\n在“层级管理器”中创建画布：右键 - “创建” - “UI 组件” - “Canvas (画布)”，并重命名为“CavansStart” 在“场景编辑器”中，切换到“2D”视图，点击“3D”或者按下2快捷键 在“层级管理器”中空接口，UI 节点都放到这个节点下：“Canvas”右键 - “创建” - “空节点”，并命名为“MenuView”，代表一个菜单 在“层级管理器”中创建按钮：“Canvas”右键 - “创建” - “2D 对象” - “Sprite (精灵)”，主要是用于设置背景 在精灵的“属性检测器”中，调整精灵的大小、背景和组件： 大小：“Content Size”中，宽600（像素），高700（像素） 背景：“Sprite Frame”中，选择“default_panel”；“type”中，选择“SLICED” 增加“cc.Widget”组件，并设置为“水平居中”、“垂直居中” 在“层级管理器”中创建按钮：“Canvas”右键 - “创建” - “UI 组件” - “Button (按钮)”，并重命名为“BtnStart” 在按钮的“属性检测器”中，调整按钮大小和文案（“Label”的内容） 接下来，对“开始游戏”按钮增加点击事件，点击则切换到“场景二”界面：\n在“资源管理器”面板中，在assets中右键新建目录scripts，用户存放小游戏 TypeScript 脚本文件，我们新建一个名为BtnStart的脚本并双击打开它。 在BtnStart的脚本内容如下： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 import { _decorator, Component, director, Node } from \u0026#39;cc\u0026#39;; const { ccclass, property } = _decorator; @ccclass(\u0026#39;BtnStart\u0026#39;) export class BtnStart extends Component { start() { this.node.on(Node.EventType.TOUCH_END, this.toScene1, this); } protected onDestroy(): void { this.node.off(Node.EventType.TOUCH_END, this.toScene1, this); } private toScene1() { director.loadScene(\u0026#39;SceneLevel1\u0026#39;); } } 我们用director.loadScene方法切换不同的场景，从脚本可以看出：点击开始按钮，则切换到SceneLevel1场景。 接下来，在“层级管理器”中选择“BtnStart”按钮，在“属性检查器”中，绑定点击事件： 点击事件：设置“Click Events”为1，即代表只有 1 个事件 点击函数：选择“BtnStart”节点，选择“BtnStart”脚本，最后选择“toScene1”函数 最终，我们运行“场景一”，就可以看到场景切换效果了。\n场景二：游戏场景（关卡一） 在“资源管理器”面板中，在assets中右键新建目录scenes，用户存放小游戏场景内容，我们新建一个名为SceneLevel1的场景并双击打开它。\n然后，在“层级管理器”中创建画布：右键 - “创建” - “3D 对象” - “Cube (立方体)”，并命名为cube\n为了使“场景二”中“立方体”的效果更明确，我们给他创建红色材质：\n在“资源管理器”面板中，在assets中右键新建目录materials，用户存放小游戏节点的材质 在materials创建材质：右键 - “创建” - “材质”，并命名为cube 在cube材质的“属性检测器”中，设置“Albedo”颜色为红色（或者：其他颜色，只要看起来有效果即可），并保存材质 拖动cube材质到cube立方体上，可以看到，立方体变成了红色 其他场景和问题 我们可以采用同样的方式，创建“场景三”、“场景四”和“场景五”，保证我们的场景之间就能串联起来。\n各个场景串联是正常的了，但是老牛同学发现个问题：在点击按钮，即使用director.loadScene()进行场景切换时有点生硬，老牛同学感觉有应该有更好的方案解决。\n老牛同学一个新的想法：一个关卡一个场景，UI 通过显示和隐藏的方式呈现，包括开始菜单 UI、游戏失败提示 UI 和游戏成功提示 UI 等。比如“场景一”包含了开始菜单，点击“开始游戏”之后，隐藏菜单。\n若大家有其他想法或经验，特别欢迎大家与老牛同学进行交流，非常感谢~\n总结 今天的分享先到这里，在小游戏的创意没有明确之前，我们就先搭建小游戏的整体流程，同时在过程中不断进行优化，因为无论小游戏的创意如何，这些流程和优化经验都是比不可少的。\n最后，给大家拜个早年，祝大家：新年快乐，🐍 蛇年吉祥～ 😆 笑口常开～ 🎉 好运连连～心想事成～ 🧧 福气满满！\n老牛同学友情提示：新春写对联、对下联，就用“对联王”小程序 ↓↓↓\n【微信搜索“老牛同学”小程序，即可打开“对联王”页面】\nCocos 0基础小游戏：\n01.技术选型 丨 02.研发流程\nTransformers 框架序列：\n01.包和对象加载中的设计巧思与实用技巧\n02.AutoModel 初始化及 Qwen2.5 模型加载全流程\n03.Qwen2.5 大模型的 AutoTokenizer 技术细节\n04.Qwen2.5/GPT 分词流程与 BPE 分词算法技术细节详解\n05.嵌入（Embedding）机制和 Word2Vec 实战\n06.位置嵌入（Positional Embedding）\nPipeline NLP 任务序列：\n零·概述 丨 01.文本转音频 丨 02.文本分类 丨 03.词元分类和命名实体识别 丨 04.问答 丨 05.表格问答 | 06.填充蒙版\n往期推荐文章：\nCline 免费插件 + Qwen2.5 大模型，零经验也能开发“对联王”微信小程序\n使用Cursor + Qwen2.5 大模型 零经验研发微信小程序：自由构建个性化节拍器应用实战\nBolt.new 用一句话快速构建全栈应用：本地部署与应用实战（Ollama/Qwen2.5 等）\n基于 Qwen2.5-Coder 模型和 CrewAI 多智能体框架，实现智能编程系统的实战教程\nvLLM CPU 和 GPU 模式署和推理 Qwen2 等大语言模型详细教程\n基于 Qwen2/Lllama3 等大模型，部署团队私有化 RAG 知识库系统的详细教程（Docker+AnythingLLM）\n使用 Llama3/Qwen2 等开源大模型，部署团队私有化 Code Copilot 和使用教程\n基于 Qwen2 大模型微调技术详细教程（LoRA 参数高效微调和 SwanLab 可视化监控）\nChatTTS 长音频合成和本地部署 2 种方式，让你的“儿童绘本”发声的实战教程\n","date":"2025-01-25T00:00:00Z","image":"https://ntopic.cn/p/2025012501/00_huaacfb4a4b3954e1274812d81e3c089e7_94172_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/2025012501/","title":"新春“码”启 | Cocos 3D 开发微信小游戏（第3天）：场景搭建与游戏链路基础开发"},{"content":"我们在新春小游戏计划的第 1 天确定了技术选型，并通读了微信小游戏的研发手册，了解了一些游戏研发的基本概念：新春“码”启 | 0 基础开发微信小游戏，Cocos 游戏引擎 + AI 辅助编程（第 1 天）\n今天是第 2 天，老牛同学开始正式进入研发了，主要包括以下 2 部分：\n精心设计一个游戏创意：明确我们最终的交付物是什么 熟悉 Cocos 游戏研发流程 设计游戏创意 说实话，我目前还不知道该给小朋友们研发一款什么主题的游戏。我还是优先选择腾讯元宝来帮忙，针对这款小游戏，我有如下要求：\n无损身心健康：不能出现暴力、杀戮、血腥、战争、死亡、打斗等场景和画面。 禁止破坏环境：不能出现砍伐树木、践踏草地、采摘花朵、挖矿挖煤等场景和画面。 生命必须平等：游戏中，无论是人类还是动物，生命面前人人平等，禁止出现以大欺小、恃强凌弱、大鱼吃小鱼等场景和画面。 有益生长发育：最好是 3D 游戏，能培养小朋友们的空间立体感；最好不要出现竞争排名、打榜等场景和界面，避免小朋友争强好胜。 游戏场景设计：游戏场景需要足够简单，便于初学者 1 周之内完成研发；需要设计 3 个场景，难度由易到难，逐步升级；单人游戏，游戏中不需要互动，同时单手即可进行游戏操作。 在和元宝来来回回对话了好几次，感觉都不太满意，要么就是游戏场景太复杂，要么就是任务过于简单，不具备挑战性。\n对于游戏创意，老牛同学感觉一时半会儿也无法确定，因此先暂时放放，优先熟悉一下 Cocos 研发界面。\nCocos 研发体验 建议：现在就去买个鼠标吧，TouchPad 操作很不方便。\n首先去官网下载 Cocos Creator：https://www.cocos.com/creator-download\n安装之后打开后是一个 Cocos Dashboard，也就是它只是一个壳。接着就去安装编辑器，老牛同学选择的是最新版本：3.8.5\nCocos 编辑器安装好之后，就可以创建项目了，也就是我们的小游戏了：\n进入小游戏编辑器之后，我们就可以放飞自我了，先瞎点一通在说：左键、右键、中键、滚动、上下左右移动、界面上的小图标等等，先看看都有什么效果。\n天旋地转，一顿操作猛如虎之后，老牛同学发现自己有点晕乎了，不知道到底在哪儿了！\n只要快速浏览一遍 Cocos 官方文档，找到了编辑器的使用文档，一定要先好好熟悉一下：https://docs.cocos.com/creator/3.8/manual/zh/editor/scene/\n其实所有的操作，主要就是上图的红框部分内容，根据官网文档介绍和我的实际操作，总结如下：\n视图：分为 2D 和 3D，可相互切换，2为快捷键。 变换工具组： 包括平移（W）、旋转（E）、缩放（R）、矩形（T）、吸附、位置（锚点/中心）、坐标系（本地/世界坐标系）这些操作工具，快捷键Q则退出这些变换工具。 平移：可以沿着X/Y/Z轴上移动，每个轴的颜色都不一样。 旋转：和平移类似，可以分别沿着 3 个轴进行 360° 旋转。 缩放：和平移类似，可以分别沿着 3 个轴放大和缩小。 矩形：只适合于 UI 节点，也可以放大和缩小，它和缩放的区别是啥？ 吸附：在使用移动/旋转/缩放变换工具时，按照设定的步长进行操作；或者矩阵工具对齐操作。 位置：变换工具的操作锚点是在中心还是坐标系，操作下来没有看出有啥效果，暂时先忽略。 坐标系：有参照物的相对坐标系还是绝对的世界坐标系。 Gizmo：显示场景相机的方向，可以点击切换观察角度（6 个角度：上下左右前后）。 总的来说：缩放只是改变节点尺寸大小，而矩形既可以改变节点尺寸大小、还可以调整对象和位置和旋转，它是一个复合操作。\n经验之谈：以下是老牛同学在不断瞎操作后得出的经验，可以借鉴一下。\n牢记 Gizmo 的设置（默认：Y 垂直向上，X 右下角，Z 左下角）：当我们转晕了，不知道在哪儿了的时候，首先把 Gizmo 的坐标还原。 在左边“层级管理器”中，随便选择一个节点，然后按一下F键（Focus 聚焦），你就会发现，我们又回来了，又可以瞎点瞎按了。 用鼠标吧，TouchPad 操作是真的不方便，建议前期把鼠标的灵活度稍微调低一点，这样能保证你晕得慢一点。 对于老牛同学来说，0 基础就直接上手开发 3D 游戏，感觉起点有点高，有点 Hold 不住了~\nTypeScript 了解 微信小游戏采用 TypeScript 编程语言，提前去了解一下：https://www.runoob.com/typescript/ts-tutorial.html\n总体来说，TypeScript 是 JavaScript 的超集，扩展了 JavaScript 的语法，因此如果熟悉 JavaScript 语音的话，学习起来问题不大：\n今天的分享就先到这里，主要是熟悉 Cocos 编辑器，它是后续研发的关键。同时小游戏的创意，目前还没有头绪，还得好好想想！\nTransformers 框架序列：\n01.包和对象加载中的设计巧思与实用技巧\n02.AutoModel 初始化及 Qwen2.5 模型加载全流程\n03.Qwen2.5 大模型的 AutoTokenizer 技术细节\n04.Qwen2.5/GPT 分词流程与 BPE 分词算法技术细节详解\n05.嵌入（Embedding）机制和 Word2Vec 实战\n06.位置嵌入（Positional Embedding）\nPipeline NLP 任务序列：\n零·概述 丨 01.文本转音频 丨 02.文本分类 丨 03.词元分类和命名实体识别 丨 04.问答 丨 05.表格问答 | 06.填充蒙版\n往期推荐文章：\nCline 免费插件 + Qwen2.5 大模型，零经验也能开发“对联王”微信小程序\n使用Cursor + Qwen2.5 大模型 零经验研发微信小程序：自由构建个性化节拍器应用实战\nBolt.new 用一句话快速构建全栈应用：本地部署与应用实战（Ollama/Qwen2.5 等）\n基于 Qwen2.5-Coder 模型和 CrewAI 多智能体框架，实现智能编程系统的实战教程\nvLLM CPU 和 GPU 模式署和推理 Qwen2 等大语言模型详细教程\n基于 Qwen2/Lllama3 等大模型，部署团队私有化 RAG 知识库系统的详细教程（Docker+AnythingLLM）\n使用 Llama3/Qwen2 等开源大模型，部署团队私有化 Code Copilot 和使用教程\n基于 Qwen2 大模型微调技术详细教程（LoRA 参数高效微调和 SwanLab 可视化监控）\nChatTTS 长音频合成和本地部署 2 种方式，让你的“儿童绘本”发声的实战教程\n","date":"2025-01-24T00:00:00Z","image":"https://ntopic.cn/p/2025012401/00_hu66848577af15ce3ec1fae86582ec8951_95362_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/2025012401/","title":"新春“码”启 | 0 基础开发微信小游戏，Cocos 游戏引擎 + AI 辅助编程（第2天）"},{"content":"好羡慕小朋友们，已经放了快两周的寒假了，还好从今天开始，我这个“大朋友”也要开始放飞自我了。\n今年春节假期，我给自己定了一个小目标：给小朋友们研发一款小游戏！ 这也是我为小朋友们预定的一份特别的新春礼物！\n缘起：做个能写游戏的程序员父亲 我一直对游戏兴趣不大，总觉得一个真实世界的人在一个虚拟的世界里玩得不亦乐乎，除了是浪费时间之外，实在是无可取之处。\n可是，每当我看到小朋友站在别的小朋友身边，聚精会神的看着别的小朋友玩游戏的时候，我总觉得我是不是对游戏有什么偏见？也许游戏还真的挺不错。\n我完全不懂写游戏，但是又有点担心小朋友误入暴力、杀戮、血腥等这些有害身心的游戏，于是硬着头皮承诺小朋友们：今年春节，我给他们写一个更好玩的游戏。 谁叫她们有个程序员的父亲呢？程序员不应该懂得电脑的一切吗，包括修电脑、装系统、布网线、做黑客，还有写游戏等……\n虽然我从没有写过游戏代码，但我还是有那么点信心，因为今年已经是 2025 年了，AI 时代早已来临，有 AI 辅助编程，应该可行！\n选择 Cocos 游戏引擎 于是乎，我信心满满地打开了微信小游戏的官方指导文档，以为能和微信小程序一样，立马就可以动手干了：https://developers.weixin.qq.com/minigame/dev/guide/\n第 1 章：起步 账号注册，这个后续上线的时候可以再补，不着急；微信开发者工具目前已经有了。 第 2 章：游戏引擎 到这章，感觉计划要泡汤了，小游戏是一个不同于浏览器的 JavaScript 运行环境，没有 BOM 和 DOM API ，也就是说，我之前的这些 Java GUI、Flutter、Web 等可视化编程的经验全都用不上了！ 要想编写微信小游戏，我必须得学习一个全新的游戏引擎，幸运的是，微信小游戏已经适配好了好几个游戏引擎：Cocos/Egret/LayaBox、Unity和Cocos2d-x\n那么现在我需要做的是，从这几个已经适配好的游戏引擎中，选择一个更适合自己的游戏引擎即可。为了了解这几个引擎的优劣，我首先使用“腾讯元宝”大模型进行分析，因为考虑到对于微信小游戏来说，元宝预训练的数据也行比其他大模型要丰富一些，毕竟微信小游戏是自家产品。\nPrompt 提示词：\n1 2 你是一个微信小游戏市场分析专家，你总是根据用户的分析需求，为用户提供客观准确的分析数据，并提供参考建议。 现在，我想学习一个小游戏引擎进行游戏研发，请根据目前已经上线的微信小程序所使用的引擎，详细分析一下这几个引擎的优劣并提供学习建议：Cocos、Egret、LayaBox、Unity和Cocos2d-x 同时，我通过 Bing 同步搜索了这个几个引擎的优劣，加上元宝的分析结果，总结如下：\nUnity 丰富的生态，被全球广泛使用，适合做 3D 游戏，可惜采用的是 C# 研发语音。 Cocos 社区比较活跃，文档齐全，性能较好，2D 和 3D 游戏均可，重点是开源免费和采用JavaScript或TypeScript研发语音。 Egret 专注于 HTML5 游戏开发，学习门槛低，社区不如 Cocos 引擎。 LayaBox 可研发 2D 和 3D 游戏，社区不如 Cocos 引擎。 Cocos2d-x 同 Cocos 引擎，是它的 2D 版本，统一到 Cocos 引擎即可，无需单独分析。 对于初学者来说，可预料到会遇到很多的问题，因此活跃的社区和丰富的文档就显得相当重要。同时，春节时间有限，需要减少不确定性，不宜专门学习一门 C# 编程语言。\n因此，老牛同学最终选择的是：Cocos游戏引擎\n了解微信小游戏 选好了 Cocos 游戏研发引擎，接下来就是通读一遍微信小游戏的研发手册，看看还有什么不理解的地方，避免成为后续研发的卡点。\n打包发布相关的内容，老牛同学暂时都先跳过了，因为这些是最后发布才需要学习的内容。首先重点学习了解游戏框架部分内容：\nScenes 场景，顾名思义，就是一个游戏界面；它是一种资源，每个关卡可以代表一个场景。 Entities 实体，也叫节点，分为 2D 节点和 3D 节点，游戏中的承载功能的对象，而功能通过组件来实现。 Prefabs 预制体，它是一种资源，是创建节点实例的模板。 关键类型 最常用的有 engine 对象，节点类 Entity 等。 脚本组件 代码脚本（样例全部都是 TypeScript 脚本），操纵实体（Entity），实现游戏逻辑。 序列化系统 实例存储和加载，如保存与加载组件或是资源。 生命周期 声明周期钩子函数。 上面几个概念理解感觉都不难，简单理解：场景就是游戏操作界面，实体/节点就是界面上的内容，预制体实例化后生成实体/节点；在整个生命周期钩子函数中，通过 TypeScript 脚本组件控制实体/节点，这样实现了游戏操作功能；而在脚本组件中，engine 和 Entity 会比较常用，因此后续我们需要重点关注他们的 API 和相关属性。\n对于一个游戏引擎来说，当一个游戏界面的展示内容、操控脚步都准备好了，那么解下来就是展示给用户和用户操作了，即：渲染\n资源，即需要展示的内容。 渲染器，包括网格、蒙皮网格、线段和尾迹共 4 种渲染器。 相机，简单理解就是窗口，即用户可以看到的游戏界面部分。 光照，让游戏界面更加真实，包括不同光源、阴影等。 Gizmos 可视化工具，用于绘制基础图形，包括网格、实体选中后的辅助线等。 初识 Cocos 游戏引擎 微信小游戏的官方文档中，对小游戏的整体进行了介绍，示例中使用大量的 TypeScript 脚本，老牛同学目前还没有细看，等后续用到了详细了解。\n接下来，来看看 Cocos 游戏研发引擎：https://docs.cocos.com/creator/3.8/manual/zh/\n简单理解：Cocos是游戏引擎，Cocos Creator是制作游戏的可视化工具，它包含了游戏引擎、资源管理、场景编辑、游戏预览和发布等游戏开发所需的全套功能，感觉像是个 Cocos 游戏研发的超级 IDE。\n同时，Cocos 在 B 站还发布有学习视频，同时还有 2D 和 3D 游戏实例教程，果然文档很丰富。\n在功能模块部分，和微信小游戏的文档基本能一一对应上，包括：图像渲染、2D 对象、动画系统、音频系统、物理系统、粒子系统、缓动系统、地形系统、资源管理，还有多语音系统。\n针对每个功能章节，都有详细的文字和截图使用说明，非常详细。\n老牛同学今天先初略浏览一番，明天开始，正式启动小游戏的构思和研发之旅。敬请期待！\nTransformers 框架序列：\n01.包和对象加载中的设计巧思与实用技巧\n02.AutoModel 初始化及 Qwen2.5 模型加载全流程\n03.Qwen2.5 大模型的 AutoTokenizer 技术细节\n04.Qwen2.5/GPT 分词流程与 BPE 分词算法技术细节详解\n05.嵌入（Embedding）机制和 Word2Vec 实战\n06.位置嵌入（Positional Embedding）\nPipeline NLP 任务序列：\n零·概述 丨 01.文本转音频 丨 02.文本分类 丨 03.词元分类和命名实体识别 丨 04.问答 丨 05.表格问答 | 06.填充蒙版\n往期推荐文章：\nCline 免费插件 + Qwen2.5 大模型，零经验也能开发“对联王”微信小程序\n使用 Cursor + Qwen2.5 大模型 零经验研发微信小程序：自由构建个性化节拍器应用实战\nBolt.new 用一句话快速构建全栈应用：本地部署与应用实战（Ollama/Qwen2.5 等）\n基于 Qwen2.5-Coder 模型和 CrewAI 多智能体框架，实现智能编程系统的实战教程\nvLLM CPU 和 GPU 模式署和推理 Qwen2 等大语言模型详细教程\n基于 Qwen2/Lllama3 等大模型，部署团队私有化 RAG 知识库系统的详细教程（Docker+AnythingLLM）\n使用 Llama3/Qwen2 等开源大模型，部署团队私有化 Code Copilot 和使用教程\n基于 Qwen2 大模型微调技术详细教程（LoRA 参数高效微调和 SwanLab 可视化监控）\nChatTTS 长音频合成和本地部署 2 种方式，让你的“儿童绘本”发声的实战教程\n","date":"2025-01-23T00:00:00Z","image":"https://ntopic.cn/p/2025012301/00_hu801d7d4a91fbb4a30826d045c1c4223f_166408_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/2025012301/","title":"新春“码”启 | 0 基础开发微信小游戏，Cocos 游戏引擎 + AI 辅助编程（第1天）"},{"content":"创作是孤独的坚持，在2025春节来临之际，我收到了CSDN技术博客给我推送的年度总结，实在是有心了。\n让我意想不到的是，我竟然在CSDN技术博客中积累了这么多的收获，我想把这份惊喜与大家一同分享：\n原来我是在2007年4月2号开通了CSDN技术博客，到今天已经有18个年头了，这个日子我应该牢记于心：\n2024年5月我开始学习大模型，从0开始，持续学习也持续在CSDN中分享自己的所学。在CSDN技术博客中，2024年，我的原力值已经达到了1420，超越了**99.98%**的用户，给自己点个赞：\n我在CSDN技术博客中的每一篇文章，都来自我的精心创作，同样的，也得到了CSDN技术社区、CSDN读者的认可：\n同样的，CSDN技术社区也给了我很多技术帮助，我也在CSDN学习了大量知识：\n我觉得是自己兴趣，更是CSDN技术博客的广大读者的支持给予了我无尽的动力。每一次阅读、每一个点赞、每一条评论，都让我感受到背后的期待和温暖的心。\n特别感谢那些一直默默支持我的忠实读者，是你们的鼓励让我持续前行，不断探索新的领域，提升自我。\n2025年，我将继续在技术这条道路上，发现更多的美好，分享更多的内容。\n再次感谢每一位陪伴在我身边的你。\n","date":"2025-01-22T00:00:00Z","image":"https://ntopic.cn/p/2025012201/00_hu80b1db25e0ce5c797275f288a4fafc11_71605_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/2025012201/","title":"感谢有你，共赴未来：2024年CSDN技术博客创作总结 · 老牛同学"},{"content":"在春节来临之际，微信公众号给我推送了年度总结，实在是有心了。\n2024年5月我开始学习大模型，同时开通了“老牛同学”微信公众号，想不到短短的几个月，竟然积累了这么多的收获！\n就像总结里面提到的：‘创作是孤独的坚持’。是你们的支持给予了我无尽的动力。每一次阅读、每一个点赞、每一条评论，都让我感受到背后的期待和温暖的心。\n特别感谢那些一直默默支持我的忠实读者，是你们的鼓励让我持续前行，不断探索新的领域，提升自我。\n2025年，我将继续在技术这条道路上，发现更多的美好，分享更多的内容。\n再次感谢每一位陪伴在我身边的你。\n使用 Llama3/Qwen2 等开源大模型，部署团队私有化 Code Copilot 和使用教程\n使用 Llama3/Qwen2 等开源大模型，部署团队私有化 Code Copilot 和使用教程\n基于 Qwen2/Lllama3 等大模型，部署团队私有化 RAG 知识库系统的详细教程（Docker+AnythingLLM）\n使用 Llama3/Qwen2 等开源大模型，部署团队私有化 Code Copilot 和使用教程\n基于 Qwen2/Lllama3 等大模型，部署团队私有化 RAG 知识库系统的详细教程（Docker+AnythingLLM）\n","date":"2025-01-15T00:00:00Z","image":"https://ntopic.cn/p/2025011501/00_hu7bb0965328bb68cebfda073b0f72a6ff_142474_120x120_fill_box_smart1_3.png","permalink":"https://ntopic.cn/p/2025011501/","title":"感谢有你，共赴未来：2024年微信公众号创作总结 · 老牛同学"},{"content":"小朋友即将放寒假了，意味着春节就不远了。在我们湖北老家，当我还是学生的时候，每年临近春节，写春联、贴春联和对春联是读书人一件乐事。\n如今，老牛同学早已不是读书人，但还是怀念那时快乐时光，因此想在春节前撸一个“对联王”微信小程序，专门用于创作春联和对春联。\n上次零经验的我们，借助 Cursor 写了一个“节拍器”小程序，本次我们来用免费的 Cline 插件，辅助完成“对联王”微信小程序编程。下面是已发布的小程序，欢迎大家来体验：\n【对联王微信小程序】\n大家也可以在微信中搜索“老牛同学”找到“对联王”小程序：\n同时，本次借助 Cline 插件完成的“对联王”小程序的演示录屏如下：\n【对联王演示视频，请到公众号或视频号查看】\n接下来，老牛同学就把 Cline 插件的使用主要过程，与大家分享。\n1. Cline 与 Cursor 区别 老牛同学根据自己编写 2 个小程序页面的经验，先说说两者的区别、使用感受和建议：\n工具形态：Cline 是一个 VS Code 插件，在 VS Code 直接安装或卸载，无需注册即可使用，灵活方便；而 Cursor 则是一个专门的 IDE，需要单独下载并安装，还需要用户注册和登陆方可使用，稍微笨重一些。 费用：Cline 是开源免费的插件（https://github.com/cline/cline）；而 Cursor 则是闭源的，且是收费的，它为新注册用户提供 2 周 2000 次调用的免费试用期。 支持的大模型：Cline 支持多种大模型，只需要兼容 OpenAI 接口规范且支持 Stream 流式输出即可，比如 Qwen 大模型；而 Cursor 内置了几种大模型，对兼容模型的支持并不很友好，比如总是有一些警告等。 功能和体验：两者在功能上感觉差不太多，Cline 在请求上下文、文件 Diff 比较界面稍逊 Cursor 一筹，但其实对编程影响并不大，总体来说 Cursor 体验上更优一点。 总体体感：这 2 个工具的功能都差不多，那么它对我们的实际帮助，其实还是依赖于底层大模型的能力，好马还得配好鞍。老牛同学之前常用 Continue 插件，在体验上，感觉 Cline 有点像增强版的 Continue 插件，只是它额外提供了文件生成、差异比对、系统调用的能力。在“对联王”小程序的整个研发过程中，老牛同学其实同时使用了这 2 个插件。\nCline 主要用于页面框架搭建、页面主要元素的布局、小程序主要逻辑代码等生成。 Continue 则用于细节部分调优，主要是对 Cline 生成的代码进行微调，比如：右下角“收藏夹”悬浮按钮的上下移动细节逻辑、悬浮按钮最后移动位置存储等逻辑。 使用建议：如果大家预算充裕，且使用比较频繁，建议直接上付费的 Cursor IDE（20 美元每月和 40 美元每人每月两种收费价格）；如果使用免费的 Cline，建议搭配 Continue 一起使用。\n关于 Continue 的使用教程，可以参考老牛同学之前文章：Code Copilot 和使用教程\n2. Cline 插件配置 安装插件 安装好 VS Code 软件之后，Cline 插件安装就比较简单了，搜索“Cline”安装即可：\n安装成功之后，我们可以看到再左侧有个“机器人”的小图标。\n配置插件 首次点击安装好的 Cline 插件图标，或者后续点击 Cline 插件的齿轮状的配置按钮，均可进入配置页面：\n几个配置项比较简单，其中API Provider如果选择的是OpenAI Compatible，则需要保证接口符合 OpenAI 规范且支持 Stream 流式输出。\n其他配置 微信开发者工具：和上次“节拍器”小程序一样，我们需要下载微信开发者工具，用于实时预览和调试小程序，确保代码可以在微信环境中正常运行。 官方下载链接\nContinue 插件：关于插件的配置和详细使用方法，可以参考老牛同学之前文章。 使用 Llama3/Qwen2 等开源大模型，部署团队私有化 Code Copilot 和使用教程\n3. “对联王”实战 准备就绪，现在，让我们一起探索如何利用 AI 的力量来加速开发进程，让基本零经验的新书也能开发一个小程序等应用。\n打开 Chat 面板，输入精心设计的 Prompt 提示词。以下是老牛同学首次尝试时所用的提示内容：\n1 2 3 4 5 6 7 在春节活动中，创作对联或者对对联是一项有趣的活动：请帮忙研发一个名为“对联王”的微信小程序页面，文件名为\u0026#34;couplet\u0026#34;，目录为“pages/couplet”，实现以下功能： 1. 用户输入上联，或者用户输入创意，回车或点击发送按钮，生成下联或者创作一副对联 2. 回车后的函数名为`sendMessage`，这个函数逻辑由用户实现 3. 整个页面的布局与微信App的聊天界面一致，布局如下： - 用户输入的消息，头像在右边 - API返回的消息，头像在左边 - 每一条消息的下面，均展示收藏、复制和删除操作按钮，同时展示消息的时间，时间格式为`小时:分钟:秒钟`，如`12:07:05`。 提交后，等待 Cline 生成代码文件（一般为 3 个文件），对于每个文件代码差异，我们可以选择“Save”接受或者“Reject”操作。\n之后，我们切换回微信开发者工具界面，默认会自动刷新，或者按Ctrl + r强制刷新页面，即可查看最新的页面效果。\n如果页面布局和我们期望差异较大，那么我们继续使用 Cline 插件生成代码 如果页面布局和我们期望差不多，需要进行局部微调，就可以使用 Continue 插件进行部分调整 不断迭代上述过程，直到小程序完全符合预期为止。\n由于提示老牛同学提示词太多了，后面调整的提示词没有保存，就不一一展示了，期望大家均能快速研发出满意的小程序。\n4. 结语 最后，当我们使用外部 API 服务时，对于涉及敏感数据或有较高保密需求的项目，请务必注意保护用户隐私和数据安全。\n通过上述步骤，即使是没有专业背景的人也能轻松上手，借助现代 AI 工具的力量，快速且高效地开发出属于自己的微信小程序。\nTransformers 框架序列：\n01.包和对象加载中的设计巧思与实用技巧\n02.AutoModel 初始化及 Qwen2.5 模型加载全流程\n03.Qwen2.5 大模型的 AutoTokenizer 技术细节\n04.Qwen2.5/GPT 分词流程与 BPE 分词算法技术细节详解\n05.嵌入（Embedding）机制和 Word2Vec 实战\n06.位置嵌入（Positional Embedding）\nPipeline NLP 任务序列：\n零·概述 丨 01.文本转音频 丨 02.文本分类 丨 03.词元分类和命名实体识别 丨 04.问答 丨 05.表格问答 | 06.填充蒙版\n往期推荐文章：\n使用Cursor + Qwen2.5 大模型 零经验研发微信小程序：自由构建个性化节拍器应用实战\nBolt.new 用一句话快速构建全栈应用：本地部署与应用实战（Ollama/Qwen2.5 等）\n基于 Qwen2.5-Coder 模型和 CrewAI 多智能体框架，实现智能编程系统的实战教程\nvLLM CPU 和 GPU 模式署和推理 Qwen2 等大语言模型详细教程\n基于 Qwen2/Lllama3 等大模型，部署团队私有化 RAG 知识库系统的详细教程（Docker+AnythingLLM）\n使用 Llama3/Qwen2 等开源大模型，部署团队私有化 Code Copilot 和使用教程\n基于 Qwen2 大模型微调技术详细教程（LoRA 参数高效微调和 SwanLab 可视化监控）\nChatTTS 长音频合成和本地部署 2 种方式，让你的“儿童绘本”发声的实战教程\n","date":"2025-01-12T00:00:00Z","image":"https://ntopic.cn/p/2025011201/00_hu883d5e381d46374d51c67d277ad5095c_219525_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/2025011201/","title":"Cline 免费插件 + Qwen2.5 大模型，零经验也能开发“对联王”微信小程序"},{"content":"最近，小朋友开始学习小提琴，需要一个节拍器来帮助他练习。老牛同学下载了好几个节拍器应用，里面的广告实在是太多了，简直无从下手操作，就算老牛同学是慎之又慎，但还是中招，被运营商扣费。\n因此，老牛同学决定自己动手，定制一个专属的微信小程序节拍器。虽然老牛同学没有微信小程序的研发经验，但是借助强大的 AI 工具，最终，还是成功研发完成并上线了：\n【老牛同学小程序 · 节拍器 视频】\n1. 开发环境搭建 安装必要的开发工具 为了实现高效开发，我们选择了以下两个主要工具：\n微信开发者工具：用于实时预览和调试小程序，确保代码可以在微信环境中正常运行。 Cursor：这是一个集成大模型的智能代码编辑器，能够根据提示词自动生成或优化代码，大大提高了开发效率。 下载地址：\n微信开发者工具：官方下载链接 Cursor：官方网站 请务必完成注册并登录到 Cursor 平台，以便使用其全部功能。\n配置开发环境 微信开发者工具配置 启动微信开发者工具后，创建一个新的小程序项目。选择“不使用云服务”和“不适用模板”，以获得更高的灵活性。记得填写正确的AppID，这是与微信平台关联的关键信息。\n创建完成后，你将进入如下的研发和预览界面，准备好迎接下一步的挑战。\nCursor 工具配置 打开 Cursor，按照指引配置好Qwen2.5-Coder-32B大模型，并选择刚刚创建的小程序目录作为工作区。这样，你就可以在 Cursor 中看到与微信开发者工具一致的文件结构，方便进行同步操作。\n3. AI 辅助编程实战 现在，让我们一起探索如何利用 AI 的力量来加速开发进程。在 Cursor 中按下Ctrl + i，打开 Chat 面板，输入精心设计的 Prompt 提示词。以下是老牛同学首次尝试时所用的提示内容：\n1 2 3 4 5 6 7 你是一位资深的微信小程序开发者，擅长UI设计和交互逻辑构建。当前任务是在现有基础上完善一个名为“节拍器”的小程序，它允许用户调整BPM（每分钟拍数），并通过点击按钮控制节拍声的播放与停止。请根据以下要求继续完成页面布局和功能实现： - 页面标题：节拍器 - 内容布局： - 第一行：居中对齐，包括四个按钮（“-10”、“-5”、“+5”、“+10”）用于调节BPM值，默认值设为60。 - 第二行：显示计时器，格式为“小时:分:秒”。 - 第三行：“开始”按钮，点击后启动节拍声，频率由BPM值决定。 - 第四行：“停止”按钮，用于暂停节拍声。 提交后，等待 AI 生成代码片段，并通过Apply将其应用到项目中。之后，我们切换回微信开发者工具界面，默认会自动刷新，或者按Ctrl + r强制刷新页面，即可查看最新的页面效果。\n不断迭代这一过程，直到小程序完全符合预期为止。\n4. 结语 最后，当我们使用外部 API 服务时，对于涉及敏感数据或有较高保密需求的项目，请务必注意保护用户隐私和数据安全。\n通过上述步骤，即使是没有专业背景的人也能轻松上手，借助现代 AI 工具的力量，快速且高效地开发出属于自己的微信小程序。\nTransformers 框架序列：\n01.包和对象加载中的设计巧思与实用技巧\n02.AutoModel 初始化及 Qwen2.5 模型加载全流程\n03.Qwen2.5 大模型的 AutoTokenizer 技术细节\n04.Qwen2.5/GPT 分词流程与 BPE 分词算法技术细节详解\n05.嵌入（Embedding）机制和 Word2Vec 实战\n06.位置嵌入（Positional Embedding）\nPipeline NLP 任务序列：\n零·概述 丨 01.文本转音频 丨 02.文本分类 丨 03.词元分类和命名实体识别 丨 04.问答 丨 05.表格问答 | 06.填充蒙版\n往期推荐文章：\nBolt.new 用一句话快速构建全栈应用：本地部署与应用实战（Ollama/Qwen2.5 等）\n基于 Qwen2.5-Coder 模型和 CrewAI 多智能体框架，实现智能编程系统的实战教程\nvLLM CPU 和 GPU 模式署和推理 Qwen2 等大语言模型详细教程\n基于 Qwen2/Lllama3 等大模型，部署团队私有化 RAG 知识库系统的详细教程（Docker+AnythingLLM）\n使用 Llama3/Qwen2 等开源大模型，部署团队私有化 Code Copilot 和使用教程\n基于 Qwen2 大模型微调技术详细教程（LoRA 参数高效微调和 SwanLab 可视化监控）\nChatTTS 长音频合成和本地部署 2 种方式，让你的“儿童绘本”发声的实战教程\n","date":"2025-01-03T00:00:00Z","image":"https://ntopic.cn/p/2025010301/01_hu5b4a41f27c7fe518c1493f019fcc5463_79100_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/2025010301/","title":"使用Cursor + Qwen2.5 大模型 零经验研发微信小程序：自由构建个性化节拍器应用实战"},{"content":"在上一篇中，我们探讨了 词嵌入（Word Embedding） ，它根据词嵌入矩阵将文本序列转换为数值向量，使得计算机能够理解和处理自然语言。现在，让我们进一步了解位置嵌入（Positional Embedding），这是让 Transformer 模型“知晓”词语顺序的关键。\n1. 位置嵌入的作用 想象一下，如果我们只用词嵌入，那么无论一个词出现在句子的开头还是结尾，它的表示都是相同的。然而，在自然语言中，词语的位置往往影响其意义。例如，“苹果”在“我吃了一个苹果”和“苹果公司发布了新产品”这两个句子中的含义截然不同。因此，我们需要一种机制来告诉模型这些信息，这就是位置嵌入的作用。\n位置嵌入通过给每个词赋予一个与它在句子中位置相关的独特向量，使得模型不仅能够捕捉到词语的语义，还能理解它们之间的相对顺序，从而更好地建模句子结构和依赖关系。\n2. 位置嵌入的原理 为了让模型能够学习到位置信息，最直接的方法是为每个位置分配一个固定的、预定义的向量。在原始的 Transformer 模型中，位置嵌入是由正弦和余弦函数组成的，这样设计的原因在于它具有周期性，可以帮助模型处理比训练时更长的序列，同时保持一定的泛化能力。\n具体来说，对于模型维度 d 、位置 pos 和维度 i，位置嵌入 PE(pos, 2i)（偶数维）和 PE(pos, 2i+1) （奇数维）分别由以下公式计算：\n下面是位置嵌入计算的 Python 代码实现：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 import torch import torch.nn as nn import math class PositionalEncoding(nn.Module): def __init__(self, d_model, max_len=5000): super(PositionalEncoding, self).__init__() # 创建一个位置编码矩阵 [max_len, d_model] pe = torch.zeros(max_len, d_model) position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1) # [max_len, 1] div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) # [d_model/2] pe[:, 0::2] = torch.sin(position * div_term) # 偶数维 pe[:, 1::2] = torch.cos(position * div_term) # 奇数维 pe = pe.unsqueeze(0) # [1, max_len, d_model] self.register_buffer(\u0026#39;pe\u0026#39;, pe) # 不作为模型参数更新 def forward(self, x): seq_len = x.size(1) x = x + self.pe[:, :seq_len, :] return x 这段代码创建了一个PositionalEncoding类，用于生成位置嵌入，并将其添加到输入的词嵌入上。d_model是模型的维度，而max_len则是可以处理的最大序列长度。\n3. 词嵌入和位置嵌入的作用 为了更好地理解词嵌入和位置嵌入是如何协作的，我们以一句简单的英语句子为例：“The cat sat on the mat.”。首先，我们会将每个词转换成对应的词嵌入向量；然后，为每个词添加与其位置相关的位置嵌入；最后，我们将两者相加，得到最终的隐藏层输入向量。\n特别注意：\n为了方便演示，老牛同学此处简化为2 维，实际预训练模型的隐藏层远不止 2 维（如：Qwen2.5 有 1536 维）。 同时，我们把 Token 简化为单词，实际使用的分词算法，如 BPE 分词算法，Token 可能并不一定与单词相同。 步骤一：词嵌入 首先，我们需要将句子中的每个词转换为词嵌入，假设我们得到了如下简化版的词嵌入向量（实际预训练模型的维度远高于此）：\n1 2 3 4 5 6 W{The} = [0.1, 0.2] W{cat} = [0.3, 0.4] W{sat} = [0.5, 0.6] W{on} = [0.7, 0.8] W{the} = [0.9, 1.0] W{mat} = [1.1, 1.2] 步骤二：位置嵌入 接下来，我们需要为每个词添加位置嵌入。我们可以根据上述公式计算出每个位置的嵌入向量。假设我们得到了如下位置嵌入向量（同样简化为2 维）：\n1 2 3 4 5 6 P_0 = [0.0, 1.0] P_1 = [0.8, 0.6] P_2 = [0.5, 0.8] P_3 = [0.2, 0.9] P_4 = [0.9, 0.4] P_5 = [0.7, 0.2] 步骤三：词嵌入 + 位置嵌入 现在，我们将词嵌入和位置嵌入相加，得到最终的输入向量。这一步操作使得每个词的表示不仅包含了其语义信息，还包含了它在句子中的位置信息。具体来说，我们有：\n1 2 3 4 5 6 X{The} = W{The} + P_0 = [0.1, 0.2] + [0.0, 1.0] = [0.1, 1.2] X{cat} = W{cat} + P_1 = [0.3, 0.4] + [0.8, 0.6] = [1.1, 1.0] X{sat} = W{sat} + P_2 = [0.5, 0.6] + [0.5, 0.8] = [1.0, 1.4] X{on} = W{on} + P_3 = [0.7, 0.8] + [0.2, 0.9] = [0.9, 1.7] X{the} = W{the} + P_4 = [0.9, 1.0] + [0.9, 0.4] = [1.8, 1.4] X{mat} = W{mat} + P_5 = [1.1, 1.2] + [0.7, 0.2] = [1.8, 1.4] 步骤四：隐藏层的输入 最终，这些带有位置信息的词嵌入向量 XThe, Xcat, Xsat, Xon, Xthe, Xmat 将作为 Transformer 模型的隐藏层的输入。通过这种方式，模型不仅能够理解每个词的语义，还能捕捉到它们在句子中的相对位置，从而更好地建模句子的结构和依赖关系。\n4. 总结 位置嵌入是现代 NLP 模型中不可或缺的一部分，它使得模型能够理解词语的顺序，进而提升对文本的理解能力。通过引入位置嵌入，Transformer 架构克服了传统自注意力机制对词序“不可知”的局限，为各种自然语言处理任务提供了强有力的支持。\n希望这篇文章能帮助你更深入地理解位置嵌入及其在 Transformer 模型中的作用。如果你还有任何疑问或想要了解更多细节，请随时留言交流！\nTransformers 框架序列：\n01.包和对象加载中的设计巧思与实用技巧\n02.AutoModel 初始化及 Qwen2.5 模型加载全流程\n03.Qwen2.5 大模型的 AutoTokenizer 技术细节\n04.Qwen2.5/GPT 分词流程与 BPE 分词算法技术细节详解\n05.嵌入（Embedding）机制和 Word2Vec 实战\nPipeline NLP 任务序列：\n零·概述 丨 01.文本转音频 丨 02.文本分类 丨 03.词元分类和命名实体识别 丨 04.问答 丨 05.表格问答 | 06.填充蒙版\n往期推荐文章：\nBolt.new 用一句话快速构建全栈应用：本地部署与应用实战（Ollama/Qwen2.5 等）\n基于 Qwen2.5-Coder 模型和 CrewAI 多智能体框架，实现智能编程系统的实战教程\nvLLM CPU 和 GPU 模式署和推理 Qwen2 等大语言模型详细教程\n基于 Qwen2/Lllama3 等大模型，部署团队私有化 RAG 知识库系统的详细教程（Docker+AnythingLLM）\n使用 Llama3/Qwen2 等开源大模型，部署团队私有化 Code Copilot 和使用教程\n基于 Qwen2 大模型微调技术详细教程（LoRA 参数高效微调和 SwanLab 可视化监控）\nChatTTS 长音频合成和本地部署 2 种方式，让你的“儿童绘本”发声的实战教程\n","date":"2024-12-27T00:00:00Z","image":"https://ntopic.cn/p/2024122701/00_huf0c8e788e888de2aa4eff4cdd05ec712_142546_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/2024122701/","title":"深度解析 Transformer 模型中的位置嵌入（Positional Embedding）"},{"content":"在自然语言处理（NLP）领域，填空或填补句子中的缺失部分是一项重要的任务。它不仅考验了模型对语言的理解能力，还展示了其生成合理且语义连贯的文本的能力。Hugging Face 的 Transformers 框架通过其 Pipeline API 提供了强大的fill-mask功能，允许开发者和研究者轻松构建并应用这种填空技术。本文将详细介绍 Transformers 框架中的fill-mask任务，涵盖任务描述、应用场景、配置指南以及实战案例。\n1. 任务简介 fill-mask任务旨在根据上下文预测句子中被遮蔽（mask）的词汇。例如：\n文学创作：当给出“[MASK] is the capital of France.”时，系统应当能够预测出“Paris”。 语言学习：对于学习新语言的学生来说，可以用来练习语法结构和词汇选择，如“Je [MASK] à l\u0026rsquo;école tous les jours.”（我每天都去学校）。 为了完成这一任务，通常需要以下步骤：\n输入解析：识别句子中被标记为[MASK]的位置。 上下文理解：分析句子的上下文，包括前后词语的关系、句法结构等。 词汇预测：基于对上下文的理解，从词汇表中选出最合适的词来填补空白。 根据 Hugging Face 官网的数据，当前已有13,576 个预训练模型支持fill-mask任务，这些模型已经在大量的文本数据上进行了微调，以更好地适应特定的任务需求。\n2. 应用场景 填充蒙版任务的应用非常广泛，以下是几个典型的应用实例：\n机器翻译：帮助改进翻译质量，确保译文更加流畅自然。 文本补全：用于自动完成功能，提高用户输入效率，如在搜索引擎或聊天机器人中。 教育工具：作为语言学习辅助工具，增强学生对单词和短语的记忆。 内容推荐：根据用户的阅读历史，预测他们可能感兴趣的主题词，从而提供个性化的内容推荐。 智能写作助手：协助作家克服创作瓶颈，提供灵感启发，比如续写故事或完善段落。 3. 任务配置 在 Transformers 框架中，fill-mask任务的配置同样直观易懂。下面是配置示例代码片段（位于./transformers/pipelines/__init__.py文件）：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 SUPPORTED_TASKS = { # 其他省略...... \u0026#34;fill-mask\u0026#34;: { \u0026#34;impl\u0026#34;: FillMaskPipeline, \u0026#34;tf\u0026#34;: (TFAutoModelForMaskedLM,) if is_tf_available() else (), \u0026#34;pt\u0026#34;: (AutoModelForMaskedLM,) if is_torch_available() else (), \u0026#34;default\u0026#34;: { \u0026#34;model\u0026#34;: { \u0026#34;pt\u0026#34;: (\u0026#34;distilbert/distilroberta-base\u0026#34;, \u0026#34;fb53ab8\u0026#34;), \u0026#34;tf\u0026#34;: (\u0026#34;distilbert/distilroberta-base\u0026#34;, \u0026#34;fb53ab8\u0026#34;), } }, \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, }, # 其他省略...... } 默认情况下，Transformers 框架会使用distilroberta-base作为fill-mask任务的预训练模型。这个模型是基于 BERT 架构，在大规模英语语料库上训练而成，特别适合英文的填空任务。\n4. 填充蒙版实战 确保您已经安装了 Transformers 库和其他必要的依赖包：\n1 pip install transformers torch 方法一：自动下载模型 如果您的网络环境允许，可以直接从 Hugging Face 下载模型并创建 Pipeline 任务。如下老牛同学使用 Hugging Face 镜像网站自动下载模型：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 import os # 设置代理 os.environ[\u0026#34;HF_ENDPOINT\u0026#34;] = \u0026#34;https://hf-mirror.com\u0026#34; # 设置本地缓存目录 cache_dir = os.path.join(\u0026#39;D:\u0026#39;, os.path.sep, \u0026#39;ModelSpace\u0026#39;, \u0026#39;Cache\u0026#39;) os.environ[\u0026#39;HF_HOME\u0026#39;] = cache_dir from transformers import pipeline # 创建Pipeline任务 nlp = pipeline(\u0026#34;fill-mask\u0026#34;, model=\u0026#34;google-bert/bert-base-chinese\u0026#34;) # 执行填充蒙版任务 if __name__ == \u0026#34;__main__\u0026#34;: result = nlp(\u0026#34;生活的真谛是[MASK]。\u0026#34;) print(result) 输出结果直接给出了模型认为的最佳答案文本，在这个例子中：输入是生活的真谛是[MASK]。，给出了多个输出：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 [ { \u0026#39;score\u0026#39;: 0.34068214893341064, \u0026#39;token\u0026#39;: 5401, \u0026#39;token_str\u0026#39;: \u0026#39;美\u0026#39;, \u0026#39;sequence\u0026#39;: \u0026#39;生 活 的 真 谛 是 美 。\u0026#39; }, { \u0026#39;score\u0026#39;: 0.22919395565986633, \u0026#39;token\u0026#39;: 4263, \u0026#39;token_str\u0026#39;: \u0026#39;爱\u0026#39;, \u0026#39;sequence\u0026#39;: \u0026#39;生 活 的 真 谛 是 爱 。\u0026#39; }, { \u0026#39;score\u0026#39;: 0.03254768252372742, \u0026#39;token\u0026#39;: 727, \u0026#39;token_str\u0026#39;: \u0026#39;乐\u0026#39;, \u0026#39;sequence\u0026#39;: \u0026#39;生 活 的 真 谛 是 乐 。\u0026#39; }, { \u0026#39;score\u0026#39;: 0.022956570610404015, \u0026#39;token\u0026#39;: 782, \u0026#39;token_str\u0026#39;: \u0026#39;人\u0026#39;, \u0026#39;sequence\u0026#39;: \u0026#39;生 活 的 真 谛 是 人 。\u0026#39; }, { \u0026#39;score\u0026#39;: 0.01942041702568531, \u0026#39;token\u0026#39;: 8038, \u0026#39;token_str\u0026#39;: \u0026#39;：\u0026#39;, \u0026#39;sequence\u0026#39;: \u0026#39;生 活 的 真 谛 是 ： 。\u0026#39; } ] 对于输出结果对象的元素，详细说明如下：\nsequence：完整填充后的句子。 score：模型对该填充选项的信心程度，值越大表示越有信心。 token：填充词汇对应的 token ID，在词汇表中的位置。 token_str：填充词汇内容。 方法二：自主下载模型 如果您希望使用本地模型文件，可以按照以下步骤操作：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 from transformers import AutoTokenizer, AutoModelForMaskedLM, pipeline import os # 下载模型目录 model_dir = \u0026#39;/models/fill_mask\u0026#39; # 加载分词器和模型 tokenizer = AutoTokenizer.from_pretrained(model_dir, local_files_only=True) model = AutoModelForMaskedLM.from_pretrained(model_dir, torch_dtype=\u0026#34;auto\u0026#34;, device_map=\u0026#34;auto\u0026#34;, local_files_only=True) # 创建Pipeline任务 nlp = pipeline(\u0026#34;fill-mask\u0026#34;, tokenizer=tokenizer, model=model) # 后续用法与自动下载相同 5. WebUI 界面 为了让fill-mask Pipeline 任务更易于使用或演示，我们可以使用 Gradio 创建一个直观的 WebUI 界面。以下是创建界面的代码示例：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 import sys # 直接复用Pipeline实例 sys.path.append(\u0026#34;./\u0026#34;) pipeline = __import__(\u0026#34;06-fill-mask\u0026#34;) import gradio as gr # 定义填充蒙版函数 def fill_mask(sentence): result = pipeline.nlp(sentence) return \u0026#34;\\n\u0026#34;.join([f\u0026#34;可能的答案: {r[\u0026#39;sequence\u0026#39;]} (得分: {r[\u0026#39;score\u0026#39;]:.2f})\u0026#34; for r in result]) # 创建Gradio界面 with gr.Blocks() as demo: gr.Markdown(\u0026#34;# 填充蒙版系统\u0026#34;) gr.Markdown( \u0026#34;这是一个基于Transformers框架的填充蒙版工具。您可以输入一句含有[MASK]标记的句子，点击“提交”按钮后，系统将尝试预测最适合填入该位置的词汇。\u0026#34;) with gr.Row(): input_sentence = gr.Textbox(placeholder=\u0026#34;请输入含有[MASK]标记的句子...\u0026#34;, label=\u0026#34;句子\u0026#34;) with gr.Row(): submit_button = gr.Button(\u0026#34;提交\u0026#34;) with gr.Row(): output_answer = gr.Label(label=\u0026#34;可能的答案\u0026#34;) # 设置按钮点击事件，触发填充蒙版函数 submit_button.click(fill_mask, inputs=[input_sentence], outputs=output_answer) # 启动Gradio应用 if __name__ == \u0026#34;__main__\u0026#34;: demo.launch() 运行上述代码，我们可以看到 URL 信息：\n1 2 3 * Running on local URL: http://127.0.0.1:7860 To create a public link, set `share=True` in `launch()`. 通过浏览器打开地址，就可以通过可视化的方式进行填充蒙版实验了。\n我们可以用如下句子样例来测试 Fill Mask：\n输入：“生活的真谛是[MASK]。” 答案有：“美”、“爱”、“乐”、“人”等 如果想要分享给更多人，可以在 launch() 函数中设置 share=True 参数，Gradio 将生成一个公开链接供他人访问。\n6. 总结 本文全面介绍了 Transformers 框架中的fill-mask任务，从基础概念到实际应用，再到如何构建和部署一个交互式的填充蒙版系统。无论是在机器翻译、文本补全还是教育工具等领域，Transformers 框架都能为我们提供强有力的支持，助力我们快速实现高效的填充蒙版解决方案。\n老牛同学将持续为大家带来更多有关 Pipeline 任务的文章（共28篇）。如果您有任何疑问或建议，请随时在评论区留言交流！\nTransformers 框架：\n01.包和对象加载中的设计巧思与实用技巧\n02.AutoModel 初始化及 Qwen2.5 模型加载全流程\n03.Qwen2.5 大模型的 AutoTokenizer 技术细节\n04.Qwen2.5/GPT 分词流程与 BPE 分词算法技术细节\n05.嵌入（Embedding）机制和 Word2Vec 实战\nPipeline NLP 任务：\n零·概述 丨 01.文本转音频 丨 02.文本分类 丨 03.词元分类和命名实体识别 丨 04.问答 丨 05.表格问答\n往期推荐文章：\nBolt.new 用一句话快速构建全栈应用：本地部署与应用实战（Ollama/Qwen2.5 等）\n基于 Qwen2.5-Coder 模型和 CrewAI 多智能体框架，实现智能编程系统的实战教程\nvLLM CPU 和 GPU 模式署和推理 Qwen2 等大语言模型详细教程\n基于 Qwen2/Lllama3 等大模型，部署团队私有化 RAG 知识库系统的详细教程（Docker+AnythingLLM）\n使用 Llama3/Qwen2 等开源大模型，部署团队私有化 Code Copilot 和使用教程\n基于 Qwen2 大模型微调技术详细教程（LoRA 参数高效微调和 SwanLab 可视化监控）\nChatTTS 长音频合成和本地部署 2 种方式，让你的“儿童绘本”发声的实战教程\n","date":"2024-12-25T00:00:00Z","image":"https://ntopic.cn/p/2024122501/00_hu9ae0166db6d19f69e9ac797994b0da33_118198_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/2024122501/","title":"Transformers 框架 Pipeline 任务详解（六）：填充蒙版（fill-mask）"},{"content":"在自然语言处理领域，表格问答是一项能够从结构化数据中提取信息的关键技术。它结合了自然语言理解和表格数据处理的能力，使得用户可以通过自然语言提问来获取表格中的特定信息。Hugging Face 的 Transformers 框架通过其 Pipeline API 提供了强大的table-question-answering功能，允许开发者和研究者轻松构建高效的表格问答系统。本文将详细介绍 Transformers 框架中的table-question-answering任务，涵盖任务描述、应用场景、配置指南以及实战案例。\n1. 任务简介 table-question-answering任务旨在根据用户提供的问题，从给定的表格数据中找出最准确的答案。例如：\n财务分析：询问“2023 年第四季度的净利润是多少？”，系统应当能够从财务报表中找到对应数值。 体育统计：当问到“谁是 2022 年世界杯的最佳射手？”时，系统应能回答出具体球员的名字。 为了完成这一任务，通常需要以下步骤：\n问题解析：理解问题的意图，并确定所需的数据字段。 表格分析：识别表格中的行、列以及可能存在的关联关系。 答案生成：基于对表格的理解，生成或抽取符合问题要求的答案。 根据 Hugging Face 官网的数据，当前已有133个table-question-answering模型，这些模型已经在大量的表格数据上进行了微调，以更好地适应特定的任务需求。\n2. 应用场景 表格问答系统广泛应用于各个行业，以下是几个典型的应用实例：\n商业智能：企业可以利用表格问答工具快速查询销售数据、市场趋势等关键指标，辅助决策制定。 教育辅导：教师可以使用该技术帮助学生理解复杂的数据集，如历史事件的时间线、科学实验结果等。 金融分析：投资者和分析师可以借助表格问答系统分析股票行情、公司财报等信息，支持投资决策。 医疗健康：研究人员和医护人员可以利用表格问答来查找临床试验结果、药物效果等医学数据。 政府服务：政府部门可以通过表格问答提供公共服务信息，如人口统计数据、公共支出明细等。 3. 任务配置 在 Transformers 框架中，table-question-answering任务的配置同样直观易懂。下面是配置示例代码片段（位于./transformers/pipelines/__init__.py文件）：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 SUPPORTED_TASKS = { # 其他省略...... \u0026#34;table-question-answering\u0026#34;: { \u0026#34;impl\u0026#34;: TableQuestionAnsweringPipeline, \u0026#34;pt\u0026#34;: (AutoModelForTableQuestionAnswering,) if is_torch_available() else (), \u0026#34;tf\u0026#34;: (TFAutoModelForTableQuestionAnswering,) if is_tf_available() else (), \u0026#34;default\u0026#34;: { \u0026#34;model\u0026#34;: { \u0026#34;pt\u0026#34;: (\u0026#34;google/tapas-base-finetuned-wtq\u0026#34;, \u0026#34;e3dde19\u0026#34;), \u0026#34;tf\u0026#34;: (\u0026#34;google/tapas-base-finetuned-wtq\u0026#34;, \u0026#34;e3dde19\u0026#34;), }, }, \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, }, # 其他省略...... } 默认情况下，Transformers 框架会使用tapas-base-finetuned-wtq作为table-question-answering任务的预训练模型。这个模型是在 WebTables 数据集上进行过微调的 TAPAS 版本，特别适用于英文的表格问答任务。\n4. 表格问答实战 确保您已经安装了 Transformers 库和其他必要的依赖包：\n1 pip install transformers torch 方法一：自动下载模型 如果您的网络环境允许，可以直接从 Hugging Face 下载模型并创建 Pipeline 任务。如下老牛同学使用 Hugging Face 镜像网站自动下载模型：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 import os # 设置代理 os.environ[\u0026#34;HF_ENDPOINT\u0026#34;] = \u0026#34;https://hf-mirror.com\u0026#34; # 设置本地缓存目录 cache_dir = os.path.join(\u0026#39;D:\u0026#39;, os.path.sep, \u0026#39;ModelSpace\u0026#39;, \u0026#39;Cache\u0026#39;) os.environ[\u0026#39;HF_HOME\u0026#39;] = cache_dir from transformers import pipeline # 创建Pipeline任务 nlp = pipeline(\u0026#34;table-question-answering\u0026#34;, model=\u0026#34;google/tapas-base-finetuned-wtq\u0026#34;) # 执行表格问答任务 if __name__ == \u0026#34;__main__\u0026#34;: # 表格数据 table = [ {\u0026#34;Year\u0026#34;: \u0026#34;2020\u0026#34;, \u0026#34;Quarter\u0026#34;: \u0026#34;Q1\u0026#34;, \u0026#34;Revenue\u0026#34;: \u0026#34;500M\u0026#34;, \u0026#34;Profit\u0026#34;: \u0026#34;100M\u0026#34;}, {\u0026#34;Year\u0026#34;: \u0026#34;2020\u0026#34;, \u0026#34;Quarter\u0026#34;: \u0026#34;Q2\u0026#34;, \u0026#34;Revenue\u0026#34;: \u0026#34;550M\u0026#34;, \u0026#34;Profit\u0026#34;: \u0026#34;110M\u0026#34;}, {\u0026#34;Year\u0026#34;: \u0026#34;2020\u0026#34;, \u0026#34;Quarter\u0026#34;: \u0026#34;Q3\u0026#34;, \u0026#34;Revenue\u0026#34;: \u0026#34;600M\u0026#34;, \u0026#34;Profit\u0026#34;: \u0026#34;120M\u0026#34;}, {\u0026#34;Year\u0026#34;: \u0026#34;2020\u0026#34;, \u0026#34;Quarter\u0026#34;: \u0026#34;Q4\u0026#34;, \u0026#34;Revenue\u0026#34;: \u0026#34;650M\u0026#34;, \u0026#34;Profit\u0026#34;: \u0026#34;130M\u0026#34;}, # 更多行... ] # 执行任务 result = nlp(table=table, query=\u0026#34;What was the profit in Q4 of 2020?\u0026#34;) print(result) # 输出：{\u0026#39;answer\u0026#39;: \u0026#39;130M\u0026#39;, \u0026#39;coordinates\u0026#39;: [(3, 3)], \u0026#39;cells\u0026#39;: [\u0026#39;130M\u0026#39;], \u0026#39;aggregator\u0026#39;: \u0026#39;NONE\u0026#39;} 输出结果如下：\n1 {\u0026#39;answer\u0026#39;: \u0026#39;130M\u0026#39;, \u0026#39;coordinates\u0026#39;: [(3, 3)], \u0026#39;cells\u0026#39;: [\u0026#39;130M\u0026#39;], \u0026#39;aggregator\u0026#39;: \u0026#39;NONE\u0026#39;} 输出结果直接给出了模型认为的答案文本，在这个例子中，答案是“130M”。\n对于输出结果对象的元素，详细说明如下：\nanswer：模型给出的答案文本，在这个例子中是“130M”，表示 2020 年第四季度的利润。 coordinates：是一个列表，其中每个元素是一个元组，代表了表格中与答案相关的单元格的位置。元组的第一个值是行索引，第二个值是列索引，都是从 0 开始计数。在这个例子中，(3, 3)意味着答案位于表格的第 4 行（因为索引从 0 开始，所以 3 代表的是第四行）和第 4 列。 cells：列出所有参与构建答案的单元格内容。在这个例子中，只有单个单元格的内容“130M”被用作答案，因此列表中只有一个元素。 aggregator：指明了是否以及如何对多个单元格或数值进行了聚合操作以生成答案。可能的值包括但不限于\u0026quot;NONE\u0026quot;、\u0026quot;SUM\u0026quot;、\u0026quot;AVERAGE\u0026quot;、\u0026quot;COUNT\u0026quot;等。在这个例子中，\u0026quot;NONE\u0026quot;表示没有执行任何聚合操作，答案直接来自单个单元格的内容。 方法二：自主下载模型 如果您希望使用本地模型文件，可以按照以下步骤操作：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 from transformers import AutoTokenizer, AutoModelForTableQuestionAnswering, pipeline import os # 下载模型目录 model_dir = \u0026#39;/models/table_question_answering\u0026#39; # 加载分词器和模型 tokenizer = AutoTokenizer.from_pretrained(model_dir, local_files_only=True) model = AutoModelForTableQuestionAnswering.from_pretrained(model_dir, torch_dtype=\u0026#34;auto\u0026#34;, device_map=\u0026#34;auto\u0026#34;, local_files_only=True) # 创建Pipeline任务 nlp = pipeline(\u0026#34;table-question-answering\u0026#34;, tokenizer=tokenizer, model=model) # 后续用法与自动下载相同 5. WebUI 界面 为了使table-question-answering Pipeline 任务更易于使用或演示，我们可以使用 Gradio 创建一个直观的 WebUI 界面。以下是创建界面的代码示例：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 import sys # 直接复用Pipeline实例 sys.path.append(\u0026#34;./\u0026#34;) pipeline = __import__(\u0026#34;05-table-question-answering\u0026#34;) import gradio as gr # 定义表格问答函数 def answer_table_question(query, table): # 将字符串形式的表格转换为列表字典 table_list = [dict(zip(table.split(\u0026#39;\\n\u0026#39;)[0].split(\u0026#39;,\u0026#39;), row.split(\u0026#39;,\u0026#39;))) for row in table.split(\u0026#39;\\n\u0026#39;)[1:]] result = pipeline.nlp(query=query, table=table_list) return f\u0026#34;答案: {result[\u0026#39;answer\u0026#39;]}\u0026#34; # 创建Gradio界面 with gr.Blocks() as demo: gr.Markdown(\u0026#34;# 表格问答系统\u0026#34;) gr.Markdown( \u0026#34;这是一个基于Transformers框架的表格问答工具。您可以输入一个问题和一段CSV格式的表格文本，点击“提交”按钮后，系统将尝试从中找到答案。\u0026#34;) with gr.Row(): input_table = gr.Textbox(placeholder=\u0026#34;请输入表格数据（CSV格式）...\u0026#34;, label=\u0026#34;表格\u0026#34;) with gr.Row(): input_query = gr.Textbox(placeholder=\u0026#34;请输入您的问题...\u0026#34;, label=\u0026#34;问题\u0026#34;) with gr.Row(): submit_button = gr.Button(\u0026#34;提交\u0026#34;) with gr.Row(): output_answer = gr.Label(label=\u0026#34;答案\u0026#34;) # 设置按钮点击事件，触发表格问答函数 submit_button.click(answer_table_question, inputs=[input_query, input_table], outputs=output_answer) # 启动Gradio应用 if __name__ == \u0026#34;__main__\u0026#34;: demo.launch() 运行上述代码，我们可以看到 URL 信息：\n1 2 3 * Running on local URL: http://127.0.0.1:7860 To create a public link, set `share=True` in `launch()`. 通过浏览器打开地址，就可以通过可视化的方式进行表格问答了。\n我们可以用如下表格样例数据来测试 Table QA：\n1 2 3 4 5 6 7 8 9 Year,Quarter,Revenue,Profit 2020,Q1,500M,100M 2020,Q2,550M,110M 2020,Q3,600M,120M 2020,Q4,650M,130M 2021,Q1,700M,140M 2021,Q2,750M,150M 2021,Q3,800M,160M 2021,Q4,850M,170M 问题：Which quarter had the highest profit in 2021?\n预期答案：Q4 (因为 2021 年第四季度的利润最高，为 170M)\n如果想要分享给更多人，可以在 launch() 函数中设置 share=True 参数，Gradio 将生成一个公开链接供他人访问。\n6. 总结 本文全面介绍了 Transformers 框架中的table-question-answering任务，从基础概念到实际应用，再到如何构建和部署一个交互式的表格问答系统。无论是在商业智能、教育辅导还是金融分析等领域，Transformers 框架都能为我们提供强有力的支持，助力我们快速实现高效的表格问答解决方案。\n老牛同学将持续为大家带来更多有关 Pipeline 任务的文章（共28篇）。如果您有任何疑问或建议，请随时在评论区留言交流！\nTransformers 框架：\n01.包和对象加载中的设计巧思与实用技巧\n02.AutoModel 初始化及 Qwen2.5 模型加载全流程\n03.Qwen2.5 大模型的 AutoTokenizer 技术细节\n04.Qwen2.5/GPT 分词流程与 BPE 分词算法技术细节\n05.嵌入（Embedding）机制和 Word2Vec 实战\nPipeline NLP 任务：\n零·概述 丨 01.文本转音频 丨 02.文本分类 丨 03.词元分类和命名实体识别 丨 04.问答\n往期推荐文章：\nBolt.new 用一句话快速构建全栈应用：本地部署与应用实战（Ollama/Qwen2.5 等）\n基于 Qwen2.5-Coder 模型和 CrewAI 多智能体框架，实现智能编程系统的实战教程\nvLLM CPU 和 GPU 模式署和推理 Qwen2 等大语言模型详细教程\n基于 Qwen2/Lllama3 等大模型，部署团队私有化 RAG 知识库系统的详细教程（Docker+AnythingLLM）\n使用 Llama3/Qwen2 等开源大模型，部署团队私有化 Code Copilot 和使用教程\n基于 Qwen2 大模型微调技术详细教程（LoRA 参数高效微调和 SwanLab 可视化监控）\nChatTTS 长音频合成和本地部署 2 种方式，让你的“儿童绘本”发声的实战教程\n","date":"2024-12-22T00:00:00Z","image":"https://ntopic.cn/p/2024122201/00_hua2f92f56a371a9250c7ce7c705f4b06c_77473_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/2024122201/","title":"Transformers 框架 Pipeline 任务详解（五）：表格问答（table-question-answering）"},{"content":"在自然语言处理领域，问答系统是一项关键的技术，它旨在根据给定的问题从文本中找到最准确的答案。借助 Hugging Face 的 Transformers 框架的 Pipeline API，我们可以快速地搭建一个强大的问答系统，而无需深入理解背后的复杂模型结构和算法细节。本文将详细探讨 Transformers 框架中的question-answering任务，包括任务描述、应用场景、配置指南以及实战案例。\n1. 任务简介 question-answering任务的目标是，对于一个给定的问题，从上下文中抽取最合适的答案。例如：\n信息检索：用户询问“谁是《哈利·波特》的作者？”，系统应当能够从提供的文本或文档集合中找出答案“J.K.罗琳”。 知识查询：当问到“太阳是由什么组成的？”时，系统应能回答出“氢和氦”。 为了完成这一任务，通常需要以下步骤：\n问题解析：理解问题的意图和要求。 上下文分析：识别并理解包含潜在答案的文本段落。 答案提取：从上下文中定位并提取最符合问题的答案。 根据 Hugging Face 官网的数据，当前已有超过12,624个question-answering模型，这些模型已经在大量的文本数据上进行了微调，以更好地适应特定的任务需求：\n2. 应用场景 问答系统广泛应用于多个行业和领域，以下是几个典型的应用实例：\n智能助手：像 Siri、Alexa 和 Google Assistant 这样的虚拟助手可以使用问答技术来响应用户的提问，提供即时的帮助和信息。 教育辅导：在线学习平台可以集成问答功能，帮助学生解决学习过程中遇到的问题，或者为教师提供教学辅助。 客户服务：企业可以通过聊天机器人自动回复客户咨询，提高服务效率，降低运营成本。 医疗健康：通过问答系统，患者可以获得关于疾病预防、治疗方案等基本医疗信息，促进自我健康管理。 法律咨询：律师或法律顾问可以利用问答工具快速查找相关的法律法规和案例，提升工作效率。 3. 任务配置 在 Transformers 框架中，question-answering任务的配置相对简单。下面是从源代码中摘录的相关配置示例（位于./transformers/pipelines/__init__.py文件）：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 SUPPORTED_TASKS = { # 其他省略...... \u0026#34;question-answering\u0026#34;: { \u0026#34;impl\u0026#34;: QuestionAnsweringPipeline, \u0026#34;tf\u0026#34;: (TFAutoModelForQuestionAnswering,) if is_tf_available() else (), \u0026#34;pt\u0026#34;: (AutoModelForQuestionAnswering,) if is_torch_available() else (), \u0026#34;default\u0026#34;: { \u0026#34;model\u0026#34;: { \u0026#34;pt\u0026#34;: (\u0026#34;distilbert/distilbert-base-cased-distilled-squad\u0026#34;, \u0026#34;564e9b5\u0026#34;), \u0026#34;tf\u0026#34;: (\u0026#34;distilbert/distilbert-base-cased-distilled-squad\u0026#34;, \u0026#34;564e9b5\u0026#34;), }, }, \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, }, # 其他省略...... } 默认情况下，Transformers 框架会使用distilbert-base-cased-distilled-squad作为question-answering任务的预训练模型。这个模型是在 SQuAD 数据集上进行过微调的 DistilBERT 版本，特别适用于英文的问答任务。\n4. 问答实战 确保您已经安装了 Transformers 库和其他必要的依赖包：\n1 pip install transformers torch 方法一：自动下载模型 如果您的网络环境允许，可以直接从 Hugging Face 下载模型并创建 Pipeline 任务。如下老牛同学使用 Hugging Face 镜像网站自动下载模型：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 import os # 设置代理 os.environ[\u0026#34;HF_ENDPOINT\u0026#34;] = \u0026#34;https://hf-mirror.com\u0026#34; # 设置本地缓存目录 cache_dir = os.path.join(\u0026#39;D:\u0026#39;, os.path.sep, \u0026#39;ModelSpace\u0026#39;, \u0026#39;Cache\u0026#39;) os.environ[\u0026#39;HF_HOME\u0026#39;] = cache_dir from transformers import pipeline # 创建Pipeline任务 nlp = pipeline(\u0026#34;question-answering\u0026#34;, model=\u0026#34;distilbert/distilbert-base-cased-distilled-squad\u0026#34;) # 执行问答任务 if __name__ == \u0026#34;__main__\u0026#34;: # 上下文 context = r\u0026#34;\u0026#34;\u0026#34; Extractive Question Answering is the task of extracting an answer from a text given a question. An example of a question answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune a model on a SQuAD task, you may leverage the examples/pytorch/question-answering/run_squad.py script. \u0026#34;\u0026#34;\u0026#34; # 执行任务 result = nlp(context=context, question=\u0026#34;What is a good example of a question answering dataset?\u0026#34;) print(result) # 输出：{\u0026#39;score\u0026#39;: 0.5152314901351929, \u0026#39;start\u0026#39;: 155, \u0026#39;end\u0026#39;: 168, \u0026#39;answer\u0026#39;: \u0026#39;SQuAD dataset\u0026#39;} 输出结果包含了四个关键部分：\nscore: 表示模型对于所找到答案的信心程度，范围从 0 到 1。数值越接近 1，意味着模型对这个答案的准确性越有信心。在这个例子中，score: 0.5152314901351929表明模型比较确信“SQuAD dataset”是正确答案。\nstart: 指定了答案在上下文字符串中的起始位置（以字符为单位）。例如，在提供的上下文中，“SQuAD dataset”的第一个字符位于索引 155 处。注意，这个索引是从 0 开始计数的，也就是说，文本的第一个字符的位置是 0。\nend: 给出了答案在上下文字符串中的结束位置（同样以字符为单位），但需要注意的是，这个索引是指答案最后一个字符之后的位置。\nanswer: 给出了模型认为的答案文本。在这个例子中，答案是“SQuAD dataset”，即数据集样本。\n方法二：自主下载模型 如果您希望使用本地模型文件，可以按照以下步骤操作：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 from transformers import AutoTokenizer, AutoModelForQuestionAnswering, pipeline import os # 下载模型目录 model_dir = \u0026#39;/models/question_answering\u0026#39; # 加载分词器和模型 tokenizer = AutoTokenizer.from_pretrained(model_dir, local_files_only=True) model = AutoModelForQuestionAnswering.from_pretrained(model_dir, torch_dtype=\u0026#34;auto\u0026#34;, device_map=\u0026#34;auto\u0026#34;, local_files_only=True) # 创建Pipeline任务 nlp = pipeline(\u0026#34;question-answering\u0026#34;, tokenizer=tokenizer, model=model) # 后续用法与自动下载相同 5. WebUI 界面 为了能使用question-answering Pipeline 任务的体验更好，或者进行模型演示，我们可以使用 Gradio 创建一个直观的 WebUI 界面。以下是创建界面的代码示例：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 import sys # 直接复用Pipeline实例 sys.path.append(\u0026#34;./\u0026#34;) pipeline = __import__(\u0026#34;04-question-answering\u0026#34;) import gradio as gr # 定义问答函数 def answer_question(question, context): result = pipeline.nlp(question=question, context=context) return f\u0026#34;答案: {result[\u0026#39;answer\u0026#39;]}, 置信度: {result[\u0026#39;score\u0026#39;]:.4f}\u0026#34; # 创建Gradio界面 with gr.Blocks() as demo: gr.Markdown(\u0026#34;# 问答系统\u0026#34;) gr.Markdown( \u0026#34;这是一个基于Transformers框架的问答工具。您可以输入一个问题和一段文本，点击“提交”按钮后，系统将尝试从中找到答案。\u0026#34;) with gr.Row(): input_context = gr.Textbox(placeholder=\u0026#34;请输入相关文本...\u0026#34;, label=\u0026#34;上下文\u0026#34;) with gr.Row(): input_question = gr.Textbox(placeholder=\u0026#34;请输入您的问题...\u0026#34;, label=\u0026#34;问题\u0026#34;) with gr.Row(): submit_button = gr.Button(\u0026#34;提交\u0026#34;) with gr.Row(): output_answer = gr.Label(label=\u0026#34;答案\u0026#34;) # 设置按钮点击事件，触发问答函数 submit_button.click(answer_question, inputs=[input_question, input_context], outputs=output_answer) # 启动Gradio应用 if __name__ == \u0026#34;__main__\u0026#34;: demo.launch() 运行上述代码，我们可以看到 URL 信息：\n1 2 3 * Running on local URL: http://127.0.0.1:7860 To create a public link, set `share=True` in `launch()`. 通过浏览器打开地址，就可以通过可视化的方式进行词元分类了：\n如果想要分享给更多人，可以在 launch() 函数中设置 share=True 参数，Gradio 将生成一个公开链接供他人访问。\n6. 总结 本文全面介绍了 Transformers 框架中的question-answering任务，从基础概念到实际应用，再到如何构建和部署一个交互式的问答系统。无论是在智能助手、教育辅导还是客户服务等领域，Transformers 框架都能为我们提供强有力的支持，助力我们快速实现高效的问答解决方案。\n老牛同学将持续为大家带来更多有关 Pipeline 任务的文章（共28篇）。如果您有任何疑问或建议，请随时在评论区留言交流！\nTransformers 框架：\n01.包和对象加载中的设计巧思与实用技巧\n02.AutoModel 初始化及 Qwen2.5 模型加载全流程\n03.Qwen2.5 大模型的 AutoTokenizer 技术细节\n04.Qwen2.5/GPT 分词流程与 BPE 分词算法技术细节\n05.嵌入（Embedding）机制和 Word2Vec 实战\nPipeline NLP 任务：\n零·概述 丨 01.文本转音频 丨 02.文本分类 丨 03.词元分类和命名实体识别\n往期推荐文章：\nBolt.new 用一句话快速构建全栈应用：本地部署与应用实战（Ollama/Qwen2.5 等）\n基于 Qwen2.5-Coder 模型和 CrewAI 多智能体框架，实现智能编程系统的实战教程\nvLLM CPU 和 GPU 模式署和推理 Qwen2 等大语言模型详细教程\n基于 Qwen2/Lllama3 等大模型，部署团队私有化 RAG 知识库系统的详细教程（Docker+AnythingLLM）\n使用 Llama3/Qwen2 等开源大模型，部署团队私有化 Code Copilot 和使用教程\n基于 Qwen2 大模型微调技术详细教程（LoRA 参数高效微调和 SwanLab 可视化监控）\nChatTTS 长音频合成和本地部署 2 种方式，让你的“儿童绘本”发声的实战教程\n","date":"2024-12-18T00:00:00Z","image":"https://ntopic.cn/p/2024121801/00_hude2f169f0297ce1d5a001841fdf5521a_78453_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/2024121801/","title":"Transformers 框架 Pipeline 任务详解（四）：问答（question-answering）"},{"content":"通过前面几篇关于 Transformers 框架的技术文章，我们探讨了大模型的配置、分词器和 BPE（Byte-Pair Encoding）分词算法。这些技术帮助我们将一段文本序列处理成一个 Token（词元）列表，并为每个 Token 分配一个唯一的 Token ID。然而，这仅仅是大模型输入层工作的第一步。接下来，我们将深入探讨嵌入（Embedding），这是将离散的符号映射到连续向量空间的关键步骤，尤其是词嵌入（Token Embeddings），它是整个流程中最基础也是最关键的环节。\n1. 嵌入：从离散符号到连续向量 在人工智能领域，特别是在自然语言处理（NLP）和深度学习中，“嵌入”（Embedding）是一个至关重要的概念。它指的是将离散的符号（如单词、字符或句子）映射到连续向量空间的过程。这些向量通常被称为“嵌入向量”，使得计算机能够更有效地理解和处理人类语言。\n嵌入的核心概念包括：\n高维向量表示：每个符号（例如单词）被表示为一个固定维度的实数向量。这个向量的空间通常具有较高的维度（如 100, 10000 甚至更高），以便能够充分表达复杂的语义信息。\n语义相似性：在嵌入空间中，语义上相似的词汇往往具有接近的向量表示。例如，“猫”和“狗”的嵌入向量可能会非常接近，因为它们都是宠物动物；而“苹果”作为一个水果，其嵌入向量会与上述两者有一定距离。\n上下文依赖：现代嵌入模型，不仅仅考虑单个词的静态意义，还会根据词语出现的具体上下文来动态调整其嵌入向量。这意味着同一个词在不同的句子中可能会有不同的表示，从而更好地捕捉其多义性和使用场景。例如，在“这个苹果很甜。”和“我使用的是苹果手机。”这两个句子中，“苹果”的含义并不相同。\n嵌入层（Embedding Layer）是神经网络模型中的一种特殊层，它的作用就是实现嵌入（Embedding）。嵌入层的输入是 Token ID，输出则是对应的嵌入向量。在大语言模型中，嵌入层本质上是一个嵌入矩阵，矩阵的行代表词表中的 Token ID，矩阵的列则代表嵌入向量的维度。当给定一个 Token 时，嵌入层会根据 Token ID，直接从嵌入矩阵中取出相应的嵌入向量。\n2. Transformers 框架中的嵌入机制 为了更好地理解嵌入机制，我们以Qwen2.5-1.5B大模型为例，看看它是如何实现嵌入的。\n首先，我们根据大模型config.json配置文件中的hidden_size和vocab_size配置项，可以看出嵌入的维度是1536维，词表大小是151936个：\n1 2 3 4 5 { /**...其他省略...*/ \u0026#34;hidden_size\u0026#34;: 1536, \u0026#34;vocab_size\u0026#34;: 151936 } 这意味着，Qwen2.5-1.5B的嵌入层形状是(151936, 1536)，即嵌入矩阵有 151936 行，1536 列。我们可以通过以下代码来验证这一点：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 import os from transformers import AutoTokenizer, AutoModel # 本地模型目录 model_dir = os.path.join(\u0026#39;D:\u0026#39;, os.path.sep, \u0026#39;ModelSpace\u0026#39;, \u0026#39;Qwen2.5\u0026#39;, \u0026#39;Qwen2.5-1.5B-Instruct\u0026#39;) # 初始化分词器和模型 tokenizer = AutoTokenizer.from_pretrained( model_dir, local_files_only=True, ) model = AutoModel.from_pretrained( model_dir, torch_dtype=\u0026#39;auto\u0026#39;, device_map=\u0026#39;auto\u0026#39;, local_files_only=True, ) # 查看嵌入矩阵和大小 embeddings = model.get_input_embeddings() print(f\u0026#39;嵌入矩阵大小: {embeddings.weight.size()}\u0026#39;) 结果输出：\n1 嵌入矩阵大小: torch.Size([151936, 1536]) 我也可以通过 Transformers 框架的源代码，查看嵌入矩阵的构建代码：\n有了嵌入矩阵后，我们可以进一步查看文本序列（如：Hi, 你好~）的嵌入向量内容：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 # 分词 word = \u0026#39;Hi, 你好~\u0026#39; tokens = tokenizer.tokenize(word) print(f\u0026#39;{word} 分词: {tokens}\u0026#39;) # 输出：Hi, 你好~ 分词: [\u0026#39;Hi\u0026#39;, \u0026#39;,\u0026#39;, \u0026#39;Ġ\u0026#39;, \u0026#39;ä½łå¥½\u0026#39;, \u0026#39;~\u0026#39;] token_ids = tokenizer.convert_tokens_to_ids(tokens) print(f\u0026#39;{word} Token IDs: {token_ids}\u0026#39;) # 输出：Hi, 你好~ Token IDs: [13048, 11, 220, 108386, 93] # 获取该单词的嵌入向量 word_embedding = embeddings.weight[token_ids] print(f\u0026#39;{word} 的嵌入形状：{word_embedding.shape}\u0026#39;) # 输出：Hi, 你好~ 的嵌入形状：torch.Size([5, 1536]) print(f\u0026#39;{word} 的嵌入内容: {word_embedding}\u0026#39;) # 输出： # Hi, 你好~ 的嵌入内容: tensor([[ 0.0261, 0.0048, -0.0043, ..., 0.0193, -0.0493, -0.0020], # [-0.0303, -0.0159, -0.0107, ..., -0.0198, -0.0020, -0.0129], # [-0.0236, -0.0254, 0.0325, ..., -0.0317, -0.0082, 0.0137], # [ 0.0270, 0.0042, 0.0014, ..., 0.0425, -0.0195, 0.0011], # [-0.0205, -0.0408, -0.0013, ..., 0.0272, -0.0060, 0.0032]], # dtype=torch.bfloat16, grad_fn=\u0026lt;IndexBackward0\u0026gt;) 从最终的输出可以看出，文本序列Hi, 你好~被分词成 5 个 Token，即对应 5 个 Token ID，最终它们的嵌入向量形状为(5, 1536)。\n3. Word2Vec：经典的词嵌入技术 在预训练大语言模型中，词嵌入通常使用大规模语料库进行学习来生成。其中，Word2Vec 是一种非常流行的词嵌入技术，广泛应用于自然语言处理（NLP）领域。Word2Vec 的核心思想是“分布假设”（Distributional Hypothesis），即词汇的意义可以通过其出现的上下文来推断。具体来说，如果两个词汇经常出现在相似的上下文中，那么它们可能具有相似的意义。Word2Vec 通过训练神经网络模型来学习词汇的分布式表示，使得这些表示能够反映词汇间的语义关系。\nWord2Vec 提供了两种主要的模型架构，分别是 CBOW (Continuous Bag-of-Words) 和 Skip-gram。这两种模型的目标都是预测词汇或其上下文，但它们的工作方式略有不同：\n特征 CBOW Skip-gram 目标 给定上下文，预测目标词汇 给定目标词汇，预测上下文词汇 训练速度 较快 较慢 对罕见词汇的表现 较差 较好 上下文窗口 固定，对称 固定或灵活 适用场景 常见词汇丰富的文本，快速原型开发，资源有限的环境 罕见词汇较多的文本，长尾分布的词汇，高质量词向量生成，多语言或多领域任务 接下来，我们将通过一个简单的实例，使用 gensim 库训练一个 Word2Vec 模型。\n4. 使用 gensim 训练 Word2Vec 模型 4.1 安装依赖包 首先，确保你已经安装了 gensim 库。你可以通过以下命令安装：\n1 pip install gensim 4.2 准备语料 假设我们有一个 train-data.txt 文件，每行代表一个句子。文件内容如下：\n1 2 3 4 5 I love programming in Python. Python is a versatile language. Machine learning is fascinating. Deep learning is a subset of machine learning. Natural language processing is a field of AI. 4.3 加载和预处理数据 使用 gensim.utils.simple_preprocess 来加载和预处理数据，通常包括分词、去除标点符号、转换为小写、去掉下划线开头、去掉过短（长度小于 2 个）、去掉过程（长度大于 15 个）等。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 import gensim from gensim.models import Word2Vec from gensim.utils import simple_preprocess # 读取语料库 with open(\u0026#39;train-data.txt\u0026#39;, \u0026#39;r\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;) as file: sentences = [line.strip() for line in file] # 预处理：分词并去除标点符号 processed_sentences = [simple_preprocess(sentence) for sentence in sentences] # 打印预处理后的句子 for sentence in processed_sentences: print(sentence) 输出结果：\n1 2 3 4 5 [\u0026#39;love\u0026#39;, \u0026#39;programming\u0026#39;, \u0026#39;in\u0026#39;, \u0026#39;python\u0026#39;] [\u0026#39;python\u0026#39;, \u0026#39;is\u0026#39;, \u0026#39;versatile\u0026#39;, \u0026#39;language\u0026#39;] [\u0026#39;machine\u0026#39;, \u0026#39;learning\u0026#39;, \u0026#39;is\u0026#39;, \u0026#39;fascinating\u0026#39;] [\u0026#39;deep\u0026#39;, \u0026#39;learning\u0026#39;, \u0026#39;is\u0026#39;, \u0026#39;subset\u0026#39;, \u0026#39;of\u0026#39;, \u0026#39;machine\u0026#39;, \u0026#39;learning\u0026#39;] [\u0026#39;natural\u0026#39;, \u0026#39;language\u0026#39;, \u0026#39;processing\u0026#39;, \u0026#39;is\u0026#39;, \u0026#39;field\u0026#39;, \u0026#39;of\u0026#39;, \u0026#39;ai\u0026#39;] 特别说明：本示例仅做演示，分词算法采用simple_preprocess方法进行简单处理，而实际大模型（如：Qwen2.5）采用的如BPE分词算法等更复杂逻辑。\n4.4 训练 Word2Vec 模型 使用 gensim.models.Word2Vec 类来训练 Word2Vec 模型。你可以选择使用 CBOW 或 Skip-gram 模型，并设置一些超参数，如嵌入维度、窗口大小、最小词频等。\n1 2 3 4 5 6 7 8 9 10 11 12 # 训练 Word2Vec 模型（使用 Skip-gram） model = Word2Vec( sentences=processed_sentences, # 输入数据 vector_size=100, # 嵌入维度 window=5, # 上下文窗口大小 min_count=1, # 忽略出现频率低于此值的词汇 workers=4, # 使用多线程加速训练 sg=1 # 使用 Skip-gram 模型 (sg=0 表示使用 CBOW) ) # 保存模型 model.save(\u0026#34;word2vec.model\u0026#34;) 4.5 加载预训练模型 你可以使用 load 方法加载一个已经训练好的模型：\n1 2 # 加载预训练的 Word2Vec 模型 model = Word2Vec.load(\u0026#34;word2vec.model\u0026#34;) 4.6 使用 Word2Vec 模型 使用模型进行各种操作，如查找相似词汇、计算词汇之间的相似度、获取词汇的嵌入向量等。\n查找相似词汇 1 2 3 # 查找与 \u0026#34;python\u0026#34; 最相似的词汇 similar_words = model.wv.most_similar(\u0026#34;python\u0026#34;, topn=5) print(similar_words) 输出结果：\n1 2 3 4 5 [(\u0026#39;subset\u0026#39;, 0.2529045641422272), (\u0026#39;machine\u0026#39;, 0.1701909899711609), (\u0026#39;deep\u0026#39;, 0.15016482770442963), (\u0026#39;language\u0026#39;, 0.13887961208820343), (\u0026#39;love\u0026#39;, 0.10852647572755814)] 计算词汇之间的相似度 1 2 3 # 计算 \u0026#34;python\u0026#34; 和 \u0026#34;language\u0026#34; 之间的相似度 similarity = model.wv.similarity(\u0026#34;python\u0026#34;, \u0026#34;language\u0026#34;) print(f\u0026#34;\u0026#39;python\u0026#39;和\u0026#39;language\u0026#39;相似度: {similarity:.4f}\u0026#34;) 输出结果：\n1 \u0026#39;python\u0026#39;和\u0026#39;language\u0026#39;相似度: 0.1389 获取词汇的嵌入向量 1 2 3 # 获取 \u0026#34;python\u0026#34; 的嵌入向量 vector = model.wv[\u0026#34;python\u0026#34;] print(vector) 输出结果（共 100 个）：\n1 [ -0.00713902 0.00124103 -0.00717672 ... 0.00481889 0.00078719 0.00301345] 5. 总结 本文详细介绍了 Transformers 框架中的嵌入机制，特别是词嵌入（Token Embeddings）的作用和实现方式。我们通过具体的代码示例，展示了如何使用 Qwen2.5-1.5B 大模型的嵌入矩阵来将文本序列转换为嵌入向量。此外，我们还介绍了经典的 Word2Vec 词嵌入技术，并通过gensim库演示了如何训练和使用 Word2Vec 模型。\n嵌入是自然语言处理中不可或缺的一环，它将离散的符号映射到连续的向量空间，使得计算机能够更好地理解和处理人类语言。无论是传统的 Word2Vec，还是现代的 Transformer 模型，嵌入技术都在不断提升自然语言处理的效果。未来，随着更多先进技术的涌现，嵌入技术将继续演进，为自然语言处理带来更多的可能性。\nTransformers 框架序列：\n01.包和对象加载中的设计巧思与实用技巧\n02.AutoModel 初始化及 Qwen2.5 模型加载全流程\n03.Qwen2.5 大模型的 AutoTokenizer 技术细节\n04.Qwen2.5/GPT 分词流程与 BPE 分词算法技术细节详解\nPipeline NLP 任务序列：\n零·概述 丨 01.文本转音频 丨 02.文本分类 丨 03.词元分类和命名实体识别\n往期推荐文章：\nBolt.new 用一句话快速构建全栈应用：本地部署与应用实战（Ollama/Qwen2.5 等）\n基于 Qwen2.5-Coder 模型和 CrewAI 多智能体框架，实现智能编程系统的实战教程\nvLLM CPU 和 GPU 模式署和推理 Qwen2 等大语言模型详细教程\n基于 Qwen2/Lllama3 等大模型，部署团队私有化 RAG 知识库系统的详细教程（Docker+AnythingLLM）\n使用 Llama3/Qwen2 等开源大模型，部署团队私有化 Code Copilot 和使用教程\n基于 Qwen2 大模型微调技术详细教程（LoRA 参数高效微调和 SwanLab 可视化监控）\nChatTTS 长音频合成和本地部署 2 种方式，让你的“儿童绘本”发声的实战教程\n","date":"2024-12-13T00:00:00Z","image":"https://ntopic.cn/p/2024121301/00_hu7a3dd240ebe555fb82c7f930e02b377e_116105_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/2024121301/","title":"深入解析 Transformers 框架（五）：嵌入（Embedding）机制和 Word2Vec 词嵌入模型实战"},{"content":"在自然语言处理（NLP）领域，Token-Classification（词元分类）任务是一项关键的技术，这项技术广泛应用于命名实体识别（NER）、分词、词性标注等场景。借助Transformers框架的Pipeline API，我们可以轻松地使用预训练模型执行词元分类，而无需深入了解底层复杂的模型结构和算法。本文将详细介绍Transformers框架中token-classification任务的各个方面，包括任务简介、应用场景、任务配置、实战案例以及Web UI界面。\n1. 任务简介 词元分类是指根据给定的文本内容，为文本中的每一个词元分配一个类别标签的过程。词元分类的核心思想是通过机器学习模型从大量标注好的文本数据中学习特征，并根据这些特征对新的文本中的每个词元进行分类。具体步骤如下：\n数据准备：收集并标注大量的文本数据，每个词元都对应一个类别标签。 特征提取：将文本转换为模型可以理解的数值表示，通常是通过分词、向量化等方式。 模型训练：使用标注好的数据训练一个分类模型，模型会学习如何根据输入的文本特征预测正确的类别。 模型推理：对于新的未见过的文本，模型会根据学到的特征为每个词元进行分类预测。 根据Hugging Face官网的数据，当前已有超过20,531个词元分类模型供选择，涵盖了多种语言和应用场景：\n2. 应用场景 词元分类技术广泛应用于各个领域，以下是几个典型的应用场景：\n命名实体识别（NER）：NER可以帮助企业或研究机构自动识别文本中的重要信息，如人物、地点、组织等。这在新闻报道、法律文件分析、情报收集等领域尤为有用。 分词：分词是中文和其他一些非空格分隔的语言的基本预处理步骤。准确的分词对于后续的NLP任务至关重要。 词性标注：词性标注有助于理解句子的结构和含义，可应用于机器翻译、问答系统、文本摘要等多个方面。 情感分析：虽然情感分析通常被视为文本分类任务，但在某些情况下，了解哪些具体的词语表达了情感也是非常重要的，这时词元级别的分类就显得尤为重要了。 医疗记录解析：在医疗领域，词元分类可以用于解析病历，帮助医生快速定位关键信息，比如症状、疾病名称、药物名称等。 3. 任务配置 在 Transformers 框架中，ner和token-classification都是词元分类任务的不同名称。尽管两者在 Pipeline 配置中有别名关系，但在实际使用时没有区别，框架最终统一使用token-classification作为任务名称。\n我们可以在 Transformers 框架的源代码中看到以下配置（源代码文件：./transformers/pipelines/__init__.py）：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 TASK_ALIASES = { # 其他省略...... \u0026#34;ner\u0026#34;: \u0026#34;token-classification\u0026#34;, # 其他省略...... } SUPPORTED_TASKS = { # 其他省略...... \u0026#34;token-classification\u0026#34;: { \u0026#34;impl\u0026#34;: TokenClassificationPipeline, \u0026#34;tf\u0026#34;: (TFAutoModelForTokenClassification,) if is_tf_available() else (), \u0026#34;pt\u0026#34;: (AutoModelForTokenClassification,) if is_torch_available() else (), \u0026#34;default\u0026#34;: { \u0026#34;model\u0026#34;: { \u0026#34;pt\u0026#34;: (\u0026#34;dbmdz/bert-large-cased-finetuned-conll03-english\u0026#34;, \u0026#34;4c53496\u0026#34;), \u0026#34;tf\u0026#34;: (\u0026#34;dbmdz/bert-large-cased-finetuned-conll03-english\u0026#34;, \u0026#34;4c53496\u0026#34;), }, }, \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, }, # 其他省略...... } 从上面的配置可以看出，Transformers框架默认使用的是dbmdz/bert-large-cased-finetuned-conll03-english模型，这是一个在CoNLL-2003 NER数据集上微调过的BERT大模型，专门用于英文命名实体识别任务。该模型是Hugging Face上下载量较大的词元分类模型之一，具有较高的准确性和效率。\n4. 实体类型解释 在命名实体识别任务中，实体（entity）指的是文本中被标记为特定类别的词或短语。常用的实体标签遵循一种叫做 IOB 标记格式，其中 I 表示 Inside，O 表示 Outside，B 表示 Begin。IOB 标记法允许我们区分实体内部的词和实体开始的词。以下是一些常见的实体标签及其含义：\nO: 不属于任何命名实体。 B-PER: 个人名字的开始部分，例如 \u0026ldquo;B-PER\u0026rdquo; 可能标记 \u0026ldquo;John\u0026rdquo;。 I-PER: 个人名字的中间或结尾部分，例如 \u0026ldquo;I-PER\u0026rdquo; 可能标记 \u0026ldquo;Doe\u0026rdquo;。 B-ORG: 组织机构名称的开始部分，例如公司、政府机关等。 I-ORG: 组织机构名称的中间或结尾部分。 B-LOC: 地理位置的开始部分，例如国家、城市、州等。 I-LOC: 地理位置的中间或结尾部分。 B-MISC: 各种杂项实体的开始部分，如事件、产品等，不属于上述三类。 I-MISC: 杂项实体的中间或结尾部分。 此外，还有可能遇到更细化的标签，例如：\nB-DATE: 日期的开始部分。 I-DATE: 日期的中间或结尾部分。 B-TIME: 时间的开始部分。 I-TIME: 时间的中间或结尾部分。 B-MONEY: 货币金额的开始部分。 I-MONEY: 货币金额的中间或结尾部分。 B-PERCENT: 百分数的开始部分。 I-PERCENT: 百分数的中间或结尾部分。 B-LANGUAGE: 语言名称的开始部分。 I-LANGUAGE: 语言名称的中间或结尾部分。 请注意，不同的数据集可能会有不同的标签集合。例如，CoNLL-2003 数据集主要关注 PER（人名）、ORG（组织）、LOC（地点）和 MISC（杂项）四个类别。而在其他数据集中，可能会有更多或更少的类别，以及不同类型的实体。\n5. 词元分类实战 首先，确保安装了Transformers库和其他必要的依赖包：\n1 pip install transformers torch 方法一：自动下载模型 我们可以直接从Hugging Face下载模型。如果您的网络环境允许，可以直接下载；否则，可以通过设置镜像来加速下载过程：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 import os # 设置代理 os.environ[\u0026#34;HF_ENDPOINT\u0026#34;] = \u0026#34;https://hf-mirror.com\u0026#34; # 设置本地缓存目录 cache_dir = os.path.join(\u0026#39;D:\u0026#39;, os.path.sep, \u0026#39;ModelSpace\u0026#39;, \u0026#39;Cache\u0026#39;) os.environ[\u0026#39;HF_HOME\u0026#39;] = cache_dir from transformers import pipeline # 创建Pipeline任务 nlp = pipeline(\u0026#34;token-classification\u0026#34;, model=\u0026#34;dbmdz/bert-large-cased-finetuned-conll03-english\u0026#34;) # 执行词元分类任务 result = nlp(\u0026#34;My name is Wolfgang and I live in Berlin.\u0026#34;) for entity in result: print(f\u0026#34;Word: {entity[\u0026#39;word\u0026#39;]}, Entity: {entity[\u0026#39;entity\u0026#39;]}\u0026#34;) # 输出：Word: Wolfgang, Entity: I-PER # 输出：Word: Berlin, Entity: I-LOC Pipeline任务的输出结果将类似于以下格式：\n1 [{\u0026#39;word\u0026#39;: \u0026#39;Wolfgang\u0026#39;, \u0026#39;entity\u0026#39;: \u0026#39;I-PER\u0026#39;}, {\u0026#39;word\u0026#39;: \u0026#39;Berlin\u0026#39;, \u0026#39;entity\u0026#39;: \u0026#39;I-LOC\u0026#39;}] 其中，word表示被分类的词元，entity表示对应的类别标签，例如I-PER表示一个人名中间或结尾部分，I-LOC表示一个地名的中间或结尾部分。\n方法二：自主下载模型 如果您希望通过本地模型文件进行推理，可以按照以下步骤操作。实际上，与自动下载相比，唯一的区别是指定分词器和模型即可。假设我们下载的模型目录是/models/pipeline，则用法如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline import os # 下载模型目录 model_dir = \u0026#39;/models/pipeline\u0026#39; # 加载分词器和模型 tokenizer = AutoTokenizer.from_pretrained(model_dir, local_files_only=True) model = AutoModelForTokenClassification.from_pretrained(model_dir, torch_dtype=\u0026#34;auto\u0026#34;, device_map=\u0026#34;auto\u0026#34;, local_files_only=True) # 创建Pipeline任务 nlp = pipeline(\u0026#34;token-classification\u0026#34;, tokenizer=tokenizer, model=model) # 后续用法与自动下载相同 我们可以看到，任务的输出结果，和自动下载是一样的。\n6. WebUI 页面 通过Gradio，我们可以轻松地为Transformers框架中的token-classification任务创建一个可视化的WebUI界面，用户可以通过浏览器输入文本并实时获得分类结果。\n首先，我们需要安装依赖包：\n1 pip install gradio 接下来，我们开始创建Web页面，我们可以复用Pipeline实例：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 import sys # 直接复用Pipeline实例 sys.path.append(\u0026#34;./\u0026#34;) pipeline = __import__(\u0026#34;03-token-classification\u0026#34;) import gradio as gr # 定义分类函数 def classify_tokens(text): # 使用Pipeline进行分类 result = pipeline.nlp(text) # 提取分类标签和词元 output = [] for entity in result: word = entity[\u0026#39;word\u0026#39;] label = entity[\u0026#39;entity\u0026#39;] output.append((word, label)) # 返回格式化后的结果 return output # 创建Gradio界面 with gr.Blocks() as demo: gr.Markdown(\u0026#34;# 词元分类器\u0026#34;) gr.Markdown( \u0026#34;这是一个基于Transformers框架的命名实体识别工具。您可以输入任意文本，点击“提交”按钮后，系统将自动标记文本中的实体。\u0026#34;) with gr.Row(): input_text = gr.Textbox(placeholder=\u0026#34;请输入要分类的文本...\u0026#34;, label=\u0026#34;输入文本\u0026#34;) with gr.Row(): submit_button = gr.Button(\u0026#34;提交\u0026#34;) with gr.Row(): output_label = gr.HighlightedText(label=\u0026#34;分类结果\u0026#34;) # 设置按钮点击事件，触发分类函数 submit_button.click(classify_tokens, inputs=input_text, outputs=output_label) # 启动Gradio应用 if __name__ == \u0026#34;__main__\u0026#34;: demo.launch() 运行上述代码，我们可以看到URL信息：\n1 2 3 * Running on local URL: http://127.0.0.1:7860 To create a public link, set `share=True` in `launch()`. 通过浏览器打开地址，就可以通过可视化的方式进行词元分类了：\n7. 总结 本文详细介绍了Transformers框架中的token-classification任务，涵盖了任务描述、应用场景、示例代码以及具体的实战案例。通过使用Transformers的Pipeline API，我们可以轻松地实现词元分类任务，而无需深入了解复杂的模型结构和算法。无论是命名实体识别、分词还是词性标注，Transformers框架都能提供强大的支持，帮助您快速构建高效的文字处理系统。\n老牛同学将继续推出更多关于Transformers框架Pipeline任务的文章，敬请期待！大家若有任何问题或建议，欢迎在评论区留言交流！\nPipeline任务：\nTransformers 框架任务概览（零）：从零开始掌握 Pipeline（管道）与 Task（任务）\nTransformers框架 Pipeline 任务详解（一）：文本转音频（text-to-audio或text-to-speech）\nTransformers 框架 Pipeline 任务详解（二）：文本分类（text-classification）\n往期推荐文章：\n深入解析 Transformers 框架（一）：包和对象加载中的设计巧思与实用技巧\n深入解析 Transformers 框架（二）：AutoModel 初始化及 Qwen2.5 模型加载全流程\n深入解析 Transformers 框架（三）：Qwen2.5 大模型的 AutoTokenizer 技术细节\n深入解析 Transformers 框架（四）：Qwen2.5/GPT 分词流程与 BPE 分词算法技术细节详解\n基于 Qwen2.5-Coder 模型和 CrewAI 多智能体框架，实现智能编程系统的实战教程\nvLLM CPU 和 GPU 模式署和推理 Qwen2 等大语言模型详细教程\n基于 Qwen2/Lllama3 等大模型，部署团队私有化 RAG 知识库系统的详细教程（Docker+AnythingLLM）\n使用 Llama3/Qwen2 等开源大模型，部署团队私有化 Code Copilot 和使用教程\n基于 Qwen2 大模型微调技术详细教程（LoRA 参数高效微调和 SwanLab 可视化监控）\nChatTTS 长音频合成和本地部署 2 种方式，让你的“儿童绘本”发声的实战教程\n","date":"2024-12-11T00:00:00Z","image":"https://ntopic.cn/p/2024121101/00_hu9de955f945922937b4d2ba40cf097258_80533_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/2024121101/","title":"Transformers 框架 Pipeline 任务详解（三）：词元分类（token-classification）和命名实体识别"},{"content":"2024 年 12 月 10 日，OpenAI CEO Sam Altman（山姆・奥特曼）在“12 Days of OpenAI”活动的第三天正式发布了一项震撼业界的产品——Sora 文生视频大模型正式发布。\n随着这一消息的公布，OpenAI 官方网站因用户流量激增而暂时无法访问，这不仅展示了 Sora 的强大吸引力，也反映出市场对这项新技术的高度期待。\n【第 00 段视频链接】\n下面，我们将深入了解 Sora 的核心功能、应用场景及其未来发展的潜力。\n1. 强大的视频生成功能 Sora Turbo，作为 Sora 的增强版本，将文本到视频的转换提升到了一个全新的水平。它不仅能够根据用户的提示词生成视频，还引入了故事板模式、视频剪辑、扩展和混合等多种编辑工具，为创作者提供了丰富的选择。Sora Turbo 支持最高 1080p 分辨率、多种宽高比及最长 20 秒的视频生成，所有内容均带有 C2PA 元数据，确保了内容的安全性和透明度。\n【第 01 段视频链接】\n提示词描述：镜头中雾气弥漫，色彩对比强烈，捕捉低能见度下的即时感与混乱。场景从 17 世纪海盗船上的水手视角出发，海怪突然出现，带来紧张刺激的视觉体验。\n2. 模拟现实世界的视觉效果 Sora 在风景镜头处理方面表现卓越，能够生成高质量的无人机航拍镜头，展现出媲美专业素材的视觉效果。无论是时尚女性漫步东京街头，还是 SUV 行驶于山路之间，Sora 都能呈现出细腻且真实的风景元素。尽管如此，知名博主 Marques Brownlee 在其 YouTube 频道上指出，Sora 在某些细节处理上仍有改进空间，例如人物动作的自然度和物理运动的真实感。\n【第 03 段视频链接】\n3. 不断优化，迎接未来 尽管 Sora 已经在视频生成领域取得了显著进展，但并非没有挑战。身体动作的不自然、物体变形、文字乱码等问题都是需要解决的技术难题。此外，Sora 最初发布的版本并不包含音频生成功能，这对追求完整视听体验的用户来说是一个遗憾。不过，这些不足也为 Sora 的未来发展指明了方向。\n【第 12 段视频链接】\n4. 热情背后的基础设施挑战 Sora 的发布吸引了大量用户的关注，尤其是那些已经是 ChatGPT Plus 和 Pro 会员的用户，他们无需额外付费即可直接使用 Sora。然而，这也给 OpenAI 的服务器带来了巨大的压力，导致官网一度崩溃。CEO Sam Altman 对此表示：“我们正在全力以赴应对这一情况，并会间歇性地关闭新用户注册，以确保现有用户的服务质量。”\n5. 总结：Sora 带来的变革与展望 Sora 的问世标志着视频创作进入了一个新的时代。它不仅为创意人员提供了一个强大的工具，同时也推动了人工智能系统交互能力的发展。虽然 Sora 目前仍存在一些需要改进的地方，但它无疑为视频制作领域注入了新的活力。\n就如奥特曼所宣称：视频版的 GPT-1 时刻来了！\n【第 02 段视频链接】\nPipeline任务：\nTransformers 框架任务概览：从零开始掌握 Pipeline（管道）与 Task（任务）\nTransformers框架 Pipeline 任务详解：文本转音频（text-to-audio或text-to-speech）\nTransformers 框架 Pipeline 任务详解：文本分类（text-classification）\n往期推荐文章：\n深入解析 Transformers 框架（一）：包和对象加载中的设计巧思与实用技巧\n深入解析 Transformers 框架（二）：AutoModel 初始化及 Qwen2.5 模型加载全流程\n深入解析 Transformers 框架（三）：Qwen2.5 大模型的 AutoTokenizer 技术细节\n深入解析 Transformers 框架（四）：Qwen2.5/GPT 分词流程与 BPE 分词算法技术细节详解\n基于 Qwen2.5-Coder 模型和 CrewAI 多智能体框架，实现智能编程系统的实战教程\nvLLM CPU 和 GPU 模式署和推理 Qwen2 等大语言模型详细教程\n基于 Qwen2/Lllama3 等大模型，部署团队私有化 RAG 知识库系统的详细教程（Docker+AnythingLLM）\n使用 Llama3/Qwen2 等开源大模型，部署团队私有化 Code Copilot 和使用教程\n基于 Qwen2 大模型微调技术详细教程（LoRA 参数高效微调和 SwanLab 可视化监控）\nChatTTS 长音频合成和本地部署 2 种方式，让你的“儿童绘本”发声的实战教程\n","date":"2024-12-10T00:00:00Z","image":"https://ntopic.cn/p/2024121001/00_hu624a672558a3a7240700bb5ca4fe7bf2_12185_120x120_fill_box_smart1_3.png","permalink":"https://ntopic.cn/p/2024121001/","title":"OpenAI Sora 震撼发布：20 秒内生成专业级视频，开启创作新时代！"},{"content":"随着 AI 编程工具的迅猛发展，从早期的 Code Copilot（代码辅助）到如今备受瞩目的 Cursor、v0、Windsurf 和 Bolt.new 等全栈开发平台。这些创新工具旨在加速项目开发、简化工作流程并提高研发效率。然而，访问这些工具通常依赖于“通畅的网络”和海外 LLM 模型，在某些情况下可能成为使用这些工具的障碍。\n作为一位大模型的爱好者和学习者，老牛同学今天分享一条不同的路径——如何利用本地 Ollama 和国内的大模型 API，在本地部署和使用 Bolt.new？\n以下是老牛同学录制的本地部署和使用 Bolt.new 的视频：通过一句话，即可自动完成整个小项目的代码编写和部署预览。\n【微信公众号视频链接】\n1. Bolt.new 概览 Bolt.new是由 StackBlitz 推出的一款革新性的 AI 驱动全栈开发平台，它以几个关键特性脱颖而出：\n即时全栈环境：借助 WebContainer 技术，Bolt.new 能够在浏览器中直接运行真实的 Node.js 环境，支持 npm 包安装、服务器配置及第三方 API 交互，为开发者提供了前所未有的便捷性。 智能 AI 助手：内置的强大 AI 功能可以理解并执行复杂的指令，无论是创建文件、编辑代码还是解决问题，都能显著提高工作效率。特别是其一键修复错误的功能，能够自动处理编译或运行时出现的问题，极大地节省了时间。 简易部署流程：集成的聊天界面让用户可以直接上传代码至云端，并选择合适的托管服务（如 Vercel）进行部署。生成的应用程序可以通过 URL 轻松分享，促进团队协作和成果展示。 尽管 Bolt.new 带来了诸多便利，但也存在一些局限：\n缺乏版本控制：代码调整可能导致原有版本被覆盖，增加了数据丢失的风险。 频繁重新生成和部署：每次修改需求时，Bolt.new 会重新生成整个代码库并部署，需要较长时间。 对于快速原型设计和全栈功能开发，Bolt.new 凭借其完整的开发环境、智能化的辅助工具和简便的协作机制，是一个不错的选择。\n2. 本地部署 Bolt.new 准备本地大模型 Bolt.new 底层依赖 LLM，我们先准备 2 个 LLM 选项：本地运行 Ollama，和 API 调用的远程 LLM 服务（非必须）\n本地 Ollama：关于 Ollama 详细使用教程，请参考之前文章（Ollama 完整教程），建议下载和启动Qwen2.5-Coder-7B模型： 1 ollama run qwen2.5-coder:7b LLM 服务 API：Ollama 依赖电脑硬件配置，如果电脑硬件条件有限，我们还可以直接用户大模型服务 API，只需要兼容 OpenAPI 接口标准即可（老牛同学用的是百炼平台 Qwen2.5-Coder-32B 大模型）。 下载和配置 Bolt.new 官方提供的 Bolt.new 并不直接支持本地 LLM 或自定义 API 设置。幸运的是，社区牛人coleam00基于官方版本开发了一个增强版——bolt.new-any-llm，该版本不仅兼容多种 LLM，还能灵活配置 API 接口。\n克隆项目仓库： 1 2 git clone https://github.com/coleam00/bolt.new-any-llm bolt.new-any-LLM cd bolt.new-any-LLM 配置环境变量：复制.env.example为.env，然后根据实际情况编辑.env配置文件中的 API 地址和密钥。例如，Ollama 需要设置OLLAMA_API_BASE_URL参数，国内模型 API 服务，则需要设置OPENAI_LIKE_API_BASE_URL和OPENAI_LIKE_API_KEY这 2 个参数。 1 2 # 复制配置文件 cp .env.example .env 然后，打开.env配置文件，可以看到支持的模型列表，包括 GROQ、HuggingFace、Open AI 等，根据需要进行内容修改：\n1 2 3 4 5 6 # Ollama配置 OLLAMA_API_BASE_URL=http://localhost:11434 # 【可选】 老牛同学使用的是百炼平台 OPENAI_LIKE_API_BASE_URL=https://dashscope.aliyuncs.com/compatible-mode/v1 OPENAI_LIKE_API_KEY=真实Key内容 说明：OPENAI_LIKE_API_BASE_URL和OPENAI_LIKE_API_KEY意思就是兼容 OpenAI 接口标准的大模型地址和 API Key，目前国内厂商基本都支持 OpenAPI 接口标准。\nBolt.new 项目部署 为了加快 Node.js 包下载速度，我们可以设置一下镜像源（老牛同学使用的是淘宝镜像）：\n1 npm config set registry https://registry.npmmirror.com 其他镜像源如下列表，请按需选择：\n1 2 3 4 5 6 7 8 NPM官方: https://registry.npmjs.org 淘宝镜像: http://registry.npmmirror.com 阿里云镜像: https://npm.aliyun.com 腾讯云: https://mirrors.cloud.tencent.com/npm 华为云: https://mirrors.huaweicloud.com/repository/npm 网易: https://mirrors.163.com/npm 中科大: http://mirrors.ustc.edu.cn 清华: https://mirrors.tuna.tsinghua.edu.cn 然后，我们执行以下命令来安装依赖并启动 Bolt.new：\n1 2 3 4 5 6 7 8 # 安装pnpm包管理工具 npm install -g pnpm # 安装项目依赖包 pnpm install # 启动Bolt.new pnpm run dev 启动成功后，我们可以看到如下输出信息：\n1 2 3 4 5 6 7 8 \u0026gt;pnpm run dev \u0026gt; bolt@ dev D:\\CodeSpace\\bolt.new \u0026gt; remix vite:dev ➜ Local: http://localhost:5173/ ➜ Network: use --host to expose ➜ press h + enter to show help 接下来，我们开始体验本地化的 Bolt.new！\n3. 使用 Bolt.new 进行开发 通过浏览器打开 Bolt.new 本地地址：http://localhost:5173\n首先可以看到如下页面，与官方相比，多了一个Model Settings的选项，在这里我们可以选择自己的模型：\n我们可以选择 Ollama 模型（如上图），也可以选择配置过OPENAI_LIKE_API尝试模型（如老牛同学百炼平台 API 模型）：\n选择完模型，我可以输入我们的需求：写一个计算器页面\n接下来的过程，就是老牛同学上面录制的视频所示了。\nBolt.new 可以根据我们的一句话内容，自动拆分成不同的小步骤：\n然后，自动生成完整的项目结构和执行步骤，包括文件名等：\n在右侧，显示源文件列表和动态展示每个文件生成过程：\n最终，所有源代码研发完成，自动部署整个和提供预览：\n接下来，如果我们觉得哪里需要修改、或者有什么报错，直接提问，Bolt.new 会自动进行修改并部署和预览！\n4. 总结 Bolt.new只需通过自然语音，就能实现全栈研发和自动部署的能力，对于追求高效开发和快速交付的团队而言，这是一个值得尝试的工具。\n友情提示：对于保密性较高、或数据安全要求较高的项目，通过调用外部大模型 API 服务使用 Bolt.new 工具时，请注意数据安全问题！\nPipeline 任务：\nTransformers 框架任务概览：从零开始掌握 Pipeline（管道）与 Task（任务）\nTransformers 框架 Pipeline 任务详解：文本转音频（text-to-audio 或 text-to-speech）\nTransformers 框架 Pipeline 任务详解：文本分类（text-classification）\n往期推荐文章：\n深入解析 Transformers 框架（一）：包和对象加载中的设计巧思与实用技巧\n深入解析 Transformers 框架（二）：AutoModel 初始化及 Qwen2.5 模型加载全流程\n深入解析 Transformers 框架（三）：Qwen2.5 大模型的 AutoTokenizer 技术细节\n深入解析 Transformers 框架（四）：Qwen2.5/GPT 分词流程与 BPE 分词算法技术细节详解\n基于 Qwen2.5-Coder 模型和 CrewAI 多智能体框架，实现智能编程系统的实战教程\nvLLM CPU 和 GPU 模式署和推理 Qwen2 等大语言模型详细教程\n基于 Qwen2/Lllama3 等大模型，部署团队私有化 RAG 知识库系统的详细教程（Docker+AnythingLLM）\n使用 Llama3/Qwen2 等开源大模型，部署团队私有化 Code Copilot 和使用教程\n基于 Qwen2 大模型微调技术详细教程（LoRA 参数高效微调和 SwanLab 可视化监控）\nChatTTS 长音频合成和本地部署 2 种方式，让你的“儿童绘本”发声的实战教程\n","date":"2024-12-07T00:00:00Z","image":"https://ntopic.cn/p/2024120701/00_hu31ac7d48ee715cbdbcf6bcc64e7e1d4c_130069_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/2024120701/","title":"Bolt.new 用一句话快速构建全栈应用：本地部署与应用实战（Ollama/Qwen2.5 等）"},{"content":"在自然语言处理（NLP）领域，Text-Classification（文本分类）是一项非常重要的任务，它涉及到将文本数据分配到预定义的类别中。借助 Transformers 框架的 Pipeline API，我们可以轻松地使用预训练模型进行文本分类，而无需深入了解复杂的模型结构和算法。老牛同学将通过本文详细介绍 Transformers 框架中的text-classification任务，涵盖任务描述、应用场景、示例代码以及具体的实战案例。\n1. 任务简介 文本分类是指根据给定的文本内容，将其归类到一个或多个预定义的类别中的过程。例如：\n情感分析：判断一段文本的情感倾向，如正面、负面或中性。 垃圾邮件检测：识别一封电子邮件是否为垃圾邮件。 主题分类：确定一篇文章属于哪个主题，如科技、体育、娱乐等。 文本分类的核心思想是通过机器学习模型从大量标注好的文本数据中学习特征，并根据这些特征对新的文本进行分类。具体步骤如下：\n数据准备：收集并标注大量的文本数据，每个文本都对应一个类别标签。 特征提取：将文本转换为模型可以理解的数值表示，通常是通过分词、向量化等方式。 模型训练：使用标注好的数据训练一个分类模型，模型会学习如何根据输入的文本特征预测正确的类别。 模型推理：对于新的未见过的文本，模型会根据学到的特征进行分类预测。 根据 Hugging Face 官网的数据，当前已有超过74,737个文本分类模型供选择，涵盖了多种语言和应用场景：\n2. 应用场景 文本分类技术广泛应用于各个领域，以下是几个典型的应用场景：\n情感分析：情感分析可以帮助企业了解用户对产品或服务的态度。例如，电商平台可以通过分析用户评论来判断产品的受欢迎程度，社交媒体平台可以监控用户的舆论动态，甚至政府机构可以评估公众对政策的看法。\n垃圾邮件检测：垃圾邮件检测是文本分类的一个经典应用。通过训练模型识别垃圾邮件的特征，系统可以自动过滤掉不必要的邮件，保护用户的邮箱免受骚扰。\n主题分类：新闻网站或博客平台可以使用文本分类技术自动为文章打上标签，帮助用户更快找到感兴趣的内容。例如，一篇关于人工智能的文章可以被自动分类到“科技”类别，而一篇关于足球比赛的文章则会被分类到“体育”类别。\n客服系统：智能客服系统可以通过文本分类技术自动识别用户的意图，提供相应的帮助。例如，当用户询问某个产品的价格时，系统可以自动将问题分类为“销售咨询”，并提供相关的产品信息。\n医疗诊断：在医疗领域，文本分类可以用于分析病历记录，帮助医生快速定位患者的病情。例如，通过对病历中的症状描述进行分类，系统可以提示医生可能的疾病类型，辅助诊断过程。\n3. 任务配置 在 Transformers 框架中，text-classification和sentiment-analysis都是文本分类任务的不同名称。尽管两者在 Pipeline 配置中有别名关系，但在实际使用时没有区别，框架最终统一使用text-classification作为任务名称。\n我们可以在 Transformers 框架的源代码中看到以下配置（源代码文件：./transformers/pipelines/__init__.py）：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 TASK_ALIASES = { \u0026#34;sentiment-analysis\u0026#34;: \u0026#34;text-classification\u0026#34;, # 其他省略...... } SUPPORTED_TASKS = { # 其他省略...... \u0026#34;text-classification\u0026#34;: { \u0026#34;impl\u0026#34;: TextClassificationPipeline, \u0026#34;tf\u0026#34;: (TFAutoModelForSequenceClassification,) if is_tf_available() else (), \u0026#34;pt\u0026#34;: (AutoModelForSequenceClassification,) if is_torch_available() else (), \u0026#34;default\u0026#34;: { \u0026#34;model\u0026#34;: { \u0026#34;pt\u0026#34;: (\u0026#34;distilbert-base-uncased-finetuned-sst-2-english\u0026#34;, \u0026#34;714eb0f\u0026#34;), \u0026#34;tf\u0026#34;: (\u0026#34;distilbert-base-uncased-finetuned-sst-2-english\u0026#34;, \u0026#34;714eb0f\u0026#34;), }, }, \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, }, # 其他省略...... } 从上面的配置可以看出，Transformers 框架默认使用的是distilbert-base-uncased-finetuned-sst-2-english模型，这是一个在 Stanford Sentiment Treebank (SST-2) 数据集上微调过的 DistilBERT 模型，专门用于英文情感分类任务。该模型是 Hugging Face 上下载量最大的文本分类模型之一，具有较高的准确性和效率。\n4. 文本分类实战 首先，确保安装了 Transformers 库和其他必要的依赖包：\n1 pip install transformers torch 方法一：自动下载模型 我们可以直接从 Hugging Face 下载模型。如果您的网络环境允许，可以直接下载；否则，可以通过设置镜像来加速下载过程：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 import os # 设置代理 os.environ[\u0026#34;HF_ENDPOINT\u0026#34;] = \u0026#34;https://hf-mirror.com\u0026#34; # 设置本地缓存目录 cache_dir = os.path.join(\u0026#39;D:\u0026#39;, os.path.sep, \u0026#39;ModelSpace\u0026#39;, \u0026#39;Cache\u0026#39;) os.environ[\u0026#39;HF_HOME\u0026#39;] = cache_dir from transformers import pipeline # 创建Pipeline任务 nlp = pipeline(\u0026#34;text-classification\u0026#34;, model=\u0026#34;distilbert/distilbert-base-uncased-finetuned-sst-2-english\u0026#34;) # 执行文本分类任务 result = nlp(\u0026#34;I like Transformers.\u0026#34;) print(result) # 输出：[{\u0026#39;label\u0026#39;: \u0026#39;POSITIVE\u0026#39;, \u0026#39;score\u0026#39;: 0.9973547458648682}] result = nlp(\u0026#34;I don\u0026#39;t like overtime.\u0026#34;) print(result) # 输出：[{\u0026#39;label\u0026#39;: \u0026#39;NEGATIVE\u0026#39;, \u0026#39;score\u0026#39;: 0.9958478212356567}] Pipeline 任务的输出结果将类似于以下格式：\n1 [{\u0026#39;label\u0026#39;: \u0026#39;POSITIVE\u0026#39;, \u0026#39;score\u0026#39;: 0.9876}] 其中，label表示分类结果，score表示模型对该分类的信心分数，范围在 0 到 1 之间，值越接近 1 表示模型越确信该分类是正确的。\n运行上述代码后，我们可以得到比较准确的结果：喜欢属于正面（POSITIVE）情绪，不喜欢属于负面（NEGATIVE）情绪。\n方法二：自主下载模型 如果您希望通过本地模型文件进行推理，可以按照以下步骤操作。实际上，与自动下载相比，唯一的区别是指定分词器和模型即可。假设我们下载的模型目录是/models/pipeline，则用法如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline import os # 下载模型目录 model_dir = \u0026#39;/models/pipeline\u0026#39; # 加载分词器和模型 tokenizer = AutoTokenizer.from_pretrained(model_dir, local_files_only=True) model = AutoModelForSequenceClassification.from_pretrained(model_dir, torch_dtype=\u0026#34;auto\u0026#34;, device_map=\u0026#34;auto\u0026#34;, local_files_only=True, num_labels=2) # 创建Pipeline任务 nlp = pipeline(\u0026#34;text-classification\u0026#34;, tokenizer=tokenizer, model=model) # 后续用法与自动下载相同 我们可以看到，任务的输出结果，和自动下载是一样的。\n5. WebUI 页面 通过 Gradio，我们可以轻松地为 Transformers 框架中的text-classification任务创建一个可视化的 WebUI 界面，用户可以通过浏览器输入文本并实时获得分类结果。\n首先，我们需要安装依赖包：\n1 pip install gradio 接下来，我们开始创建 Web 页面，我们可以复用 Pipeline 实例：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 import sys # 直接复用Pipeline实例 sys.path.append(\u0026#34;./\u0026#34;) pipeline = __import__(\u0026#34;02-text-classification\u0026#34;) import gradio as gr # 定义分类函数 def classify_text(text): # 使用Pipeline进行分类 result = pipeline.nlp(text) # 提取分类标签和置信度 label = result[0][\u0026#39;label\u0026#39;] score = result[0][\u0026#39;score\u0026#39;] # 返回格式化后的结果 return f\u0026#34;分类结果: {label}, 置信度: {score:.4f}\u0026#34; # 创建Gradio界面 with gr.Blocks() as demo: gr.Markdown(\u0026#34;# 文本分类器\u0026#34;) gr.Markdown( \u0026#34;这是一个基于Transformers框架的情感分析工具。您可以输入任意文本，点击“提交”按钮后，系统将自动判断其情感倾向（正面或负面）。\u0026#34;) with gr.Row(): input_text = gr.Textbox(placeholder=\u0026#34;请输入要分类的文本...\u0026#34;, label=\u0026#34;输入文本\u0026#34;) with gr.Row(): submit_button = gr.Button(\u0026#34;提交\u0026#34;) with gr.Row(): output_label = gr.Label(label=\u0026#34;分类结果\u0026#34;) # 设置按钮点击事件，触发分类函数 submit_button.click(classify_text, inputs=input_text, outputs=output_label) # 启动Gradio应用 if __name__ == \u0026#34;__main__\u0026#34;: demo.launch() 运行上述代码，我们可以看到 URL 信息：\n1 2 3 * Running on local URL: http://127.0.0.1:7860 To create a public link, set `share=True` in `launch()`. 通过浏览器打开地址，就可以通过可视化的方式进行文本分类了：\n6. 总结 本文详细介绍了 Transformers 框架中的text-classification任务，涵盖了任务描述、应用场景、示例代码以及具体的实战案例。通过使用 Transformers 的 Pipeline API，我们可以轻松地实现文本分类任务，而无需深入了解复杂的模型结构和算法。无论是情感分析、垃圾邮件检测还是主题分类，Transformers 框架都能提供强大的支持，帮助您快速构建高效的文字处理系统。\n老牛同学将继续推出更多关于 Transformers 框架 Pipeline 任务的文章，敬请期待！大家若有任何问题或建议，欢迎在评论区留言交流！\nPipeline任务：\nTransformers 框架任务概览：从零开始掌握 Pipeline（管道）与 Task（任务）\nTransformers框架 Pipeline 任务详解：文本转音频（text-to-audio或text-to-speech）\n往期推荐文章：\n深入解析 Transformers 框架（一）：包和对象加载中的设计巧思与实用技巧\n深入解析 Transformers 框架（二）：AutoModel 初始化及 Qwen2.5 模型加载全流程\n深入解析 Transformers 框架（三）：Qwen2.5 大模型的 AutoTokenizer 技术细节\n深入解析 Transformers 框架（四）：Qwen2.5/GPT 分词流程与 BPE 分词算法技术细节详解\n基于 Qwen2.5-Coder 模型和 CrewAI 多智能体框架，实现智能编程系统的实战教程\nvLLM CPU 和 GPU 模式署和推理 Qwen2 等大语言模型详细教程\n基于 Qwen2/Lllama3 等大模型，部署团队私有化 RAG 知识库系统的详细教程（Docker+AnythingLLM）\n使用 Llama3/Qwen2 等开源大模型，部署团队私有化 Code Copilot 和使用教程\n基于 Qwen2 大模型微调技术详细教程（LoRA 参数高效微调和 SwanLab 可视化监控）\nChatTTS 长音频合成和本地部署 2 种方式，让你的“儿童绘本”发声的实战教程\n","date":"2024-12-04T00:00:00Z","image":"https://ntopic.cn/p/2024120401/00_huaabab090f83155d3b38f36ae12d9a15a_81387_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/2024120401/","title":"Transformers 框架 Pipeline 任务详解：文本分类（text-classification 或 sentiment-analysis）"},{"content":"在上一篇文章（Transformers 框架任务概览：从零开始掌握 Pipeline（管道）与 Task（任务）），我们深入探讨了 Transformers 框架中的 Pipeline 和 Task 分类、配置及执行流程。今天，我们将聚焦于五大任务之一的text-to-audio或text-to-speech（TTS），探索如何利用 Transformers 将文本转换为自然流畅的语音。\n1. 应用场景 文本转音频技术通过深度学习模型从海量语音数据中学习发音规律，能够生成逼真的声音。根据 Hugging Face 官网的数据，当前已有超过2,148个文本转音频模型供选择：\n这项技术广泛应用于多个领域，以下是一些具有代表性的应用场景：\n有声读物与电子书：将电子书或文章的内容转化为音频格式，使用户能够在通勤、运动或其他无法专注阅读的时间段聆听，提供了一种更加便捷和灵活的知识获取方式。\n在线教育与学习平台：自动为课程内容生成配套的音频讲解，帮助学生练习外语发音，增强了学习的互动性和趣味性，丰富了教育资源的形式。\n虚拟助手与智能家居：智能音箱、智能手机和其他物联网设备可以使用文本转音频技术响应用户的命令或查询，提供天气预报、新闻更新等信息，提升了设备的易用性和用户体验。\n客服系统与自动化服务：企业可以集成文本转音频技术来自动生成语音应答，用于客户服务热线、电话会议记录转写以及自动化的消息通知，降低了人力成本并提高了服务效率。\n广播与媒体制作：广播电台、电视节目和网络视频可以快速生成旁白、广告词等需要语音播报的内容，加速了内容生产流程，减少了对专业配音演员的依赖，并可根据目标受众的特点定制不同的声音风格。\n2. 任务配置 在 Transformers 框架中，text-to-audio（T2A）和text-to-speech（TTS）是同一任务的不同称呼。尽管两者在 Pipeline 配置中有别名关系，但在实际使用时没有区别，框架最终统一使用text-to-audio作为任务名称。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 TASK_ALIASES = { # 其他省略...... \u0026#34;text-to-speech\u0026#34;: \u0026#34;text-to-audio\u0026#34;, } SUPPORTED_TASKS = { # 其他省略...... \u0026#34;text-to-audio\u0026#34;: { \u0026#34;impl\u0026#34;: TextToAudioPipeline, \u0026#34;tf\u0026#34;: (), \u0026#34;pt\u0026#34;: (AutoModelForTextToWaveform, AutoModelForTextToSpectrogram) if is_torch_available() else (), \u0026#34;default\u0026#34;: {\u0026#34;model\u0026#34;: {\u0026#34;pt\u0026#34;: (\u0026#34;suno/bark-small\u0026#34;, \u0026#34;1dbd7a1\u0026#34;)}}, \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, }, # 其他省略...... } 3. 文本转音频实战 3.1 ChatTTS 模型 之前的文章中，我们介绍了如何使用ChatTTS将儿童绘本内容合成音频。ChatTTS支持中英文、停顿、笑声等多种元素的合并，生成的声音非常流畅。对于更详细的步骤，请参考老牛同学的教程：ChatTTS 长音频合成和本地部署 2 种方式，让你的“儿童绘本”发声的实战教程。\n【此处为音频文件】\n需要注意的是，ChatTTS目前尚不支持 Transformers 框架的 Pipeline 任务，因此需要按照老牛同学提供的指南进行下载、配置和部署推理模型。\n3.2 Bark 模型 Transformers 框架默认使用的是由 Suno AI 研发的suno/bark-small模型，该模型以其高质量、自然流畅且富有表现力的语音生成能力而闻名。除了英语外，它还支持包括简体中文、法语、德语、西班牙语在内的 13 种语言。\n接下来，我们将介绍两种使用 Bark 模型的方法。\n方法一：自动下载模型 首先，确保安装必要的依赖包：\n1 pip install --upgrade transformers scipy 然后，我们可以直接从 Hugging Face 下载模型。如果您的网络环境允许，可以直接下载；否则，可以通过设置镜像来加速下载过程：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 import os from transformers import pipeline import scipy # 设置代理和本地缓存目录 os.environ[\u0026#34;HF_ENDPOINT\u0026#34;] = \u0026#34;https://hf-mirror.com\u0026#34; cache_dir = os.path.join(\u0026#39;D:\u0026#39;, os.path.sep, \u0026#39;ModelSpace\u0026#39;, \u0026#39;Cache\u0026#39;) os.environ[\u0026#39;HF_HOME\u0026#39;] = cache_dir # 创建Pipeline任务 nlp = pipeline(\u0026#34;text-to-audio\u0026#34;, model=\u0026#34;suno/bark-small\u0026#34;) # 执行文本转音频任务 speech = nlp(\u0026#34;Hello, my name is Suno. And, uh — and I like pizza. [laughs] But I also have other interests such as playing tic tac toe.\u0026#34;, forward_params={\u0026#34;do_sample\u0026#34;: True}) # 存储音频文件 scipy.io.wavfile.write(\u0026#34;./output/01.bark.wav\u0026#34;, rate=speech[\u0026#34;sampling_rate\u0026#34;], data=speech[\u0026#34;audio\u0026#34;].ravel()) 运行上述代码后，您将在output目录下找到生成的音频文件：01.bark.wav。\n【此处为音频文件】\n方法二：自主下载模型 如果您希望通过本地模型文件进行推理，可以按照以下步骤操作：\n1 2 3 4 5 6 7 8 9 10 11 12 from transformers import AutoTokenizer, AutoModel, pipeline import os # 本地模型文件目录 model_dir = os.path.join(\u0026#39;D:\u0026#39;, os.path.sep, \u0026#39;ModelSpace\u0026#39;, \u0026#39;Bark-small\u0026#39;) # 加载分词器和模型 tokenizer = AutoTokenizer.from_pretrained(model_dir, local_files_only=True) model = AutoModel.from_pretrained(model_dir, torch_dtype=\u0026#34;auto\u0026#34;, device_map=\u0026#34;auto\u0026#34;, local_files_only=True) # 创建Pipeline任务 nlp = pipeline(\u0026#34;text-to-audio\u0026#34;, tokenizer=tokenizer, model=model) 这样，您可以基于本地存储的模型文件进行推理，避免了网络下载的限制。\n总结 本文详细介绍了 Transformers 框架中text-to-audio任务的应用场景、配置方法以及实战案例。无论是使用默认的suno/bark-small模型还是其他模型，Pipeline 都提供了简单而强大的接口来实现文本到音频的转换。未来，老牛同学将继续推出更多关于 Transformers 框架 Pipeline 任务的文章，敬请期待！\n好书推荐\n最后，老牛同学向大家推荐一本非常适合大模型学习者的基础书籍。这本书不仅内容详实，而且易于理解，是构建扎实理论基础的好帮手。虽然本次不再举办免费送书活动，但强烈建议有兴趣的朋友阅读：\nPipeline任务：\nTransformers 框架任务概览：从零开始掌握 Pipeline（管道）与 Task（任务）\n往期推荐文章：\n深入解析 Transformers 框架（一）：包和对象加载中的设计巧思与实用技巧\n深入解析 Transformers 框架（二）：AutoModel 初始化及 Qwen2.5 模型加载全流程\n深入解析 Transformers 框架（三）：Qwen2.5 大模型的 AutoTokenizer 技术细节\n深入解析 Transformers 框架（四）：Qwen2.5/GPT 分词流程与 BPE 分词算法技术细节详解\n基于 Qwen2.5-Coder 模型和 CrewAI 多智能体框架，实现智能编程系统的实战教程\nvLLM CPU 和 GPU 模式署和推理 Qwen2 等大语言模型详细教程\n基于 Qwen2/Lllama3 等大模型，部署团队私有化 RAG 知识库系统的详细教程（Docker+AnythingLLM）\n使用 Llama3/Qwen2 等开源大模型，部署团队私有化 Code Copilot 和使用教程\n基于 Qwen2 大模型微调技术详细教程（LoRA 参数高效微调和 SwanLab 可视化监控）\nChatTTS 长音频合成和本地部署 2 种方式，让你的“儿童绘本”发声的实战教程\n","date":"2024-12-01T00:00:00Z","image":"https://ntopic.cn/p/2024120101/00_hubfb6e18fd0e7a280cb3d55a3ec77bd99_85933_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/2024120101/","title":"Transformers 框架 Pipeline 任务详解：文本转音频（text-to-audio 或 text-to-speech）"},{"content":"在前面的四篇 Transformers 框架文章中，老牛同学基于 Qwen2.5 大模型，通过走读源代码，对 Transformers 框架进行了初步分析，主要涵盖了以下几个方面：\nTransformers 模块的懒加载设计 AutoConfig、AutoModel 和 AutoTokenizer 这三个基础组件 BPE 分词算法 这些基础组件是预训练大模型推理的基石。在此基础上，Transformers 框架提供了更高层次的组件——Pipeline（管道），它封装了模型加载、数据预处理、模型推理和结果后处理的完整流程。通过 Pipeline，用户可以极简地使用预训练模型来处理具体的自然语言处理（NLP）任务，而无需深入了解模型的内部细节。\n在 Hugging Face 官网上，这些任务被归为六大类：多模态（Multimodal）、自然语言处理（Natural Language Processing，NLP）、计算机视觉（Computer Vision）、音频（Audio）、表格（Tabular）和强化学习（Reinforcement Learning）。\n本文我们将初步了解一下 Pipeline 和 Task 的配置和技术，老牛同计划在后续文章进行详细分析介绍。\n1. Task 配置源代码 我们打开 Transformers 包的源代码，就可以看到当前支持的 Task 列表和配置（模块：./pipelines/__init__.py，常量：SUPPORTED_TASKS字典）：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 # Transformers框架支持的Task列表和配置 SUPPORTED_TASKS = { \u0026#34;audio-classification\u0026#34;: { \u0026#34;impl\u0026#34;: AudioClassificationPipeline, \u0026#34;tf\u0026#34;: (), \u0026#34;pt\u0026#34;: (AutoModelForAudioClassification,) if is_torch_available() else (), \u0026#34;default\u0026#34;: {\u0026#34;model\u0026#34;: {\u0026#34;pt\u0026#34;: (\u0026#34;superb/wav2vec2-base-superb-ks\u0026#34;, \u0026#34;372e048\u0026#34;)}}, \u0026#34;type\u0026#34;: \u0026#34;audio\u0026#34;, }, \u0026#34;automatic-speech-recognition\u0026#34;: { \u0026#34;impl\u0026#34;: AutomaticSpeechRecognitionPipeline, \u0026#34;tf\u0026#34;: (), \u0026#34;pt\u0026#34;: (AutoModelForCTC, AutoModelForSpeechSeq2Seq) if is_torch_available() else (), \u0026#34;default\u0026#34;: {\u0026#34;model\u0026#34;: {\u0026#34;pt\u0026#34;: (\u0026#34;facebook/wav2vec2-base-960h\u0026#34;, \u0026#34;22aad52\u0026#34;)}}, \u0026#34;type\u0026#34;: \u0026#34;multimodal\u0026#34;, }, \u0026#34;text-to-audio\u0026#34;: { \u0026#34;impl\u0026#34;: TextToAudioPipeline, \u0026#34;tf\u0026#34;: (), \u0026#34;pt\u0026#34;: (AutoModelForTextToWaveform, AutoModelForTextToSpectrogram) if is_torch_available() else (), \u0026#34;default\u0026#34;: {\u0026#34;model\u0026#34;: {\u0026#34;pt\u0026#34;: (\u0026#34;suno/bark-small\u0026#34;, \u0026#34;1dbd7a1\u0026#34;)}}, \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, }, \u0026#34;text-classification\u0026#34;: { \u0026#34;impl\u0026#34;: TextClassificationPipeline, \u0026#34;tf\u0026#34;: (TFAutoModelForSequenceClassification,) if is_tf_available() else (), \u0026#34;pt\u0026#34;: (AutoModelForSequenceClassification,) if is_torch_available() else (), \u0026#34;default\u0026#34;: { \u0026#34;model\u0026#34;: { \u0026#34;pt\u0026#34;: (\u0026#34;distilbert/distilbert-base-uncased-finetuned-sst-2-english\u0026#34;, \u0026#34;714eb0f\u0026#34;), \u0026#34;tf\u0026#34;: (\u0026#34;distilbert/distilbert-base-uncased-finetuned-sst-2-english\u0026#34;, \u0026#34;714eb0f\u0026#34;), }, }, \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, }, # 其他任务省略 } SUPPORTED_TASKS字典配置了 Transformers 框架支持的所有任务和 Pipeline 实现，每个字典的元素配置内容如下：\n字典键：代表任务名，应用时代表这个任务。 type：代表任务分类，如audio表示这是一个处理音频数据的任务。 impl：代表Pipeline实现类，它封装了任务的所有逻辑，包括数据预处理、模型推理和结果后处理。 tf或pt：代表支持该任务的 TensorFlow 模型类或 PyTorch 模型类。 default：代表任务的默认配置，包括默认使用的模型及其版本。 2. Pipeline 和 Task 使用示例 下面，我们通过一个常见的情感分类任务来演示如何使用 Pipeline 进行任务处理：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 import os from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification # 本地模型文件目录 model_dir = os.path.join(\u0026#39;D:\u0026#39;, os.path.sep, \u0026#39;ModelSpace\u0026#39;, \u0026#39;Qwen2.5\u0026#39;, \u0026#39;Qwen2.5-1.5B-Instruct\u0026#39;) # 加载分词器 tokenizer = AutoTokenizer.from_pretrained( model_dir, local_files_only=True, ) # 加载本地模型 model = AutoModelForSequenceClassification.from_pretrained( model_dir, torch_dtype=\u0026#34;auto\u0026#34;, device_map=\u0026#34;auto\u0026#34;, local_files_only=True, ) # 创建一个情感分析Pipeline nlp = pipeline(\u0026#34;text-classification\u0026#34;, tokenizer=tokenizer, model=model) # 对一段文本进行情感分析 result = nlp(\u0026#34;我觉得使用Transformers库Pipeline处理NLP任务非常方便！\u0026#34;) # 输出：[{\u0026#39;label\u0026#39;: \u0026#39;LABEL_0\u0026#39;, \u0026#39;score\u0026#39;: 0.9700134992599487}] print(result) 上面的代码展示了使用 Pipeline 处理任务的标准框架：\ntokenizer：分词器，我们在前面的文章中详细介绍了其技术细节和 BPE 算法逻辑，这里不再赘述。 model：与前面介绍的 AutoModel 使用方式完全一致，AutoModelForSequenceClassification是 Task 配置中 PyTorch 模型类。AutoModelForSequenceClassification.from_pretrained()的逻辑与我们前面介绍的 AutoModel 基本一致。对于 Qwen2 大模型，它最终返回Qwen2ForSequenceClassification实例（具体实现老牛同学将在接下来的系列文章中进行详细分析）。 nlp：Pipeline 的实例化，可以指定任务名处理单个任务，也可以指定任务列表处理多个任务。 result：任务结果，返回label标签和score置信度分数。 3. Pipeline 技术细节 接下来，我们通过阅读源代码，分析 Pipeline 执行的技术细节。\n第 1 行代码：nlp = pipeline(\u0026quot;text-classification\u0026quot;, tokenizer=tokenizer, model=model) Pipeline 初始化时，根据任务名text-classification映射到任务配置信息，然后根据配置中的impl字段获取到 Pipeline 的实现类（如TextClassificationPipeline类）。\nPipeline 初始化完成后，加载对应的模型（如AutoModelForSequenceClassification，如果是 TensorFlow 模型则是TFAutoModelForSequenceClassification）。\n最终，Pipeline 实例化：pipeline_class(model=model, framework=framework, task=task, **kwargs)\n第 2 行代码：result = nlp(\u0026quot;我觉得使用Transformers库Pipeline处理NLP任务非常方便！\u0026quot;) 开始执行 Pipeline 任务。\nTask 执行过程由三个模板方法完成：preprocess()前置处理、forward()执行和postprocess()后置处理。\n从上面的 Pipeline 执行流程可以看出，与 AutoModel 非常相似：模型初始化、分词、模型推理、收集结果。\n4. Transformers 任务列表 SUPPORTED_TASKS常量字典内容丰富，我们通过一段代码来统计任务列表：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 from transformers.pipelines import SUPPORTED_TASKS # 收集分类 task_types = [\u0026#39;text\u0026#39;, \u0026#39;image\u0026#39;, \u0026#39;audio\u0026#39;, \u0026#39;video\u0026#39;, \u0026#39;multimodal\u0026#39;] # 打印表格 order = 0 print(\u0026#39;| # | 分类 | Task名 | Pipeline实现 |\u0026#39;) print(\u0026#39;|:---:|:----:|:------|:------------|\u0026#39;) for task_type in task_types: for key, task in SUPPORTED_TASKS.items(): if task[\u0026#39;type\u0026#39;] == task_type: order += 1 print(f\u0026#39;| {\u0026#34;{:02d}\u0026#34;.format(order)} | {task[\u0026#34;type\u0026#34;]} | {key} | {task[\u0026#34;impl\u0026#34;].__name__} |\u0026#39;) # 分类 Task 名 Pipeline 实现 01 text text-to-audio TextToAudioPipeline 02 text text-classification TextClassificationPipeline 03 text token-classification TokenClassificationPipeline 04 text question-answering QuestionAnsweringPipeline 05 text table-question-answering TableQuestionAnsweringPipeline 06 text fill-mask FillMaskPipeline 07 text summarization SummarizationPipeline 08 text translation TranslationPipeline 09 text text2text-generation Text2TextGenerationPipeline 10 text text-generation TextGenerationPipeline 11 text zero-shot-classification ZeroShotClassificationPipeline 12 image image-classification ImageClassificationPipeline 13 image image-feature-extraction ImageFeatureExtractionPipeline 14 image depth-estimation DepthEstimationPipeline 15 image image-to-image ImageToImagePipeline 16 audio audio-classification AudioClassificationPipeline 17 video video-classification VideoClassificationPipeline 18 multimodal automatic-speech-recognition AutomaticSpeechRecognitionPipeline 19 multimodal feature-extraction FeatureExtractionPipeline 20 multimodal visual-question-answering VisualQuestionAnsweringPipeline 21 multimodal document-question-answering DocumentQuestionAnsweringPipeline 22 multimodal zero-shot-image-classification ZeroShotImageClassificationPipeline 23 multimodal zero-shot-audio-classification ZeroShotAudioClassificationPipeline 24 multimodal image-segmentation ImageSegmentationPipeline 25 multimodal image-to-text ImageToTextPipeline 26 multimodal object-detection ObjectDetectionPipeline 27 multimodal zero-shot-object-detection ZeroShotObjectDetectionPipeline 28 multimodal mask-generation MaskGenerationPipeline 在接下来的 Pipeline 系列文章中，老牛同学将针对以上任务（共 28 个任务），分别进行详细介绍。敬请期待！如果你有任何问题或建议，欢迎留言交流！\n往期推荐文章：\n深入解析 Transformers 框架（一）：包和对象加载中的设计巧思与实用技巧\n深入解析 Transformers 框架（二）：AutoModel 初始化及 Qwen2.5 模型加载全流程\n深入解析 Transformers 框架（三）：Qwen2.5 大模型的 AutoTokenizer 技术细节\n深入解析 Transformers 框架（四）：Qwen2.5/GPT 分词流程与 BPE 分词算法技术细节详解\n基于 Qwen2.5-Coder 模型和 CrewAI 多智能体框架，实现智能编程系统的实战教程\nvLLM CPU 和 GPU 模式署和推理 Qwen2 等大语言模型详细教程\n基于 Qwen2/Lllama3 等大模型，部署团队私有化 RAG 知识库系统的详细教程（Docker+AnythingLLM）\n使用 Llama3/Qwen2 等开源大模型，部署团队私有化 Code Copilot 和使用教程\n基于 Qwen2 大模型微调技术详细教程（LoRA 参数高效微调和 SwanLab 可视化监控）\nChatTTS 长音频合成和本地部署 2 种方式，让你的“儿童绘本”发声的实战教程\n","date":"2024-11-19T00:00:00Z","image":"https://ntopic.cn/p/2024111901/00_hucbe7527809a9a573bfe399636ed41f4d_79053_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/2024111901/","title":"Transformers 框架任务概览：从零开始掌握 Pipeline（管道）与 Task（任务）"},{"content":"最近，阿里巴巴集团、Skywork AI 携手香港科技大学及其广州校区、浙江大学、加州大学伯克利分校，联合推出一款超厉害的文生图多模态模型 ——Meissonic！它仅有 1B 参数量，却能在普通电脑上轻松运行推理，生成高质量图像，未来甚至有望在无线端实现文本到图像的生成，简直是文生图领域的 “小钢炮”：\n轻量高效：仅 1B 参数量，打破大模型高参数量的常规，在资源有限的普通电脑甚至未来的无线端设备上都能运行，大大降低了对硬件的依赖，为个人创作者和小型企业提供了更经济实惠的文生图解决方案。\n技术创新：通过引入改进的 Transformer 架构、高级位置编码策略以及动态采样条件，对非自回归图像生成器（MIM）在文本到图像（T2I）合成任务中的性能和效率进行了大幅提升，能够更精准地理解文本语义并转化为高质量图像。\nMeissonic 融合了先进技术与方法，为 MIM 技术带来了重大革新。\n在大模型厂商纷纷追逐高参数量的当下，Meissonic 却独树一帜，将目标定位在小参数、可在无线端和普通电脑运行上。今天，老牛同学就带大家在笔记本电脑上本地部署 Meissonic 模型，并进行推理生成图片（本文的完整源代码地址放在评论区，同时参与本文评论赢新书《计算机视觉之 PyTorch 数字图像处理》）。\n下载模型文件 首先，我们要把模型文件下载到本地（目录：MeissonFlow/Meissonic）：\n1 2 3 cd MeissonFlow git lfs install git clone https://www.modelscope.cn/MeissonFlow/Meissonic.git Python 虚拟环境配置 我们使用 Miniconda 来管理 Python 虚拟环境，关于 Miniconda 的安装和使用，可以参考老牛同学之前的文章：大模型应用研发基础环境配置（Miniconda、Python、Jupyter Lab、Ollama 等）\n1 2 3 4 5 # Python虚拟环境名：Meissonic，Python版本号：3.10 conda create -n Meissonic python=3.10 -y # 激活虚拟环境 conda activate Meissonic 安装 Python 依赖包 完整的 Python 依赖包列表如下，为了方便安装，我们可以存储为requirements.txt文件，然后执行命令批量进行安装：pip install -r requirements.txt\n1 2 3 4 5 6 7 8 9 10 --extra-index-url https://download.pytorch.org/whl/cu124 accelerate pytorch-lightning torch torchvision tqdm transformers numpy gradio git+https://github.com/huggingface/diffusers.git Meissonic 模型生成图片 基础环境准备就绪，我们开始准备图片生产代码。首先下载 Meissonic 源代码（目录：Meissonic）：\n1 git clone https://github.com/viiika/Meissonic.git 然后，修改源代码目录Meissonic下的app.py文件，以下是老牛同学根据本地部署修改后的代码内容，大家可直接覆盖即可：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 # app.py import os import sys sys.path.append(\u0026#34;./\u0026#34;) import torch from src.transformer import Transformer2DModel from src.pipeline import Pipeline from src.scheduler import Scheduler from transformers import ( CLIPTextModelWithProjection, CLIPTokenizer, ) from diffusers import VQModel import gradio as gr # 运行环境 device = \u0026#39;cuda\u0026#39; if torch.cuda.is_available() else \u0026#39;cpu\u0026#39; # 模型文件目录 model_path = os.path.join(os.path.expanduser(\u0026#39;~\u0026#39;), \u0026#39;ModelSpace\u0026#39;, \u0026#39;MeissonFlow\u0026#39;, \u0026#39;Meissonic\u0026#39;) # 模型初始化 model = Transformer2DModel.from_pretrained(model_path, local_files_only=True, subfolder=\u0026#34;transformer\u0026#34;) vq_model = VQModel.from_pretrained(model_path, local_files_only=True, subfolder=\u0026#34;vqvae\u0026#34;) text_encoder = CLIPTextModelWithProjection.from_pretrained(model_path, local_files_only=True, subfolder=\u0026#34;text_encoder\u0026#34;) tokenizer = CLIPTokenizer.from_pretrained(model_path, local_files_only=True, subfolder=\u0026#34;tokenizer\u0026#34;) scheduler = Scheduler.from_pretrained(model_path, local_files_only=True, subfolder=\u0026#34;scheduler\u0026#34;) pipe = Pipeline(vq_model, tokenizer=tokenizer, text_encoder=text_encoder, transformer=model, scheduler=scheduler) pipe.to(device) MAX_SEED = 2 ** 32 - 1 MAX_IMAGE_SIZE = 1024 def generate_image(prompt, negative_prompt, seed, randomize_seed, width, height, guidance_scale, num_inference_steps, progress=gr.Progress(track_tqdm=True)): if randomize_seed or seed == 0: seed = torch.randint(0, MAX_SEED, (1,)).item() torch.manual_seed(seed) image = pipe( prompt=prompt, negative_prompt=negative_prompt, height=height, width=width, guidance_scale=guidance_scale, num_inference_steps=num_inference_steps ).images[0] return image, seed # Default negative prompt default_negative_prompt = \u0026#34;worst quality, low quality, low res, blurry, distortion, watermark, logo, signature, text, jpeg artifacts, signature, sketch, duplicate, ugly, identifying mark\u0026#34; css = \u0026#34;\u0026#34;\u0026#34; #col-container { margin: 0 auto; max-width: 640px; } \u0026#34;\u0026#34;\u0026#34; examples = [ \u0026#34;Modern Architecture render with pleasing aesthetics.\u0026#34;, \u0026#34;An image of a Pikachu wearing a birthday hat and playing guitar.\u0026#34;, \u0026#34;A statue of a lion stands in front of a building.\u0026#34;, \u0026#34;A white and blue coffee mug with a picture of a man on it.\u0026#34;, \u0026#34;A metal sculpture of a deer with antlers.\u0026#34;, \u0026#34;A bronze statue of an owl with its wings spread.\u0026#34;, \u0026#34;A white table with a vase of flowers and a cup of coffee on top of it.\u0026#34;, \u0026#34;A woman stands on a dock in the fog.\u0026#34;, \u0026#34;A lion\u0026#39;s head is shown in a grayscale image.\u0026#34;, \u0026#34;A sculpture of a Greek woman head with a headband and a head of hair.\u0026#34; ] with gr.Blocks(css=css) as demo: with gr.Column(elem_id=\u0026#34;col-container\u0026#34;): gr.Markdown(\u0026#34;# Meissonic Text-to-Image Generator\u0026#34;) with gr.Row(): prompt = gr.Text( label=\u0026#34;Prompt\u0026#34;, show_label=False, max_lines=1, placeholder=\u0026#34;Enter your prompt\u0026#34;, container=False, ) run_button = gr.Button(\u0026#34;Run\u0026#34;, scale=0, variant=\u0026#34;primary\u0026#34;) result = gr.Image(label=\u0026#34;Result\u0026#34;, show_label=False) with gr.Accordion(\u0026#34;Advanced Settings\u0026#34;, open=False): negative_prompt = gr.Text( label=\u0026#34;Negative prompt\u0026#34;, max_lines=1, placeholder=\u0026#34;Enter a negative prompt\u0026#34;, value=default_negative_prompt, ) seed = gr.Slider( label=\u0026#34;Seed\u0026#34;, minimum=0, maximum=MAX_SEED, step=1, value=0, ) randomize_seed = gr.Checkbox(label=\u0026#34;Randomize seed\u0026#34;, value=True) with gr.Row(): width = gr.Slider( label=\u0026#34;Width\u0026#34;, minimum=256, maximum=MAX_IMAGE_SIZE, step=32, value=1024, ) height = gr.Slider( label=\u0026#34;Height\u0026#34;, minimum=256, maximum=MAX_IMAGE_SIZE, step=32, value=1024, ) with gr.Row(): guidance_scale = gr.Slider( label=\u0026#34;Guidance scale\u0026#34;, minimum=0.0, maximum=20.0, step=0.1, value=9.0, ) num_inference_steps = gr.Slider( label=\u0026#34;Number of inference steps\u0026#34;, minimum=1, maximum=100, step=1, value=64, ) gr.Examples(examples=examples, inputs=[prompt]) gr.on( triggers=[run_button.click, prompt.submit], fn=generate_image, inputs=[ prompt, negative_prompt, seed, randomize_seed, width, height, guidance_scale, num_inference_steps, ], outputs=[result, seed], ) # 启动WebUI界面 demo.launch() 接着，我们启动 WebUI 界面：python app.py\n1 2 3 4 (Meissonic) $ python app.py * Running on local URL: http://127.0.0.1:7860 To create a public link, set `share=True` in `launch()`. 启动成功后，打开浏览器：http://127.0.0.1:7860\nWebUI 页面主要分为 4 部分：Prompt 提示词输入框、图片展示部分、参数设置和样例 Prompt 提示词。\n我们先输入一个简单的提示词：1 boy, smile, looking at viewer, blue eyes,（一个男孩、微笑、看着镜头、蓝色眼睛）\n点击大大的橙色的“Run”按钮，就开始图片生成：\n老牛同学的电脑配置一般，经过大概 5 分钟左右，图片生成好了：\n整体来看，图片效果还不错，Prompt 提示中的男孩、微笑、蓝眼睛、正脸等都表现得比较细致，不过鼻子、牙齿、衣服等部分就稍显逊色了。但在笔记本电脑资源有限的情况下能有这样的成果，老牛同学感觉已经很不错！\n再看看这张比卡丘戴着生日帽弹吉他的图片，图片效果就感觉细致多了：\n从上面的例子可以看出，对于 Prompt 提示词中明确的内容，Meissonic 表现相当出色。所以在实际应用时，提示词越具体，可能就越能得到我们想要的结果！欢迎大家尝试体验！\n往期推荐文章：\n基于 Qwen2.5-Coder 模型和 CrewAI 多智能体框架，实现智能编程系统的实战教程\nvLLM CPU 和 GPU 模式署和推理 Qwen2 等大语言模型详细教程\n基于 Qwen2/Lllama3 等大模型，部署团队私有化 RAG 知识库系统的详细教程（Docker+AnythingLLM）\n使用 Llama3/Qwen2 等开源大模型，部署团队私有化 Code Copilot 和使用教程\n基于 Qwen2 大模型微调技术详细教程（LoRA 参数高效微调和 SwanLab 可视化监控）\nChatTTS 长音频合成和本地部署 2 种方式，让你的“儿童绘本”发声的实战教程\n深入解析 Transformers 框架（一）：包和对象加载中的设计巧思与实用技巧\n深入解析 Transformers 框架（二）：AutoModel 初始化及 Qwen2.5 模型加载全流程\n深入解析 Transformers 框架（三）：Qwen2.5 大模型的 AutoTokenizer 技术细节\n深入解析 Transformers 框架（四）：Qwen2.5/GPT 分词流程与 BPE 分词算法技术细节详解\n","date":"2024-11-16T00:00:00Z","image":"https://ntopic.cn/p/2024111601/00_hu142be318d759fb7240397bc41b49c8da_142414_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/2024111601/","title":"Meissonic 文生图模型：小参数，超轻量，本地部署推理教程"},{"content":"前面我们已经通过三篇文章，详细介绍了 Qwen2.5 大语言模型在 Transformers 框架中的技术细节，包括包和对象加载、模型初始化和分词器技术细节：\n深入解析 Transformers 框架（一）：包和对象加载中的设计巧思与实用技巧 深入解析 Transformers 框架（二）：AutoModel 初始化及 Qwen2.5 模型加载全流程 深入解析 Transformers 框架（三）：Qwen2.5 大模型的 AutoTokenizer 技术细节 在第 3 篇文章中，我们简要介绍了 Qwen2.5 使用的 Byte Pair Encoding (BPE) 分词算法，并用英文序列 \u0026ldquo;Hello World.\u0026rdquo; 举例说明了分词过程。然而，这只是 BPE 分词的一部分内容。今天，我们将继续深入探讨 Qwen2.5 的分词流程，重点介绍 Qwen2.5 如何处理中英文混合的文本序列，并解析 BPE 分词算法的具体实现。\n一、中英文分词示例与初步分析 为深入理解 Qwen2.5 的分词过程，我们以一段中英文混合的文本序列“Transformers分词：台风又双叒叕来了！”为例进行剖析。以下是相关的代码示例：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 import os from transformers import AutoTokenizer # 初始化分词器，从本地文件加载模型 model_dir = os.path.join(\u0026#39;D:\u0026#39;, os.path.sep, \u0026#39;ModelSpace\u0026#39;, \u0026#39;Qwen2.5\u0026#39;, \u0026#39;Qwen2.5-1.5B-Instruct\u0026#39;) tokenizer = AutoTokenizer.from_pretrained( model_dir, local_files_only=True, ) text = \u0026#39;Transformers分词：台风又双叒叕来了！\u0026#39; tokens = tokenizer.tokenize(text) print(tokens) # 输出：[\u0026#39;Transform\u0026#39;, \u0026#39;ers\u0026#39;, \u0026#39;åĪĨ\u0026#39;, \u0026#39;è¯į\u0026#39;, \u0026#39;ï¼ļ\u0026#39;, \u0026#39;åı°é£İ\u0026#39;, \u0026#39;åıĪ\u0026#39;, \u0026#39;åıĮ\u0026#39;, \u0026#39;åı\u0026#39;, \u0026#39;Ĵ\u0026#39;, \u0026#39;åıķ\u0026#39;, \u0026#39;æĿ¥äºĨ\u0026#39;, \u0026#39;ï¼ģ\u0026#39;] ids = tokenizer.convert_tokens_to_ids(tokens) print(ids) # 输出：[8963, 388, 17177, 99689, 5122, 108118, 99518, 99493, 5758, 240, 122378, 101161, 6313] 我们先看看tokenizer.convert_tokens_to_ids()方法：它的实现比较简单，就是从词表中获取Token对应的ID；而词表其实就是一个字典，即vocab.json文件内容，每个 Token 都对应一个数字，即 Token ID。\n如下随便挑选了上面结果几个 Token，在vocab.json文件中都可以找到他们的对应关系：\n1 2 3 \u0026#34;Transform\u0026#34;: 8963, \u0026#34;åıķ\u0026#34;: 122378, \u0026#34;æĿ¥äºĨ\u0026#34;: 101161, 第 1 个问题：为什么英文序列分词之后仍可以明确地识别出来，而中文分词之后看起来像是乱码呢？\n实际上，这并非真正的乱码，而是字节 Byte 的 Unicode 字符表示。这也揭示了 BPE 算法的核心特性——基于 Byte 字节进行分词。无论是英文、中文，还是其他任何语言的字符序列，在计算机的存储与计算体系中，均以字节为基本单元进行处理。这一特性使得 BPE 算法具备了天然的通用性，能够跨越不同语言的界限，为大语言模型的多语言处理能力奠定了坚实基础。\n二、Tokenize 分词流程详解 接下来，我们重点聚焦于tokenizer.tokenize()方法，深入探究其如何实现分词操作，其中 BPE 算法的具体实现过程尤为关键。通过对 Transformers 框架源代码的逐步解读，可以将其分词流程分解为以下几个关键步骤：\n执行PreTrainedTokenizer.tokenize()父类方法：在这一初始步骤中，借助 Python 内置的unicodedata.normalize()函数对输入的文本序列进行规范化处理。其主要目的在于消除因字符编码差异可能引发的后续处理问题，确保文本序列在字符编码层面的一致性与规范性。在当前示例中，由于输入的文本序列本身已经是规范化的文本，因此该步骤直接返回原文本序列，未进行实质性的修改操作。\n处理特殊 Token：特殊 Token 在分词过程中具有特殊的处理方式，它们将直接作为分词结果的一部分被返回，无需经过复杂的分词逻辑处理。\n执行Qwen2Tokenizer._tokenize()方法：这一步骤构成了分词的核心逻辑部分，其中 BPE 算法将被实际应用于文本序列的分词操作。从该方法的注释Copied from transformers.models.gpt2.tokenization_gpt2.GPT2Tokenizer._tokenize 可以清晰地看出，Qwen2.5 的分词逻辑实现，是复制GPT2的实现代码。\n第一行代码：re.findall(self.pat, text) 通过正则表达式对文本序列进行初步分割：(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+，该正则表达式从基础语法层面进行初步分割，包括英文缩写词、字母数字、空白符和换行符等。\n如，对于我们的输入序列Transformers分词：台风又双叒叕来了！，会被分割成[Transformers分词, ：台风又双叒叕来了, ！]三个文本片段。\n第二行代码：token = \u0026quot;\u0026quot;.join(self.byte_encoder[b] for b in token.encode(\u0026quot;utf-8\u0026quot;)) 对正则表达式分割的文本片段进行处理\n首先，通过token.encode(\u0026quot;utf-8\u0026quot;)方法编码为 UTF-8 格式的字节序列，对于 ASCII 字符（0-127）字符，UTF-8 编码与其相同，这也是为什么分词结果，英文单词我们可以明确识别出来；而对于中文等非 ASCII 字符（128 及以上），UTF-8 会编码使用 2 到 4 个字节，如果 BPE 按照字节分词的话，一个汉字会被拆分几个部分，因此看起来就像是乱码了。如：Transformers分词字节序列的 16 进展表示为Transformers\\xe5\\x88\\x86\\xe8\\xaf\\x8d。\n然后，通过self.byte_encoder[b]字典映射成字符，然后把每个 Byte 的字符表示拼接起来。\n那么，self.byte_encoder字典又是如何实现的呢？\n寥寥几行代码，Qwen2.5 继续复制GPT2的实现代码：\n对于!到~、¡到¬和®到ÿ这些可见字符，字典键就是这些字符，值为对应的 Byte 值。\n对于其他的非可见字符（空白和控制字符），字典键为256+序号，值为对应的 Byte 值。\n第 2 个问题：为什么可见字符和非可见字符需要分开处理？其实原因也比较好理解：因为 BPE 代码无法处理的空白和控制字符，所以需要特殊处理。\n因此，第 2 行代码的结果是，通过正则表达式初步分割的序列片段，把他们的每个字节按照其对应的 Unicode 字符拼接起来了，这也是为什么最终结果看起来像乱码的原因了。\n第三行代码：bpe_tokens.extend(bpe_token for bpe_token in self.bpe(token).split(\u0026quot; \u0026quot;)) 对于 Unicode 字符串，执行 BPE 算法，进行分词操作。\nBPE 分词算法的核心逻辑，都在这个方法里面，下面老牛同学进行详细介绍！\n三、BPE 分词算法核心逻辑解析 bpe()方法作为 BPE 分词算法的核心枢纽，其代码实现主要借鉴了 GPT2 的相关代码：\n在深入探讨其核心代码之前，有必要对 BPE 算法的基本思想进行简要回顾（可参考本系列文章的第三篇：深入解析 Transformers 框架（三）：Qwen2.5 大模型的 AutoTokenizer 技术细节）。\n首先，算法会检查缓存中是否已经存在当前 Unicode 字符串的分词结果。若缓存命中，则直接返回缓存中的分词后的 Token 列表，从而避免重复计算，提高分词效率。\n若缓存未命中，则进入以下核心逻辑处理流程：\n第 1 行代码：word = tuple(token)：将输入的 Unicode 字符串拆分为单个 Unicode 字符的元组形式。这一操作将原始字符串打散为最基本的Unicode字符单元，为后续基于字符对的合并操作奠定了基础。\n第 2 行代码：pairs = get_pairs(word)：该方法同样源自 GPT2 的代码实现，其功能是获取给定 Unicode 字符串中所有可能的字符对组合。例如，对于“ABCD”这样的 Unicode 字符串，其返回的结果将是[(A,B),(B,C),(C,D)]，即包含了字符串中相邻两个字符组成的所有字符对。\n第 3 行代码：bigram = min(pairs, key=lambda pair: self.bpe_ranks.get(pair, float(\u0026quot;inf\u0026quot;))) 对于pairsUnicode字符对集合中的每个元素，取bpe_ranks的最小值，如果不存在，则返回float(\u0026quot;inf\u0026quot;)无限大值。\n那么，self.bpe_ranks又是什么呢？它也是一个字典，它的键是merges.txt文件内容去掉第 1 行版本号行后的每一行内容，值就是对应的行号。\n也就是说，bigram的内容是pairs中的所有的 Unicode 字符对从merges.txt匹配到的最先出现的行号（即：最小值）。\n第 3 个问题：为什么首先合并的 Unicode 字符对要取merges.txt文件中最小行号的字符对？\n这需要我们理解merges.txt文件中存储的是什么内容：它存储的就是 GPT2/Qwen2.5 等大语言模型在预训练时，使用 BPE 算法进行分词时 Unicode 字符对的合并顺序。最小行号，意味着最先合并的 Unicode 字符对，也意味着出现频次最高的字符对。在我们推理时，保持同样的合并顺序，就可以保证合并结果和预训练时一致，同样保证了词表中有我们合并的Token！\nBPE 算法在while True:循环中，持续进行字符对的合并操作：若在当前循环中成功合并了字符对，则将原有的单个 Unicode 字符对从待处理列表中移除，并将合并后的结果添加到待合并 Unicode 字符列表中。例如，对于初始字符列表：['T','r','a','n','s','f','o','r','m','e','r','s']，最小行号 Unicode 字符对是('e','r')，则在本次循环中会将“er”进行合并，下一轮循环时word的内容将更新为['T','r','a','n','s','f','o','r','m','er','s']。循环将持续进行，直到无法再找到可合并的字符对，或者所有字符对在merges.txt中均无法匹配到有效行号（即：bigram=float(\u0026quot;inf\u0026quot;)）时，循环终止，此时word中的内容即为最终的 Token 列表，代表了经过 BPE 算法分词后的结果。\n至此，Qwen2.5 分词流程和 BPE 分词算法实现逻辑基本完成，下面进行简单总结。\n总结：Qwen2.5 分词流程与 BPE 算法要点 Qwen2.5 的分词流程可以概括为以下几个关键步骤：\n规范化预处理：对输入的文本序列运用规范化处理手段，消除因字符编码差异可能带来的干扰因素，确保文本序列在进入分词流程前的一致性与规范性。\n正则表达式初步分割：借助精心设计的正则表达式，依据常见的语义和语法规则，将文本序列初步分割为多个片段序列，为后续的精细化分词操作提供基础框架。\nBPE 算法分词：针对每个片段序列，运用 BPE 算法进行深度分词处理，将片段序列进一步分解为 Token 列表，从而完成整个文本序列的分词任务。\nBPE 分词算法的核心要点包括：\n文本序列转换：将输入的文本序列通过 UTF-8 编码转换为 Byte 字节序列，再将每个字节映射为 Unicode 字符，实现文本序列在字符表示层面的转换，为基于字节的分词操作提供基础数据格式。\n预训练阶段字符对合并与记录：在 Qwen2.5/GPT2 等大模型的预训练阶段，对语料中的 Unicode 字符进行逐对分析与合并操作。按照字符对在语料中出现的频次高低，逐步合并出现频次最高的 Unicode 字符对，并将每次合并的字符对信息逐行记录到“merges.txt”文件中，形成预训练阶段的字符对合并顺序知识库。\n推理阶段字符对匹配与合并：在模型推理阶段，将每个 Unicode 字符转换为字符对形式，然后依据“merges.txt”文件中的合并顺序信息，每次仅合并最小行号（即最先合并）的字符对，持续进行合并操作直至无法再进行合并为止。最终得到的合并结果即为分词后的 Token 列表，完成对输入文本序列的分词任务，并确保推理阶段的分词结果与预训练阶段保持高度一致，从而为大语言模型在多语言文本处理中的准确性与稳定性提供有力保障。\n【书籍购买链接】\n往期推荐文章：\n基于 Qwen2.5-Coder 模型和 CrewAI 多智能体框架，实现智能编程系统的实战教程\nvLLM CPU 和 GPU 模式署和推理 Qwen2 等大语言模型详细教程\n基于 Qwen2/Lllama3 等大模型，部署团队私有化 RAG 知识库系统的详细教程（Docker+AnythingLLM）\n使用 Llama3/Qwen2 等开源大模型，部署团队私有化 Code Copilot 和使用教程\n基于 Qwen2 大模型微调技术详细教程（LoRA 参数高效微调和 SwanLab 可视化监控）\nChatTTS 长音频合成和本地部署 2 种方式，让你的“儿童绘本”发声的实战教程\n深入解析 Transformers 框架（一）：包和对象加载中的设计巧思与实用技巧\n深入解析 Transformers 框架（二）：AutoModel 初始化及 Qwen2.5 模型加载全流程\n深入解析 Transformers 框架（三）：Qwen2.5 大模型的 AutoTokenizer 技术细节\n","date":"2024-11-08T00:00:00Z","image":"https://ntopic.cn/p/2024110801/00_huaf4d4384d74399782110fa538598e630_129596_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/2024110801/","title":"深入解析 Transformers 框架（四）：Qwen2.5/GPT 分词流程与 BPE 分词算法技术细节详解"},{"content":"大家是不是和老牛同学一样，每到年初、年中或年终，都需要制作 PPT 或其他演示文档，用来总结过去的工作成果和规划未来的工作方向呢？\n在准备这些总结和规划文档时，我们是不是经常面临着一个难题：想要粘贴大量的文字，以确保信息传达完整，但又希望能够做到图文并茂，避免页面显得过于单调乏味，从而纠结于选择什么样的配图最为合适？\n在参加了多次会议和分享后，相信大家一定注意到，那些优秀的汇报和演示文档有一个共同的特点——它们总是能够巧妙地利用图表和图片来增强信息的可读性和吸引力。优秀的展示，往往离不开精心挑选的配图。\n今天，老牛为大家推荐一款强大的文本自动配图工具——Napkin.ai。这款 AI 应用不仅能帮助我们快速找到合适的配图，还能轻松编辑和自定义配图样式，让我们的演示文档更加生动有趣。\nNapkin.ai 的主要亮点包括：\n文本自动摘要：只需输入描述性文本，无需额外进行摘要处理。 自动生成对应配图：根据输入的文本，Napkin.ai 提供多个风格各异的图片选项，供你选择最合适的那一张。 配图支持自定义：你可以自由调整配图的样式、添加或修改图片上的文字，甚至对图片的局部细节进行编辑。 配图支持多种方式下载：无论是直接复制到文档中，还是下载不同格式的图片文件，Napkin.ai 都能满足你的需求。 通过 Napkin.ai，你可以轻松解决配图难题，让每一次汇报和分享都更加出色！\nNapkin.ai 使用秘籍大公开 Napkin.ai是一款在线应用，无需下载安装，打开官网https://www.napkin.ai就能用，是不是超方便？\n注册和登陆：轻松开启 Napkin.ai 之旅 第一次使用的时候需要注册，只需要输入用户名、邮箱和密码就可以了，连手机号都不用哦。邮箱是用来接收注册码的，老牛同学用的是 QQ 邮箱，接收注册码毫无压力。注册和登录地址：https://app.napkin.ai/signin。\n创建 Napkin：开启你的创意之旅 第一次注册或登陆成功之后，会有一个欢迎页面，屏幕中间有一个醒目的大按钮“Create my first Napkin”，点击它，我们就可以创建我们的第一个 Napkin。\n选择配置：打造专属配图 老牛同学使用一段文本作为样例，看看如何创建一个插图：\nNapkin.ai 有几个亮点功能：1. 文本内容自动摘要；2. 自动生成对应的图片；3. 还可用选择图片；4. 图片文本支持修改；5. 最终定稿，还可以复制、下载图片。\n点击文本段左边的“闪电”图标，Napkin 就开始为我们创建可选的配图样式。\nNapkin 创建完配图之后，我们可以滑动来选择，而且还能实时预览，就像在逛一个装满精美图片的画廊。\n当我们选择了满意的配图之后，可以点击“STYLES”进一步设置配图样式，然后点击选择最终的配图样式。\n设置配置 选择了配图和样式之后，我们还可以进一步对配图修改和配置。当我们选中了配图之后，配图上下均会出现调整面部：\n上方面板按钮分别为：轮廓颜色、风格、边框、局部选择调整和配图下载。下方面板按钮分别为：样式搜索、输入文字、画笔和上次图片。\n老牛同学选择花边轮廓，效果如下：\n我们确定配图之后，可以点击“下载”按钮使用配图了：\n在下载配置页，我可以选择文件格式，包括：PNG 图片、SVG 矢量图和 PDF 文档。还可以选择白天和黑暗模式，是否需要背景和图片的尺寸。\n我们还可以下载到本地或者复制到粘贴板，老牛同学选择复制，最终粘贴到出一张完整图片：\n怎么样？是不是感觉操作非常简单，并且功能还特别强大呢？是不是有了这款神器，后续在做 PPT 感觉压力能变小好多呢？\n小结：Napkin.ai——演示文档的得力助手 Napkin.ai 是一款强大的工具，能根据你的内容快速生成概念示意图初稿，让展示更加形象生动，同时减少时间和精力的投入。\nNapkin.ai 提供多种图示选项，帮助你找到最佳表达方式，轻松修改和优化。你可以调整布局、添加文字、修改颜色和样式，快速制作出专业且个性化的图表。\n有了 Napkin.ai，再也不用在 PPT 中堆砌大段文字。通过直观的图示，更有效地传达信息，让观众更容易理解和记住你的观点。赶快试试吧，让视觉化概念表达轻松愉快，充满乐趣！\n特别注意：老牛同学在此提醒大家，需要特别注意数据安全，针对内部的私密数据，务必脱敏后再使用本工具！！！\n最后：又来给大家免费包邮送几本新书 前面我们已经举办过 4 次免费包邮送书活动了，朋友们反馈还不错。老牛同学继续和出版社朋友合作，举办第 5 次送书福利小活动，新书是：《OpenAI API 接口应用实战》\n本期送书小活动的规则和之前类似，总体如下：\n关注老牛同学微信公众号，因为这是老牛同学公众号关注者的小福利 在本文的评论区进行留言，留言的点赞数排名前 3 名的朋友，免费且包邮获得 1 本书（即：默认送 3 本书） 若本文的阅读量和老牛同学 CSDN 同文的阅读量之和达到了2000（含2000），则留言的点赞数排名前 6 名的朋友均免费包邮送书（即：最多送 6 本书） 活动时间：2024 年 11 月 4 日到 2024 年 11 月 10 号上午 12 点整（寄送新书好购物） 老牛同学不参与本次活动（因为老牛同学默认就有 1 本） 为方便大家购买本书，老牛同学贴上京东商品链接：\n【书籍购买链接】\n往期推荐文章：\n基于 Qwen2.5-Coder 模型和 CrewAI 多智能体框架，实现智能编程系统的实战教程\nvLLM CPU 和 GPU 模式署和推理 Qwen2 等大语言模型详细教程\n基于 Qwen2/Lllama3 等大模型，部署团队私有化 RAG 知识库系统的详细教程（Docker+AnythingLLM）\n使用 Llama3/Qwen2 等开源大模型，部署团队私有化 Code Copilot 和使用教程\n基于 Qwen2 大模型微调技术详细教程（LoRA 参数高效微调和 SwanLab 可视化监控）\nChatTTS 长音频合成和本地部署 2 种方式，让你的“儿童绘本”发声的实战教程\n深入解析 Transformers 框架（一）：Transformers 包和对象加载\n深入解析 Transformers 框架（二）：Qwen2.5 大模型的 AutoModel 技术细节\n深入解析 Transformers 框架（三）：Qwen2.5 大模型的 AutoTokenizer 技术细节\n","date":"2024-11-04T00:00:00Z","image":"https://ntopic.cn/p/2024110401/00_hub20bbea5694ca3a59339a693ae8ca887_113920_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/2024110401/","title":"告别 PPT 配图难题！Napkin.ai 轻松打造生动演示文档（文末免费送书）"},{"content":"前面 2 篇文章，我们通过查看 Transformers 包代码，学习了 Transformer 包模块 API 设计、模型初始化和加载流程：\n第 1 篇：transformers 推理 Qwen2.5 等大模型技术细节详解(一)transformers 包和对象加载 第 2 篇：transformers 推理 Qwen2.5 等大模型技术细节详解(二)AutoModel 初始化和模型加载 本文是 Transformers 推理 LLM 大语言模型技术细节的第 3 篇，我们将基于 Qwen2.5 大模型，通过走读 Transformers 源代码的方式，来学习AutoTokenizer技术细节：\n环境准备：配置虚拟环境，下载 Qwen2.5 模型文件 AutoTokenizer分词器介绍、初始化和存储代码流程的技术细节 Qwen2.5使用的分词算法介绍，和一些常用的 Token 操作用法 环境准备：配置虚拟环境和下载模型文件 【配置虚拟环境】 我们可以继续使用在上一篇中我们已经配置好的虚拟环境：\n1 2 3 4 5 6 7 8 9 10 # Python虚拟环境名：Qwen2.5，版本号：3.10 conda create -n Qwen2.5 python=3.10 -y # 激活虚拟环境 conda activate Qwen2.5 # 安装必要的Python依赖包 pip install torch pip install \u0026#34;transformers\u0026gt;=4.43.1\u0026#34; pip install \u0026#34;accelerate\u0026gt;=0.26.0\u0026#34; 【下载 Qwen2.5 模型文件】 我们也可以继续使用在上一篇中下载好的模型文件：\n1 2 3 4 5 6 7 8 9 10 11 12 13 # Git大文件系统 git lfs install # 下载模型文件 git clone https://www.modelscope.cn/qwen/Qwen2.5-1.5B-Instruct.git Qwen2.5-1.5B-Instruct # 若下载过程中异常中断，可以通过`git lfs install`命令继续下载： # 切换到Git目录 cd Qwen2.5-1.5B-Instruct # 中断继续下载 git lfs install git lfs pull AutoTokenizer 初始化和存储流程 在大模型中，分词就是把模型的输入内容（如：文本序列）转换为Token（也称：词元）序列，Token 是最小的语义单元，且每个 Token 都有相对完整的语义。\n如下代码示例，我们可以通过AutoTokenizer.from_pretrained方法初始化分词器：\n1 2 3 4 5 6 7 8 9 10 import os from transformers import AutoTokenizer # 初始化分词器，从本地文件加载模型 model_dir = os.path.join(\u0026#39;D:\u0026#39;, os.path.sep, \u0026#39;ModelSpace\u0026#39;, \u0026#39;Qwen2.5\u0026#39;, \u0026#39;Qwen2.5-1.5B-Instruct\u0026#39;) tokenizer = AutoTokenizer.from_pretrained( model_dir, local_files_only=True, ) 根据第 1 篇Transformers 包模块设计，我们可以找到AutoTokenizer类定义在./models/auto/tokenization_auto.py模块中，我们可以走读from_pretrained方法执行流程：\n第 1 步：AutoTokenizer.from_pretrained解析tokenizer_config.json配置文件，获取tokenizer_class配置项，Qwen2.5 的配置文件中的值为Qwen2Tokenizer：\n第 2 步：默认情况下，Transformers 优先使用带有Fast结尾的、性能更好的分词器实现。因此会先把Qwen2Tokenizer类型转为Qwen2TokenizerFast类，并调用tokenizer_class_from_name()方法加载Qwen2TokenizerFast类：\n最终，成功加载Qwen2TokenizerFast类后，调用Qwen2TokenizerFast.from_pretrained进一步完成初始化。\n其中，tokenizer_class_from_name()是一个重要的方法，它的定义如下，我们可以看到它的实现和我们第 1 篇中动态模块加载非常类似：\n执行逻辑是从TOKENIZER_MAPPING_NAMES常量中，循环匹配到Qwen2TokenizerFast类型，并且得到qwen2模块名称，为动态加载提供完整的模块路径：\n1 2 3 4 5 6 7 8 9 10 # ...省略... ( \u0026#34;qwen2\u0026#34;, ( \u0026#34;Qwen2Tokenizer\u0026#34;, \u0026#34;Qwen2TokenizerFast\u0026#34; if is_tokenizers_available() else None, ), ), # 说明：is_tokenizers_available() 方法定义在 transformers.utils.import_utils.py 模块中，其值为 True # ...省略... 从动态加载代码可以看出，Qwen2TokenizerFast类定义在transformers.models.qwen2.tokenization_qwen2_fast.py模块中：\n第 3 步：分词器执行Qwen2TokenizerFast.from_pretrained方法，由于Qwen2TokenizerFast -\u0026gt; PreTrainedTokenizerFast -\u0026gt; PreTrainedTokenizerBase类继承链，from_pretrained方法实际在PreTrainedTokenizerBase类定义。\n在PreTrainedTokenizerBase.from_pretrained方法中，主要是在收集配置参数文件列表，最终执行Qwen2TokenizerFast._from_pretrained方法，实际还是PreTrainedTokenizerBase._from_pretrained方法：\n最终收集到的配置文件不一定都存在，其中 vocab_file/merges_file/tokenizer_file/tokenizer_config_file 存在对应的文件，而 added_tokens_file/special_tokens_map_file 文件却并不存在。文件不存在其实不影响接下来的处理逻辑，因为收集文件的目的是为了解析且内，只要内容存在就可以了，接下来我们将会看到。\n第 4 步：解析tokenizer_config.json配置文件，收集初始化参数（init_kwargs变量）：\n其实在第 1 步的时候，为了获取tokenizer_class配置项，这个配置文件就解析过一次。然而在这里再次解析了一次，并且再次获取了一次该配置项！\n第 5 步：继续收集参数，包括 3 个配置文件路径，和tokenizer_config.json配置文件中的added_tokens_decoder配置项字典元素内容：\n收集的 3 个文件为 vocab_file/merges_file/tokenizer_file，而added_tokens_decoder配置项内容为特殊 Token ID 和映射。\n第 6 步：规整化收集到的特殊 Token 参数，最后进行Qwen2TokenizerFast类实例化：\n至此，AutoTokenizer.from_pretrained初始化完成，其实我们也可以看到，其实我们直接使用Qwen2TokenizerFast.from_pretrained方法结果一样，并且还可以直接跳过第 1 步和第 2 步解析Qwen2TokenizerFast的处理过程，因此代码执行效率会更高一些。\n最后，老牛同学用一张图对上面步骤进行简单总结：\n接下来，我们可以通过XXXTokenizer.save_pretrained方法存储分词器：\n1 2 3 # 存储分词器 save_dir = os.path.join(\u0026#39;D:\u0026#39;, os.path.sep, \u0026#39;ModelSpace\u0026#39;, \u0026#39;Qwen2.5\u0026#39;, \u0026#39;Qwen2.5-1.5B-Instruct-COPY\u0026#39;) tokenizer.save_pretrained(save_dir) 执行完成，我们可以看到Qwen2.5-1.5B-Instruct-COPY目录中 6 个文件，而这 6 个文件，正是初始化过程中收集的那 6 个文件：\n分词器存储的文件列表中，vocab.json 就是我们的词表文件，文件内容是一个大字典，字典键为 Token，字典值就是对应的 Token ID（从 0 开始）值。\nQwen2.5 字节对编码（Byte Pair Encoding, BPE）分词算法 接着，我们尝试用编辑器打开vocab.json词表文件可以看到：第 1 个 Token 是!，接着是一些数字、字母、标点符号等 Token，这些还好理解；接下来是一些如ort、ass等英语短语，它们不是完整的单词，当然也能看到一些如at、Check、inner等完整单词；在接下来感觉就开始是乱码了，应该不是完整的汉字。最大的 Token ID 为151642代表了词表的大小。\n根据上面看到的内容初步判断：Qwen2.5 并不是按照单词或者汉字粒度进行分词，我们也可以从Qwen2TokenizerFast类源代码注释也可以佐证（Based on byte-level Byte-Pair-Encoding.）。\n首先有个疑惑：大模型为什么不能按照单个完整的单词或者汉字的粒度进行分词，这样分词的方法不是更加便于理解、同时分词结果也更加直观吗？\n老牛同学认为有 2 个主要的考虑因素：\n能有效控制 Token 总数量，不至于随着单词或者汉字等词汇的增长而膨胀，可以有效地节省内存和计算资源；同时，当有新造词出现时，无需更新模型的词表。 能有效处理预训练时未遇见或罕见词汇，因为分词算法将这些词汇分解为已知的 Token 单元。 字节对编码（Byte Pair Encoding, BPE）是一种流行的分词算法，它的主要思想是通过迭代合并最常见的字符对来生成词汇表。主要步骤：\n初始化词汇表：从字符级别开始，词汇表包含所有出现的字符。 统计字符对频率：统计文本中所有字符对的出现频率。 合并最常见的字符对：将出现频率最高的字符对合并为一个新的 Token，并更新词汇表。 重复步骤 2 和 3：重复上述过程，直到达到预定的词汇表大小或满足预设的停止条件。 举一个简单例子：假设我们的语料库就一句话Hello World.，我们首先统计单词出现频率（“Hello”:1 次，“ ”:1 次，“World”:1 次， “.”:1 次）\n第 1 步，初始化词汇表：{H，e，l，o，空格，W，r，d，句号} 第 2 步，统计字符对频率：{H:1 次，e:1 次，l:3 次，o:2 次，空格:1 次，W:1 次, r:1 次，d:1 次，句号:1 次} 第 3 步，合并最常见的字符对：l 和 o 频次最高，组合的 Token 为llo 继续第 2 步，此时词汇表：{H:1 次，e:1 次，l:1 次，llo:1 次，o:1 次，空格:1 次，W:1 次, r:1 次，d:1 次，句号:1 次} 继续第 3 步，假设合并 H 和 e 组成新 Token 为He 继续第 2 步，此时词汇表：{He:1 次，l:1 次，llo:1 次，o:1 次，空格:1 次，W:1 次, r:1 次，d:1 次，句号:1 次} 假设预设停止合并条件为：词汇表大小不超过 9 个词汇，则此时即完成了词汇表的生成过程。\n以上是英文构建词汇表，对于中文来说类是，比如中文语料库：台风又双叒叕来了！\n第 1 步，初始化词汇表：\u0026lsquo;台\u0026rsquo;, \u0026lsquo;风\u0026rsquo;, \u0026lsquo;又\u0026rsquo;, \u0026lsquo;双\u0026rsquo;, \u0026lsquo;叒\u0026rsquo;, \u0026lsquo;叕\u0026rsquo;, \u0026lsquo;来\u0026rsquo;, \u0026lsquo;了\u0026rsquo;, \u0026lsquo;！\u0026rsquo; 第 2 步，统计所有相邻字符对的频率：(\u0026lsquo;台\u0026rsquo;, \u0026lsquo;风\u0026rsquo;): 1，(\u0026lsquo;风\u0026rsquo;, \u0026lsquo;又\u0026rsquo;): 1，(\u0026lsquo;又\u0026rsquo;, \u0026lsquo;双\u0026rsquo;): 1，(\u0026lsquo;双\u0026rsquo;, \u0026lsquo;叒\u0026rsquo;): 1，(\u0026lsquo;叒\u0026rsquo;, \u0026lsquo;叕\u0026rsquo;): 1，(\u0026lsquo;叕\u0026rsquo;, \u0026lsquo;来\u0026rsquo;): 1，(\u0026lsquo;来\u0026rsquo;, \u0026lsquo;了\u0026rsquo;): 1，(\u0026lsquo;了\u0026rsquo;, \u0026lsquo;！\u0026rsquo;): 1 第 3 步，合并最常见的字符对：由于字符对的频率相同，因此可选择任意一个进行合并，比如合并(\u0026lsquo;台\u0026rsquo;, \u0026lsquo;风\u0026rsquo;)为：\u0026lsquo;台风\u0026rsquo;, \u0026lsquo;又\u0026rsquo;, \u0026lsquo;双\u0026rsquo;, \u0026lsquo;叒\u0026rsquo;, \u0026lsquo;叕\u0026rsquo;, \u0026lsquo;来\u0026rsquo;, \u0026lsquo;了\u0026rsquo;, \u0026lsquo;！\u0026rsquo; 继续第 2 步，统计频率；然后第 3 步合并字符对，直到达到终止条件 Token 常用操作：分词、编码和解码、添加 Token 等 有了分词器和词表，我们就可以对输入的文件进行分词、映射 ID、根据 Token ID 解码成文本、往词表中添加 Token 等操作。\n老牛同学下面展示的代码片段，多次使用 tokenizer 实例，建议使用 Jupyter Lab 编辑器：大模型应用研发基础环境配置（Miniconda、Python、Jupyter Lab、Ollama 等）\n1 2 3 4 5 6 text = \u0026#39;Transformers分词：台风又双叒叕来了！\u0026#39; tokens = tokenizer.tokenize(text) print(tokens) # 输出：[\u0026#39;Transform\u0026#39;, \u0026#39;ers\u0026#39;, \u0026#39;åĪĨ\u0026#39;, \u0026#39;è¯į\u0026#39;, \u0026#39;ï¼ļ\u0026#39;, \u0026#39;åı°é£İ\u0026#39;, \u0026#39;åıĪ\u0026#39;, \u0026#39;åıĮ\u0026#39;, \u0026#39;åı\u0026#39;, \u0026#39;Ĵ\u0026#39;, \u0026#39;åıķ\u0026#39;, \u0026#39;æĿ¥äºĨ\u0026#39;, \u0026#39;ï¼ģ\u0026#39;] 从上述输出可以看出：Transformers单词被分成了Transform和ers两个 Token。我们可以把上面的 Token 映射其 Token ID：\n1 2 3 4 5 ids = tokenizer.convert_tokens_to_ids(tokens) print(ids) # 输出：[8963, 388, 17177, 99689, 5122, 108118, 99518, 99493, 5758, 240, 122378, 101161, 6313] 以上 2 步操作的结果，其实可以通过编码方法一步完成：\n1 2 3 4 5 6 # 编码 token_ids = tokenizer.encode(text) print(token_ids) # 输出：[8963, 388, 17177, 99689, 5122, 108118, 99518, 99493, 5758, 240, 122378, 101161, 6313] 可以看出：tokenizer.encode编码操作，其实是上面tokenizer.tokenize分词和tokenizer.convert_tokens_to_ids映射 2 个操作的组合。\nToken ID 是计算机识别的，我们可以通过词表和分词器把 Token ID解码成文本内容：\n1 2 3 4 5 6 # 解码 token_text = tokenizer.decode(token_ids) print(token_text) # 输出：Transformers分词：台风又双叒叕来了！ 接下来，我们来看看如何往词表中增加 Token：添加普通 Token 和添加特殊 Token。\n1 2 3 4 5 6 7 8 9 # 添加普通Token，词表中已存在的Token会被忽略 new_tokens = [\u0026#34;老牛同学\u0026#34;, \u0026#34;imxulin\u0026#34;] new_tokens = set(new_tokens) - set(tokenizer.vocab.keys()) num_add_tokens = tokenizer.add_tokens(list(new_tokens)) print(f\u0026#39;新增加 {num_add_tokens}个普通Token到词表。\u0026#39;) # 输出：新增加 2个普通Token到词表。 添加特殊 Token 的方法是：add_special_tokens，入参是字典，键只能从bos_token, eos_token, unk_token, sep_token, pad_token, cls_token, mask_token, additional_special_tokens中选择：\n1 2 3 4 5 6 7 8 9 10 11 # 添加特殊Token，词表存在则忽略 mew_special_tokens = {\u0026#39;cls_token\u0026#39;: \u0026#39;[LNTX]\u0026#39;} num_add_spec_tokens = tokenizer.add_special_tokens(mew_special_tokens) print(f\u0026#39;新增加 {num_add_spec_tokens}个特殊Token到词表。\u0026#39;) print(f\u0026#39;特殊Token值：{tokenizer.cls_token}\u0026#39;) # 输出： # 新增加 1个特殊Token到词表。 # 特殊Token值：[LNTX] 解下来，我们可以验证以下我们添加的 Token 了：\n1 2 3 4 5 6 text = \u0026#39;[LNTX]大家[LNTX]好，我是老牛同学，他是一位[LNTX]大模型[LNTX]爱好者！\u0026#39; tokens = tokenizer.tokenize(text) print(tokens) # 输出：[\u0026#39;å¤§å®¶\u0026#39;, \u0026#39;[LNTX]\u0026#39;, \u0026#39;å¥½\u0026#39;, \u0026#39;ï¼Į\u0026#39;, \u0026#39;æĪĳæĺ¯\u0026#39;, \u0026#39;老牛同学\u0026#39;, \u0026#39;ï¼Į\u0026#39;, \u0026#39;ä»ĸ\u0026#39;, \u0026#39;æĺ¯ä¸Ģä½į\u0026#39;, \u0026#39;å¤§\u0026#39;, \u0026#39;æ¨¡åŀĭ\u0026#39;, \u0026#39;[LNTX]\u0026#39;, \u0026#39;çĪ±å¥½èĢħ\u0026#39;, \u0026#39;ï¼ģ\u0026#39;] 根据输出我们可以看到：新添加的[LNTX]特殊 Token，和老牛同学普通 Token，在分词结果中都直接作为了一个完整的 Token，没有被进一步的切分。\n最后，当我们更新了词表后，为了能让大模型推理过程能正常进行，我们还需要调整模型的 embedding 矩阵大小：\n1 2 3 4 5 6 7 8 9 print(f\u0026#39;调整前：{model.model.embed_tokens.weight.size()}\u0026#39;) model.resize_token_embeddings(len(tokenizer)) print(f\u0026#39;调整后：{model.model.embed_tokens.weight.size()}\u0026#39;) # 输出： # 调整前：torch.Size([151936, 1536]) # 调整后：torch.Size([151668, 1536]) 分词器的其他用法，如编码和解码多段文本、Token ID 张量填充对齐、超长截断等，请大家阅读官网，有中文版：https://hf-mirror.com/docs/transformers/v4.46.0/zh/index\n往期推荐文章：\n基于 Qwen2.5-Coder 模型和 CrewAI 多智能体框架，实现智能编程系统的实战教程\nvLLM CPU 和 GPU 模式署和推理 Qwen2 等大语言模型详细教程\n基于 Qwen2/Lllama3 等大模型，部署团队私有化 RAG 知识库系统的详细教程（Docker+AnythingLLM）\n使用 Llama3/Qwen2 等开源大模型，部署团队私有化 Code Copilot 和使用教程\n基于 Qwen2 大模型微调技术详细教程（LoRA 参数高效微调和 SwanLab 可视化监控）\nChatTTS 长音频合成和本地部署 2 种方式，让你的“儿童绘本”发声的实战教程\ntransformers 推理 Qwen2.5 等大模型技术细节详解(一)transformers 包和对象加载\ntransformers 推理 Qwen2.5 等大模型技术细节详解(二)AutoModel 初始化和模型加载\n","date":"2024-10-31T00:00:00Z","image":"https://ntopic.cn/p/2024103101/00_huc89b245422ea1f5d6529b8e431bfaabe_174035_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/2024103101/","title":"深入解析 Transformers 框架（三）：Qwen2.5 大模型的 AutoTokenizer 技术细节"},{"content":"接上文：transformers 推理 Qwen2.5 等大模型技术细节详解(一)transformers 包和对象加载\n老牛同学和大家通过 Transformers 框架的一行最常见代码from transformers import AutoModelForCausalLM，走读了transformers 包初始化代码的整个流程。从中体会到了dummy对象、LazyModule延迟模块和按需加载、API 简化使用等这些transformers 包的设计美学，为我们设计超大 Python 包提供了很好的借鉴思路。\n本文是使用 Transformers 推理 LLM 技术细节的第 2 篇文章，我们将基于 Qwen2.5 大模型，通过模型配置和from_pretrained代码，了解AutoModel模型初始化的技术细节：\n环境准备：配置虚拟环境，下载 Qwen2.5 模型文件 Qwen2.5 模型配置文件config.json各个配置项说明，并介绍AutoConfig加载文件到Qwen2Config配置 走读AutoModelForCausalLM.from_pretrained方法代码，详细介绍模型初始化流程 在最后，老牛同学又来搞个小活动，免费包邮送几本新书给大家，期望大家踊跃参与 在以上 Transformers 框架推理 LLM 的核心框架代码中，我们本次将走读的代码是from_pretrained方法：\n1 2 3 4 5 6 7 from transformers import AutoModelForCausalLM model = AutoModelForCausalLM.from_pretrained( model_dir, torch_dtype=\u0026#34;auto\u0026#34;, device_map=\u0026#34;auto\u0026#34;, local_files_only=True, ) 环境准备：配置虚拟环境和下载 Qwen2.5 模型文件 【配置虚拟环境】 在上一篇我们已经配置好了虚拟环境，老牛同学就只简单罗列一下配置脚本：\n1 2 3 4 5 6 7 8 9 10 # Python虚拟环境名：Qwen2.5，版本号：3.10 conda create -n Qwen2.5 python=3.10 -y # 激活虚拟环境 conda activate Qwen2.5 # 下载Python依赖包 pip install torch pip install \u0026#34;transformers\u0026gt;=4.43.1\u0026#34; pip install \u0026#34;accelerate\u0026gt;=0.26.0\u0026#34; 【下载 Qwen2.5 模型文件】 老牛同学存放模型文件目录：Qwen2.5-1.5B-Instruct\n1 2 3 4 5 # Git大文件系统 git lfs install # 下载模型文件 git clone https://www.modelscope.cn/qwen/Qwen2.5-1.5B-Instruct.git Qwen2.5-1.5B-Instruct 若下载过程中异常中断，可以通过git lfs install命令继续下载：\n1 2 3 4 5 6 # 切换到Git目录 cd Qwen2.5-1.5B-Instruct # 中断继续下载 git lfs install git lfs pull 下载完成，可以看到如下文件结构：\n针对本文大模型初始化来说，我们重点关注config.json模型配置文件和model.safetensors模型文件：\nmodel.safetensors文件是模型文件（主要为权重参数数据），它存储了验证文件完整性的校验和、模型张量元数据和张量权重参数等。我们将在代码流程中详细说明这些数据。 config.json文件是模型的配置文件，包括了模型框架、训练设置、各种超参数等信息： config.json参数详解和 AutoConfig 初始化 老牛同学对config.json文件中的每个参数进行简单说明：\n参数名 配置值 参数说明 architectures Qwen2ForCausalLM 模型的架构名称，Qwen2ForCausalLM是 Qwen 推理模型，在模型初始化方式将会实际使用 attention_dropout 0.0 注意力机制中的 Dropout 操作概率，0.0代表不对注意力权重进行随机置零操作，即保留所有的注意力权重 bos_token_id 151643 文本序列开始 Token ID 标记（Begin of Sentence Token ID），代表文本片段开始位置 eos_token_id 151645 文本序列结束 Token ID 标记（End of Sentence Token ID），代表文本片段结束位置 hidden_act silu 模型隐藏层使用SiLU激活函数（其他激活函数：ReLU，Tanh，Sigmoid 等） hidden_size 1536 隐藏层的维度（或每个隐藏层中神经元的数量），数值越大意味着模型能学习更复杂的特征和模式 initializer_range 0.02 模型训练时初始化权重的标准差，0.02代表权重从均值为 0、标准差为 0.02 的正态分布中随机初始化 intermediate_size 8960 前馈神经网络中间层的维度，通常要比hidden_size大得多，用于增加模型学习能力 max_position_embeddings 32768 模型可以处理的最大序列长度，32768代表模型可以处理最长为 32768 个 Token 的输入序列 max_window_layers 21 模型在处理长序列时，最多可以应用多少层的滑动窗口策略，即可以对长序列进行多少次分隔和处理 model_type qwen2 Qwen2 模型类型标识，用于映射 Qwen2Config 和 Qwen2ForCausalLM 等实际类 num_attention_heads 12 多头注意力机制中注意力头的数量，12代表每个隐藏层中使用 12 个注意力头 num_hidden_layers 28 模型隐藏层的数量 num_key_value_heads 2 多头自注意力机制中键（Key）和值（Value）的注意力头数量，共12个注意头而键值只有2个头，则意味着查询（Query）头可以共享相同的键和值头 rms_norm_eps 1e-06 使用 RMSNorm 归一化技术处理，分母增加一个极小值，避免除以零的情况，保证归一化的有效性 rope_theta 1000000.0 RoPE 旋转位置编码的周期性因子，用于捕捉长距离依赖关系，提高模型的数值稳定性 sliding_window 32768 模型在处理长序列时采用的滑动窗口序列长度，即通过滑动窗口的方式处理超过max_position_embeddings的长序列策略 tie_word_embeddings true 模型的输入和输出层之间共享词嵌入矩阵标识 torch_dtype bfloat16 模型权重采用 bfloat16 浮点数格式（1 位符号位+8 位指数位+7 位尾数位），它区别于 float16 半精度浮点数格式（1 位符号位 5 位指数位+10 位尾数位），通过增加指数位数以提高数字精度 transformers_version 4.43.1 Transformers 推理模型的版本号，通常是指最低版本，pip install \u0026quot;transformers\u0026gt;=4.43.1\u0026quot;，我们依赖包版本为4.45.1 use_cache true 模型推理过程中使用缓存机制标识，用于缓存中间结果，避免重复计算 use_sliding_window false 模型处理长序列时是否使用滑动窗口机制，false代表禁用滑动窗口机制，则意味着模型将尝试一次性处理整个长序列，可能会导致内存占用和计算复杂度增加 vocab_size 151936 模型词汇表的大小，AutoTokenizer 我们将会介绍 特别注意：在上述配置参数中，use_sliding_window = false意味着模型禁用滑动窗口机制，即sliding_window = 32768参数设置不生效，也就是模型能够处理的最大序列长度为max_position_embeddings = 32768个 Token，超过这个长度的序列将无法正确处理；如果我们需要处理接近或超过32768 个 Token 的长序列，建议启用滑动窗口机制（use_sliding_window = true）！\n模型配置参数，我们可以通过AutoConfig进行加载和打印出各个参数项：\n1 2 3 4 5 6 7 8 9 10 11 12 import os from transformers import AutoConfig # 加载配置文件 config_path = os.path.join(\u0026#39;D:\u0026#39;, os.path.sep, \u0026#39;ModelSpace\u0026#39;, \u0026#39;Qwen2.5\u0026#39;, \u0026#39;Qwen2.5-1.5B-Instruct\u0026#39;, \u0026#39;config.json\u0026#39;) config = AutoConfig.from_pretrained(config_path) # 查看词汇表大小，输出: 151936 print(config.vocab_size) # 查看配置所有内容 print(config) 最终输出结果如下，config返回结果为Qwen2Config实例：\n是不是感觉有点奇怪，我们并没有Qwen2Config相关配置，但最终结果就有了它？接下来，我们来一起看下代码AutoConfig.from_pretrained流程：\n首先，根据我们第 1 篇文章的介绍，我们能快速定位到AutoConfig所在模块为models.auto：\n1 2 3 4 5 6 7 8 _import_structure = { # .... \u0026#34;models.auto\u0026#34;: [ # .... \u0026#34;AutoConfig\u0026#34;, # .... ] } 然后，我们打开 Transformers 包下 ./models/auto/__init__.py 文件，可以定位到AutoConfig类型所在模块configuration_auto.py：\n1 2 3 4 5 _import_structure = { # ... \u0026#34;configuration_auto\u0026#34;: [\u0026#34;CONFIG_MAPPING\u0026#34;, \u0026#34;MODEL_NAMES_MAPPING\u0026#34;, \u0026#34;AutoConfig\u0026#34;], # ... } AutoConfig.from_pretrained方法定义如下，红框代码比较关键：\n下面对红框代码进行详细说明：\nconfig_dict, * = PretrainedConfig.get_config_dict(config_path, _)代码行：这行代码逻辑稍微比较简单，就是解析config.jsonJSON 格式文件，返回字典对象 config_class = CONFIG_MAPPING[config_dict[\u0026quot;model_type\u0026quot;]]代码行：前面的配置文件，config_dict[\u0026quot;model_type\u0026quot;]的结果是qwen2，而CONFIG_MAPPING是一个字典常量，它的定义如下： 1 2 3 4 5 6 7 8 9 10 11 CONFIG_MAPPING_NAMES = OrderedDict( [ # ... (\u0026#34;qwen2\u0026#34;, \u0026#34;Qwen2Config\u0026#34;), (\u0026#34;qwen2_audio\u0026#34;, \u0026#34;Qwen2AudioConfig\u0026#34;), (\u0026#34;qwen2_audio_encoder\u0026#34;, \u0026#34;Qwen2AudioEncoderConfig\u0026#34;), (\u0026#34;qwen2_moe\u0026#34;, \u0026#34;Qwen2MoeConfig\u0026#34;), (\u0026#34;qwen2_vl\u0026#34;, \u0026#34;Qwen2VLConfig\u0026#34;), # ... ] ) 因此，这行代码的最终结果config_class内容为Qwen2Config，因此它就是从这里来的。\n根据我们第 1 篇文章的介绍，我们快速定位到Qwen2Config类所在模块为 ./models/qwen2/configuration_qwen2.py，它的定义如下：\n从Qwen2Config类的定义我们可以看到：它没有具体方法，因此它的方法均来自PretrainedConfig父类；同时，类构造函数，有vocab_size/hidden_size等命名参数，而这些命名参数，正是前面介绍的config.json配置文件参数项。\nreturn config = class.from_dict(config_dict, _)代码行：最终返回**Qwen2Config.from_dict()方法调用，我们打开PretrainedConfig.from_dict()**方法： 核心代码行config = cls(**config_dict)即实例化Qwen2Config类，入参即config.json的配置项，最终完成了我们最开始的print(config)输出结果闭环。\n到这里，我们基本完成了config.json配置项说明和用法，老牛同学整理了AutoConfig.from_pretrained(config_path)代码行的执行流程：\nAutoModelForCausalLM.from_pretrained 模型加载 我们还是根据第 1 篇文章方法，找到AutoModelForCausalLM所在模块为 ./models/auto/modeling_auto.py，它的定义如下：\n1 2 3 4 5 class AutoModelForCausalLM(_BaseAutoModelClass): _model_mapping = MODEL_FOR_CAUSAL_LM_MAPPING AutoModelForCausalLM = auto_class_update(AutoModelForCausalLM, head_doc=\u0026#34;causal language modeling\u0026#34;) 其中，MODEL_FOR_CAUSAL_LM_MAPPING是一个常量字典，存放的是模型类型和模型类的映射关系。根据配置我们可以知道，Qwen2ForCausalLM是实际的模型类：\n1 2 3 4 5 6 7 8 MODEL_FOR_CAUSAL_LM_MAPPING_NAMES = OrderedDict( [ # ... (\u0026#34;qwen2\u0026#34;, \u0026#34;Qwen2ForCausalLM\u0026#34;), (\u0026#34;qwen2_moe\u0026#34;, \u0026#34;Qwen2MoeForCausalLM\u0026#34;), # ... ] ) auto_class_update(XXXLM, _)方法是一个非常有趣的方法，这个方法把 _BaseAutoModelClass类的from_config和from_pretrained方法复制给了XXXLM类。因此，当我们调用XXXLM.from_config或者XXXLM.from_pretrained方法时，方法的内部逻辑与 _BaseAutoModelClass.from_config和 _BaseAutoModelClass.from_pretrained实际方法一致！\n接下来，我们来看看 AutoModelForCausalLM.from_pretrained() 的执行逻辑：\n1 2 3 4 5 6 model = AutoModelForCausalLM.from_pretrained( model_dir, torch_dtype=\u0026#34;auto\u0026#34;, device_map=\u0026#34;auto\u0026#34;, local_files_only=True, ) _BaseAutoModelClass.from_pretrained 方法有点长，老牛同学截图最关键的几行代码：\n模型配置加载：AutoConfig.from_pretrained，结果为Qwen2Config配置类 通过同样的方式，根据配置文件model_type模型类型映射到实际类，最终执行Qwen2ForCausalLM.from_pretrained方法 Qwen2ForCausalLM类定义：继承Qwen2PreTrainedModel父类，包含了Qwen2Config配置，和Qwen2Model为实际的模型类\n我们继续查看Qwen2PreTrainedModel类：它继承PreTrainedModel类，并没有定义方法。PreTrainedModel类是所有模型的基类，它继承nn.Module类，它定义了**from_pretrained()**方法：\n这个方法代码行超级长，老牛同学把它大致分为以下4 部分：\n解析模型文件路径，即我们下载的权重文件：./Qwen2.5-1.5B-Instruct/model.safetensors 1 2 3 4 5 6 7 8 9 10 11 12 13 elif use_safetensors is not False and os.path.isfile( os.path.join(pretrained_model_name_or_path, subfolder, _add_variant(SAFE_WEIGHTS_NAME, variant)) ): # Load from a safetensors checkpoint archive_file = os.path.join( pretrained_model_name_or_path, subfolder, _add_variant(SAFE_WEIGHTS_NAME, variant) ) # ...省略... if is_local: logger.info(f\u0026#34;loading weights file {archive_file}\u0026#34;) resolved_archive_file = archive_file 其中，SAFE_WEIGHTS_NAME常量值为model.safetensors，archive_file变量即为权重文件完整路径，最终赋值给了resolved_archive_file变量。\n解析模型文件，获取模型元数据，模型格式为pt即为 PyTorch 深度学习框架，并加载模型权重： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 if ( is_safetensors_available() and isinstance(resolved_archive_file, str) and resolved_archive_file.endswith(\u0026#34;.safetensors\u0026#34;) ): with safe_open(resolved_archive_file, framework=\u0026#34;pt\u0026#34;) as f: metadata = f.metadata() if metadata.get(\u0026#34;format\u0026#34;) == \u0026#34;pt\u0026#34;: pass # ...省略... from_pt = not (from_tf | from_flax) # load pt weights early so that we know which dtype to init the model under if from_pt: if not is_sharded and state_dict is None: # Time to load the checkpoint state_dict = load_state_dict(resolved_archive_file) 实例化AutoModelForCausalLM类，model变量即为对象实例： 1 2 3 4 5 6 with ContextManagers(init_contexts): # Let\u0026#39;s make sure we don\u0026#39;t run the init function of buffer modules model = cls(config, *model_args, **model_kwargs) # make sure we use the model\u0026#39;s config since the __init__ call might have copied it config = model.config 加载模型，并设置模型为推理模式，同时设置模型配置，最后分发模型： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 elif from_pt: # restore default dtype if dtype_orig is not None: torch.set_default_dtype(dtype_orig) ( model, missing_keys, unexpected_keys, mismatched_keys, offload_index, error_msgs, ) = cls._load_pretrained_model( model, state_dict, # ...省略... # ...省略... # Set model in evaluation mode to deactivate DropOut modules by default model.eval() # ...省略... model.generation_config = GenerationConfig.from_pretrained( pretrained_model_name_or_path, cache_dir=cache_dir, force_download=force_download, resume_download=resume_download, proxies=proxies, local_files_only=local_files_only, token=token, revision=revision, subfolder=subfolder, _from_auto=from_auto_class, _from_pipeline=from_pipeline, **kwargs, ) # ...省略... if not is_fsdp_enabled() and not is_deepspeed_zero3_enabled(): dispatch_model(model, **device_map_kwargs) 最终，当我们执行model = AutoModelForCausalLM.from_pretrained()代码的结果，即为Qwen2ForCausalLM类的实例，它就是 PyTorch 的nn.Module的实例。老牛同学把整个处理流程整理如下：\n至此，使用 Transformers 初始化大模型部分就基本结束了，老牛同学在接下来的文章，将与大家一起看看文本AutoTokenizer技术，它也是大模型推理的关键部分~\n最后：又来给大家免费包邮送几本新书 前面我们已经举办过 3 次免费包邮送书活动：\nChatTTS 长音频合成和本地部署 2 种方式，让你的“儿童绘本”发声的实战教程（文末有福利） 使用世界领先的 Qwen2.5-Math 开源模型当 AI 数学老师，让奥数解题辅导父慈子孝（文末有福利） transformers 推理 Qwen2.5 等大模型技术细节详解(一)transformers 包和对象加载（文末免费送书） 本期，老牛同学继续和出版社朋友合作，举办第 4 次送书福利小活动，新书是：《Python 金融大数据分析快速入门和案例详解》\n本期送书小活动的规则和之前类似，总体如下：\n关注老牛同学微信公众号，因为这是老牛同学公众号关注者的小福利 在本文的评论区进行留言，留言的点赞数排名前 3 名的朋友，免费且包邮获得 1 本书（即：默认送 3 本书） 若本文的阅读量和老牛同学 CSDN 同文的阅读量之和达到了2000（含2000），则留言的点赞数排名前 6 名的朋友均免费包邮送书（即：最多送 6 本书） 活动时间：2024 年 10 月 27 日到 2024 年 11 月 3 号上午 12 点整（正好 1 周，周末可寄送新书） 老牛同学不参与本次活动（因为老牛同学默认就有 1 本） 为方便大家购买本书，老牛同学贴上京东商品链接：\n【书籍购买链接】\n往期推荐文章：\n基于 Qwen2.5-Coder 模型和 CrewAI 多智能体框架，实现智能编程系统的实战教程\nvLLM CPU 和 GPU 模式署和推理 Qwen2 等大语言模型详细教程\n基于 Qwen2/Lllama3 等大模型，部署团队私有化 RAG 知识库系统的详细教程（Docker+AnythingLLM）\n使用 Llama3/Qwen2 等开源大模型，部署团队私有化 Code Copilot 和使用教程\n基于 Qwen2 大模型微调技术详细教程（LoRA 参数高效微调和 SwanLab 可视化监控）\nChatTTS 长音频合成和本地部署 2 种方式，让你的“儿童绘本”发声的实战教程\ntransformers 推理 Qwen2.5 等大模型技术细节详解(一)transformers 包和对象加载\n","date":"2024-10-26T00:00:00Z","image":"https://ntopic.cn/p/2024102601/00_hu9af697bca531050727e02c25c2fd1c01_132520_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/2024102601/","title":"transformers 推理 Qwen2.5 等大模型技术细节详解(二)AutoModel 初始化和模型加载（免费送新书）"},{"content":"上周收到一位网友的私信，希望老牛同学写一篇有关使用 transformers 框架推理大模型的技术细节的文章。\n老牛同学刚开始以为这类的文章网上应该会有很多，于是想着百度几篇质量稍高一点的回复这位网友。结果，老牛同学搜索后发现，类似文章确实不少，但是总觉得不太满意，要么细节深度不够，要么介绍不够全面，感觉达不到网友希望的技术细节要求。为了不辜负这位网友的期望，老牛同学决定自己动手，丰衣足食。\n其实，我们使用 transformers 框架推理大模型的技术细节非常多，仅主链路就包括了模型加载、数据预处理、数据转换、模型推理、解码输出等。老牛同学本着宁缺毋滥尽善尽美的原则，期望能把这些技术细节完整全面介绍一遍，考虑到文章篇幅长度，计划共分为3 篇完成，希望能启动抛砖引玉之用：\n第 1 篇：也就是本文，核心介绍transformers包 API 设计美学、如何初始化、LazyModule 模块、按需导入指定对象等 第 2 篇：基于 Qwen2.5 大模型，核心介绍AutoModel模型初始化from_pretrained过程细节（PyTorch/TensorFlow/Flax 深度学习框架的其他模型也类似） 第 3 篇：同样基于 Qwen2.5 大模型，核心介绍AutoTokenizer初始化from_pretrained，和结合 Qwen2.5 大模型的完整推理过程技术细节 请原谅老牛同学水平有限，文中难免会出现遗漏和错误之处，恳请大家及时留言指出，以帮助老牛同学和其他阅读到本文的网友，让我们共同学习进步，在此万分感谢。\n下面截图的寥寥几行代码，相信大家已经比较熟悉，因为它们经常出现在老牛同学的文章中。也就是这么几行代码，却涵盖了使用 transformers 框架进行大模型推理的核心代码框架。本系列的 3 篇文章，也将围绕这几行代码逐步展开：\n要想详细介绍 Transformers 推理技术细节，我们就从 transformers 推理框架本身开始，老牛同学觉得这行代码就已足够：from transformers import XXX，代码中的XXX具体是哪个对象其实并不太重要，但为了后续介绍和演示方便，在本文中老牛同学就以AutoModelForCausalLM代替XXX。\n是的，你没有看错，本文主要就只介绍这 1 行代码，这行在我们看来在简单熟悉不过的代码。想想看，对于 Transformers 这个大模型研究者和使用者们首选的框架来说，按理我们使用起来应该会很复杂，但从上面推理程序代码可以看到，我们使用这个框架时却似乎很简单，那么可能的解释是：Transformers 框架设计得特别的精妙！\n老牛同学可以这么说：如果能把这行看似简单的代码真正读懂，在我们目前或将来设计像 transformers 这种超级 Python 包时，我们将会得心应手、游刃有余！\n我们把本文分为以下几个主要章节部分，最终完成这行代码的介绍：\n前期环境准备，主要是下载 transformers 包代码，包括环境配置和包安装 transformers 包的初始化过程，即代码前半部分：from transformers，我们可以看到transformers 包的设计和我们日常研发很不一样 transformers 包导入类过程，即代码后半部分：import AutoModelForCausalLM，让我们看看 transformers 包在设计上的精妙之处 在最后，老牛同学又来搞个小活动，免费包邮送几本新书给大家，期望大家踊跃参与 环境准备，下载 transformers 包代码 我们依然使用Miniconda来管理 Python 虚拟环境，Miniconda的安装和使用可以参考老牛同学之前的文章：大模型应用研发基础环境配置（Miniconda、Python、Jupyter Lab、Ollama 等）\n首先，我们配置虚拟环境：\n1 2 3 4 5 # Python虚拟环境名：Qwen2.5，版本号：3.10 conda create -n Qwen2.5 python=3.10 -y # 激活虚拟环境 conda activate Qwen2.5 然后，在虚拟环境中下载依赖包：\n1 2 3 pip install torch pip install \u0026#34;transformers\u0026gt;=4.37.0\u0026#34; pip install \u0026#34;accelerate\u0026gt;=0.26.0\u0026#34; 环境准备好了，我们接下来就来看前半部分代码了~\n代码行：from transformers 代码详解 这行代码意思是引入 transformers 模块，它是 Python 从一个模块导入指定模块或对象的标准语法。\n那么，Python 怎么知道去哪里找到transformers这个模块呢？\n首先，Python 检查sys.modules模块字典中是否存在名transformers的模块，这个字典存放着内置模块和已经导入过的模块，如果存在则直接返回，否则 进一步搜索模块，逐一遍历sys.path目录列表，最后在site-packages目录下找到transformers包（文件夹） 我们可以通过以下代码，查看 Python 解释器启动时，默认的搜索模块目录列表，和获取site-packages目录位置的方式：\n1 2 3 4 5 6 7 8 # 包或者模块搜索目录列表 import sys print(sys.path) # 查看 site-packages 目录的位置 import site for directory in site.getsitepackages(): print(directory) site-packages目录作用：它存放我们安装的第三方包和模块，我们通过pip install 模块名命令安装包和模块，默认都存放在该目录中。\nPython 研发小技巧： 当我们希望动态改变模块搜索目录，用于加载自定义模块时，我们可以在sys.path列表中添加自定义的模块目录即可实现 如：把自定义模块目录添加模块搜索目录列表中：sys.path.append('/a/b/c/my-modules')\n接下来，Python 会对transformers包进行初始化，即执行目录下面的__init__.py文件代码。对于 Python 来说，__init__.py代码文件主要有 2 个作用：\n告诉 Python 该目录不是一个普通目录，它是一个 Python 包 该文件内容是这个包的初始化代码，第一次加载包时需要执行这些代码 接下来，我们来看看transformers 包的初始化__init__.py代码文件内容（开始进入主题了）：\n文件内容总共有9 千多行代码，前面是一些普通的变量定义和对象导入等代码，和我们日常研发无异，无需过多关注，直到_import_structure变量首次出现： _import_structure变量是一个字典，它其实是在收集模块名和对象（类型、方法和变量）关系的字典。字典的键是模块名（包名和模块名），字典值是对象名。\n我们前面有提到，Transformer 支持 PyTorch/TensorFlow/Flax 这 3 个深度学习框架，对于每个框架它都有对应着收集不同的对象： Transformer 根据当前环境支持的不同深度学习框架，收集不同的字典内容，三个框架是否支持的判断条件：is_torch_available()、is_torch_available()和is_flax_available()\n同时，通过try/except/else的方式，如果当前环境支持某个深度学习框架，那么在else代码块里收集该框架对应的字典内容；否则抛出OptionalDependencyNotAvailable错误，然后通过except代码块捕获错误，并通过utils.dummy_XX_objects模块收集字典内容，保证最终字典内容包含了完整模块对象。\n我们打开dummy模块（如：dummy_pt_objects.py）代码可以看到，它定义了else块中的对象，但是没有任何方法，简单理解它其实就是一个占位符！\n问题 1：当我们使用 Transformer 框框推理某个预训练模型（如 Qwen2.5），它使用的深度学习框架只会属于某一个，我们只收集当前所支持的对象不可以吗，为什么还需要设置dummy模块，一定要保证完整的收集到所有对象呢，并且这些dummy对象也没有实际可用的方法？\n解读 1：Transformers 框架是一个功能齐全的超级库，它除了数据集、模型、训练等常用功能之外，还有很多其他功能和模块，比如配置、分词器等。如果我们程序只需要其他模块（如：与框架无关的工具类），但是因不小心或者代码注解的需要，引入了与框架相关的类（即else块中或者dummy中的那些模型或对象），如果没有dummy中对象定义，就直接ImportError了，而有了dummy我们就可以正常使用。同时，而当我们实际使用了 dummy 对象时，我们也可以收到一个明确的错误提示，进而帮助我们进一步排查处理。\n收集_import_structure字典的代码行非常多，模式都一样，基本都是try/except/else代码块。接下来，我们以AutoModel类为例，来看看 3 个深度学习框架它们对应的类名：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 # PyTorch框架 _import_structure[\u0026#34;models.auto\u0026#34;].extend( [ # .... \u0026#34;AutoModel\u0026#34;, \u0026#34;AutoModelForCausalLM\u0026#34;, # ... ] ) # TensorFlow框架 _import_structure[\u0026#34;models.auto\u0026#34;].extend( [ # .... \u0026#34;TFAutoModel\u0026#34;, \u0026#34;TFAutoModelForCausalLM\u0026#34;, # ... ] ) # Flax框架 _import_structure[\u0026#34;models.auto\u0026#34;].extend( [ # .... \u0026#34;FlaxAutoModel\u0026#34;, \u0026#34;FlaxAutoModelForCausalLM\u0026#34;, # ... ] ) 从上面代码可以看出，它们的模块名都是models.auto，但是它们的类名却不一样，TensorFlow 和 Flax 分别增加了TF和Flax前缀。其实这可以理解，否则就有 3 个或更多重名的类了，下一节我们还会看到，_LazyModule初始化时会反转键值对，因此必须不能重名。\n问题 2：在__init__.py代码文件中，还有个重要常量TYPE_CHECKING，在if TYPE_CHECKING:代码块中，它显示的导入了和_import_structure收集的一样的所有对象，一个显示导入，一个只是字符串收集，这又是为什么呢？\n解读 2：TYPE_CHECKING是在 Python 内置的typing.py模块中定义的常量（Python 一般约定：变量名大写则称该变量为常量），当 PyCharm/mypy 等工具进行类型检查时，该常量的值为True，而当程序执行时，它的值为False。Transformer 包非常大，功能非常多，如果我们默认导入全部的模块和对象，势必会大大增加程序的启动时间，而通过if/else条件判断，即保证了类型检查能正常运行，又可以节省程序运行的启动时间和提高执行效率。\n我们继续__init__.py代码文件，_import_structure字典收集完模块和对象之后，又出现了一段有段有意思的代码：\ntransformers 包竟然在自己初始化的时候，自己把自己设置到sys.modules中缓存起来：sys.modules[__name__] = _LazyModule(...)\n问题 3：我们一般在设计包的初始化__init__.py代码文件时，在完成模块导入或对象定义之后，代码正常结束即可，Python 自动把我们的模块放到sys.modules中缓存起来。为什么transformers 包需要自己设置呢？\n解读 3：从上面的 9 千行代码看到，初始化过程只是在收集_import_structure字典内容，并没有导入实际对象。如果__init__.py就此结束，from transformers import XXX必然会报错。其实，Transformer 包设计的核心就在_LazyModule(...)里，下面我们就揭开它的就是细节。\n代码行：import AutoModelForCausalLM 代码详解 transformers模块初始化__init__.py代码文件的结果，就是自定义模块类_LazyModule，其中_import_structure作为非常重要的初始化参数，它的定义如下：\n_LazyModuel 继承自ModuleType类，说明它是一个模块类型，这也是为什么它可以放入sys.modules的原因 _LazyModuel 有 3 个非常关键的方法，下面会逐一介绍：__init__、__getattr__和_get_module 【__init__初始化方法：把_import_structure字典进行键值反转】\n前面_import_structure字典收集的模块和对象，在_LazyModuel 进行了反转self._class_to_module，在这个字典中，我们可以通过对象名获取到它所在模块名。\n【__getattr__属性获取魔法方法：为动态加载模块提供了机会】\n我们回到最开始的 Python 语句from transformers import AutoModelForCausalLM，在transformers模块初始化的结果是一个自定义的_LazyModule类型模块。我们需要从_LazyModule中查找AutoModelForCausalLM信息：\n首先，Python 在_LazyModule实例的字典__dict__中查找，很明显没有AutoModelForCausalLM属性 然后，Python 在_LazyModule类本身中查找，很明显也没有 最终，若对象实现了__getattr__魔法方法，Python 会调用这个魔法方法获取属性（如果还找不到则会抛出AttributeError异常） 在__getattr__魔方方法中，通过刚才键值反转的字典_class_to_module获取到模块路径，然后调用self._get_module私有方法动态加载模块。在这个私有方法中，通过importlib.import_module动态导入模块。\n问题 4：动态导入模块，又是怎么实现的呢？\n解读 4：我们打开models.auto包的__init__.py初始化文件，可以看到，它的处理方式，和transformers 包的初始化方式同出一辙，也是_LazyModule延迟模块，它们是通过递归的方式，最终获取到了实际的对象：\n下面老牛同学抽取了三个不同框架，AutoModel类的配置：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 # PyTorch框架 _import_structure[\u0026#34;modeling_auto\u0026#34;].extend( [ # .... \u0026#34;AutoModel\u0026#34;, \u0026#34;AutoModelForCausalLM\u0026#34;, # ... ] ) # TensorFlow框架 _import_structure[\u0026#34;modeling_tf_auto\u0026#34;].extend( [ # .... \u0026#34;TFAutoModel\u0026#34;, \u0026#34;TFAutoModelForCausalLM\u0026#34;, # ... ] ) # Flax框架 _import_structure[\u0026#34;modeling_flax_auto\u0026#34;].extend( [ # .... \u0026#34;FlaxAutoModel\u0026#34;, \u0026#34;FlaxAutoModelForCausalLM\u0026#34;, # ... ] ) 也就是说：最终AutoModel在文件./models/auto/modeling_auto.py中定义：\n1 2 3 4 5 6 7 8 # ./models/auto/modeling_auto.py # ... class AutoModelForCausalLM(_BaseAutoModelClass): _model_mapping = MODEL_FOR_CAUSAL_LM_MAPPING AutoModelForCausalLM = auto_class_update(AutoModelForCausalLM, head_doc=\u0026#34;causal language modeling\u0026#34;) # ... 最后一步，也是非常重要的一步：setattr(self, name, value)把对象放入了transformers模块属性列表中，它把本不是transformers模块中的对象，与自己建立了关联！\n问题 5：到这里，transformers包的初始化流程基本完成了，它为什么要搞这么复杂呢？直接使用from transformers.models.auto.modeling_auto import AutoModelForCausalLM不可以吗？\n解答 5：老牛同学认为在使用上完全可以，但是 Transformers 作为一个有望一统天下的大模型推理框架，如果它把 API 设计得越简单、越方便使用，就越容易被使用者接受；同时，内部代码调整对已有的程序也没有任何的影响。是的，它就在提高我们使用的便利性和升级的影响最小化，它处处在为使用者着想，老牛同学觉得这也许就是超级框架的大格局！\n最后，老牛同学试着用一张图，把上面流程总结一下（为了展示方便，把递归给拆开了）：\n我们可以看到，虽然只是一行代码，但是却有着很多值得参考的设计技巧：\n设计dummy对象，让对象的导入能顺序进行，让报错延迟，尽可能让程序能执行 _LazyModule延迟动态模块，按需加载对象，在节省启动时间的同时，减少内存使用 API 设计简单化，把困难留给自己，把便利留给使用者，这样才能得到用户的认可，也只有这样才能走的更稳更远 最后：又来给大家免费包邮送几本新书 前面我们已经举办过 2 次面试包邮送书活动：\nChatTTS 长音频合成和本地部署 2 种方式，让你的“儿童绘本”发声的实战教程（文末有福利） 使用世界领先的 Qwen2.5-Math 开源模型当 AI 数学老师，让奥数解题辅导父慈子孝（文末有福利） 老牛同学继续和出版社朋友合作，举办第 3 次送书福利小活动，新书名为：《硅谷 Python 工程师面试指南》\n大模型应用固然重要，但是 Python 基础知识不容忽视，这本书作者根据亲身经历，讲解了面试技巧、流程和策略等。同时，针对 Python 的数据结构、算法等基础知识，通过丰富的实例，帮忙我们逐步完成系统设计。\n本期送书小活动的规则和之前类似，总体如下：\n关注老牛同学微信公众号，因为这是老牛同学公众号关注者的小福利 在本文的评论区进行留言，留言的点赞数排名前 3 名的朋友，免费且包邮获得 1 本书（即：默认送 3 本书） 若本文的阅读量和老牛同学 CSDN 同文的阅读量之和达到了2000（含2000），则留言的点赞数排名前 6 名的朋友均免费包邮送书（即：最多送 6 本书） 活动时间：2024 年 10 月 18 日到 2024 年 10 月 24 号上午 12 点整（正好 1 周，周末可寄送新书） 老牛同学不参与本次活动（因为老牛同学默认就有 1 本） 为方便大家购买本书，老牛同学贴上京东商品链接（大家也可在购书网站搜索“硅谷 Python 工程师面试指南”直接购买）：\n【书籍购买链接】\n往期推荐文章：\n基于 Qwen2.5-Coder 模型和 CrewAI 多智能体框架，实现智能编程系统的实战教程\nvLLM CPU 和 GPU 模式署和推理 Qwen2 等大语言模型详细教程\n基于 Qwen2/Lllama3 等大模型，部署团队私有化 RAG 知识库系统的详细教程（Docker+AnythingLLM）\n使用 Llama3/Qwen2 等开源大模型，部署团队私有化 Code Copilot 和使用教程\n基于 Qwen2 大模型微调技术详细教程（LoRA 参数高效微调和 SwanLab 可视化监控）\nChatTTS 长音频合成和本地部署 2 种方式，让你的“儿童绘本”发声的实战教程\n","date":"2024-10-18T00:00:00Z","image":"https://ntopic.cn/p/2024101801/00_hu0fbce01b01350acd5184d40c8b394d68_277717_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/2024101801/","title":"transformers 推理 Qwen2.5 等大模型技术细节详解(一)transformers 初始化和对象加载（文末免费送书）"},{"content":"在上篇文章中，我们使用Qwen2.5-Coder编写了一个自动编程的多智能体系统（基于 Qwen2.5-Coder 模型和 CrewAI 多智能体框架，实现智能编程系统的实战教程），着实感受到了Qwen2.5-Coder和CrewAI强强联合所发挥出来的强大威力。\n我们知道最新发布的Qwen2.5全家桶共有 3 个模型，除常规的大语言模型 Qwen2.5 和专门针对编程的 Qwen2.5-Coder 模型之外，还有一个专门针对数学的 Qwen2.5-Math 模型，它号称是世界领先的数学开源大语言模型，我们今天就通过奥数题目来实实在在的感受一下它的强大之处。\n因此，我们将通过以下 3 个主要部分来完成 Qwen2.5-Math 模型的实战之旅：\nQwen2.5-Math的基本介绍，主要是简单了解一下Qwen2.5-Math的特点和能力 本地部署Qwen2.5-Math-1.5B模型，通过一个一元一次方程简单数学题，体验一下其数学推理能力 我们选取小学和初中阶段经典的几道奥数题，让 Qwen2.5-Math-72B 作为 AI 老师，来实战验证其逻辑推导能力 Qwen2.5-Math 基本介绍、CoT 和 TIR 推理方式 Qwen2.5-Math 明确说明：Qwen2.5-Math 主要被设计用于通过CoT或TIR的方式解中英数学题，不推荐在其他任务上使用该系列模型。那么，什么是 CoT 和 TIR 推理方式呢？\n【CoT 推理方式：】即思维链（Chain of Thought），主要目的是让大模型一步一步的展现出其推理过程，而不是直接给答案，就像我们人类逻辑思维过程一样，通过多步分解的方式，能更好的理解和解决复杂问题。如下数学题目的解答过程：\n题目：小明有 10 个苹果，他给了小红 3 个苹果，然后又买了 5 个苹果，请问小明现在有几个苹果？ CoT 推理过程： 第一步：小明最初有 10 个苹果。 第二步：小明给了小红 3 个苹果，所以现在剩下 10-3=7 个苹果。 第三步：小明又买了 5 个苹果，所以现在共有 7+5=12 个苹果。 答案：现在小明有 12 个苹果。 【TIR 推理方式：】即工具集成推理（Tool integrated Reasoning），就是在推理过程中使用外部工具（如：使用 Python 执行代码获取结果）。TIR 是Qwen2.5-Math的新特性，它能显著提升中英文的数学解题能力，包括精确计算、符号操作和算法操作等方面。具体用法可以阅读样例代码：https://github.com/QwenLM/Qwen-Agent\n另外，有关 Qwen2.5-Math 的预训练架构设计，有个特别有趣的地方：Qwen2.5-Math的部分预训练数据，竟然是由Qwen2-Math-Instruct 模型提供的，感觉它们已经开始在左右互搏了，因此世界领先也就不为奇了。\n更多 Qwen2.5-Math 的详细介绍，可查看官网：https://qwenlm.github.io/zh/blog/qwen2.5-math/\nQwen2.5-Math 本地部署和体验 我们先本地部署 Qwen2.5-Math 模型，然后进行简单的数学题推理。由于老牛同学电脑配置不够强悍，因此本次演示我们使用1.5B参数版本（大家可根据自己硬件配置，选择不同的参数量版本）。我们通过以下三步完成整个流程：\n【第一步：下载 Qwen2.5-Math 模型权重文件】\n存放权重文件目录：Qwen2.5-Math-1.5B-Instruct\n1 2 3 4 5 # Git大文件系统 git lfs install # 下载模型权重文件 git clone https://www.modelscope.cn/qwen/Qwen2.5-math-1.5B-Instruct.git Qwen2.5-Math-1.5B-Instruct 若下载过程中异常中断，可以通过git lfs install命令继续下载：\n1 2 3 4 5 6 # 切换到Git目录 cd Qwen2.5-Math-1.5B-Instruct # 继续下载 git lfs install git lfs pull 【第二步：设置 Python 虚拟环境和安装依赖】\n工欲善其事，必先利其器，我们通过Miniconda管理 Python 虚拟环境，Miniconda的安装和使用可以参考老牛同学之前的文章：大模型应用研发基础环境配置（Miniconda、Python、Jupyter Lab、Ollama 等）\n1 2 3 4 5 # Python虚拟环境名：Qwen2.5，版本号：3.10 conda create -n Qwen2.5 python=3.10 -y # 激活虚拟环境 conda activate Qwen2.5 接下来，在虚拟环境中下载依赖包：\n1 2 3 4 pip install torch pip install modelscope pip install \u0026#34;transformers\u0026gt;=4.37.0\u0026#34; pip install \u0026#34;accelerate\u0026gt;=0.26.0\u0026#34; 【第三步：使用CoT方式进行数学推理验证】\n我们先通过一个简单数学方程：4X+5=6X+7，验证一下 Qwen2.5-Math 的推理能力：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 # Qwen2.5-Math-Eval-01.py import os from modelscope import AutoModelForCausalLM, AutoTokenizer # 权重文件目录 model_dir = os.path.join(\u0026#39;D:\u0026#39;, os.path.sep, \u0026#39;ModelSpace\u0026#39;, \u0026#39;Qwen2.5\u0026#39;, \u0026#39;Qwen2.5-Math-1.5B-Instruct\u0026#39;) print(f\u0026#39;权重目录: {model_dir}\u0026#39;) # 初始化模型 model = AutoModelForCausalLM.from_pretrained( model_dir, torch_dtype=\u0026#39;auto\u0026#39;, device_map=\u0026#39;auto\u0026#39;, local_files_only=True, ) # 初始化分词器 tokenizer = AutoTokenizer.from_pretrained( model_dir, local_files_only=True, ) # Prompt提示词 prompt = \u0026#39;请计算等式中的X值: 4X+5=6X+7\u0026#39; messages = [ {\u0026#39;role\u0026#39;: \u0026#39;system\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;你是一位数学专家，特别擅长解答数学题。\u0026#39;}, {\u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: prompt} ] text = tokenizer.apply_chat_template( messages, tokenize=False, add_generation_prompt=True, ) model_inputs = tokenizer( [text], return_tensors=\u0026#39;pt\u0026#39;, ).to(model.device) print(f\u0026#39;开始推理: {prompt}\u0026#39;) generated_ids = model.generate( **model_inputs, max_new_tokens=512, ) print(\u0026#39;推理完成.\u0026#39;) generated_ids = [ output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids) ] response = tokenizer.batch_decode( generated_ids, skip_special_tokens=True, )[0] print(f\u0026#39;推理结果: {response}\u0026#39;) 执行本 Python 程序：python Qwen2.5-Math-Eval-01.py，可以看到 Qwen2.5-Math 模型通过 CoT 的推理方式给出了结果：\n从最终结果来看，Qwen2.5-Math 推理过程很清晰，和我们的思维模型比较相近（唯一不足就是老牛同学电脑配置有点吃力）。\nQwen2.5-Math-72B 推理小学和初中奥数题挑战 上面一元一次方程只是简单体验，接下来我们通过奥数题目，对 Qwen2.5-Math 发出挑战。因此，我们将采用 Qwen2.5-Math-72B 目前地表最强数学模型，来解答小学和初中奥数题目。（那为啥没有高中奥数题呢？因为老牛同学有点担心自己不会，当不了裁判，因此就不好去挑战大模型了^_^）\n我们将通过以下 3 个步骤，完成 Qwen2.5-Math 整个奥数题挑战验证：\n【第一步：整理奥数题目，小学和初中分别 3 道题目】\n老牛同学在网上分别找了 3 道奥数题目，我们通过 JSON 格式文件存储每道题目，每道题由等级、题目和答案组成：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 [ { \u0026#34;level\u0026#34;: \u0026#34;小学\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;小明和小红共有100元钱，小明比小红多20元。请问小明和小红分别有多少钱？\u0026#34;, \u0026#34;answer\u0026#34;: \u0026#34;小明：60元，小红：40元\u0026#34; }, { \u0026#34;level\u0026#34;: \u0026#34;小学\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;小明围绕长方形操场跑步，跑了3圈共480米，操场的长比宽多20米。请问操场面积是多少平方米？\u0026#34;, \u0026#34;answer\u0026#34;: \u0026#34;操场面积：1500平方米\u0026#34; }, { \u0026#34;level\u0026#34;: \u0026#34;小学\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;甲乙二人从两地沿直线同时相对而行，经过4小时，在距离中点4千米处相遇，甲比乙的速度快。请问甲每小时比乙快多少千米？\u0026#34;, \u0026#34;answer\u0026#34;: \u0026#34;甲每小时比乙快：2千米\u0026#34; }, { \u0026#34;level\u0026#34;: \u0026#34;初中\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;小明和小红以同样多的钱买了同一种铅笔，小明要了13支，小红要了7支，小明又给小红0.6元钱。请问每支铅笔多少钱？\u0026#34;, \u0026#34;answer\u0026#34;: \u0026#34;每支铅笔：0.2元\u0026#34; }, { \u0026#34;level\u0026#34;: \u0026#34;初中\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;父亲今年45岁，儿子今年15岁。请问多少年前父亲的年龄是儿子年龄的11倍？\u0026#34;, \u0026#34;answer\u0026#34;: \u0026#34;12年前\u0026#34; }, { \u0026#34;level\u0026#34;: \u0026#34;初中\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;商店有一套运动服，成本价为100元，按标价的8折出售仍可获利20元。请问这套运动服的标价是多少元？\u0026#34;, \u0026#34;answer\u0026#34;: \u0026#34;这套运动服的标价：150元\u0026#34; } ] 【第二步：调用 Qwen2.5-Math-72B API 完成推理】\n由于老牛同学本地推理比较慢，接下来将使用阿里云百炼平台 API 完成奥数推理。大家如果对自己电脑配置有信心的话，可以直接使用本地模型进行验证，可以免费完全接下来的推理。\n有关阿里云百炼平台的介绍，可参考老牛同学之前的文章：太卷了，阿里云免费 1 个月大模型算力额度，玩转 Llama3.1/Qwen2 等训练推理\n奥数题目的推理程序逻辑有 3 部分组成（Qwen2.5-Math-奥数推理.py）：\n读取奥数题目 JSON 文件，并循环每一道题 使用 Qwen2-Math 模型进行推理（或者：百炼平台 API），获取结果 存储奥数题目和推理结果到新文件，便于我们进行查看检测 如果大家是通过调用 API 完成推理，请先安装依赖包：pip install OpenAI\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 # Qwen2.5-Math-奥数推理.py import os import json from openai import OpenAI # 初始化客户端：提前配置好`DASHSCOPE_API_KEY`环境变量 client = OpenAI( api_key=os.getenv(\u0026#39;DASHSCOPE_API_KEY\u0026#39;), base_url=\u0026#39;https://dashscope.aliyuncs.com/compatible-mode/v1\u0026#39;, ) # 读取奥数题目 input_file = \u0026#39;Qwen2.5-Math-奥数题目.json\u0026#39; with open(input_file, \u0026#39;r\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;) as file: data = json.load(file) # 循环每道题目，请求推理服务 output_data = [] for item in data: print(\u0026#39;\u0026#39;) print(f\u0026#39;奥数题目-\u0026gt; {item[\u0026#34;title\u0026#34;]}\u0026#39;) print(f\u0026#39;期望答案-\u0026gt; {item[\u0026#34;answer\u0026#34;]}\u0026#39;) completion = client.chat.completions.create( model=\u0026#39;qwen2.5-math-72b-instruct\u0026#39;, messages=[ {\u0026#39;role\u0026#39;: \u0026#39;system\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;你是一位数学专家，特别擅长解答数学题。\u0026#39;}, {\u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: item[\u0026#34;title\u0026#34;]} ], ) # 获取推理结果 result = json.loads(completion.model_dump_json()) content = result[\u0026#39;choices\u0026#39;][0][\u0026#39;message\u0026#39;][\u0026#39;content\u0026#39;] print(f\u0026#39;推理结果-\u0026gt; {content}\u0026#39;) # 暂存结果，后面统一存储到文件 output = { \u0026#39;level\u0026#39;: item[\u0026#39;level\u0026#39;], \u0026#39;title\u0026#39;: item[\u0026#39;title\u0026#39;], \u0026#39;answer\u0026#39;: item[\u0026#39;answer\u0026#39;], \u0026#39;result\u0026#39;: content } output_data.append(output) print(\u0026#39;\u0026#39;) # 保存推理结果 output_file = \u0026#39;Qwen2.5-Math-推理结果.md\u0026#39; with open(output_file, \u0026#34;w\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as file: for output in output_data: file.write(f\u0026#39;{output[\u0026#34;level\u0026#34;]}题目：{output[\u0026#34;title\u0026#34;]}\\n\\n期望答案：{output[\u0026#34;answer\u0026#34;]}\\n\\n\u0026#39;) file.write(f\u0026#39;{output[\u0026#34;result\u0026#34;]}\\n\\n---\\n\\n\u0026#39;) Qwen2.5-Math-72B 果然没有让人失望，推理过程较快，并且推理的每一步都清晰明了：\n【第三步：验证推理结果的正确性，给出评分】\n最后一步，我们打开 Qwen2.5-Math-72B 推理结果文件Qwen2.5-Math-推理结果.md，确定其准确率：100%（奥数题全对）\n说实话，老牛同学还是挺吃惊的，它的推理速度，比老牛同学要快得多，并且推理的步骤都非常清晰明了。\n对于有娃的朋友们，如果您在辅导家庭作业感到头疼的话，Qwen2.5-Math 赶紧使用起来吧，让我们真正体验一下父慈子孝的和谐乐趣~\n最后：又来给大家免费包邮送几本新书 送书小活动我们举办过 1 次，送的是大模型书：ChatTTS 长音频合成和本地部署 2 种方式，让你的“儿童绘本”发声的实战教程（文末有福利）\n本期老牛同学和出版社朋友合作，继续举办送书福利小活动，送的书是《AI 绘画基础与商业实战》\n这本新出版的书比较偏应用，通过 Midjourney 和 Stable Diffusion 等 AI 绘图大模型，协助我们在日常的餐饮、服饰等各个领域落地的应用教程，值得我们学习（正所谓肚中有货，从容不迫）。\n那么本期福利小活动的规则如下，和上期基本类似：\n关注老牛同学微信公众号，因为这是老牛同学公众号关注者的小福利 本文评论区留言，留言的点赞数排名前 3 名的朋友，免费且包邮获得 1 本书（即：默认送 3 本书） 若本文的阅读量和老牛同学 CSDN 同文的阅读量之和达到了2000（含2000），则留言的点赞数排名前 6 名的朋友均免费包邮送书（即：最多送 6 本书） 活动时间：2024 年 9 月 28 日到 2024 年 10 月 5 号上午 12 点整（正好 1 周，恰逢国庆假期，便于老牛同学统计和收集寄件地址） 老牛同学不参与本次活动（因为老牛同学默认就有 1 本） 最后的最后，为了方便大家购买，老牛同学贴上京东商品链接（大家也可在购书网站搜索“AI 绘画基础与商业实战”直接购买）：\n【书籍购买链接】\n往期推荐文章：\n基于 Qwen2.5-Coder 模型和 CrewAI 多智能体框架，实现智能编程系统的实战教程\nvLLM CPU 和 GPU 模式署和推理 Qwen2 等大语言模型详细教程\n基于 Qwen2/Lllama3 等大模型，部署团队私有化 RAG 知识库系统的详细教程（Docker+AnythingLLM）\n使用 Llama3/Qwen2 等开源大模型，部署团队私有化 Code Copilot 和使用教程\n基于 Qwen2 大模型微调技术详细教程（LoRA 参数高效微调和 SwanLab 可视化监控）\nChatTTS 长音频合成和本地部署 2 种方式，让你的“儿童绘本”发声的实战教程\n","date":"2024-09-28T00:00:00Z","image":"https://ntopic.cn/p/2024092801/00_huf5e64d048c9e740a245b54ea56ec0df5_108371_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/2024092801/","title":"使用世界领先的 Qwen2.5-Math 开源模型当 AI 数学老师，让奥数解题辅导父慈子孝（文末有福利）"},{"content":"9 月 19 日，阿里开源了 Qwen2.5 系列大模型全家桶：除常规的语言模型 Qwen2.5 之外，还发布了专门针对编程的Qwen2.5-Coder模型和数学的 Qwen2.5-Math 模型，并且针对每个模型都提供了不同规模参数版本，包括：\nQwen2.5 语音模型: 0.5B, 1.5B, 3B, 7B, 14B, 32B 和 72B Qwen2.5-Coder 编程模型: 1.5B, 7B, 以及即将推出的 32B Qwen2.5-Math 数学模型: 1.5B, 7B 和 72B Qwen2.5 系列模型性能和推理能力对比，老牛同学不在赘述，我们可以到 Qwen 官网博客查看详细介绍：https://qwenlm.github.io/zh/blog/qwen2.5/\n在国内开源大模型中，老牛同学用遥遥领先做为 Qwen2.5 模型系列总结一点也不为过，国内开源大模型能真打的实在是太少了！\n老牛同学对Qwen2.5-Coder比较感兴趣，可能是老牛同学专业的原因，非常期待能有一个智能编程的模型或应用，真所谓：码无止境，学无止境！\n今天，老牛同学就基于Qwen2.5-Coder模型，结合CrewAI多智能体框架，实现一个能根据软件需求编写程序代码的系统，同时感受一下Qwen2.5-Coder的能力。因此本文大致分为以下三部分：\nQwen2.5-Coder本地部署和简单验证 CrewAI多智能体设计、和基于Qwen2.5-Coder的实现代码 最终，验证CrewAI智能体编写的程序 Qwen2.5-Coder 本地部署 通过老牛同学前面的文章可以看到，我们至少有 3 种方式在本地部署和推理大模型，本文就是不在详细介绍其过程了（感兴趣的朋友可以翻看一下老牛同学之前文章）。\n为了尽快实现我们的目标，我们将使用Ollama快速在本地部署Qwen2.5-Coder模型；关于 Ollama 的详细介绍和使用教程，可以参考老牛同学之前文章：Ollama 完整教程：本地 LLM 管理、WebUI 对话、Python/Java 客户端 API 应用\n我们安装好Ollama程序之后，通过以下一条命令，即可完成大模型的下载和部署：\n1 ollama run qwen2.5-coder:7b 下载和部署成功之后，Ollama默认自带了一个终端对话窗口：\n至此，Qwen2.5-Coder模型本地部署就成功了，接下来我们来实现多智能体代码逻辑~\nCrewAI 多智能体设计和实现 CrewAI是一个多智能体框架，关于它的详细介绍和使用教程，可以参考老牛同学之前文章：基于 CrewAI 多智能体框架，手把手构建一个自动化写作应用实战\n工欲善其事，必先利其器，我们先准备一下CrewAI的研发环境，我们通过Miniconda管理 Python 虚拟环境，Miniconda的安装和使用可以参考老牛同学之前的文章：大模型应用研发基础环境配置（Miniconda、Python、Jupyter Lab、Ollama 等）\n1 2 3 4 5 6 7 8 # Python虚拟环境名：CrewAI，版本号：3.10 conda create -n CrewAI python=3.10 -y # 激活虚拟环境 conda activate CrewAI # 安装 CrewAI 框架依赖 pip install crewai 我们的编程智能体为游戏智能编程团队（姑且就叫这个名吧），这个团队由3 个角色组成（即：3 个智能体），他们的工作职责设定如下：\n软件工程师：负责根据需求编写程序代码 质量工程师：发现并修复软件工程师的代码 Bug 和错误 首席质量工程师：检测程序完整性，并检测是否符合软件需求 我们通过以下步骤，完成游戏智能编程团队整体逻辑（Python 文件：CrewAI-Game.py），首先我们需要导入程序的均依赖类：\n1 2 3 import os from textwrap import dedent from crewai import Agent, Task, Crew 【第一步：配置Qwen2.5-Coder大模型】\n1 2 3 4 # 配置模型（qwen2.5-coder:7b） os.environ[\u0026#34;OPENAI_API_BASE\u0026#34;] = \u0026#39;http://127.0.0.1:11434/v1\u0026#39; os.environ[\u0026#34;OPENAI_MODEL_NAME\u0026#34;] = \u0026#39;qwen2.5-coder:7b\u0026#39;, os.environ[\u0026#34;OPENAI_API_KEY\u0026#34;] = \u0026#39;EMPTY\u0026#39; 【第二步：定义3 个角色（即：3 个智能体）】\n3 个智能体分别是：高级软件工程师、高级质量工程师和首席质量工程师\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 # # 3个智能体逻辑 # def senior_engineer_agent(): \u0026#34;\u0026#34;\u0026#34;高级软件工程师智能体\u0026#34;\u0026#34;\u0026#34; return Agent( role=\u0026#39;高级软件工程师\u0026#39;, goal=\u0026#39;根据需求完成软件编程\u0026#39;, backstory=dedent(\u0026#39;\u0026#39;\u0026#39;你是一位国际领先的科技公司的高级软件工程师。 你非常擅长Python编程，并尽自己的最大努力编写功能齐全、运行良好的完美代码。 \u0026#39;\u0026#39;\u0026#39;), allow_delegation=False, verbose=True ) def qa_engineer_agent(): \u0026#34;\u0026#34;\u0026#34;高级软件质量工程师智能体\u0026#34;\u0026#34;\u0026#34; return Agent( role=\u0026#39;高级软件质量工程师\u0026#39;, goal=\u0026#39;分析程序代码，找出其中的错误，并修复这些错误代码\u0026#39;, backstory=dedent(\u0026#39;\u0026#39;\u0026#39;你是一位检测代码的高级工程师。 你对代码细节很敏锐，非常擅长找出代码中的Bug，包括检查是否缺少导入、变量声明、不匹配括号和语法错误等。 您还能检查出代码的安全漏洞和逻辑错误。 \u0026#39;\u0026#39;\u0026#39;), allow_delegation=False, verbose=True ) def chief_qa_engineer_agent(): \u0026#34;\u0026#34;\u0026#34;首席软件质量工程师智能体\u0026#34;\u0026#34;\u0026#34; return Agent( role=\u0026#39;首席软件质量工程师\u0026#39;, goal=\u0026#39;确保代码实现了需求\u0026#39;, backstory=\u0026#39;\u0026#39;\u0026#39;你怀疑程序员没有按照需求编写软件，你特别专注于编写高质量的代码。\u0026#39;\u0026#39;\u0026#39;, allow_delegation=True, verbose=True ) 【第三步：定义3 个任务（即：3 个智能体的任务）】\n与 3 个智能体对应，每个智能体均对应有 1 个任务：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 # # 3个任务逻辑 # def code_task(agent, game): return Task(description=dedent(f\u0026#39;\u0026#39;\u0026#39;你将按照软件需求，使用Python编写程序: 软件需求 ------------ {game} \u0026#39;\u0026#39;\u0026#39;), expected_output=\u0026#39;你的输出是完整的Python代码, 特别注意只需要输出Python代码，不要输出其他任何内容！\u0026#39;, agent=agent ) def review_task(agent, game): return Task(description=dedent(f\u0026#39;\u0026#39;\u0026#39;你将按照软件需求，进一步使用Python完善给定的程序: 软件需求 ------------ {game} 根据给定的Python程序代码，检查其中的错误。包括检查逻辑错误语法错误、缺少导入、变量声明、括号不匹配，以及安全漏洞。 \u0026#39;\u0026#39;\u0026#39;), expected_output=\u0026#39;你的输出是完整的Python代码, 特别注意只需要输出Python代码，不要输出其他任何内容！\u0026#39;, agent=agent ) def evaluate_task(agent, game): return Task(description=dedent(f\u0026#39;\u0026#39;\u0026#39;你将按照软件需求，进一步使用Python完善给定的程序: 软件需求 ------------ {game} 查看给定的Python程序代码，确保程序代码完整，并且符合软件需求。 \u0026#39;\u0026#39;\u0026#39;), expected_output=\u0026#39;你的输出是完整的Python代码, 特别注意只需要输出Python代码，不要输出其他任何内容！\u0026#39;, agent=agent ) 【第四步：定义团队，组织智能体协同执行任务】\nCrewAI中，Crew代表团队，它有agents和tasks这 2 个核心属性，最终通过kickoff()方法触发执行整个流程：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 # # 团队逻辑 # print(\u0026#39;\u0026#39;) game = input(\u0026#39;# 您好，我们是游戏智能编程团队，请输入游戏的详细描述：\\n\\n\u0026#39;) print(\u0026#39;\u0026#39;) # 智能体 senior_engineer_agent = senior_engineer_agent() qa_engineer_agent = qa_engineer_agent() chief_qa_engineer_agent = chief_qa_engineer_agent() # 任务 code_game = code_task(senior_engineer_agent, game) review_game = review_task(qa_engineer_agent, game) approve_game = evaluate_task(chief_qa_engineer_agent, game) # 团队 crew = Crew( agents=[ senior_engineer_agent, qa_engineer_agent, chief_qa_engineer_agent ], tasks=[ code_game, review_game, approve_game ], verbose=True ) # 执行 game_code = crew.kickoff() 最后，我们拿到 Python 代码之后，希望能保存到本地文件（Python 文件：Game.py）：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # 输出 print(\u0026#34;\\n\\n########################\u0026#34;) print(\u0026#34;## 游戏代码结果\u0026#34;) print(\u0026#34;########################\\n\u0026#34;) print(game_code) # 存储代码 filename = \u0026#39;Game.py\u0026#39; print(\u0026#34;\\n\\n########################\\n\u0026#34;) with open(filename, \u0026#39;w\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;) as file: file.write(game_code) print(f\u0026#34;游戏代码已经存储到文件： {filename}\u0026#34;) print(f\u0026#39;你可以运行游戏：python {filename}\u0026#39;) 以上步骤即完成了我们只能编程团队代码，接下就可以执行了：python CrewAI-Game.py\nCrewAI 执行过程中，我们看到 3 个智能体执行详细信息：\n软件工程师执行过程信息 质量工程师执行过程信息 首席工程师执行过程信息 最后，我们运行本地存储的小游戏程序：python Game.py\n因为程序使用到了Pygame包，如果虚拟环境没有的话，需要安装一下：pip install pygame\n最终我们运行起来了贪吃蛇游戏：\n至此，本次我们需求基本完成了，我们通过定义的 3 个智能体和对应的任务，最终智能体与Qwen2.5-Coder交互完成软件需求程序的编写。\n程序最终还能跑起来，这也进一步体现了Qwen2.5-Coder模型的语音和代码推理能力！\n往期推荐文章：\nvLLM CPU 和 GPU 模式署和推理 Qwen2 等大语言模型详细教程\n基于 Qwen2/Lllama3 等大模型，部署团队私有化 RAG 知识库系统的详细教程（Docker+AnythingLLM）\n使用 Llama3/Qwen2 等开源大模型，部署团队私有化 Code Copilot 和使用教程\n基于 Qwen2 大模型微调技术详细教程（LoRA 参数高效微调和 SwanLab 可视化监控）\nChatTTS 长音频合成和本地部署 2 种方式，让你的“儿童绘本”发声的实战教程\nMiniCPM-V 2.6 面壁“小钢炮”，多图、视频理解多模态模型，部署和推理实战教程\n","date":"2024-09-21T00:00:00Z","image":"https://ntopic.cn/p/2024092101/01_hu4a69ce90e6d0acdf7131434eb5356c98_280965_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/2024092101/","title":"基于 Qwen2.5-Coder 模型和 CrewAI 多智能体框架，实现智能编程系统的实战教程"},{"content":"明月皓皓，星河灿烂，中秋佳节，团圆美满。祝大家中秋节快乐！听说台风要来了，也不知道还能不能吃着月饼赏个月？\r老牛同学在上文（Agent(智能体)和 MetaGPT，一句话实现整个需求应用代码）和大家一起初步理解了什么是 Agent(智能体)、它具备哪些特性等基础知识，然后想着使用 MetaGPT 这个国内开源的多智能体系统，用一句话让它帮忙写一个贪吃蛇小游戏程序，体验一把当“老板”的乐趣。我们的需求确实是用一句话给了 MetaGPT，只可惜“老板”的乐趣是没有体验到，MetaGPT 倒是给我们留了一个有 Bug 的程序，老牛同学又得做回程序员，修 Bug 去！\nMetaGPT 这种端到端的多智能体系统设计本身很有吸引力，但是对它所依赖的基础大语言模型（大模型）有很强的诉求，否则根据一句话的需求产出的结果很难尽人意。老牛同学感觉大模型参数量得 100B 起步才会较好的效果，且需求越复杂，对大模型的要求就会越高。\n今天，老牛同学和大家一起看看另外一个多智能体框架 CrewAI，我们使用它，可以像搭乐高积木一样，搭建自己工作流。与 MetaGPT 相比，CrewAI 更加轻量化，定制的灵活性更高，因此老牛同学称它为框架。\n大语言模型最擅长的是自然语言生成，那么我们本次就来使用 CrewAI 搭建一个自动化写作系统，本文主要包括以下内容：\nCrewAI 框架基本介绍 本地安装 CrewAI 框架依赖 使用 CrewAI 逐步地搭建我们的自动化写作系统 本文所有涉及到的源代码，不出意外的话，源代码地址老牛同学会放在评论区~\nCrewAI 框架基本介绍 CrewAI 官方开源地址：https://github.com/crewAIInc/crewAI\nCrewAI 是一个多智能体框架，主要为角色扮演中的智能体提供自动化配置，以促进智能体之间的合作，共同解决复杂问题。\nCrewAI 主要由Agent(智能体)、Task(任务)、Tool(工具)和Crew(团队)模块组成。智能体是具有角色和背景故事、目标和记忆的团队成员；任务是智能体需要完成的任务；工具是智能体使用的设备，用于辅助完成任务；团队则是智能体和任务相结合的容器，是智能体协调合作执行任务的实际场所，智能体由它共同完成实际任务。\nCrewAI 框架比较灵活，它不仅支持OpenAI标准 API，还支持Ollama标准 API，它具备如下核心特征：\n角色定制代理：可以根据不同的角色、目标和工具来量身定制代理。 自动任务委派：代理之间能够自主地分配任务和进行交流，有效提升解题效率。 任务管理灵活性：可以根据需要自定义任务和工具，并灵活地指派给不同代理。 流程导向：目前系统仅支持按顺序执行任务，但更加复杂的如基于共识和层级的流程正在研发中。 本地安装 CrewAI 框架依赖 工欲善其事，必先利其器，我们通过Miniconda管理 Python 虚拟环境，Miniconda的安装和使用可以参考老牛同学之前的文章：大模型应用研发基础环境配置（Miniconda、Python、Jupyter Lab、Ollama 等）\n1 2 3 4 5 # Python虚拟环境名：CrewAI，版本号：3.10 conda create -n CrewAI python=3.10 -y # 激活虚拟环境 conda activate CrewAI 接下来，我们就可以安装 CrewAI 框架依赖了：\n1 pip install crewai 至此，CrewAI 框架依赖就安装好了，我们接下来使用它来构建一个自动化协作应用。\n使用 CrewAI 搭建自动化写作系统 前面提到，智能体依赖的大模型既支持 OpenAI 标准接口，也支持 Ollama 标准接口。老牛同学为了方便演示，使用 Ollama 本地部署Qwen2-7B大模型，有关 Ollama 详细介绍和使用，参见老牛同学之前的文章，本文不在赘述：Ollama 完整教程：本地 LLM 管理、WebUI 对话、Python/Java 客户端 API 应用\n【第一步： 明确我们的需求】\n我们在来想当一次“老板”，还是以儿童绘本故事为例，我们只提供一个主题：由第 1 个智能体根据主题产出绘本故事的构思内容，然后由第 2 个智能体根据第 1 个智能体的构思内容，产出故事内容。\n因此，我们需要有 2 个智能体（即：需要定义 2 个角色），分别是：故事内容构思者，和故事内容协作者；他们各自都有一个任务，分别是：产出故事内容构思，和根据构思编写故事。\n接下来，我们就开始实现我们的智能体了，在此之前，我们先引入CrewAI核心模块类，同时配置大模型：\n【第二步： 定义构思者角色的智能体】\n对于构思者智能体，它的定义如下：\n我们要给它设定一个与任务相符的角色（role）：为了简单，我们就设定为构思者 我们为这个智能体设定了一个目标（goal）：要根据给定的主题（{topic}），产出儿童绘本故事内容创意 我们再为规划者智能体设定一个背景故事（backstory）：我们尽可能多地设定相关的上下文，比如它的职责、对它产出物的要求等。 我们还要设置一些其它属性：allow_delegation 代表这个智能体不允许把工作委托给其他的智能体；verbose 代表把它的工作详细记录下来，这样我们可以看到它在执行内部任务时是如何运行的 【第三步： 定义写作者角色的智能体】\n对于写作者智能体来说，它的目标就是：根据构思者所提供的儿童绘本故事创意来编写故事内容。\n【第四步： 定义每个智能体的任务】\n上面我们定义了 2 个智能体角色，现在给他们分别定义 1 个任务：\n构思者和写作者的任务，均包含了描述、期望输出和分配的智能体等 3 个属性：\n描述（description）：期望智能体能做什么 预期的输出（expected_output）：类似一种强制机制，明确任务的产出结果 智能体（agent）：即为这个任务分配智能体 【第五步： 让智能体和任务协同工作】\n智能体是按照顺序执行任务的，上一个任务的输出，会被作为下一个任务的输入。特别注意：我们提供了topic上下文参数，它在智能体和任务中可以通过{topic}使用。\n运行程序，我们可以逐步看到构思者智能体的运行过程内容：\n接下来，可以看到写作者智能体的运行过程内容：\n最终，所有的智能体执行完成，产出的目标故事内容：\n最后：CrewAI 总结说明 通过以上示例，我们了解了 CrewAI 的三个核心模块：Agent智能体，Task任务，Crew团体或者称容器，容器把他们组合在一起。\nAgent： 一般情况下，一个Agent只做一件事，明确智能体的目标和期望；当然，一个Agent可以执行多项任务。 Task： 无论是创建 Agent 还是 Task 都需要明确自己的职责，以及对自己的期望。 Crew： 最终把任务，按照串行或并行的方式组织起来。 vLLM CPU 和 GPU 模式署和推理 Qwen2 等大语言模型详细教程\n基于 Qwen2/Lllama3 等大模型，部署团队私有化 RAG 知识库系统的详细教程（Docker+AnythingLLM）\n使用 Llama3/Qwen2 等开源大模型，部署团队私有化 Code Copilot 和使用教程\n基于 Qwen2 大模型微调技术详细教程（LoRA 参数高效微调和 SwanLab 可视化监控）\nMiniCPM-V 2.6 面壁“小钢炮”，多图、视频理解多模态模型，部署和推理实战教程\n","date":"2024-09-14T00:00:00Z","image":"https://ntopic.cn/p/2024091401/00_hu25975eb2f329e262091455ae0784f007_16497_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/2024091401/","title":"基于 CrewAI 多智能体框架，手把手构建一个自动化写作应用实战"},{"content":"前面 2 篇文章，我们使用文生文、文生图和文生音频三个大模型共同实现了图文并茂的儿童绘本故事和绘本故事音频需求：\n第一篇 根据主题生成儿童绘本故事：GLM-4-Flash 大模型 API 免费了，手把手构建“儿童绘本”应用实战（附源码） 第二篇 根据儿童绘本故事生成长音频：ChatTTS 长音频合成和本地部署 2 种方式，让你的“儿童绘本”发声的实战教程（文末有福利） 友情提示：第二篇有送书活动，活动还在继续，欢迎大家参与~\n我们可以看出，儿童绘本故事的整个流程如下：\n在整个流程中，其实我们核心就做了 2 件事：设定主题明确需求、编排整个流程让应用跑起来。而其他的包括构思文案、故事内容、图片生成、长音频合并等，其实都是围绕大模型不同阶段的交付物（构思和故事内容）进一步的交付物，即我们的需求目标。\n假如我们有一个基于大模型的程序，它能准确地理解我们的需求内涵，然后进一步把需求拆分成小任务，最终组织整个任务的流程和依赖，并执行达成我们的需求目标。那么这个应用就是我们常说的大模型Agent（或者叫智能体）：大模型 Agent 是基于大语音模型（LLM）之上构建的、具备环境感知、自主理解、决策制定和执行能力的智能体。\n大模型 Agent 能够模拟独立思考过程、调用各类工具，逐步达成我们的需求目标。从我们应用架构设计上讲，大模型 Agent 从原来面向过程的架构（如：儿童绘本应用）转变为面向目标的架构（提交需求后静待结果），它可以实现复杂的目标任务。\n一般来说，大模型 Agent 主要由四个部分组成：规划、记忆、工具和行动，分别负责任务拆解与策略评估、信息存储与回忆、环境感知与决策辅助、和将思维转换为实际行动。\n我们还是以儿童绘本故事应用来举例：\n规划（Planning）：我们通过儿童绘本故事主题，期望大模型给我们生成一本儿童绘本故事；那么大模型 Agent 需要准确识别我们的意图，然后进行任务拆分（包括：构思、故事内容、插图、音频等），制定整个处理流程。 记录（Memory）：包括我们的主题、构思、故事等内容，分为短期记忆和长期记忆；短期记忆主要是上下文信息，包括多轮对话（如：绘本故事是面向多大年龄、插入风格等）；长期记忆可能包括用户特征，需要使用特征数据库存储。 工具（Tools）：包括 API 调用、图片存储等，是 Agent 执行决策的辅助手段。 行动（Action）：将规划、记忆转化为具体输出过程，在这个过程中可能需要使用到不同的工具与外部环境交互。 接下来，老牛同学和大家看看，一些常见的大模型 Agent 框架和应用：\nAgent 框架简单介绍 目前，开源和闭源大模型 Agent 可谓是百家争鸣、百花齐放：https://github.com/e2b-dev/awesome-ai-agents\n根据业务需求复杂度和大模型 Agent 的实现方式的差异，目前 Agent 框架可以简单分为单 Agent 和多 Agent 框架。单 Agent 框架包括如：BabyAGI 和 AutoGPT 等。\n接下来，老牛同学主要介绍一个多 Agent 框架：MetaGPT，它是一个国内研发开源的多 Agent 框架（https://github.com/geekan/MetaGPT）\nMetaGPT以软件公司方式组成多个 Agent，实现了一个软件公司的全过程与精心调配的标准流程；它的目的是根据老板的一句话需求，完成一个软件研发整体流程（包括：用户故事、竞品分析、需求、数据结构 API 等）；内部多个 Agent 划分为多个角色（包括：产品经理、架构师、项目经理、研发工程师等），他们按照标准流程实现业务需求。\nMetaGPT 多智能体框架应用 接下来，我们来当一回老板：我们输入一句话需求，让MetaGPT实现我们的目标。\n【第一步：MetaGPT环境准备】\n工欲善其事，必先利其器，我们通过Miniconda管理 Python 虚拟环境，Miniconda的安装和使用可以参考老牛同学之前的文章：大模型应用研发基础环境配置（Miniconda、Python、Jupyter Lab、Ollama 等）\n1 2 3 4 5 # Python虚拟环境名：MetaGPT，版本号：3.10 conda create --name MetaGPT python=3.10 -y # 激活虚拟环境 conda activate MetaGPT 【第二步：安装MetaGPT依赖】\n我们可以安装MetaGPT的稳定版本和最新的研发版本（老牛同学使用的是稳定版本）：\n稳定版本：pip install metagpt 研发版本：pip install --upgrade git+https://github.com/geekan/MetaGPT.git 如果我们想保留MetaGPT画的设计图（如：类图、序列图等），那么还需要安装 Node.js 包：sudo npm install -g @mermaid-js/mermaid-cli\n【第三步：配置MetaGPT大模型】\n前面已经介绍，大模型 Agent 是基于大模型的，因此我们需要配置一下MetaGPT的大语言模型。\n我们可以通过命令初始化配置文件：metagpt --init-config\n默认情况下，MetaGPT自动创建默认配置文件：~/.metapgt/config2.yaml\n我们修改这个配置文件，配置我们自己的大语言模型信息（老牛同学使用的是 Ollama，其它的配置类同）：\n1 2 3 4 5 llm: api_type: \u0026#34;ollama\u0026#34; model: \u0026#34;qwen2:7b\u0026#34; base_url: \u0026#34;http://127.0.0.1:11434/api\u0026#34; api_key: \u0026#34;EMPTY\u0026#34; 有关 Ollama 详细介绍和使用，参见老牛同学之前的文章，本文不在赘述：Ollama 完整教程：本地 LLM 管理、WebUI 对话、Python/Java 客户端 API 应用\n【第四步：当老板，提供一句话需求，静候结果】\n老牛同学之前使用大模型，一步一步通过 Prompt 提示词，完成了消消乐小游戏研发：AI 已来，我与 AI 一起用 Python 编写了一个消消乐小游戏\n今天老牛同学想玩一玩贪吃蛇小游戏，体验一下当老板的乐趣，需求就一句话：write a cli snake game\n1 (MetaGPT) $ metagpt \u0026#34;write a cli snake game\u0026#34; 接下来，我们就开始等待了，在MetaGPT实现需求的标准流程中，我们也可以看到它的每一步的输出：\n首先，名为 Alice 的产品经理登场：他根据老板的需求，产出产品需求文档\n有了 PRD，接下来是名为 Bob 的架构师来了：他根据需求文档，完成产品架构设计\n接下来，名为 Eve 的项目经理上场：他根据架构设计，整理研发任务\n有了研发任务，进行排期锁定资源，研发工程师开始研发了：\n代码研发完成，就需要进行代码 Review 了：\n其它研发的截图，老牛同学就省略了。经过多轮的研发任务和 CR，终于完成了整个需求，可以把代码、文件等存档：\n最后，我们就可以看到最终的产出物了：\n代码源文件目录：./workspace/cli_snake_game/cli_snake_game 1 2 3 4 5 (MetaGPT) $ tree ├── food.py ├── game.py ├── main.py └── snake.py 项目实现文档目录：./workspace/cli_snake_game/docs 项目实现资源目录：./workspace/cli_snake_game/resources 我们运行一下小游戏：python main.py\n可以看到，还真能跑起来了，可是有 Bug（如：蛇吃食物自动退出，蛇撞墙异常退出）：\n至此，MetaGPT按照标准流程，确实帮我们完成了项目，可是我们当老板的并没有享受到什么乐趣，因为我们还得去改 Bug！\n最后，(多)大模型 Agent 到底能解决什么问题？ 上面MetaGPT帮我们写了一个有 Bug 的程序，这肯定不是我们所期望的，分析其背后原因，其实也并不是大模型 Agent 的错：\n我们可以查看MetaGPT源代码：https://github.com/geekan/MetaGPT/tree/main/metagpt/roles\n期核心逻辑都在role.py文件中：https://github.com/geekan/MetaGPT/blob/main/metagpt/roles/role.py\n它的核心还是通过 Prompt 提示词和大模型进行交互，大模型产出质量的好坏，决定了需求结构的优劣！\n那么，大模型 Agent 到底在什么场景能更有优势呢？首先能肯定的是：那些底层大模型擅长的领域！\n复杂问题场景，大模型的初衷就是用来解决复杂问题的 需要多角色交互场景（如：游戏故事生成、素材生成），内容生成，部分工作提效（如：项目代码框架等） vLLM CPU 和 GPU 模式署和推理 Qwen2 等大语言模型详细教程\n基于 Qwen2/Lllama3 等大模型，部署团队私有化 RAG 知识库系统的详细教程（Docker+AnythingLLM）\n使用 Llama3/Qwen2 等开源大模型，部署团队私有化 Code Copilot 和使用教程\n基于 Qwen2 大模型微调技术详细教程（LoRA 参数高效微调和 SwanLab 可视化监控）\nMiniCPM-V 2.6 面壁“小钢炮”，多图、视频理解多模态模型，部署和推理实战教程\n","date":"2024-09-07T00:00:00Z","image":"https://ntopic.cn/p/2024090701/00_hu7a0c880590782b86405b42d88e695a08_83513_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/2024090701/","title":"Agent(智能体)和 MetaGPT，一句话实现整个需求应用代码"},{"content":"接上文（GLM-4-Flash 大模型 API 免费了，手把手构建“儿童绘本”应用实战（附源码）），老牛同学通过 GLM-4-Flash 文生文和 CogView-3 文生图大模型，和大家一起编写了一个图文并茂的儿童绘本应用，并且以《黑神话·悟空》当前热门游戏为背景，做了一本名为《悟空探秘之旅》的儿童小绘本。\n绘本我们是做好了，可是还得我们读给小朋友们听，老牛同学懒虫又犯了：能不能把绘本转换成音频，直接播放给小朋友们听（顺便我也跟着听听）？\n这让老牛同学想到了之前写的关于ChatTTS详细介绍的文章：ChatTTS 开源文本转语音模型本地部署、API 使用和搭建 WebUI 界面（建议收藏）\n老牛同学打开之前写的ChatTTS文章，发现有不少朋友的留言，包括：ChatTTS 的 API 更新了运行失败、模型文件下载不下跑不起来，希望老牛同学给打个包放到云盘等。这也不能怪 ChatTTS，都过去 2 个多月了，发布了新版也属正常，但是啥就不能兼容一下呢？\n本着负责到底和解放自己的目的，老牛同学觉得完全有必要在重头部署一遍最新版的 ChatTTS，部署过程中发现有新坑，本文逐一破解：\nChatTTS 研发环境配置 ChatTTS 的 2 种方式部署和填坑，填坑过程老牛同学已经痛了一次，本文就不在赘述了，推荐使用第 2 种方式 ChatTTS 最长生成30 秒音频，如何生成《悟空探秘之旅》小绘本3 分钟左右的音频？ 实在忍不住，必须要剧透一下：本文末有限量的福利活动，这个福利老牛同学都喜欢，大家可以顺便参与一下（其实就是一本最近上市的看起来还不错却还没有看完的大模型的书）！\nChatTTS 配置 Python 研发环境 工欲善其事，必先利其器，我们通过Miniconda管理 Python 虚拟环境，Miniconda的安装和使用可以参考老牛同学之前的文章：大模型应用研发基础环境配置（Miniconda、Python、Jupyter Lab、Ollama 等）\n1 2 3 4 5 # Python虚拟环境名：ChatTTS，版本号：3.10 conda create --name ChatTTS python=3.10 -y # 激活虚拟环境 conda activate ChatTTS 接下来，我们克隆 ChatTTS 源代码，安装 Python 依赖包（目录：ChatTTS）：\n1 2 3 4 5 6 7 8 9 10 11 12 # 克隆源代码 git clone https://github.com/2noise/ChatTTS ChatTTS # 切换到源代码目录 cd ChatTTS # 安装Python依赖包 pip install -r requirements.txt # 安装ChatTTS包：以下2种方式任一一种即可 pip install -e . # pip install git+https://github.com/2noise/ChatTTS 最后，我们还需要安装 2 个依赖，主要是文本合成音频文件，和音频文件合并（这是一个坑，老牛同学翻源代码解决）：\n1 2 pip install pysoundfile pip install pydub 通过以上步骤，我们即完成了 ChatTTS 环境准备。接下来我们采用2 种方式在本地部署 ChatTTS 模型。\n方式一：API 自动下载模型文件部署 自动下载模型文件的方式比较省事，唯一就是需要从 HF 下载文件，如果大家发现自己没有通畅的网络，那就直接跳过，采用方式二部署方式。\n打开刚刚源代码目录ChatTTS，直接运行 WebUI 程序，就会自动下载模型文件：\n1 2 3 4 5 # 切换到源代码目录 cd ChatTTS # 启动WebUI，API自动下载模型文件 python examples/web/webui.py 我们可以看到，ChatTTS API 自动下载模型文件：\n模型文件下载完成，自动启动了我们之前熟悉的 WebUI 界面：\nChatTTS WebUI 的输入部分核心包括 2 个内容：\n需要转换音频的文本内容，也就是我们“儿童绘本”故事内容 选择音色Timbre，里面有男声、女声、翘舌等 10 种不同的音色 点击那个长长的 Generate橙色按钮，即可开始根据文本合并音频，我们可以点击播放按钮在线播放音频，还可以把音频文件下载到本地磁盘。\n如果你想把绘本的内容全部放进，那是行不通的，因为：ChatTTS 最多只能合并30 秒时长的音频。我们得抛弃 WebUI 界面，通过 API 生产音频。\n方式二：手动下载模型文件部署和 API 使用（推荐） 如果方式一大家能够成功下载模型文件，且只是体验一下玩玩，且能正常启动 WebUI，那么老年同学建议就直接使用方式一了，因为它确实简单粗暴。\n如果我们想使用 API 自动化的做一下事情，那么就用方式二吧：\n【第一步： 下载模型文件到本地】\n我们通过 Git 大文件系统下载模型文件（下载目录：ChatTTS-Model）：\n1 2 git lfs install git clone https://www.modelscope.cn/ai-modelscope/chattts.git ChatTTS-Model 如果因网络不佳等原因，导致下载中断，我们可以通过以下命令在中断后继续下载：\n1 2 cd ChatTTS-Model git lfs pull 【第二步： 本地部署模型并合并音频】\n又有一个坑：ChatTTS 包中，固定死了模型文件列表的摘要内容，如果与包中摘要不一致，则本地部署失败！它这么做其实是可以理解，因为不同版本对应不同的程序，但是模型文件中为啥也不说明一下，使用哪个版本呢？\n第一步：初始化 ChatTTS 代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 import ChatTTS import torch import torchaudio from tools.logger import get_logger from pydub import AudioSegment import os logger = get_logger(\u0026#34;SunningTX\u0026#34;) # # 第一步：ChatTTS初始化 # # 1. 初始化ChatTTS实例 chat = ChatTTS.Chat( get_logger(\u0026#34;ChatTTS\u0026#34;), ) 第二步：加载模型文件，坑就在这里出现了，老牛同学就直接填了\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # 2. 坑：本地下载模型文件，需要魔改文件摘要，否则模型加载失败 chat.sha256_map = { \u0026#34;sha256_asset_Decoder_pt\u0026#34;: \u0026#34;9964e36e840f0e3a748c5f716fe6de6490d2135a5f5155f4a642d51860e2ec38\u0026#34;, \u0026#34;sha256_asset_DVAE_full_pt\u0026#34;: \u0026#34;553eb75763511e23f3e5f86303e2163c5ca775489d637fb635d979c8ae58bbe5\u0026#34;, \u0026#34;sha256_asset_GPT_pt\u0026#34;: \u0026#34;d7d4ee6461ea097a2be23eb40d73fb94ad3b3d39cb64fbb50cb3357fd466cadb\u0026#34;, \u0026#34;sha256_asset_Vocos_pt\u0026#34;: \u0026#34;09a670eda1c08b740013679c7a90ebb7f1a97646ea7673069a6838e6b51d6c58\u0026#34;, \u0026#34;sha256_asset_tokenizer_special_tokens_map_json\u0026#34;: \u0026#34;30e1f3dfbaef18963d9298e9a70e5f4be9017b4f64ec67737d0bccc2dbeba8c9\u0026#34;, \u0026#34;sha256_asset_tokenizer_tokenizer_config_json\u0026#34;: \u0026#34;ac0eb91b587bd4c627c2b18ac1652bdf3686c7a6cd632d4b00feb9f34828dfdc\u0026#34;, \u0026#34;sha256_asset_tokenizer_tokenizer_json\u0026#34;: \u0026#34;dddc3d54016d6cb75ed66dde3be50287afe6dee679c751538373feb75f950020\u0026#34; } # 3. 加载模型文件：指定模型文件路径 chat.load( compile=True, source=\u0026#39;custom\u0026#39;, force_redownload=False, custom_path=\u0026#39;D:\\ModelSpace\\ChatTTS-Model\u0026#39;, ) 特别注意： source值为custom代表使用自定义模型文件，custom_path代表自定义模型文件目录，即我们下载的模型文件根目录！\n经过以上 2 步，我们即加载完成 ChatTTS 模型，可以看到如下日志输出：\n1 2 3 4 5 6 7 8 9 10 [+0800 20240831 15:37:35] [INFO] ChatTTS | core | try to load from local: D:\\ModelSpace\\ChatTTS-Model [+0800 20240831 15:37:35] [INFO] ChatTTS | dl | checking assets... [+0800 20240831 15:37:36] [INFO] ChatTTS | dl | all assets are already latest. [+0800 20240831 15:37:36] [WARN] ChatTTS | gpu | no GPU found, use CPU instead [+0800 20240831 15:37:36] [INFO] ChatTTS | core | use device cpu [+0800 20240831 15:37:36] [INFO] ChatTTS | core | vocos loaded. [+0800 20240831 15:37:36] [INFO] ChatTTS | core | dvae loaded. [+0800 20240831 15:37:39] [INFO] ChatTTS | core | gpt loaded. [+0800 20240831 15:37:39] [INFO] ChatTTS | core | decoder loaded. [+0800 20240831 15:37:39] [INFO] ChatTTS | core | tokenizer loaded. 接下来，我们测试一下合成一个简单的音频文件：\n1 2 3 4 5 6 7 8 # 需要转化为音频的文本内容 text = \u0026#39;大家好，我是老牛，微信公众号：老牛同学，微信号：imxulin（奔跑的蜗牛），很高兴与您相遇！\u0026#39; # 文本转为音频 wavs = chat.infer(text) # 保存音频文件到本地文件（采样率为24000Hz） torchaudio.save(\u0026#34;./output/output-01.wav\u0026#34;, torch.from_numpy(wavs[0]), 24000) 运行程序，我们可以看到如下日志输出：\n1 2 20%|██████████████████████████▌ | 76/384 [00:08\u0026lt;00:35, 8.62it/s] 26%|██████████████████████████████████▌ | 536/2048 [00:48\u0026lt;02:17, 10.98it/s] 同时，在本地目录生成了./output/output-01.wav音频文件，打开该音频文件，就可以听到非常自然流畅的语音了！\nChatTTS 应用：把《悟空探秘之旅》绘本故事转换为长音频文件 有了前面辛苦的准备，我们终于可以来实现我们的目标了：把《悟空探秘之旅》绘本故事合并音频文件！\n现在摆在我们面前的问题是：ChatTTS 一次最多只能合成30 秒长的音频，如何让它合并整个绘本呢？\n老牛同学的办法也简单粗暴：先按照故事每个小段落，分别合成较短时长的音频，最终把短音频合成长音频。（大家如果有更好的办法，恳请留言，谢谢大家）\n【第一步： 我们先把故事内容分成文本行】\n需要注意的是：有些符合 ChatTTS 无法识别，我们把它踢除或者替换成空格，否则不知 ChatTTS 会发出什么音！\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 # # 第二步：按行准备故事文本内容，并转换为音频文件 # # 1. 按行读取故事内容（·《》：符号无法识别，需要过滤掉） lines = [] with open(\u0026#39;./Story.txt\u0026#39;, \u0026#39;r\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;) as file: for line in file: # 去掉或者替换不支持的字符 line = line.replace(\u0026#39;·\u0026#39;, \u0026#39; \u0026#39;) line = line.replace(\u0026#39;《\u0026#39;, \u0026#39;\u0026#39;).replace(\u0026#39;》\u0026#39;, \u0026#39;\u0026#39;) line = line.replace(\u0026#39;：\u0026#39;, \u0026#39; \u0026#39;) # 最后末尾增加一个空格，以暂停一下 line = line.strip() + \u0026#39; \u0026#39; if len(lines) \u0026gt; 2: pass if len(line) \u0026gt; 0: lines.append(line) logger.info(line) 【第二步： 按照故事段落，分别合成音频文件】\n需要注意的是：以下代码torchaudio.save()的参数backend代表本地音频处理扩展，总共支持 3 种类型：\n最简单的方式，就是使用soundfile，前面已经介绍：pip install pysoundfile\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 # 2. 音频转换，按照每行文本转换为一个音频文件 temp_dir = os.path.join(os.getcwd(), \u0026#39;temp\u0026#39;) os.makedirs(temp_dir, exist_ok=True) # 存在单个音频文件 def save_wav(idx: int, src: torch.Tensor): wav_path = f\u0026#39;./temp/{idx}.wav\u0026#39; try: torchaudio.save( wav_path, torch.from_numpy(src).unsqueeze(0), 24000, format=\u0026#39;wav\u0026#39;, backend=\u0026#39;soundfile\u0026#39; ) except: torchaudio.save( wav_path, torch.from_numpy(src), 24000, format=\u0026#39;wav\u0026#39;, backend=\u0026#39;soundfile\u0026#39; ) return wav_path # 逐行合成音频文件 wav_list = [] for i in range(len(lines)): line = lines[i] logger.info(f\u0026#39;合成音频：{line}\u0026#39;) res_gen = chat.infer( lines[i], skip_refine_text=False, refine_text_only=False, params_refine_text=ChatTTS.Chat.RefineTextParams( temperature=0.3, top_P=0.7, top_K=20, manual_seed=222, ), ) wav_list.append(save_wav(i, res_gen)) 重点说明一下，chat.infer()函数的几个参数，因为真的很重要：\nskip_refine_text和refine_text_only分别代表忽略文本处理和仅文本预处理标，预处理就是方式一种 WebUI 中，Output Text内容，主要去掉不支持字符、插入停顿、笑声等内容 temperature大模型温度值，越越越严谨，越高则代表大模型会有更多创意，比如加入一些字符等，让音频更加口语化等 top_K和top_P代表大模型采样策略，它们都是用于控制模型输出；K代表概率分布最高的 K 个词，然后从 K 个中随机取一个作为输出；P代表累计概率和超过 P 的词，然后随机取一个作为输出 manual_seed代表音色，不同的男声和女声的不同音色，老牛同学随机定了一个值 【第三步： 最后，合并短音频文件为长音频文件】\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # # 第三步：合并所有短音频为长音频文件 # def merge_wav_files(input_files, output_file): combined_audio = AudioSegment.empty() for input_file in input_files: audio = AudioSegment.from_wav(input_file) combined_audio += audio combined_audio.export(output_file, format=\u0026#39;wav\u0026#39;) # 合并音频文件 merge_wav_files(wav_list, \u0026#39;./Story.wav\u0026#39;) 合并以上步骤代码片段，我们执行程序：python ChatTTS-Story.py\n至此，绘本故事所有的音频代码全部完成，完整的源代码老牛同学放到评论区。\n【这里是《悟空探秘之旅》绘本故事的音频文件】\n最后：来个有限的福利活动（送书） 非常感谢大家一直关注老牛同学的公众号，是大家的支持给了老牛同学很多学习大模型的动力，最近老牛同学联系到一位出版社朋友，共同办一个送书的小福利活动。\n这本书老牛同学还没有看，但是网上看了评论和目录，里面涉及了大模型算法、应用实战，包括老牛同学之前介绍的 LoRA 参数高效微调、LangChain 大模型研发框架等，这本书80 元整体是很值。\n本次福利小活动的要求：\n关注老牛同学微信公众号 然后在公众号本文的评论区留言，留言的点赞数排名前 3 名的朋友，免费且包邮获得《多模态大模型》书籍（即：默认送 3 本书） 若公众号本文的阅读量超过了2000（含2000），则留言的点赞数排名前 6 名的朋友均免费包邮送书（即：最多送 6 本书） 活动时间：2024 年 8 月 31 日到 2024 年 8 月 15 号上午 12 点整（共 2 周，正好假期老牛同学可以统计和收集寄件地址） 老牛同学不参与本次活动（因为老牛同学默认就有 1 本） 请原谅老牛同学这么久才搞个福利活动，竟然还搞得这么小，老牛同学真的尽力了：本来出版社朋友只送 2 本，老牛同学争取到了 3 本，然后继续磨嘴皮争取到 6 本。如果本次活动效果给力，老牛同学敢保证后续肯定还有很多机会。\n大家可能会问，老牛同学你到底得到了什么福利？老牛同学会得到以下 3 个福利：\n不论本次阅读效果如何，出版社朋友都会送书1 本（可见的福利） 本次是老牛同学第 1 次搞活动，可以积累活动经验，下次也行就更容易了（这是一条路子，老牛同学就可以去找更多的出版社朋友了） 公众号可能会有新的关注着，本文和其他文章的阅读量可能会增加，出版社朋友可能会比较满意，那么下次老牛同学就可以争取多一些福利（如：默认送 10 本书等） 最后，凡是参与活动满足送书条件的朋友，麻烦添加一下老牛同学的个人微信（注明：送书活动），便于老牛同学收集邮寄地址：imxulin（奔跑的蜗牛）\n最后的最后，为了方便大家购买，老牛同学把京东商品链接也贴上：\n【书籍购买链接】\n本文示例源代码：https://gitee.com/obullxl/SunningTX/tree/master/PythonTX/ChatTTS\nvLLM CPU 和 GPU 模式署和推理 Qwen2 等大语言模型详细教程\nMiniCPM-V 2.6 面壁“小钢炮”，多图、视频理解多模态模型，部署和推理实战教程\n基于 Qwen2/Lllama3 等大模型，部署团队私有化 RAG 知识库系统的详细教程（Docker+AnythingLLM）\n使用 Llama3/Qwen2 等开源大模型，部署团队私有化 Code Copilot 和使用教程\n基于 Qwen2 大模型微调技术详细教程（LoRA 参数高效微调和 SwanLab 可视化监控）\n","date":"2024-08-31T00:00:00Z","image":"https://ntopic.cn/p/2024083101/00_huf868185b9ea8f4868876dfd4ab61d1fc_93264_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/2024083101/","title":"ChatTTS 长音频合成和本地部署2种方式，让你的“儿童绘本”发声的实战教程（文末有福利）"},{"content":"老牛同学刚刷到了一条劲爆的消息，GLM-4-Flash大模型推理 API 免费了：https://bigmodel.cn/pricing\n老牛同学一直觉得上次阿里云百炼平台为期 1 个月免费额度的“羊毛”已经够大了（太卷了，阿里云免费 1 个月大模型算力额度，玩转 Llama3.1/Qwen2 等训练推理），但经过老牛同学在智谱 AI 官网反复确认之后，竟然不是标题党，它是真的免费，感觉大模型厂商的“羊毛”简直是没有最大，只有更大（老牛同学这次要站智谱 AI 这边）！\n老牛同学本地部署推理和体验的大模型有很多，且第一个就是GLM-4-9B（是的，比Qwen2-7B还要早一些），也一直关注着智谱 AI 推出的大模型：\n第一次是 6 月 8 号：GLM-4-9B大模型开源，且推理能力亮点突出，老牛还写了一篇本地部署推理的文章（本地部署 GLM-4-9B 清华智谱开源大模型方法和对话效果体验），由于当时GLM-4本地部署门槛还有点高，因此这篇文章还获得了不少网友的关注，老牛同学也收到不少网友的关于部署过程的私信和留言 第二次是 7 月 9 号（1 个月后）：GLM-4-9B支持通过 Ollama 部署（https://ollama.com/library/glm4），本地部署和推理的门槛降低不少，因此老牛同学顺势写了一篇普及 Ollama 使用的文章，文章中案例就是用GLM-4-9B（Ollama 完整教程：本地 LLM 管理、WebUI 对话、Python/Java 客户端 API 应用） 第三次是 8 月 27 号也就是今天（差不多也是 1 个月后）：智谱 AI 大模型开放平台官网（https://bigmodel.cn/）宣布GLM-4-Flash大模型 API 免费使用，这不又激起老牛同学的兴致，也因此有了本文 从老牛同学微观视角我们可以看到，智谱 AI 从开放开源大模型、到降低大模型使用门槛、在到面向大众免费，不段在诠释着技术进步和普惠 AI，就如智谱 AI 的愿景那样：实现“让机器像人一样思考”，让技术更好地服务社会，打造支持不同场景、不同方向的智能应用程序的底层人工智能架构，不断开拓努力实现人工智能的行业普惠。\nGLM-4-Flash免费了，我们除了高兴和感慨之外，羊毛不常有，我们总得要“薅”一把。这次老牛同学就不做微调了，咱们来做一个我们生活场景：儿童绘本应用！\n大家可能会问，为什么选择儿童绘本这个方向呢？其实原因一点都不复杂：老牛同学一时半会儿也没有想到好的创意（特别期待大家留言新创意，老牛同学愿意协助实现）；再者老牛同学家绘本要又更新了（存量的绘本小孩都读腻了）；并且这个应用不是很复杂，不至于一篇文章还写不完，还要有续集。如果我们能基于免费的大模型 API 能力，随时生成不同的儿童绘本，岂不是妙哉？\n针对“儿童绘本”应用，老牛同学的构思如下：\n面向7~10小朋友，绘本构思由大模型产出，最好能与当前热点相关（如：最近火爆天的黑神话·悟空游戏） 绘本故事内容也由大模型产出，需要积极向上，饱含正能量 绘本情节结构分为 4 个段，每个段 300 个汉字左右，整个绘本不超过 1500 个汉字 每一段绘本情节均需要配一张插图 最后：绘本内容和图片存储到电脑本地磁盘 在儿童绘本应用中，我们需要用到 2 个大模型：\nGLM-4-Flash大语音模型，主要是提供绘本构思和产出绘本内容 CogView-3文生图大模型，主要是生成绘本插图 确定了应用内容，我们就开始行动了~\n注册\u0026amp;申请 API Key 想要使用大模型推理 API，我们首先得拿到 API Key，打开智谱 AI 开放平台官网：https://bigmodel.cn/\n完成注册和实名认证后，我们打开控制台，即可复制 API Key 了：https://bigmodel.cn/usercenter/apikeys\n我们可以把复制的 API Key 放到环境变量中（老牛同学的环境变量名：ZhipuAI-APIKey），免费在代码里硬编码，下面老牛同学代码演示均从环境变量获取！\nAPI 调用前置准备和配置 这部分包括 Python 环境准备、SDK 依赖包安装和 API 验证：\n【Python 研发环境】\n工欲善其事，必先利其器，我们通过Miniconda管理 Python 虚拟环境，Miniconda的安装和使用可以参考老牛同学之前的文章：大模型应用研发基础环境配置（Miniconda、Python、Jupyter Lab、Ollama 等）\n第一步：设置 Python 虚拟环境：\n1 2 3 4 5 # Python虚拟环境名：ZhipuAI，版本号：3.10 conda create --name ZhipuAI python=3.10 -y # 激活虚拟环境 conda activate ZhipuAI 第二步：安装 Python SDK 依赖包\n1 pip install zhipuai -U 以上简单的 2 步，我们即完成了前期环境准备工作了，接下来我们简单验证一下准备工作是否达标：\n【开放平台 API 验证】\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 # GLM-4-Flash-Test.py from zhipuai import ZhipuAI import os # 环境变量中获取API Key内容 client = ZhipuAI( api_key=os.environ.get(\u0026#39;ZhipuAI-APIKey\u0026#39;) ) # 使用GLM-4-Flash大模型进行验证 response = client.chat.completions.create( model=\u0026#39;glm-4-flash\u0026#39;, messages=[ {\u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;你好，我是老牛同学，请问你是谁？\u0026#39;}, ], stream=True, ) # 流式输出 for chunk in response: print(chunk.choices[0].delta.content, end=\u0026#39;\u0026#39;) print(\u0026#39;\u0026#39;) 执行验证代码，可以看到如果输出：python GLM-4-Flash-Test.py\n1 2 \u0026gt;python GLM-4-Flash-Test.py 你好，老牛同学！我是人工智能助手智谱清言，很高兴为您服务，有什么可以帮助您的吗？ 原来智谱清言就是GLM-4的应用端名，又涨了一点姿势~\n最后，我们可以查看 API 接口文档，准备我们的儿童绘本应用的研发了：\n【大模型接口文档】\nGLM-4-Flash接口文档：https://bigmodel.cn/dev/api#glm-4\nCogView-3接口文档：https://bigmodel.cn/dev/api#cogview\n“儿童绘本”实战 1：绘本构思 “儿童绘本”应用实战的第一步，就是我们给大模型提供一个主题（如：黑神话·悟空），让大模型产出绘本构思：GLM-4-Flash-Idea.py\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 # GLM-4-Flash-Idea.py # 构造客户端 def make_client(): return ZhipuAI( api_key=os.environ.get(\u0026#39;ZhipuAI-APIKey\u0026#39;) ) # # 第一步：使用GLM-4-Flash大模型产出创意 # def make_idea(): response = make_client().chat.completions.create( model=\u0026#39;glm-4-flash\u0026#39;, messages=[ {\u0026#39;role\u0026#39;: \u0026#39;system\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;你是一位儿童绘本的内容创意专家，你的任务是根据用户提供的主题，提供适合7岁到10岁小学生阅读的、专业的、有见地的绘本内容创意。\u0026#39;}, {\u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;请以“黑神话·悟空”这款最近热门的游戏为主题，提供儿童绘本创意。要求：绘本分为4个小段，每个小段需要有插图。\u0026#39;}, ], stream=True, ) # 流式输出 idea = \u0026#39;\u0026#39; for chunk in response: idea += chunk.choices[0].delta.content return idea # 运行 if __name__ == \u0026#39;__main__\u0026#39;: idea = make_idea() print(idea) 和大模型推理 API 验证的代码相比，就只是 Prompt 提示不一样。有关 Prompt 提示词编写小技巧，可以查看老牛同学之前的文章：高效编写大模型 Prompt 提示词，解锁 AI 无限创意潜能\n运行以上绘本创意代码的结果如下：python GLM-4-Flash-Idea.py\n从提供的创意结果来看，GLM-4-Flash还是有 2 把刷子，产出的创意非常完整，包括：绘本名称、绘本简介、绘本内容、故事内容和绘本特点。面面俱到，比老牛同学的设想要丰富多了：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 绘本名称：《悟空探秘之旅》 绘本简介： 这是一本以热门游戏《黑神话·悟空》为背景的儿童绘本，通过四个小故事，带领孩子们进入一个充满神话色彩的西游记世界，体验悟空 的英勇与智慧。 绘本内容创意： 第一段：《悟空的起源》 插图：孙悟空石破天惊地从石头中蹦出来，周围是五彩斑斓的花草和神秘的森林。 故事简介：绘本开始讲述孙悟空的诞生，描述了石猴如何从一块神奇的石头中蹦出来，开始了他的奇幻之旅。 第二段：《猴王争霸》 插图：孙悟空挥舞金箍棒，与群猴比试，最终成为猴王。 故事简介：孙悟空在花果山与众猴比试，凭借智慧和力量，成为猴王，得到众猴的尊敬。 第三段：《取经之路》 插图：孙悟空带领唐僧、猪八戒和沙僧四人西行取经，路上遇到妖魔鬼怪。 故事简介：孙悟空带着唐僧师徒四人踏上取经之路，他们历经九九八十一难，勇敢地战胜了各种妖魔鬼怪，保护师傅安全。 第四段：《悟空归来》 插图：孙悟空在取得真经后，回到花果山，与众猴欢聚一堂。 故事简介：孙悟空取得真经后，回到花果山，与众猴庆祝胜利，展示了他英勇无畏、智慧过人的品质。 绘本特点： 1. 故事情节紧凑，富有想象力，吸引孩子们阅读。 2. 插图色彩鲜艳，画面生动，有助于培养孩子们的审美观。 3. 故事融入了传统神话元素，传承了中华民族的优秀文化。 4. 通过孙悟空的成长历程，教育孩子们勇敢、智慧、团结的重要性。 “儿童绘本”实战 2：绘本内容 “儿童绘本”应用实战的第二步，就是根据第一步提供的创意，完善故事内容情节：GLM-4-Flash-Content.py\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 # GLM-4-Flash-Content.py # ..... 前面省略 # # 第二步：使用GLM-4-Flash大模型生成故事内容 # def make_content(idea:str): response = make_client().chat.completions.create( model=\u0026#39;glm-4-flash\u0026#39;, messages=[ {\u0026#39;role\u0026#39;: \u0026#39;system\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;你是一位儿童绘本的故事内容编写专家，你编写的故事幽默有趣，特别适合7岁到10岁的小学生阅读，你的任务是根据用户提供的儿童绘本创意，完成编写的整个故事内容。\u0026#39;}, {\u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: f\u0026#39;请根据儿童绘本创意，完成编写整个故事内容。\\n\\n故事内容要求：\\n故事内容分为4个小段，每个小段500个汉字左右，故事总长度不得超过2000个汉字。\\n\\n儿童绘本创意：\\n{idea}\u0026#39;}, ], stream=True, ) # 流式输出 content = \u0026#39;\u0026#39; for chunk in response: content += chunk.choices[0].delta.content return content # 运行 if __name__ == \u0026#39;__main__\u0026#39;: idea = make_idea() content = make_content(content) print(content) 和第一步的代码类似，我们主要是调整了 Prompt 提示词。Prompt 提示中，创意的内容完全复制第一步的内容，一个字都不需要修改。\n运行以上绘本故事内容代码的结果如下：python GLM-4-Flash-Content.py\n还别说，故事内容还算有模有样：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 # 《悟空探秘之旅》 这是一本以热门游戏《黑神话·悟空》为背景的儿童绘本，通过四个小故事，带领孩子们进入一个充满神话色彩的西游记世界，体验悟空 的英勇与智慧。 # 第一段：《悟空的起源》 在一个遥远的时代，有一块五彩斑斓的石头，静静地躺在花果山的山顶上。这石头非同小可，它吸收了天地精华，蕴含着无尽的能量。一天，石头突然炸裂开来，从中蹦出一个石猴，他的眼睛里闪烁着好奇的光芒。 石猴出生后，就与周围的猴子们玩得不亦乐乎。他聪明伶俐，机智过人，很快便成为了猴群中的领袖。有一天，石猴突发奇想，他想要找到自己的起源，于是开始了他的奇幻之旅。 # 第二段：《猴王争霸》 石猴带着对未知的渴望，来到了一座神秘的山洞。他发现这个山洞里隐藏着许多宝藏，于是便决定在这里建立自己的家园。然而，这座山洞并不容易占据，其他猴子也觊觎着这里的宝藏。 石猴决定与这些猴子们一较高下，他挥舞着金箍棒，与群猴比试。凭借着过人的智慧和力量，石猴最终战胜了所有对手，成为了猴王。众猴们纷纷向他致敬，花果山也因此成为了猴子们的乐土。 # 第三段：《取经之路》 时光荏苒，石猴渐渐长大，他渴望着更多的冒险。一日，石猴在山林中遇到了一位云游四方的和尚唐僧。唐僧受佛祖旨意，要前往西天取经。石猴决定跟随唐僧，一同踏上取经之路。 在这条路上，他们遇到了无数妖魔鬼怪。孙悟空凭借着他的智慧和勇气，带领着唐僧、猪八戒和沙僧四人，战胜了一个又一个的困难。他们历经九九八十一难，终于来到了西天，取得了真经。 # 第四段：《悟空归来》 孙悟空取得真经后，带着唐僧师徒四人回到了花果山。猴群们为他们举行了盛大的庆祝活动，欢声笑语充斥着整个山林。孙悟空在这次冒险中，不仅收获了真经，更赢得了众猴的尊敬和爱戴。 孙悟空的成长历程，让所有孩子都为之动容。他英勇无畏、智慧过人的品质，成为了孩子们心中的英雄。而《悟空探秘之旅》这本书，也成为了孩子们传承中华民族优秀文化的瑰宝。 “儿童绘本”实战 3：绘本插图 绘本插图我们需要用到另外一个大模型：CogView-3，它可以根据文本生成图片。可惜的是，它并不免费，它的收费标准如下：\n我们本次需要四张插图，因此预计收费0.4 元，为了能更好的演示如何构建绘本应用，老牛同学觉得收费是值得的。\n在插图图片生成之后，我们需要保存到本地，因此我们需要安装一下 Python 依赖：\n1 pip install requests -U 下面，我们通过调用CogView-3文生图大模型，生成故事每小段的插图：GLM-4-Flash-Illustration.py\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 # GLM-4-Flash-Illustration.py from zhipuai import ZhipuAI import os import requests # ..... 前面省略 # 存储图片到本地 def download_image(url, save_path): print(f\u0026#39;开始下载图片: {url}\u0026#39;) # 发送 HTTP 请求 response = requests.get(url, stream=True) # 检查请求是否成功 if response.status_code == 200: # 以二进制模式打开文件 with open(save_path, \u0026#39;wb\u0026#39;) as file: # 将图片内容写入文件 file.write(response.content) print(f\u0026#39;图片下载成功: {save_path}\u0026#39;) else: print(\u0026#39;图片下载失败.\u0026#39;) # 生成图片 def make_illustration(idea:str): item_list = [\u0026#39;第一段\u0026#39;, \u0026#39;第二段\u0026#39;, \u0026#39;第三段\u0026#39;, \u0026#39;第四段\u0026#39;] for item in item_list: response = make_client().images.generations( model=\u0026#39;cogview-3\u0026#39;, prompt=f\u0026#39;你是一位儿童绘本插图绘画专家，你画的插图紧贴绘本的创意，插图色彩鲜艳，画面生动，有助于培养7岁到10岁的小学生的审美观。\\n\\n下面是一个儿童绘本的创意，共有4个小段，请为“{item}”画一张插图：\\n\\n{idea}\u0026#39; ) image_url = response.data[0].url print(f\u0026#39;{item}插图地址：{image_url}\u0026#39;) # 图片目录 image_dir = os.path.join(os.getcwd(), \u0026#39;assets\u0026#39;) os.makedirs(image_dir, exist_ok=True) # 图片路径 save_path = os.path.join(image_dir, f\u0026#39;{item}.jpg\u0026#39;) # 下载存储图片 download_image(image_url, save_path) # 运行 if __name__ == \u0026#39;__main__\u0026#39;: idea = make_idea() make_illustration(idea) 最终，我们可以看到本地磁盘，有 4 张绘本插图：\n“儿童绘本”实战 4：绘本存储 最后，我们只要把绘本故事内容和插图组织起来，就是一本完整的绘本了。老牛同学使用 Markdown 格式，然后用网上 Markdown 渲染，可以看到最终绘本内容：\n我们把以上代码实例编织起来，就是一个完整的基于大模型的儿童绘本应用程序了，整体结构如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 # GLM-4-Flash.py from zhipuai import ZhipuAI import os import requests # 构造客户端 def make_client(): # ...略 # # 第一步：使用GLM-4-Flash大模型产出创意 # def make_idea(): # ...略 # # 第二步：使用GLM-4-Flash大模型生成故事内容 # def make_content(idea:str): # ...略 # # 第三步：使用CogView-3大模型生成故事插图 # # 存储图片到本地 def download_image(url, save_path): # ...略 # 生成图片 def make_illustration(idea:str): # ...略 # 运行 if __name__ == \u0026#39;__main__\u0026#39;: # 1. 绘本构思 print(\u0026#39;----------------------------- STRT：绘本构思 -----------------------------\u0026#39;) idea = make_idea() print(idea) print(\u0026#39;----------------------------- 绘本构思：END ------------------------------\u0026#39;) # 2. 绘本内容 print(\u0026#39;----------------------------- STRT：绘本内容 -----------------------------\u0026#39;) content = make_content(idea) print(content) print(\u0026#39;----------------------------- 绘本内容：END ------------------------------\u0026#39;) # 3. 绘本插图 print(\u0026#39;----------------------------- STRT：绘本插图 -----------------------------\u0026#39;) make_illustration(idea) print(\u0026#39;----------------------------- 绘本插图：END ------------------------------\u0026#39;) 我们的儿童小绘本应用到此总体就完成了，老牛同学主要是介绍如何使用 AI 完成我们的构思，因此整体有点粗糙，请大家见谅，我们接下来可以进一步优化：\n能否自动生成 Markdown 文件，并且把 Markdown 自动转换为其他格式文件（如：PDF 文件） 能否对接我们的智能家居设备，自动读我们生成的故事（如：天猫精灵、小爱同学等） 更进一步，智能家居读故事的音色，能否模拟我们自己，让孩子们在听故事时，就感觉是自己的爸爸妈妈在读一样？ 在进一步，能否设计一款 APP，支持只需一个主题，自动完成绘本创作、并且能播放？ 以上几点，老牛同学后续有机会，在发文介绍，大家如果感兴趣，也可以与老牛同学私信，我们可以探讨完成！\n最后：智谱 AI 为什么要第一个吃“免费”螃蟹？ 据老牛同学所知，GLM-4-Flash是第一个免费的大模型 API，从前面我们模型部署经验来看，大模型部署和推理对资源的要求较高，那么智谱 AI 为什么要这么做呢？老牛同学觉得有以下几点：\n智谱 AI 技术进步，让GLM-4-Flash大模型推理成本降到了一个足够低的水平，以至于收费也赚不到什么钱，还不如干脆大方一点全免费，在国内大模型发展的初期，抢占先发优势，给自己赚个好口碑的同时，吸引更多的大模型使用者、爱好者、研究者。 智谱 AI 不忘初心，坚持自己的愿景，让 AI 普惠所有人。前期的开源、降门槛粒度还不够，免费有可能是终极手段。 智谱 AI 自信于自己技术，通过自己的免费举措，逐步带动其他大模型厂商降低大模型的门槛，争做 AI 大模型的弄潮儿。 本文实例源代码：https://gitee.com/obullxl/SunningTX/tree/master/PythonTX/GLM-4-Flash\nvLLM CPU 和 GPU 模式署和推理 Qwen2 等大语言模型详细教程\nMiniCPM-V 2.6 面壁“小钢炮”，多图、视频理解多模态模型，部署和推理实战教程\n基于 Qwen2/Lllama3 等大模型，部署团队私有化 RAG 知识库系统的详细教程（Docker+AnythingLLM）\n使用 Llama3/Qwen2 等开源大模型，部署团队私有化 Code Copilot 和使用教程\n基于 Qwen2 大模型微调技术详细教程（LoRA 参数高效微调和 SwanLab 可视化监控）\n","date":"2024-08-27T00:00:00Z","image":"https://ntopic.cn/p/2024082701/00_hua0f417586d7fd1a3944a298dab07c6c1_47981_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/2024082701/","title":"GLM-4-Flash 大模型API免费了，手把手构建“儿童绘本”应用实战（附源码）"},{"content":"第一段：《悟空的起源》 在一个遥远的时代，有一块五彩斑斓的石头，静静地躺在花果山的山顶上。这石头非同小可，它吸收了天地精华，蕴含着无尽的能量。一天，石头突然炸裂开来，从中蹦出一个石猴，他的眼睛里闪烁着好奇的光芒。\n石猴出生后，就与周围的猴子们玩得不亦乐乎。他聪明伶俐，机智过人，很快便成为了猴群中的领袖。有一天，石猴突发奇想，他想要找到自己的起源，于是开始了他的奇幻之旅。\n第二段：《猴王争霸》 石猴带着对未知的渴望，来到了一座神秘的山洞。他发现这个山洞里隐藏着许多宝藏，于是便决定在这里建立自己的家园。然而，这座山洞并不容易占据，其他猴子也觊觎着这里的宝藏。\n石猴决定与这些猴子们一较高下，他挥舞着金箍棒，与群猴比试。凭借着过人的智慧和力量，石猴最终战胜了所有对手，成为了猴王。众猴们纷纷向他致敬，花果山也因此成为了猴子们的乐土。\n第三段：《取经之路》 时光荏苒，石猴渐渐长大，他渴望着更多的冒险。一日，石猴在山林中遇到了一位云游四方的和尚唐僧。唐僧受佛祖旨意，要前往西天取经。石猴决定跟随唐僧，一同踏上取经之路。\n在这条路上，他们遇到了无数妖魔鬼怪。孙悟空凭借着他的智慧和勇气，带领着唐僧、猪八戒和沙僧四人，战胜了一个又一个的困难。他们历经九九八十一难，终于来到了西天，取得了真经。\n第四段：《悟空归来》 孙悟空取得真经后，带着唐僧师徒四人回到了花果山。猴群们为他们举行了盛大的庆祝活动，欢声笑语充斥着整个山林。孙悟空在这次冒险中，不仅收获了真经，更赢得了众猴的尊敬和爱戴。\n孙悟空的成长历程，让所有孩子都为之动容。他英勇无畏、智慧过人的品质，成为了孩子们心中的英雄。而《悟空探秘之旅》这本书，也成为了孩子们传承中华民族优秀文化的瑰宝。\n","date":"2024-08-25T00:00:00Z","image":"https://ntopic.cn/p/2024082501/assets/%E7%AC%AC%E4%B8%80%E6%AE%B5_hu1266b15c2c3b0f5178c4b89f32b11593_116518_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/2024082501/","title":"《悟空探秘之旅》"},{"content":"老牛同学之前偶尔刷到过InternLM大模型相关的介绍文章，因为在老牛同学心中，Qwen2千问才是国内开源模型中最适合自己的大模型，原因是自己在本地部署和应用Qwen2都非常满意，所以没有在意InternLM大模型，也就没有动力去了解它。\n今天老牛同学又刷到InternLM大模型发布1.8B新开源版本的文章，同时还知道了书生·浦语是它的中文名。因老牛同学在上海生活了十几年了，当看到浦字时有点敏感，猜测想是不是代表上海浦东的意思？所以特意去查了一下，官网介绍：书生·浦语（InternLM）大语言模型由上海人工智能实验室联合多家机构共同推出。官网并没有解释浦字的含义，因此老牛同学就算自己的猜测是对的了。\n既然是自己生活的城市发布的大语音模型，那就没有理由不去了解一下了，顺便部署体验一翻：\nInternLM 大模型的简单介绍，顺便介绍一下官网的评测数据，方便大家阅读 通过 Ollama 本地部署 InternLM 大模型，同时通过不同方式进行推理调用，包括 API 调用、WebUI 等 书生·浦语（InternLM）介绍 书生·浦语系列大模型主页：https://internlm.intern-ai.org.cn\n书生·浦语（InternLM） 由上海人工智能实验室（上海 AI 实验室）联合推出，上海 AI 实验室是我国人工智能领域的新型科研机构，它的研究方向包括：人工智能基础理论、人工智能开放平台、人工智能基础软件和基础硬件系统、人工智能应用、人工智能核心技术和人工智能伦理与政策。感觉就是个政府机构，老牛同学生活了这么多年竟然都不知道！\n书生·浦语（InternLM） 自 2023 年 6 月 7 日发布第 1 个大模型，到本月 8 月 4 号，开源发布InternLM 2.5 1.8B小尺寸模型，目前InternLM 2.5有 3 个不同尺寸：1.8B、7B和20B\n同时，针对20B参数量版本，官网提供了一些评测数据：https://github.com/InternLM/InternLM。InternLM-20B显著领先主流的 13B 量级开源模型，在语言、知识学科综合评测上都超越 Llama2-70B，在推理能力评测上和 Llama2-70B 持平，而知识方面则仍有一定差距。\n本地部署 InternLM 2.5 大模型 目前 Ollama 已经支持InternLM 2.5大模型了：https://ollama.com/internlm/internlm2.5:1.8b-chat\n关于 Ollama 详细介绍，老牛同学之前有专门的文章，本文不在累赘：Ollama 完整教程：本地 LLM 管理、WebUI 对话、Python/Java 客户端 API 应用\n我们可以根据自己的需要选择不同的参数版本，老牛同学选择的最新发布的1.8B参数量版本。\n下载模型权重文件：ollama run internlm/internlm2.5:1.8b-chat\n下载完成之后，其实我们已经有了个控制台的对话界面了：\n接下来，我们就可以通过多种方式使用推理服务了，包括：HTTP、Python 客户端、Java 客户端、WebUI 等，老牛同学简单介绍以下 WebUI 方式：\nOllama 自带控制台对话界面体验总归是不太好，接下来部署 Web 可视化聊天界面：\n下载并安装 Node.js 工具：https://nodejs.org/zh-cn 下载ollama-webui工程代码：git clone https://github.com/ollama-webui/ollama-webui-lite ollama-webui 切换ollama-webui代码的目录：cd ollama-webui 设置 Node.js 工具包镜像源（下载提速）：npm config set registry http://mirrors.cloud.tencent.com/npm/ 安装 Node.js 依赖的工具包：npm install 最后，启动 Web 可视化界面：npm run dev 1 2 3 4 5 6 7 8 9 10 11 12 \u0026gt;npm run dev \u0026gt; ollama-webui-lite@0.0.1 dev \u0026gt; vite dev --host --port 3000 VITE v4.5.2 ready in 16023 ms ➜ Local: http://localhost:3000/ ➜ Network: http://192.168.101.35:3000/ ➜ Network: http://172.27.112.1:3000/ ➜ Network: http://172.25.64.1:3000/ ➜ press h to show help 如果看到以上输出，代表 Web 可视化界面已经成功了！\n浏览器打开 Web 可视化界面：http://localhost:3000/\n总结：InternLM 其他能力 以上是老牛同学介绍如何部署和推理书生·浦语（InternLM） 大模型，并进行最简单的对话推理，InternLM的其他能力相关介绍，我们在官网都可以查到，包括：复杂的多步推理、多轮对话意图理解、对输出格式的控制和操作和复杂指令的理解。大家可以在本地进行体验。\nvLLM CPU 和 GPU 模式署和推理 Qwen2 等大语言模型详细教程\nMiniCPM-V 2.6 面壁“小钢炮”，多图、视频理解多模态模型，部署和推理实战教程\n基于 Qwen2/Lllama3 等大模型，部署团队私有化 RAG 知识库系统的详细教程（Docker+AnythingLLM）\n使用 Llama3/Qwen2 等开源大模型，部署团队私有化 Code Copilot 和使用教程\n基于 Qwen2 大模型微调技术详细教程（LoRA 参数高效微调和 SwanLab 可视化监控）\n","date":"2024-08-23T00:00:00Z","image":"https://ntopic.cn/p/2024082301/InternLM_hu41c4c6995ceabd0c37456922e7c82507_79554_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/2024082301/","title":"InternLM 2.5 书生·浦语 开源大模型本地部署体验"},{"content":"MiniCPM-V 2.6是清华和面壁智能最新发布的多模态模型，亦称面壁“小钢炮”，它是 MiniCPM-V 系列中最新、性能最佳的模型。该模型基于 SigLip-400M 和 Qwen2-7B 构建，仅 8B 参数，但却取得 20B 以下单图、多图、视频理解 3 SOTA 成绩，一举将端侧 AI 多模态能力拉升至全面对标 GPT-4V 水平。\nMiniCPM-V 2.6 的主要特点包括：\n仅 8B 参数，单图、多图、视频理解全面超越 GPT-4V ！ 小钢炮一口气将实时视频理解、多图联合理解、多图 ICL 等能力首次搬上端侧多模态模型。 端侧友好：量化后端侧内存仅占 6 GB，个人笔记本电脑可部署和推理。 更多性能和功能介绍，参见 GitHub 官网：https://github.com/OpenBMB/MiniCPM-V\n这么小的参数量，却能带来这么强悍的能力，老牛同学决定部署，和大家一起一探究竟：\n准备环境、下载源代码和模型权重文件 模型部署，进行图片理解推理、WebUI 可视化部署和推理 环境准备和模型下载 环境准备分为 3 部分：Miniconda配置、下载 GitHub 源代码、下载MiniCPM-V 2.6模型权重文件。\nMiniconda 配置 工欲善其事，必先利其器，大模型研发环境先准备好，为后面部署和推理做好准备。详细教程，大家可以参考之前的文章：大模型应用研发基础环境配置（Miniconda、Python、Jupyter Lab、Ollama 等）\n1 conda create --name MiniCPM-V python=3.10 -y 我们创建 Python 虚拟环境：MiniCPM-V，同时 Python 的主版本：3.10\n完成之后，我们激活虚拟环境：conda activate MiniCPM-V\nGitHub 源代码下载 GitHub 源代码地址：https://github.com/OpenBMB/MiniCPM-V.git\n源代码下载的目录：MiniCPM-V\n1 git clone https://github.com/OpenBMB/MiniCPM-V.git MiniCPM-V 源代码下载完成之后，我们就可以安装 Python 依赖包了，包依赖列表文件：MiniCPM-V/requirements.txt\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 packaging==23.2 addict==2.4.0 editdistance==0.6.2 einops==0.7.0 fairscale==0.4.0 jsonlines==4.0.0 markdown2==2.4.10 matplotlib==3.7.4 more_itertools==10.1.0 nltk==3.8.1 numpy==1.24.4 opencv_python_headless==4.5.5.64 openpyxl==3.1.2 Pillow==10.1.0 sacrebleu==2.3.2 seaborn==0.13.0 shortuuid==1.0.11 timm==0.9.10 torch==2.1.2 torchvision==0.16.2 tqdm==4.66.1 protobuf==4.25.0 transformers==4.40.0 typing_extensions==4.8.0 uvicorn==0.24.0.post1 sentencepiece==0.1.99 accelerate==0.30.1 socksio==1.0.0 gradio==4.41.0 gradio_client http://thunlp.oss-cn-qingdao.aliyuncs.com/multi_modal/never_delete/modelscope_studio-0.4.0.9-py3-none-any.whl eva-decord 特别注意：最后一个依赖包decord通过pip install decord安装可能会报如下错误，因此，老牛同学找到了替代的依赖包eva-decord，可正常安装。\n1 2 3 (MiniCPM-V) $ pip install decord ERROR: Could not find a version that satisfies the requirement decord (from versions: none) ERROR: No matching distribution found for decord 模型权重文件下载 模型权重文件比较大，我们需要通过 Git 大文件系统下载：\n权重文件下载的目录：MiniCPM-V2.6\n1 2 git lfs install git clone https://www.modelscope.cn/openbmb/minicpm-v-2_6.git MiniCPM-V2.6 下载过程中，如果因网络等原因中断，我们可以继续断点下载：\n1 2 cd MiniCPM-V2.6 git lfs pull 小试牛刀：单图理解体验 老牛同学网上找了一张汽车图片，先试一下“小钢炮”的威力：\n由于模型推理过程，需要用到权重模型中的 Python 模块，因此我们把推理的 Python 代码放到模型权重文件目录中：MiniCPM-V2.6/MiniCPM-V2.6-01.py\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 # MiniCPM-V2.6-01.py import torch from PIL import Image from transformers import AutoModel, AutoTokenizer # 模型权重文件目录 model_dir = \u0026#39;.\u0026#39; # 加载模型：local_files_only 加载本地模型，trust_remote_code 执行远程代码（必须） model = AutoModel.from_pretrained( model_dir, local_files_only=True, trust_remote_code=True, ) # 设置推理模式，如果有卡：model = model.eval().cuda() model = model.eval() # 加载分词器 tokenizer = AutoTokenizer.from_pretrained( model_dir, local_files_only=True, trust_remote_code=True, ) # 测试的汽车尾部图片，可以指定其它目录 image = Image.open(\u0026#39;Car-01.jpeg\u0026#39;).convert(\u0026#39;RGB\u0026#39;) # 图片理解：自然语言理解 + 图片理解 question = \u0026#39;请问这是一张什么图片？\u0026#39; msgs = [{\u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: [image, question]}] res = model.chat( image=None, msgs=msgs, tokenizer=tokenizer, sampling=True, stream=True ) # 理解结果 generated_text = \u0026#34;\u0026#34; for new_text in res: generated_text += new_text print(new_text, flush=True, end=\u0026#39;\u0026#39;) 图片理解的输出结果如下：\n可以看出，MiniCPM-V 2.6对图片的理解非常详尽：汽车、奥迪、A6L、尾部、黑色、中国、牌照区域、牌照内容等。\n如果要给图片理解推理的结果打分的话，老牛同学打99 分，另外1 分的不足是给老牛同学自己的：推理速度实在太慢了，只能怪老牛同学的笔记本电脑配置太低了！\nWebUI 可视化，推理自由 我们本地完成图片理解推理之后，在来看看 WebUI 可视化推理界面，体验会更好。同样的，我们把 WebUI 代码放到模型权重文件目录中：MiniCPM-V2.6/MiniCPM-V2.6-WebUI.py\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 # MiniCPM-V2.6-WebUI.py #!/usr/bin/env python # encoding: utf-8 import torch import argparse from transformers import AutoModel, AutoTokenizer import gradio as gr from PIL import Image from decord import VideoReader, cpu import io import os import copy import requests import base64 import json import traceback import re import modelscope_studio as mgr # 解析参数 parser = argparse.ArgumentParser(description=\u0026#39;demo\u0026#39;) parser.add_argument(\u0026#39;--device\u0026#39;, type=str, default=\u0026#39;cuda\u0026#39;, help=\u0026#39;cuda or mps\u0026#39;) parser.add_argument(\u0026#39;--multi-gpus\u0026#39;, action=\u0026#39;store_true\u0026#39;, default=False, help=\u0026#39;use multi-gpus\u0026#39;) args = parser.parse_args() device = args.device assert device in [\u0026#39;cuda\u0026#39;, \u0026#39;mps\u0026#39;] # 模型权重文件目录 model_path = \u0026#39;.\u0026#39; # 加载模型和分词器 if \u0026#39;int4\u0026#39; in model_path: if device == \u0026#39;mps\u0026#39;: print(\u0026#39;Error: running int4 model with bitsandbytes on Mac is not supported right now.\u0026#39;) exit() model = AutoModel.from_pretrained(model_path, local_files_only=True, trust_remote_code=True) else: if args.multi_gpus: from accelerate import load_checkpoint_and_dispatch, init_empty_weights, infer_auto_device_map with init_empty_weights(): model = AutoModel.from_pretrained(model_path, local_files_only=True, trust_remote_code=True, attn_implementation=\u0026#39;sdpa\u0026#39;, torch_dtype=torch.bfloat16) device_map = infer_auto_device_map(model, max_memory={0: \u0026#34;10GB\u0026#34;, 1: \u0026#34;10GB\u0026#34;}, no_split_module_classes=[\u0026#39;SiglipVisionTransformer\u0026#39;, \u0026#39;Qwen2DecoderLayer\u0026#39;]) device_id = device_map[\u0026#34;llm.model.embed_tokens\u0026#34;] device_map[\u0026#34;llm.lm_head\u0026#34;] = device_id # firtt and last layer should be in same device device_map[\u0026#34;vpm\u0026#34;] = device_id device_map[\u0026#34;resampler\u0026#34;] = device_id device_id2 = device_map[\u0026#34;llm.model.layers.26\u0026#34;] device_map[\u0026#34;llm.model.layers.8\u0026#34;] = device_id2 device_map[\u0026#34;llm.model.layers.9\u0026#34;] = device_id2 device_map[\u0026#34;llm.model.layers.10\u0026#34;] = device_id2 device_map[\u0026#34;llm.model.layers.11\u0026#34;] = device_id2 device_map[\u0026#34;llm.model.layers.12\u0026#34;] = device_id2 device_map[\u0026#34;llm.model.layers.13\u0026#34;] = device_id2 device_map[\u0026#34;llm.model.layers.14\u0026#34;] = device_id2 device_map[\u0026#34;llm.model.layers.15\u0026#34;] = device_id2 device_map[\u0026#34;llm.model.layers.16\u0026#34;] = device_id2 #print(device_map) model = load_checkpoint_and_dispatch(model, model_path, local_files_only=True, dtype=torch.bfloat16, device_map=device_map) else: model = AutoModel.from_pretrained(model_path, local_files_only=True, trust_remote_code=True) model = model.to(device=device) tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True, trust_remote_code=True) # 设置推理模式 model.eval() ERROR_MSG = \u0026#34;Error, please retry\u0026#34; model_name = \u0026#39;MiniCPM-V 2.6\u0026#39; MAX_NUM_FRAMES = 64 IMAGE_EXTENSIONS = {\u0026#39;.jpg\u0026#39;, \u0026#39;.jpeg\u0026#39;, \u0026#39;.png\u0026#39;, \u0026#39;.bmp\u0026#39;, \u0026#39;.tiff\u0026#39;, \u0026#39;.webp\u0026#39;} VIDEO_EXTENSIONS = {\u0026#39;.mp4\u0026#39;, \u0026#39;.mkv\u0026#39;, \u0026#39;.mov\u0026#39;, \u0026#39;.avi\u0026#39;, \u0026#39;.flv\u0026#39;, \u0026#39;.wmv\u0026#39;, \u0026#39;.webm\u0026#39;, \u0026#39;.m4v\u0026#39;} def get_file_extension(filename): return os.path.splitext(filename)[1].lower() def is_image(filename): return get_file_extension(filename) in IMAGE_EXTENSIONS def is_video(filename): return get_file_extension(filename) in VIDEO_EXTENSIONS form_radio = { \u0026#39;choices\u0026#39;: [\u0026#39;Beam Search\u0026#39;, \u0026#39;Sampling\u0026#39;], #\u0026#39;value\u0026#39;: \u0026#39;Beam Search\u0026#39;, \u0026#39;value\u0026#39;: \u0026#39;Sampling\u0026#39;, \u0026#39;interactive\u0026#39;: True, \u0026#39;label\u0026#39;: \u0026#39;Decode Type\u0026#39; } def create_component(params, comp=\u0026#39;Slider\u0026#39;): if comp == \u0026#39;Slider\u0026#39;: return gr.Slider( minimum=params[\u0026#39;minimum\u0026#39;], maximum=params[\u0026#39;maximum\u0026#39;], value=params[\u0026#39;value\u0026#39;], step=params[\u0026#39;step\u0026#39;], interactive=params[\u0026#39;interactive\u0026#39;], label=params[\u0026#39;label\u0026#39;] ) elif comp == \u0026#39;Radio\u0026#39;: return gr.Radio( choices=params[\u0026#39;choices\u0026#39;], value=params[\u0026#39;value\u0026#39;], interactive=params[\u0026#39;interactive\u0026#39;], label=params[\u0026#39;label\u0026#39;] ) elif comp == \u0026#39;Button\u0026#39;: return gr.Button( value=params[\u0026#39;value\u0026#39;], interactive=True ) def create_multimodal_input(upload_image_disabled=False, upload_video_disabled=False): return mgr.MultimodalInput(upload_image_button_props={\u0026#39;label\u0026#39;: \u0026#39;Upload Image\u0026#39;, \u0026#39;disabled\u0026#39;: upload_image_disabled, \u0026#39;file_count\u0026#39;: \u0026#39;multiple\u0026#39;}, upload_video_button_props={\u0026#39;label\u0026#39;: \u0026#39;Upload Video\u0026#39;, \u0026#39;disabled\u0026#39;: upload_video_disabled, \u0026#39;file_count\u0026#39;: \u0026#39;single\u0026#39;}, submit_button_props={\u0026#39;label\u0026#39;: \u0026#39;Submit\u0026#39;}) def chat(img, msgs, ctx, params=None, vision_hidden_states=None): try: print(\u0026#39;msgs:\u0026#39;, msgs) answer = model.chat( image=None, msgs=msgs, tokenizer=tokenizer, **params ) res = re.sub(r\u0026#39;(\u0026lt;box\u0026gt;.*\u0026lt;/box\u0026gt;)\u0026#39;, \u0026#39;\u0026#39;, answer) res = res.replace(\u0026#39;\u0026lt;ref\u0026gt;\u0026#39;, \u0026#39;\u0026#39;) res = res.replace(\u0026#39;\u0026lt;/ref\u0026gt;\u0026#39;, \u0026#39;\u0026#39;) res = res.replace(\u0026#39;\u0026lt;box\u0026gt;\u0026#39;, \u0026#39;\u0026#39;) answer = res.replace(\u0026#39;\u0026lt;/box\u0026gt;\u0026#39;, \u0026#39;\u0026#39;) print(\u0026#39;answer:\u0026#39;, answer) return 0, answer, None, None except Exception as e: print(e) traceback.print_exc() return -1, ERROR_MSG, None, None def encode_image(image): if not isinstance(image, Image.Image): if hasattr(image, \u0026#39;path\u0026#39;): image = Image.open(image.path).convert(\u0026#34;RGB\u0026#34;) else: image = Image.open(image.file.path).convert(\u0026#34;RGB\u0026#34;) # resize to max_size max_size = 448*16 if max(image.size) \u0026gt; max_size: w,h = image.size if w \u0026gt; h: new_w = max_size new_h = int(h * max_size / w) else: new_h = max_size new_w = int(w * max_size / h) image = image.resize((new_w, new_h), resample=Image.BICUBIC) return image ## save by BytesIO and convert to base64 #buffered = io.BytesIO() #image.save(buffered, format=\u0026#34;png\u0026#34;) #im_b64 = base64.b64encode(buffered.getvalue()).decode() #return {\u0026#34;type\u0026#34;: \u0026#34;image\u0026#34;, \u0026#34;pairs\u0026#34;: im_b64} def encode_video(video): def uniform_sample(l, n): gap = len(l) / n idxs = [int(i * gap + gap / 2) for i in range(n)] return [l[i] for i in idxs] if hasattr(video, \u0026#39;path\u0026#39;): vr = VideoReader(video.path, ctx=cpu(0)) else: vr = VideoReader(video.file.path, ctx=cpu(0)) sample_fps = round(vr.get_avg_fps() / 1) # FPS frame_idx = [i for i in range(0, len(vr), sample_fps)] if len(frame_idx)\u0026gt;MAX_NUM_FRAMES: frame_idx = uniform_sample(frame_idx, MAX_NUM_FRAMES) video = vr.get_batch(frame_idx).asnumpy() video = [Image.fromarray(v.astype(\u0026#39;uint8\u0026#39;)) for v in video] video = [encode_image(v) for v in video] print(\u0026#39;video frames:\u0026#39;, len(video)) return video def check_mm_type(mm_file): if hasattr(mm_file, \u0026#39;path\u0026#39;): path = mm_file.path else: path = mm_file.file.path if is_image(path): return \u0026#34;image\u0026#34; if is_video(path): return \u0026#34;video\u0026#34; return None def encode_mm_file(mm_file): if check_mm_type(mm_file) == \u0026#39;image\u0026#39;: return [encode_image(mm_file)] if check_mm_type(mm_file) == \u0026#39;video\u0026#39;: return encode_video(mm_file) return None def make_text(text): #return {\u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;pairs\u0026#34;: text} # # For remote call return text def encode_message(_question): files = _question.files question = _question.text pattern = r\u0026#34;\\[mm_media\\]\\d+\\[/mm_media\\]\u0026#34; matches = re.split(pattern, question) message = [] if len(matches) != len(files) + 1: gr.Warning(\u0026#34;Number of Images not match the placeholder in text, please refresh the page to restart!\u0026#34;) assert len(matches) == len(files) + 1 text = matches[0].strip() if text: message.append(make_text(text)) for i in range(len(files)): message += encode_mm_file(files[i]) text = matches[i + 1].strip() if text: message.append(make_text(text)) return message def check_has_videos(_question): images_cnt = 0 videos_cnt = 0 for file in _question.files: if check_mm_type(file) == \u0026#34;image\u0026#34;: images_cnt += 1 else: videos_cnt += 1 return images_cnt, videos_cnt def count_video_frames(_context): num_frames = 0 for message in _context: for item in message[\u0026#34;content\u0026#34;]: #if item[\u0026#34;type\u0026#34;] == \u0026#34;image\u0026#34;: # For remote call if isinstance(item, Image.Image): num_frames += 1 return num_frames def respond(_question, _chat_bot, _app_cfg, params_form): _context = _app_cfg[\u0026#39;ctx\u0026#39;].copy() _context.append({\u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: encode_message(_question)}) images_cnt = _app_cfg[\u0026#39;images_cnt\u0026#39;] videos_cnt = _app_cfg[\u0026#39;videos_cnt\u0026#39;] files_cnts = check_has_videos(_question) if files_cnts[1] + videos_cnt \u0026gt; 1 or (files_cnts[1] + videos_cnt == 1 and files_cnts[0] + images_cnt \u0026gt; 0): gr.Warning(\u0026#34;Only supports single video file input right now!\u0026#34;) return _question, _chat_bot, _app_cfg if params_form == \u0026#39;Beam Search\u0026#39;: params = { \u0026#39;sampling\u0026#39;: False, \u0026#39;num_beams\u0026#39;: 3, \u0026#39;repetition_penalty\u0026#39;: 1.2, \u0026#34;max_new_tokens\u0026#34;: 2048 } else: params = { \u0026#39;sampling\u0026#39;: True, \u0026#39;top_p\u0026#39;: 0.8, \u0026#39;top_k\u0026#39;: 100, \u0026#39;temperature\u0026#39;: 0.7, \u0026#39;repetition_penalty\u0026#39;: 1.05, \u0026#34;max_new_tokens\u0026#34;: 2048 } if files_cnts[1] + videos_cnt \u0026gt; 0: params[\u0026#34;max_inp_length\u0026#34;] = 4352 # 4096+256 params[\u0026#34;use_image_id\u0026#34;] = False params[\u0026#34;max_slice_nums\u0026#34;] = 1 if count_video_frames(_context) \u0026gt; 16 else 2 code, _answer, _, sts = chat(\u0026#34;\u0026#34;, _context, None, params) images_cnt += files_cnts[0] videos_cnt += files_cnts[1] _context.append({\u0026#34;role\u0026#34;: \u0026#34;assistant\u0026#34;, \u0026#34;content\u0026#34;: [make_text(_answer)]}) _chat_bot.append((_question, _answer)) if code == 0: _app_cfg[\u0026#39;ctx\u0026#39;]=_context _app_cfg[\u0026#39;sts\u0026#39;]=sts _app_cfg[\u0026#39;images_cnt\u0026#39;] = images_cnt _app_cfg[\u0026#39;videos_cnt\u0026#39;] = videos_cnt upload_image_disabled = videos_cnt \u0026gt; 0 upload_video_disabled = videos_cnt \u0026gt; 0 or images_cnt \u0026gt; 0 return create_multimodal_input(upload_image_disabled, upload_video_disabled), _chat_bot, _app_cfg def fewshot_add_demonstration(_image, _user_message, _assistant_message, _chat_bot, _app_cfg): ctx = _app_cfg[\u0026#34;ctx\u0026#34;] message_item = [] if _image is not None: image = Image.open(_image).convert(\u0026#34;RGB\u0026#34;) ctx.append({\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: [encode_image(image), make_text(_user_message)]}) message_item.append({\u0026#34;text\u0026#34;: \u0026#34;[mm_media]1[/mm_media]\u0026#34; + _user_message, \u0026#34;files\u0026#34;: [_image]}) else: if _user_message: ctx.append({\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: [make_text(_user_message)]}) message_item.append({\u0026#34;text\u0026#34;: _user_message, \u0026#34;files\u0026#34;: []}) else: message_item.append(None) if _assistant_message: ctx.append({\u0026#34;role\u0026#34;: \u0026#34;assistant\u0026#34;, \u0026#34;content\u0026#34;: [make_text(_assistant_message)]}) message_item.append({\u0026#34;text\u0026#34;: _assistant_message, \u0026#34;files\u0026#34;: []}) else: message_item.append(None) _chat_bot.append(message_item) return None, \u0026#34;\u0026#34;, \u0026#34;\u0026#34;, _chat_bot, _app_cfg def fewshot_respond(_image, _user_message, _chat_bot, _app_cfg, params_form): user_message_contents = [] _context = _app_cfg[\u0026#34;ctx\u0026#34;].copy() if _image: image = Image.open(_image).convert(\u0026#34;RGB\u0026#34;) user_message_contents += [encode_image(image)] if _user_message: user_message_contents += [make_text(_user_message)] if user_message_contents: _context.append({\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: user_message_contents}) if params_form == \u0026#39;Beam Search\u0026#39;: params = { \u0026#39;sampling\u0026#39;: False, \u0026#39;num_beams\u0026#39;: 3, \u0026#39;repetition_penalty\u0026#39;: 1.2, \u0026#34;max_new_tokens\u0026#34;: 2048 } else: params = { \u0026#39;sampling\u0026#39;: True, \u0026#39;top_p\u0026#39;: 0.8, \u0026#39;top_k\u0026#39;: 100, \u0026#39;temperature\u0026#39;: 0.7, \u0026#39;repetition_penalty\u0026#39;: 1.05, \u0026#34;max_new_tokens\u0026#34;: 2048 } code, _answer, _, sts = chat(\u0026#34;\u0026#34;, _context, None, params) _context.append({\u0026#34;role\u0026#34;: \u0026#34;assistant\u0026#34;, \u0026#34;content\u0026#34;: [make_text(_answer)]}) if _image: _chat_bot.append([ {\u0026#34;text\u0026#34;: \u0026#34;[mm_media]1[/mm_media]\u0026#34; + _user_message, \u0026#34;files\u0026#34;: [_image]}, {\u0026#34;text\u0026#34;: _answer, \u0026#34;files\u0026#34;: []} ]) else: _chat_bot.append([ {\u0026#34;text\u0026#34;: _user_message, \u0026#34;files\u0026#34;: [_image]}, {\u0026#34;text\u0026#34;: _answer, \u0026#34;files\u0026#34;: []} ]) if code == 0: _app_cfg[\u0026#39;ctx\u0026#39;]=_context _app_cfg[\u0026#39;sts\u0026#39;]=sts return None, \u0026#39;\u0026#39;, \u0026#39;\u0026#39;, _chat_bot, _app_cfg def regenerate_button_clicked(_question, _image, _user_message, _assistant_message, _chat_bot, _app_cfg, params_form): if len(_chat_bot) \u0026lt;= 1 or not _chat_bot[-1][1]: gr.Warning(\u0026#39;No question for regeneration.\u0026#39;) return \u0026#39;\u0026#39;, _image, _user_message, _assistant_message, _chat_bot, _app_cfg if _app_cfg[\u0026#34;chat_type\u0026#34;] == \u0026#34;Chat\u0026#34;: images_cnt = _app_cfg[\u0026#39;images_cnt\u0026#39;] videos_cnt = _app_cfg[\u0026#39;videos_cnt\u0026#39;] _question = _chat_bot[-1][0] _chat_bot = _chat_bot[:-1] _app_cfg[\u0026#39;ctx\u0026#39;] = _app_cfg[\u0026#39;ctx\u0026#39;][:-2] files_cnts = check_has_videos(_question) images_cnt -= files_cnts[0] videos_cnt -= files_cnts[1] _app_cfg[\u0026#39;images_cnt\u0026#39;] = images_cnt _app_cfg[\u0026#39;videos_cnt\u0026#39;] = videos_cnt upload_image_disabled = videos_cnt \u0026gt; 0 upload_video_disabled = videos_cnt \u0026gt; 0 or images_cnt \u0026gt; 0 _question, _chat_bot, _app_cfg = respond(_question, _chat_bot, _app_cfg, params_form) return _question, _image, _user_message, _assistant_message, _chat_bot, _app_cfg else: last_message = _chat_bot[-1][0] last_image = None last_user_message = \u0026#39;\u0026#39; if last_message.text: last_user_message = last_message.text if last_message.files: last_image = last_message.files[0].file.path _chat_bot = _chat_bot[:-1] _app_cfg[\u0026#39;ctx\u0026#39;] = _app_cfg[\u0026#39;ctx\u0026#39;][:-2] _image, _user_message, _assistant_message, _chat_bot, _app_cfg = fewshot_respond(last_image, last_user_message, _chat_bot, _app_cfg, params_form) return _question, _image, _user_message, _assistant_message, _chat_bot, _app_cfg def flushed(): return gr.update(interactive=True) def clear(txt_message, chat_bot, app_session): txt_message.files.clear() txt_message.text = \u0026#39;\u0026#39; chat_bot = copy.deepcopy(init_conversation) app_session[\u0026#39;sts\u0026#39;] = None app_session[\u0026#39;ctx\u0026#39;] = [] app_session[\u0026#39;images_cnt\u0026#39;] = 0 app_session[\u0026#39;videos_cnt\u0026#39;] = 0 return create_multimodal_input(), chat_bot, app_session, None, \u0026#39;\u0026#39;, \u0026#39;\u0026#39; def select_chat_type(_tab, _app_cfg): _app_cfg[\u0026#34;chat_type\u0026#34;] = _tab return _app_cfg init_conversation = [ [ None, { # The first message of bot closes the typewriter. \u0026#34;text\u0026#34;: \u0026#34;You can talk to me now\u0026#34;, \u0026#34;flushing\u0026#34;: False } ], ] css = \u0026#34;\u0026#34;\u0026#34; video { height: auto !important; } .example label { font-size: 16px;} \u0026#34;\u0026#34;\u0026#34; introduction = \u0026#34;\u0026#34;\u0026#34; ## Features: 1. Chat with single image 2. Chat with multiple images 3. Chat with video 4. In-context few-shot learning Click `How to use` tab to see examples. \u0026#34;\u0026#34;\u0026#34; with gr.Blocks(css=css) as demo: with gr.Tab(model_name): with gr.Row(): with gr.Column(scale=1, min_width=300): gr.Markdown(value=introduction) params_form = create_component(form_radio, comp=\u0026#39;Radio\u0026#39;) regenerate = create_component({\u0026#39;value\u0026#39;: \u0026#39;Regenerate\u0026#39;}, comp=\u0026#39;Button\u0026#39;) clear_button = create_component({\u0026#39;value\u0026#39;: \u0026#39;Clear History\u0026#39;}, comp=\u0026#39;Button\u0026#39;) with gr.Column(scale=3, min_width=500): app_session = gr.State({\u0026#39;sts\u0026#39;:None,\u0026#39;ctx\u0026#39;:[], \u0026#39;images_cnt\u0026#39;: 0, \u0026#39;videos_cnt\u0026#39;: 0, \u0026#39;chat_type\u0026#39;: \u0026#39;Chat\u0026#39;}) chat_bot = mgr.Chatbot(label=f\u0026#34;Chat with {model_name}\u0026#34;, value=copy.deepcopy(init_conversation), height=600, flushing=False, bubble_full_width=False) with gr.Tab(\u0026#34;Chat\u0026#34;) as chat_tab: txt_message = create_multimodal_input() chat_tab_label = gr.Textbox(value=\u0026#34;Chat\u0026#34;, interactive=False, visible=False) txt_message.submit( respond, [txt_message, chat_bot, app_session, params_form], [txt_message, chat_bot, app_session] ) with gr.Tab(\u0026#34;Few Shot\u0026#34;) as fewshot_tab: fewshot_tab_label = gr.Textbox(value=\u0026#34;Few Shot\u0026#34;, interactive=False, visible=False) with gr.Row(): with gr.Column(scale=1): image_input = gr.Image(type=\u0026#34;filepath\u0026#34;, sources=[\u0026#34;upload\u0026#34;]) with gr.Column(scale=3): user_message = gr.Textbox(label=\u0026#34;User\u0026#34;) assistant_message = gr.Textbox(label=\u0026#34;Assistant\u0026#34;) with gr.Row(): add_demonstration_button = gr.Button(\u0026#34;Add Example\u0026#34;) generate_button = gr.Button(value=\u0026#34;Generate\u0026#34;, variant=\u0026#34;primary\u0026#34;) add_demonstration_button.click( fewshot_add_demonstration, [image_input, user_message, assistant_message, chat_bot, app_session], [image_input, user_message, assistant_message, chat_bot, app_session] ) generate_button.click( fewshot_respond, [image_input, user_message, chat_bot, app_session, params_form], [image_input, user_message, assistant_message, chat_bot, app_session] ) chat_tab.select( select_chat_type, [chat_tab_label, app_session], [app_session] ) chat_tab.select( # do clear clear, [txt_message, chat_bot, app_session], [txt_message, chat_bot, app_session, image_input, user_message, assistant_message] ) fewshot_tab.select( select_chat_type, [fewshot_tab_label, app_session], [app_session] ) fewshot_tab.select( # do clear clear, [txt_message, chat_bot, app_session], [txt_message, chat_bot, app_session, image_input, user_message, assistant_message] ) chat_bot.flushed( flushed, outputs=[txt_message] ) regenerate.click( regenerate_button_clicked, [txt_message, image_input, user_message, assistant_message, chat_bot, app_session, params_form], [txt_message, image_input, user_message, assistant_message, chat_bot, app_session] ) clear_button.click( clear, [txt_message, chat_bot, app_session], [txt_message, chat_bot, app_session, image_input, user_message, assistant_message] ) with gr.Tab(\u0026#34;How to use\u0026#34;): with gr.Column(): with gr.Row(): image_example = gr.Image(value=\u0026#34;http://thunlp.oss-cn-qingdao.aliyuncs.com/multi_modal/never_delete/m_bear2.gif\u0026#34;, label=\u0026#39;1. Chat with single or multiple images\u0026#39;, interactive=False, width=400, elem_classes=\u0026#34;example\u0026#34;) example2 = gr.Image(value=\u0026#34;http://thunlp.oss-cn-qingdao.aliyuncs.com/multi_modal/never_delete/video2.gif\u0026#34;, label=\u0026#39;2. Chat with video\u0026#39;, interactive=False, width=400, elem_classes=\u0026#34;example\u0026#34;) example3 = gr.Image(value=\u0026#34;http://thunlp.oss-cn-qingdao.aliyuncs.com/multi_modal/never_delete/fshot.gif\u0026#34;, label=\u0026#39;3. Few shot\u0026#39;, interactive=False, width=400, elem_classes=\u0026#34;example\u0026#34;) # 启动WebUI: http://127.0.0.1:8885 demo.launch(share=False, debug=True, show_api=False, server_port=8885, server_name=\u0026#34;0.0.0.0\u0026#34;) WebUI 支持 GPU 和苹果 CPU 推理，启动方式分别为：\nGPU：python web_demo_2.6.py --device cuda 苹果 MPS：PYTORCH_ENABLE_MPS_FALLBACK=1 PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 python web_demo_2.6.py --device mps 浏览器打开地址：http://0.0.0.0:8885/ ，可以看到可视化界面：\nWebUI 功能支持：自然语言对话、上传图片、上传视频等理解。\n终极考验：视频理解体验 老牛同学受限于电脑配置，视频预计推理速度将会极慢，因此不做演示。根据官方代码，视频推理和图片推理类似，代码样例如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 import torch from PIL import Image from modelscope import AutoModel, AutoTokenizer from decord import VideoReader, cpu # 模型权重文件目录 model_dir = \u0026#39;.\u0026#39; # 加载模型：local_files_only 加载本地模型，trust_remote_code 执行远程代码（必须） model = AutoModel.from_pretrained( model_dir, local_files_only=True, trust_remote_code=True, ) # 设置推理模式，如果有卡：model = model.eval().cuda() model = model.eval() # 加载分词器 tokenizer = AutoTokenizer.from_pretrained( model_dir, local_files_only=True, trust_remote_code=True, ) MAX_NUM_FRAMES=64 def encode_video(video_path): def uniform_sample(l, n): gap = len(l) / n idxs = [int(i * gap + gap / 2) for i in range(n)] return [l[i] for i in idxs] vr = VideoReader(video_path, ctx=cpu(0)) sample_fps = round(vr.get_avg_fps() / 1) # FPS frame_idx = [i for i in range(0, len(vr), sample_fps)] if len(frame_idx) \u0026gt; MAX_NUM_FRAMES: frame_idx = uniform_sample(frame_idx, MAX_NUM_FRAMES) frames = vr.get_batch(frame_idx).asnumpy() frames = [Image.fromarray(v.astype(\u0026#39;uint8\u0026#39;)) for v in frames] print(\u0026#39;num frames:\u0026#39;, len(frames)) return frames # 视频文件路径 video_path=\u0026#34;~/Car.mp4\u0026#34; frames = encode_video(video_path) question = \u0026#34;请问这是一个什么视频？\u0026#34; msgs = [ {\u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: frames + [question]}, ] # Set decode params for video params={} params[\u0026#34;use_image_id\u0026#34;] = False params[\u0026#34;max_slice_nums\u0026#34;] = 2 # 如果cuda OOM且视频分辨率大于448*448 可设为1 answer = model.chat( image=None, msgs=msgs, tokenizer=tokenizer, **params ) print(answer) 老牛同学在这里，引用了官方的绘制兔子头像的一个视频理解：\n|\n基于 Qwen2/Lllama3 等大模型，部署团队私有化 RAG 知识库系统的详细教程（Docker+AnythingLLM）\n使用 Llama3/Qwen2 等开源大模型，部署团队私有化 Code Copilot 和使用教程\n基于 Qwen2 大模型微调技术详细教程（LoRA 参数高效微调和 SwanLab 可视化监控）\nLivePortrait 数字人：开源的图生视频模型，本地部署和专业视频制作详细教程\n本地部署 GLM-4-9B 清华智谱开源大模型方法和对话效果体验\n玩转 AI，笔记本电脑安装属于自己的 Llama 3 8B 大模型和对话客户端\nChatTTS 开源文本转语音模型本地部署、API 使用和搭建 WebUI 界面\nOllama 完整教程：本地 LLM 管理、WebUI 对话、Python/Java 客户端 API 应用\n","date":"2024-08-14T00:00:00Z","image":"https://ntopic.cn/p/2024081401/MiniCPM-V_huc8b43462814d34d316edcb67d57199f7_46284_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/2024081401/","title":"MiniCPM-V 2.6 面壁“小钢炮”，多图、视频理解多模态模型，部署和推理实战教程"},{"content":"老牛同学在前面文章中，介绍了不同大模型的部署和推理方式。有网友给老牛同学留言，希望老牛同学能写一篇使用 vLLM 通过 CPU 和 GPU 推理 Qwen2 等大语言模型的文章，老牛同学决定本期安排起来。\n由于vLLM默认并不支持 CPU 部署和推理，为了验证和演示CPU部署和推理过程，老牛同学肝了周六 1 个晚上和周日大半天，目前都成功了。因vLLM通过CPU部署推理的文档少之又少，官网有些地方也不是很明确，导致老牛同学在执行过程中走了不少弯路。因此，老牛同学希望通过本文，能帮助大家避免踩坑和走弯路。\n通过本文，老牛同学将介绍vLLM的 3 部分内容：\nCPU模式下，如果编译打包vLLM框架（你没有看错：CPU模式需要我们自己编译打包） 通过CPU模式，演示多种方式部署和推理Qwen2大模型（其他模型也一样）：本地模式、部署 API 服务、API 客户端、WebUI 等方式 最后，在GPU模式，简单介绍vLLM部署和推理；相比CPU环境，简单太多了，因为vLLM默认就只支持GPU环境 vLLM 官网源代码地址：https://github.com/vllm-project/vllm\nvLLM 支持目前主流大模型，详细列表见官网：https://docs.vllm.ai/en/latest/models/supported_models.html\nQwen2 系列大模型在vLLM支持大模型之列，本文将详细介绍通过vLLM部署和推理Qwen2-0.5B大语言模型（之所以选择0.5B小尺寸模型，纯粹下载更快、演示更方便，其他vLLM所支持的所有大模型，其部署和推理过程完全一样，大家可以根据自己的需要选择不同的模型）\nvLLM 环境准备 特别注意的是：vLLM目前只支持Linux操作系统（包括Windows WSL子系统），因此环境准备分为 2 部分：\nLinux 环境准备：我们的操作系统如果是Linux系统，那就无需其他操作；如果是Windows操作系统，需要首先安装和配置好WSL子系统，老牛同学就不介绍（本教程老牛同学也是用的Windows WSL Ubuntu 子系统） Python 研发环境准备：老牛默认使用Miniconda，需要安装和设置。老牛同学稍微进行介绍，大模型研发环境配置详细文档，参加老牛同学之前的文章：大模型应用研发基础环境配置（Miniconda、Python、Jupyter Lab、Ollama 等） 安装 Miniconda 包管理工具 Linux/Windows WSL系统，我们可以通过命令行完成安装：\n1 2 3 4 mkdir -p ~/miniconda3 wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda3/miniconda.sh bash ~/miniconda3/miniconda.sh -b -u -p ~/miniconda3 rm -rf ~/miniconda3/miniconda.sh Shell 上下文激活Miniconda相关命令：\n1 2 3 4 5 ~/miniconda3/bin/conda init bash ~/miniconda3/bin/conda init zsh source /home/obullxl/.bashrc source /home/obullxl/.zshrc 设置 Miniconda 国内镜像 配置镜像是为了提升依赖包下载速度，老牛同学强烈建议进行配置。\nMiniconda配置文件路径：~/.condarc，一般情况下配置文件不存在，我们可以创建并初始化它：conda config --set show_channel_urls yes\n然后打开配置文件，设置依赖包镜像渠道：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 show_channel_urls: true channels: - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/ - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/msys2/ - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge/ - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/ - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/ - defaults custom_channels: conda-forge: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud msys2: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud bioconda: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud menpo: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud pytorch: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud pytorch-lts: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud simpleitk: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud 保存配置文件，我可以查看配置是否生效：conda info\n设置 Python 版本和虚拟环境 接下来，我们通过Minicodan安装 Python 虚拟环境：conda create --name vLLM python=3.10 -y\n虚拟环境名：vLLM Python 版本：3.10 虚拟环境安装完成之后，我们激活环境：conda activate vLLM\n特别注意：Python 版本号建议为3.10，老牛同学刚开始使用3.12后续过程出现不兼容问题！\n下载 Qwen2 模型权重文件 接下来，我们下载Qwen2大模型权重文件，共有 3 种方式：Git 下载、SDK 下载、命令行下载（推荐：Git方式）\n下载 Qwen2 模型权重文件（Git 方式） 我们将通过Git下载权重文件，因此先检测一下Git是否安装：git\n若没有安装，则通过命令进行安装：sudo apt-get install git\n如果想更新Git版本，则可执行命令：sudo apt-get update\n同时，Qwen2模型权重文件比较大，我们需要用到 Git 大文件系统，因此需要提前安装好：\n1 sudo apt-get install git-lfs Git准备就绪，我们开始下载模型权重文件到本地（老牛同学目录：/home/obullxl/ModelSpace/Qwen2-0.5B）：\n1 2 3 4 5 6 # 创建目录 mkdir -p ~/ModelSpace \u0026amp;\u0026amp; cd ~/ModelSpace # 下载文件 git lfs install git clone https://www.modelscope.cn/qwen/qwen2-0.5b.git Qwen2-0.5B 下载过程中，如果因网络等原因中断，我们可以继续断点下载：\n1 2 cd ~/ModelSpace/Qwen2-0.5B git lfs pull` 下载成功之后，我们可以看到Qwen2的模型权重文件列表：\n下载 Qwen2 模型权重文件（SDK 和命令行方式） 当然，我们还可以通过以下 2 种方式下载模型权重文件：\n通过 SDK 下载： 1 2 3 4 pip install modelscope from modelscope import snapshot_download model_dir = snapshot_download(\u0026#39;qwen/qwen2-0.5b\u0026#39;) 通过命令行下载： 1 2 pip install modelscope modelscope download --model qwen/qwen2-0.5b 使用 vLLM 部署和推理大模型 特别注意：vLLM的依赖包默认支持 GPU 部署和推理，如果使用CPU推理，我们需要根据vLLM源代码重新编译打包！\n【GPU 部署和推理】比较简单，通过 PIP 直接安装依赖包即可：\n1 pip install vLLM 【CPU 部署和推理】我们需要下载vLLM源代码，自己编译打包和安装：\n首先，下载vLLM源代码（老牛同学目录：~/CodeSpace/vllm-project）：\n1 2 3 4 mkdir -p ~/CodeSpace cd ~/CodeSpace git clone https://github.com/vllm-project/vllm.git vllm-project 然后，我们安装源代码GCC 编译器：\n1 2 3 sudo apt-get update -y sudo apt-get install -y gcc-12 g++-12 libnuma-dev sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-12 10 --slave /usr/bin/g++ g++ /usr/bin/g++-12 接下来，我们需要安装vLLM便于打包的依赖：\n1 2 3 4 5 cd ~/CodeSpace/vllm-project pip install --upgrade pip pip install wheel packaging ninja \u0026#34;setuptools\u0026gt;=49.4.0\u0026#34; numpy pip install -v -r requirements-cpu.txt --extra-index-url https://download.pytorch.org/whl/cpu 最后，我们可以进行vLLM打包安装了：\n1 2 3 cd ~/CodeSpace/vllm-project VLLM_TARGET_DEVICE=cpu python setup.py install vLLM打包安装完成，我们就可以开始对Qwen2大模型进行部署和推理了：\nvLLM 本地大模型部署和推理 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 # Qwen2-vLLM-Local.py import os from transformers import AutoTokenizer from vllm import LLM, SamplingParams # 设置环境变量 os.environ[\u0026#39;VLLM_TARGET_DEVICE\u0026#39;] = \u0026#39;cpu\u0026#39; # 模型ID：我们下载的模型权重文件目录 model_dir = \u0026#39;/home/obullxl/ModelSpace/Qwen2-0.5B\u0026#39; # Tokenizer初始化 tokenizer = AutoTokenizer.from_pretrained( model_dir, local_files_only=True, ) # Prompt提示词 messages = [ {\u0026#39;role\u0026#39;: \u0026#39;system\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;You are a helpful assistant.\u0026#39;}, {\u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;天空为什么是蓝色的？\u0026#39;} ] text = tokenizer.apply_chat_template( messages, tokenize=False, add_generation_prompt=True, ) # 初始化大语言模型 llm = LLM( model=model_dir, tensor_parallel_size=1, # CPU无需张量并行 device=\u0026#39;cpu\u0026#39;, ) # 超参数：最多512个Token sampling_params = SamplingParams(temperature=0.7, top_p=0.8, repetition_penalty=1.05, max_tokens=512) # 模型推理输出 outputs = llm.generate([text], sampling_params) for output in outputs: prompt = output.prompt generated_text = output.outputs[0].text print(f\u0026#39;Prompt提示词: {prompt!r}, 大模型推理输出: {generated_text!r}\u0026#39;) 发布 API 服务和调用推理 本地部署推理只能在一台服务器完成，我们也通过vLLM把本地大模型部署成 OpenAI API 服务：\n1 python -m vllm.entrypoints.openai.api_server --model ~/ModelSpace/Qwen2-0.5B 默认情况下，API 服务端口为8000，我们可通过 --port 参数设置服务端口；同时，可通过--host 参数设置服务地址：\n1 python -m vllm.entrypoints.openai.api_server --model /home/obullxl/ModelSpace/Qwen2-0.5B --port 8000 --host 0.0.0.0 API 服务部署成功之后，可以通过 CURL 命令验证服务：\n1 2 3 4 5 6 7 8 9 10 11 12 # Qwen2-vLLM-CURL.py curl http://localhost:8000/v1/chat/completions -H \u0026#34;Content-Type: application/json\u0026#34; -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;/home/obullxl/ModelSpace/Qwen2-0.5B\u0026#34;, \u0026#34;messages\u0026#34;: [ {\u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;You are a helpful assistant.\u0026#34;}, {\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;天空为什么是蓝色的？\u0026#34;} ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;top_p\u0026#34;: 0.8, \u0026#34;repetition_penalty\u0026#34;: 1.05, \u0026#34;max_tokens\u0026#34;: 512 }\u0026#39; 或者，我们可以通过 Python 客户端调用API 访问服务：\n若没有安装openai依赖包，需要提前安装一下：pip install openai\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 # Qwen2-vLLM-OpenAI.py from openai import OpenAI # OpenAI初始化 client = OpenAI( api_key=\u0026#39;EMPTY\u0026#39;, base_url=\u0026#39;http://localhost:8000/v1\u0026#39;, ) chat_response = client.chat.completions.create( model=\u0026#39;/home/obullxl/ModelSpace/Qwen2-0.5B\u0026#39;, messages=[ {\u0026#39;role\u0026#39;: \u0026#39;system\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;You are a helpful assistant.\u0026#39;}, {\u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;天空为什么是蓝色的？\u0026#39;}, ], temperature=0.7, top_p=0.8, max_tokens=512, ) print(\u0026#39;Qwen2推理结果:\u0026#39;, chat_response) 我们还可以通过 WebUI 访问我们部署的 API 服务：\n1 pip install gradio 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 # Qwen2-vLLM-WebUI.py import argparse import json import gradio as gr import requests def http_bot(prompt): headers = {\u0026#34;User-Agent\u0026#34;: \u0026#34;vLLM Client\u0026#34;} pload = { \u0026#34;prompt\u0026#34;: prompt, \u0026#34;stream\u0026#34;: True, \u0026#34;max_tokens\u0026#34;: 128, } response = requests.post(args.model_url, headers=headers, json=pload, stream=True) for chunk in response.iter_lines(chunk_size=8192, decode_unicode=False, delimiter=b\u0026#34;\\0\u0026#34;): if chunk: data = json.loads(chunk.decode(\u0026#34;utf-8\u0026#34;)) output = data[\u0026#34;text\u0026#34;][0] yield output def build_demo(): with gr.Blocks() as demo: gr.Markdown(\u0026#34;# vLLM text completion demo\\n\u0026#34;) inputbox = gr.Textbox(label=\u0026#34;Input\u0026#34;, placeholder=\u0026#34;Enter text and press ENTER\u0026#34;) outputbox = gr.Textbox(label=\u0026#34;Output\u0026#34;, placeholder=\u0026#34;Generated result from the model\u0026#34;) inputbox.submit(http_bot, [inputbox], [outputbox]) return demo if __name__ == \u0026#34;__main__\u0026#34;: parser = argparse.ArgumentParser() parser.add_argument(\u0026#34;--host\u0026#34;, type=str, default=None) parser.add_argument(\u0026#34;--port\u0026#34;, type=int, default=8001) parser.add_argument(\u0026#34;--model-url\u0026#34;, type=str, default=\u0026#34;http://0.0.0.0:8000/generate\u0026#34;) args = parser.parse_args() demo = build_demo() demo.queue().launch(server_name=args.host, server_port=args.port, share=True) 启动 WebUI 服务：python Qwen2-vLLM-WebUI.py\n浏览器打开 WebUI 界面：http://localhost:8001（如是Windows WSL子系统，可以通过ifconfig命令、或者直接通过ifconfig | grep eth0 -n1 | grep inet | awk '{print $3}'命令获取 WSL 的 IP 地址），通过 WebUI 我们就可以与大模型对话了：\n1 Running on local URL: http://127.0.0.1:8001 如果我们是Windows WSL子系统，那么需要把 WebUI 设置为共享模式，否则会有如下提示：\n1 2 3 4 5 6 7 8 9 Running on local URL: http://127.0.0.1:8001 Could not create share link. Missing file: /home/obullxl/miniconda3/envs/vLLM/lib/python3.10/site-packages/gradio/frpc_linux_amd64_v0.2. Please check your internet connection. This can happen if your antivirus software blocks the download of this file. You can install manually by following these steps: 1. Download this file: https://cdn-media.huggingface.co/frpc-gradio-0.2/frpc_linux_amd64 2. Rename the downloaded file to: frpc_linux_amd64_v0.2 3. Move the file to this location: /home/obullxl/miniconda3/envs/vLLM/lib/python3.10/site-packages/gradio frpc_linux_amd64文件默认在 HF 上，需要通畅的网络，老牛同学已经下载好，请自取：\n1 2 链接: https://pan.baidu.com/s/1CYDcbkUEhhhCEuzb5z8xXA?pwd=LNTX 提取码: LNTX GPU 多卡部署和推理大模型 非常抱歉，因老牛同学没有 GPU 卡，因此本文此部分通过 GPU 部署和推理的结果无法演示和截图。但老牛同学查看和研究了官网，总结了 GPU 多卡的使用方法，大家若有卡的话可以验证一下。若本文此部分有不正确或者遗漏之处，还望留言指出，老牛同学进行修正完善，谢谢大家！\n我们可以通过tensor_parallel_size参数启用 GPU 多卡分布式并行推理能力，提高大模型推理的吞吐量。\n【vLLM 本地大模型部署和推理】\n1 2 3 4 5 6 7 8 9 10 from vllm import LLM, SamplingParams # ...... 省略部分 llm = LLM( model=model_dir, tensor_parallel_size=4, ) # ....... 其他省略 【发布 API 服务和调用推理】\n同样的，也可以通过 --tensor-parallel-size 参数部署并发布 API 服务：\n1 python -m vllm.entrypoints.api_server --model /home/obullxl/ModelSpace/Qwen2-0.5B --tensor-parallel-size 4 客户端的使用方法，与之前完全一样~\n总结：vLLM 建议生产环境部署推理 至此，通过vLLM部署和推理Qwen2等大语言模型的教程就结束了。vLLM官方默认支持 GPU 部署和推理，CPU则需要单独编译打包，比较繁琐。\n老牛同学建议：如果我们是研发、联调、WSL 等环境使用大语言模型，建议就是直接用Llama.cpp、Ollama等部署和推理工具；生产部署是 Linux 操作系统，可以使用vLLM部署和推理。\n大家可以看出，其实通过Llama.cpp、Ollama等方式部署和推理大语言模型的方式，其实都大同小异：\n本地部署和推理：先下载模型权重文件，然后通过对应的 Python 模块完成部署和推理 通过 API 部署和推理：先把本地模型部署到某个 API 服务端口，然后通过 API 客户端完成调用推理 本文所有源代码仓库地址：https://gitee.com/obullxl/SunningTX/tree/master/PythonTX/Qwen-vLLM\n基于 Qwen2 大模型微调技术详细教程（LoRA 参数高效微调和 SwanLab 可视化监控）\nLivePortrait 数字人：开源的图生视频模型，本地部署和专业视频制作详细教程\n基于 Qwen2/Lllama3 等大模型，部署团队私有化 RAG 知识库系统的详细教程（Docker+AnythingLLM）\n使用 Llama3/Qwen2 等开源大模型，部署团队私有化 Code Copilot 和使用教程\n本地部署 GLM-4-9B 清华智谱开源大模型方法和对话效果体验\n玩转 AI，笔记本电脑安装属于自己的 Llama 3 8B 大模型和对话客户端\nChatTTS 开源文本转语音模型本地部署、API 使用和搭建 WebUI 界面\nOllama 完整教程：本地 LLM 管理、WebUI 对话、Python/Java 客户端 API 应用\n","date":"2024-08-11T00:00:00Z","image":"https://ntopic.cn/p/2024081101/vllm_hub20bbea5694ca3a59339a693ae8ca887_49290_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/2024081101/","title":"vLLM CPU和GPU模式署和推理 Qwen2 等大语言模型详细教程"},{"content":"在前面的文章中，老牛同学介绍了不少大语言模型的部署、推理和微调，也通过大模型演示了我们的日常的工作需求场景。我们通过大语言模型，实实在在的感受到了它强大的功能，同时也从中受益颇多。\n今天，老牛同学想和大家一起来训练一个自定义的、但是功能齐全的简单的神经网络模型。这个模型虽然在参数规模、训练数据集、应用场景等方面均无法与大语言模型相媲美，但是我们旨在通过这个模型的训练过程，一窥神经网络模型的训练全貌。正所谓“麻雀虽小，五脏俱全”，同时老牛同学也希望能通过本文，与大家一起学习加深对训练神经网络的理解，逐步做到“肚里有货，从容不迫”！\n由于模型训练过程的代码可能会反复调试和修改，老牛同学强烈建议大家使用Jupyter Lab来编写和调试代码。如果还没有配置好Jupyter Lab环境，请先移步老牛同学之前的文章，首先完成大模型研发的基础环境配置：大模型应用研发基础环境配置（Miniconda、Python、Jupyter Lab、Ollama 等）\n定义神经网络模型 本文重在演示训练过程，因此为了方便我们训练，我们模型定义如下：\n它是一个简单的线性计算模型 它只有3 个权重参数 它输出一个数值结果 根据以上定义，我们的模型的线性运算公式定义为：y = W1*x1 + W2*x2 + W3*x3 + b\ny 为模型输出，在训练时，则代表模型的目标训练数据集 x 为模型输入，在训练时，则代表模型的输入训练数据集 W 为模型权重，是模型训练的最终结果 b 为调整线性运算结果的偏置向量 我们将根据 y 和 x 训练数据集，逐步训练得出模型权重 W 和 b 值。\n本文的源码地址，老牛同学放到评论区。如果大家不想一步一步地跟着老牛同学进行模型训练，也可以直接看源代码，一步到位看完整代码（源代码中还有 1 个权重的样例）。\n准备训练数据集 首先，打开 Jupyter Lab 编辑器：\n1 2 conda activate PY3.12 jupyter-lab . 为了后面创建数据集、创建模型、模型训练等操作，我们直接引入所有的依赖包：\n1 2 3 4 5 import torch import torch.nn as nn import random import torch.optim as optim import numpy as np 为了方便构建数据集，我们先假设W的内容（偏置向量b值初始化为0）：\n1 2 3 # 随便写几个数字 weights = [1.3, 2.9, 3.7] w_count = len(weights) 大家可能会有疑问，既然我们都已经知道模型权重了，那我们还训练个啥呢？\n别着急，老牛同学提前定义它，有 2 个目的，后面在实际训练时不会使用它：\n方便构造我们的训练数据集：因为我们已经确定了模型的线性运算公式，那么我们只需要随机一些x，就可以容易得到训练数据集y了 模型训练结束之后，方便后面做个比对，看下我们训练结果我们预期值是否符合我们预期 我们先构建我们输入数据集，即线性运算公式x的内容，我们通过随机函数构建了 100 个随机数字，并转换模型训练时 PyTorch 张量类型：\n1 2 3 4 5 x_list = [] for _ in range(100): x_list.append([random.randint(1, 50) for _ in range(w_count)]) inputs = torch.tensor(x_list, dtype=torch.float32, requires_grad=True) 然后，我们根据的输入数据集 x，构建目标训练数据集，同样转换模型训练时 PyTorch 张量类型：\n1 2 3 4 5 y_list = [] for x in x_list: y_list.append(np.dot(x, weights)) targets = torch.tensor(y_list, dtype=torch.float32) 结果 y 就是输入 x 和 模型权重的点积运算：\n至此，我们的训练数据集已经构建完成（我们可以忘掉权重了）。接下来，我们来构建神经网络模型。\n构建神经网络模型 我们把即将构建的神经网络模型定义为LNTXModel（即：老牛同学线性模型）：\n1 2 3 4 5 6 7 8 9 10 11 12 # 线性模型 class LNTXModel(nn.Module): def __init__(self): super(LNTXModel, self).__init__() self.linear = nn.Linear(in_features=w_count, out_features=w_count) def forward(self, x): return self.linear(x) # 实例化模型 model = LNTXModel() model 在本模型定义中，我们直接使用了nn.Linear线性层，它有 2 个参数：\nin_features: 输入特征的数量（即输入向量的维度） out_features: 输出特征的数量（即输出向量的维度） bias：偏置向量参数默认为True 然后我们初始化了模型（大语言模型一般为加载模型）。接下来，我们就可以开始使用训练数据集来训练这个模型了。\n训练神经网络模型 神经网络模型的训练过程，通常包括以下几步：\n首先，进行前向传播以预测结果（即forward函数） 然后，将预测结果与目标结果进行比较，即计算损失值 接着，利用反向传播算法计算损失值的梯度 最后，根据梯度更新模型的参数 首先，我们定义损失函数和优化器：损失函数用于根据模型的预测结果和目标结果计算损失值，而优化器则用于根据计算出的梯度更新模型的权重，以最小化损失。\n1 2 3 4 5 # 定义损失函数 loss_fn = nn.MSELoss() # 定义优化器 optimizer = optim.SGD(model.parameters(), lr=0.0005) nn.MSELoss() 损失函数常用于回归任务中，用来衡量模型预测值与实际目标值之间的差距。函数返回一个标量张量，代表了所有输入的均方误差。\ntorch.optim.SGD 是随机梯度下降优化算法，一般用于最小化损失函数。与标准的梯度下降算法相比，随机梯度下降算法每一步更新只基于一个或一小批样本的梯度估计。这种方法能够更快地收敛，并且有助于跳出局部极小点。\n其中，lr 学习率（Learning Rate）参数是优化算法中的一个重要超参数，它决定了模型参数在每次更新时的变化幅度。较高的学习率可以加快收敛的速度，但可能会导致优化过程震荡或者无法稳定在最小值附近；而较低的学习率有助于更精确地找到最小值，但可能会陷入局部最小值或者导致训练过程非常缓慢。确定最佳学习率通常需要基于模型、数据集和问题的特性进行反复试验。一般情况下，我们可以从一个较小的学习率开始（比如老牛同学本次设置为0.0005），然后根据模型的收敛情况逐渐增加或减少学习率。\n现在，所有准备工作都已经完成，我们可以开始训练我们的模型了。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 # 训练循环，迭代1000次 num_epochs = 1000 for epoch in range(num_epochs): for i, x in enumerate(inputs): # 前向传播 predictions = model(x) # 计算损失 loss = loss_fn(predictions, targets[i]) # 清空梯度 optimizer.zero_grad() # 反向传播 loss.backward() # 更新参数 optimizer.step() if (epoch+1) % 100 == 0: print(\u0026#39;Epoch [{}/{}], Loss: {:.4f}\u0026#39;.format(epoch+1, num_epochs, loss.item())) # 训练完成 print(\u0026#39;Train done.\u0026#39;) 可以看到，经过900 轮的训练，预测损失接近为0：\n我们可以打印出模型的训练结果，与我们预期结果进行比较：\n1 2 print(f\u0026#39;Final weights:{model.linear.weight.data}\u0026#39;) print(f\u0026#39;Final bias:{model.linear.bias.data}\u0026#39;) 可以看出，模型权重与我们预期结果基本吻合，模型巡检结果基本符合预期！\n使用神经网络模型 模型训练完成，我们就可以使用我们的模型了：model(x)\n1 model(torch.tensor([float(1), float(1), float(1)])) 总结：扩展模型大小 至此，我们整个训练过程已经完成了。在上面演示案例中，我们只是用了 3 个权重参数的简单模型，我们可以根据需求，进一步扩大模型参数。但是不论模型权重参数扩大到多少，他们的训练流程基本是一样的：\n初始化模型（大语言模型成为加载模型） 根据x输入预测输出y值 通过损失函数计算损失梯度值 最后根据梯度更新模型参数值 直到训练结束，模型权重符合预期 最后的最后，8 月开始了，大家S1 绩效基本都沟通确定了吧？公众号回复都是匿名的，最终绩效结果如何，大家若感觉兴趣，欢迎在评论区留言分享~\n基于 Qwen2 大模型微调技术详细教程（LoRA 参数高效微调和 SwanLab 可视化监控）\nLivePortrait 数字人：开源的图生视频模型，本地部署和专业视频制作详细教程\n基于 Qwen2/Lllama3 等大模型，部署团队私有化 RAG 知识库系统的详细教程（Docker+AnythingLLM）\n使用 Llama3/Qwen2 等开源大模型，部署团队私有化 Code Copilot 和使用教程\n本地部署 GLM-4-9B 清华智谱开源大模型方法和对话效果体验\n玩转 AI，笔记本电脑安装属于自己的 Llama 3 8B 大模型和对话客户端\nChatTTS 开源文本转语音模型本地部署、API 使用和搭建 WebUI 界面\nOllama 完整教程：本地 LLM 管理、WebUI 对话、Python/Java 客户端 API 应用\n","date":"2024-08-02T00:00:00Z","image":"https://ntopic.cn/p/2024080201/01_hu7ba85a2c2f05ca1e327d4e897c945d88_53556_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/2024080201/","title":"PyTorch 训练自定义功能齐全的神经网络模型的详细教程"},{"content":"早上收到朋友转发的阿里云公众号推文，阿里云为用户免费提供 1 个月的训练推理等算力额度（上阿里云，免费玩转 Llama 405B 「超大杯」！）。想想上周老牛同学为了制作微调技术教程，演示训练Qwen2-0.5B小尺寸大模型就跑了一个晚上（基于 Qwen2 大模型微调技术详细教程（LoRA 参数高效微调和 SwanLab 可视化监控）），如今阿里云竟然免费提供 1 个月训练推理算力，而且还支持Llama3.1-405B超大尺寸模型，标题和内容确实把老牛同学给够吸住了。\n虽然老牛同学非常相信阿里云在中国市场的地位，但还是有那么一点点担心是标题党，因此老牛同学决定验证一下，走一遍完整开通和使用流程，最后给出自己的感受给大家做个参考。\n开通阿里云百炼服务 推文写得很清楚，免费额度是为阿里云百炼平台用户提供的，因此先开通注册：https://www.aliyun.com/product/bailian\n说实话，取名真的很重要，不仅要好听好记还要切实际功能。比如千问：作为你的人工智能助理，随你问，我都能答；百炼：千锤百炼，大模型训练微调，就是不断打磨和调整优化过程。\n阿里云百炼：是基于通义大模型、行业大模型以及三方大模型的一站式大模型开发平台。面向企业客户和个人开发者，提供完整的模型服务工具和全链路应用开发套件，预置丰富的能力插件，提供 API 及 SDK 等便捷的集成方式，高效完成大模型应用构建。\n产品定价：阿里云百炼大模型服务平台在调用 API 后将产生计量和计费。各个领域的模型采用不同的计量单元，不同模型单独制定各自的计费单价和免费额度等规则。（特别注意：不同模型是单独计价和单价免费额度规则，算力额度有可能针对不同模型不一样！）\n点击“立即开通”橙色大按钮，按照提示，一步一步完成注册即可：\n我们可以直接通过支付宝、淘宝等方式登录（老牛同学使用支付宝扫描登录）。登录成功，需要同意“阿里云百炼服务协议”，我们主要是用于学习和研究，肯定不会干那些超纲的事情，无脑点击“同意”按钮就行了。\n然后，我们就进入了百炼平台控制台首页：\n看到“免费额度：100/100 次”小字样，老牛同学之前的担心加重了一点，回头想想还是继续往下看看，毕竟是阿里云，应该是不能随便开玩笑的。\n点击“去开通 →”按钮，又来个弹框确认开通。不过支持的模型倒是挺多的，支持 139 个推理（包括 Llama3.1-405B 大模型）、17 个部署和 14 个训练，应该来说我们对学习和研究类需求足够了：\n勾选“我已阅读并同意《模型管理服务协议》”，然后点击“确认开通”按钮，整个开通流程就结束了！\n使用 API 调用推理服务 首先体验一下大模型的推理服务，选择“模型广场”，选择任意一个模型（如：通义千问-Max），点击“API 调用示例”：\n可以看到，API 调用示例提供了Python、Java和Curl共 3 种方式，对于我们来说也应该足够了（老牛同学就不演示了）。\n最关键的是，点击“模型详情”，可以看到计费方式和免费额度信息：\n可以看到，免费额度信息如下：\n免费的 Token 数量有限制（如：通义千问-Max 是 100 万个） 之前推文中提到的，免费 1 个月，是指这些免费额度仅 1 个月有效期，不是老牛同学想象的免费使用 1 个月！ 到这里，老牛同学感觉有一点点失望了，但是老牛同学对推理要求不多，在看看模型训练如何免费的。\n使用百炼平台进行模型训练 选择“模型工具”-“模型调优”，然后点击“训练新模型”按钮，然后在弹框选择“高效训练”（就是老牛同学上周文章中的微调）\n点击“下一步”，选择“预置模型”，在下拉框中选择“通义千问-开源版-7B”模型（模型没有强制要求）\n点击“下一步”，需要选择训练数据集，因之前没有训练过，还没有数据集，因此我们点击“管理训练集”增加数据集\n数据集是jsonl文件，单个 JSON 的格式需要符合 ChatML 格式，样例如下：\n1 {\u0026#34;messages\u0026#34;: [{\u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;You are a helpful assistant\u0026#34;}, {\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;谁在文艺复兴时期绘制人体?\u0026#34;}, {\u0026#34;role\u0026#34;: \u0026#34;assistant\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;文艺复兴时期是一个关于艺术、文化和学术的复兴运动，在这个时期，许多艺术家都绘制了人体。\u0026#34;}]} 因此，老牛同学上周微调用的文本分类的原始数据集需要格式化处理一下（评论区有最终文件链接），Python 代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 import os import json # 训练数据集文件 BAS_DATA_DIR = \u0026#39;D:\\ModelSpace\\Qwen2\u0026#39; RAW_TRAIN_FILE_PATH = os.path.join(BAS_DATA_DIR, os.path.join(\u0026#39;zh_cls_fudan-news\u0026#39;, \u0026#39;train.jsonl\u0026#39;)) NEW_TRAIN_FILE_PATH = os.path.join(BAS_DATA_DIR, \u0026#39;train-ChatML.jsonl\u0026#39;) # 将原始数据集转换为ChatML格式的新数据集 message_list = [] # 读取原JSONL文件 with open(RAW_TRAIN_FILE_PATH, \u0026#34;r\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as file: for line in file: # 解析每一行原始数据（每一行均是一个JSON格式） data = json.loads(line) text = data[\u0026#34;text\u0026#34;] catagory = data[\u0026#34;category\u0026#34;] output = data[\u0026#34;output\u0026#34;] message_part_1 = {\u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;You are a helpful assistant\u0026#34;} message_part_2 = {\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: f\u0026#34;你是一个文本分类领域的专家，你会接收到一段文本和几个潜在的分类选项列表，请输出文本内容的正确分类。\\n{text}\\n分类选项列表:{catagory}\u0026#34;} message_part_3 = {\u0026#34;role\u0026#34;: \u0026#34;assistant\u0026#34;, \u0026#34;content\u0026#34;: output} message = { \u0026#34;messages\u0026#34;: [message_part_1, message_part_2, message_part_3] } message_list.append(message) # 保存处理后的JSONL文件，每行也是一个JSON格式 with open(NEW_TRAIN_FILE_PATH, \u0026#34;w\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as file: for message in message_list: file.write(json.dumps(message, ensure_ascii=False) + \u0026#34;\\n\u0026#34;) 如果大家不想对原始的文件进行处理了，可以到评论区直接下载老牛同学已经处理好的数据集文件。\n在新打开页面，点击“新增数据集”，勾选“训练集”（文件最大支持 20MB），然后处理好的文件上次即可（裁剪一下，留下 1000 行）。\n导入成功之后，可以查看数据样例，还可以单行进行处理：\n确认无误之后，点击“发布”数据集，后面就可以使用了。\n然后我们继续回到刚才模型训练页面，选择我们创建的数据集，操作菜单：模型工具-模型调优，选择刚才发布的训练数据集：\n点击“下一步”，验证数据，选择“自动切分”，我们就不单独创建数据集了。\n在“混合训练”选择中，针对中文-对话和中文-通用均选择0.5倍（因为我们训练集都是中文，就简单点点设置了）\n最后，可以看到本次模型训练的详情和预估费用：\n点击“开始训练”就是开始训练了，预估费用不足50 元还算不错，比老牛同学自己电脑训练所用的电费感觉贵不了多少！\n最后：想谈点大模型产品的感想 最近和朋友聊天，聊到最近很火爆的大模型，目前市面上有那么多感觉非常惊艳的开源大模型，利用这些开源大模型来创业岂不美哉？！\n朋友对自己的想法自信满满，滔滔不绝介绍自己的创意，其中之一就是：基于大语言模型研发一个老年人伴侣 APP，专门解决老年人孤单无人陪伴的问题，背景是经调研机构研究和评测，中国老龄化逐渐严重，老年人服务市场巨大，等等……\n老牛同学听了一会儿实在没有忍住，强制打断给出判断：行不通，极有可能被大模型厂商碾压，没有生存空间！\n下面是老牛同学的一些不成熟的想法判断示意图：凡是大模型厂商正在做、未来可能会做的事情，小团体、创业团队等如果不想被他们碾压的话，趁早不要做！！！\n想想看，如果存在一个巨大的大模型市场，那些大模型厂商会不涉猎吗？\n阿里云为了拉新用户使用百炼平台，推广百炼平台，提供免费的算力额度，这些额度难道不是钱吗？大模型厂商之间都已经卷成这样了，小团体还敢挡路？ 大模型厂商他们基本是要人有人、要技术有技术、要资源有资源、要卡有卡，而小团体呢，大多情况只是有一个创意想法而已！ 那么，创业小团体的出路在哪儿呢：细分领域，那些大模型厂商看不上的领域、那些和大模型厂商前进方向不冲突的领域。\n比如准爸准妈领域：有没有可能，根据准妈的产检信息，利用大模型预判孩子出生日期、时间、体重、性别等，以便准爸准妈提前做好准备？\n比如护肤品领域：有没有可能，根据女性皮肤照片，利用大模型推荐最合适的护肤品？\n等等，以上只是老牛同学的一些胡思乱想，若有不适宜之处，请帮忙指正，老牛同学第一时间修正或者删除本文，谢谢大家！\n最后的最后，回应本文开头，对阿里云百炼推文做个总结：虽然有那么一点点标题党嫌疑，但总体还是诚意满满（单模型，100 万 Token，有效期 30 天），试问国内还有别家这么干吗？如果我们只是作为一个学习者、体验者或者临时使用者，Token 数量和有效期我想也完全足够。\n因此，如果你只是想体验一下大模型、或者产品上线之前的联调、或者临时小需求需要使用大模型服务，这个免费的羊毛就是为你准备的，抓紧大胆的去薅吧！\n基于 Qwen2 大模型微调技术详细教程（LoRA 参数高效微调和 SwanLab 可视化监控）\nLivePortrait 数字人：开源的图生视频模型，本地部署和专业视频制作详细教程\n基于 Qwen2/Lllama3 等大模型，部署团队私有化 RAG 知识库系统的详细教程（Docker+AnythingLLM）\n使用 Llama3/Qwen2 等开源大模型，部署团队私有化 Code Copilot 和使用教程\n本地部署 GLM-4-9B 清华智谱开源大模型方法和对话效果体验\n玩转 AI，笔记本电脑安装属于自己的 Llama 3 8B 大模型和对话客户端\nChatTTS 开源文本转语音模型本地部署、API 使用和搭建 WebUI 界面\nOllama 完整教程：本地 LLM 管理、WebUI 对话、Python/Java 客户端 API 应用\n","date":"2024-07-26T00:00:00Z","image":"https://ntopic.cn/p/2024072601/00_hu495ae1cd16dc9d03dd5f36c665248bf2_45404_120x120_fill_box_smart1_3.png","permalink":"https://ntopic.cn/p/2024072601/","title":"太卷了，阿里云免费1个月大模型算力额度，玩转Llama3.1/Qwen2等训练推理"},{"content":"老牛同学在之前的介绍大模型 Prompt 提示词的文章中（高效编写大模型 Prompt 提示词，解锁 AI 无限创意潜能），曾把大模型比作成一位无所不能无所不知且不知疲惫的“大师”。我们在日常工作、学习中等一些通用知识方面的问题，通常情况下，我们均可通过 Prompt 提示词就能从“大师”那里得到期望的结果。\n但是，在某些垂直场景的特定任务（包括：个性化服务、内部私有数据等）中，这位“大师”可能就不一定能胜任了：\n数据隐私安全： 保密项目、创业团体和企业内部数据是需要保证绝对安全的，“大师”的知识来自预训练的公开数据，在推理时就缺乏这方面知识。 Prompt 长度和截取： 使用清晰详细的 Prompt 提示词，确实能帮助“大师”理解我们需求，从而更好的输出结果。但是大模型对输入序列的长度有限制，超长会被截断，同时超长的 Prompt 提示意味着推理成本更高、推理效率更低，可能达不到预期的效果。 个性化需求： 预训练的大模型，其对问题的理解和输出方式基本固定，无法满足个性化的需求。 这个时候，我们可以通过标记好的结构化数据，让“大师”进一步学习（即：微调），通过调整“大师”的知识（即：调整大模型参数），达到处理特定任务的能力。\n根据我们需要调整的大模型的参数量，微调技术大致可以分为 2 种：\n全量微调 即FFT(Full Fine-Tuning)，它使用特定领域的数据集对模型的所有参数进行调整，微调的参数量跟预训练时一样多，训练成本和资源会很高，同时可能因数据集等原因出现过拟合问题，导致发生灾难性遗忘(Catastrophic Forgetting)，即我们可能会让大模型在某个领域的能力变的更好，但也可能会让原来其它表现好领域的能力变差。 参数高效微调 即PEFT(Parameter-Efficient Fine-Tuning)，它仅更新模型中的小部分参数，保持大部分预训练权重不变，在保持模型性能的同时减少所需的计算资源和存储空间，可在有效避免过拟合问题的同时，还有助于保留模型在广泛任务上的通用知识（即：泛化能力）。 LoRA(Low-Rank Adaptation)是一种高效的大模型PEFT微调技术，它是通过在预训练模型的关键层（如全连接层和自注意力层）之间添加低秩矩阵来完成微调。这些低秩矩阵的引入使得模型能够适应新的任务，而无需改变原有的大量参数。由于低秩矩阵的参数数量远小于原有层的参数数量，这就大大减少了需要训练的参数总数。\nLoRA的优势在于，即使在资源有限的情况下，也可以有效地对大型预训练模型进行微调，使其适应各种下游任务，如文本分类、命名实体识别等。此外，由于 LoRA 的微调通常只需要较少的数据，这也使得它成为小数据集场景下的一个有力工具。\n老牛同学将通过本教程，基于Qwen2-0.5B开源的预训练大模型，和大家一起进行一次大模型文本分类能力的微调。在 AI 蓬勃发展的今天，老牛同学期望能通过本教程，与大家一起在我们的 AI 知识库里新增储备微调知识，逐步做到肚里有货，从容不迫。\n完成一次完整的大模型微调，大致需要以下几个步骤：\n环境准备： 主要是 Python 依赖库安装 数据集准备： 针对特定任务，准备相关的数据，数据内容包含Prompt 提示词和输出即可 准备大模型： 我们可以通过 HF、ModelScope 等下载预训练大模型权重 大模型微调： 包括加载大模型、数据集格式化处理、LoRA 参数准备等。最后，微调过程我们通过swanlab可视化界面查看 环境准备和安装依赖包 首先，我们需要通过Miniconda安装 Python 依赖库：\n1 2 3 4 5 # 切换环境 conda activate PY3.12.2 # 安装依赖库 pip install transformers datasets peft accelerate modelscope swanlab 如果我们还没有安装好Miniconda包管理工具，请先移步此文完成大模型基础环境配置：大模型应用研发基础环境配置（Miniconda、Python、Jupyter Lab、Ollama 等）\n以上 6 个库的主要用途简单介绍：\ntransformers HuggingFace 出品的深度学习框架，是 NLP（自然语言处理）领域最流行的训练与推理框架。在本教程中主要用于加载模型、训练以及推理。 datasets HuggingFace 出品的数据集工具，在本教程中主要用于加载数据集。 peft HuggingFace 出品的微调工具，是一个流行的实现 LoRA 和其他微调技术的库。本教程中主要用于微调训练，与微调后模型推理。 accelerate HuggingFace 出品的帮助简化分布式训练和混合精度训练的库。本教程中主要用于支持混合精度训练。 modelscope ModelScope 库使开发人员能够通过丰富的 API 设计执行推理、训练和评估，从而促进跨不同 AI 领域的最先进模型的统一体验。代码中将主要用于在国内环境中下载 Qwen 大模型。 swanlab 西安电子科技大学出品，深度学习实验管理与训练的可视化工具，可记录整个实验的超参数、指标、训练环境、Python 版本等，并通过可视化图表展示，帮助我们分析训练的结果。本教程中主要用于记录指标和可视化界面。 数据集准备和处理 ModelScope 上有很多公开免费的数据集供我们使用：datasets\n本教程我们使用的是一个开放性问题进行分类的数据集：zh_cls_fudan-news\n1 2 git lfs install git clone https://www.modelscope.cn/datasets/swift/zh_cls_fudan-news.git 下载完成之后，我们会看到 2 个后缀为.jsonl的文件：\n1 2 3 4 5 zh_cls_fudan-news ├── README.md ├── dataset_infos.json ├── test.jsonl └── train.jsonl .jsonl文件一般存储的是多行文本，每一行文本是一个 JSON 格式内容，即是多行 JSON 格式内容组合的文件。\ntrain.jsonl是训练的输入文件，而test.jsonl则是训练的验证文件。他们每行 JSON 格式内容都包含text、category和output共 3 个属性，分代表模型输入、可选的分类列表和最终模型输出的分类。\n我们对大模型微调的目标，就是希望微调后的大模型能够根据text和category组成的提示词，输出正确的output分类。\n预训练大模型准备 本教程中，老牛同学使用的是Qwen2-0.5B模型，我们把大模型下载到本地（目录：Qwen2-0.5B）：\n1 2 git lfs install git clone https://www.modelscope.cn/qwen/Qwen2-0.5B.git 如果 Git 克隆失败中断，可以继续克隆下载：\n1 2 cd Qwen2-0.5B git lfs pull 大模型微调 大模型微调包括：包括加载大模型、数据集格式化处理、LoRA 参数准备等。最后，微调过程我们通过SwanLab可视化界面监控整个微调过程。\n微调可视化配置 我们使用SwanLab来监控整个训练过程，并评估最终的模型效果。如果是第一次使用 SwanLab，则需要注册 SwanLab 账号：https://swanlab.cn，注册成功之后，在用户设置页面复制API Key，在训练开始时需要用到。\n为了便于我们查看我们微调的数据，我们还需要创建一个项目（项目名称：Qwen2-FineTuning）：\n大模型加载、设置和微调 由于微调涉及到好几步，老牛同学强烈建议大家使用Jupyter Lab工具进行代码调试和验证，它可以把整个代码分成多个区块，单个区块可以多次执行。若还没有配置Jupyter Lab工具，建议先移步此文完成大模型基础环境配置：大模型应用研发基础环境配置（Miniconda、Python、Jupyter Lab、Ollama 等）\n因为涉及到不同的代码片段，老牛同学直接粘贴完整代码，通过代码注释和代码后面进行说明（文件名：Qwen2-0.5B-train.py，完整的代码和数据，老牛同学在评论区提供仓库地址）：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 # Qwen2-0.5B-train.py import json import pandas as pd import torch from datasets import Dataset from modelscope import AutoTokenizer from swanlab.integration.huggingface import SwanLabCallback from peft import LoraConfig, TaskType, get_peft_model from transformers import AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForSeq2Seq import os import swanlab # 权重根目录 BASE_DIR = \u0026#39;D:\\ModelSpace\\Qwen2\u0026#39; # 设备名称 device = \u0026#39;cuda\u0026#39; if torch.cuda.is_available() else \u0026#39;cpu\u0026#39; # device = torch.device(\u0026#39;cuda\u0026#39; if torch.cuda.is_available() else \u0026#39;cpu\u0026#39;) # 数据集处理函数，包括：训练数据集和测试数据集 def dataset_jsonl_transfer(origin_path, new_path): \u0026#34;\u0026#34;\u0026#34; 将原始数据集转换为大模型微调所需数据格式的新数据集 \u0026#34;\u0026#34;\u0026#34; messages = [] # 读取原JSONL文件 with open(origin_path, \u0026#34;r\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as file: for line in file: # 解析每一行原始数据（每一行均是一个JSON格式） data = json.loads(line) text = data[\u0026#34;text\u0026#34;] catagory = data[\u0026#34;category\u0026#34;] output = data[\u0026#34;output\u0026#34;] message = { \u0026#34;input\u0026#34;: f\u0026#34;文本:{text},分类选项列表:{catagory}\u0026#34;, \u0026#34;output\u0026#34;: output, } messages.append(message) # 保存处理后的JSONL文件，每行也是一个JSON格式 with open(new_path, \u0026#34;w\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as file: for message in messages: file.write(json.dumps(message, ensure_ascii=False) + \u0026#34;\\n\u0026#34;) # 在使用数据集训练大模型之前，对每行数据进行预处理 def process_func(example): \u0026#34;\u0026#34;\u0026#34; 将数据集进行预处理 \u0026#34;\u0026#34;\u0026#34; MAX_LENGTH = 384 input_ids, attention_mask, labels = [], [], [] instruction = tokenizer(f\u0026#34;\u0026lt;|im_start|\u0026gt;system\\n你是一个文本分类领域的专家，你会接收到一段文本和几个潜在的分类选项列表，请输出文本内容的正确分类\u0026lt;|im_end|\u0026gt;\\n\u0026lt;|im_start|\u0026gt;user\\n{example[\u0026#39;input\u0026#39;]}\u0026lt;|im_end|\u0026gt;\\n\u0026lt;|im_start|\u0026gt;assistant\\n\u0026#34;, add_special_tokens=False) # add_special_tokens 不在开头加 special_tokens response = tokenizer(f\u0026#34;{example[\u0026#39;output\u0026#39;]}\u0026#34;, add_special_tokens=False) input_ids = instruction[\u0026#34;input_ids\u0026#34;] + response[\u0026#34;input_ids\u0026#34;] + [tokenizer.pad_token_id] attention_mask = instruction[\u0026#34;attention_mask\u0026#34;] + response[\u0026#34;attention_mask\u0026#34;] + [1] # 因为eos token咱们也是要关注的所以 补充为1 labels = [-100] * len(instruction[\u0026#34;input_ids\u0026#34;]) + response[\u0026#34;input_ids\u0026#34;] + [tokenizer.pad_token_id] if len(input_ids) \u0026gt; MAX_LENGTH: # 做一个截断 input_ids = input_ids[:MAX_LENGTH] attention_mask = attention_mask[:MAX_LENGTH] labels = labels[:MAX_LENGTH] return { \u0026#34;input_ids\u0026#34;: input_ids, \u0026#34;attention_mask\u0026#34;: attention_mask, \u0026#34;labels\u0026#34;: labels } # 加载预训练模型和分词器 model_dir = os.path.join(BASE_DIR, \u0026#39;Qwen2-0.5B\u0026#39;) tokenizer = AutoTokenizer.from_pretrained(model_dir, use_fast=False, trust_remote_code=True) model = AutoModelForCausalLM.from_pretrained(model_dir, device_map=device, torch_dtype=torch.bfloat16) model.enable_input_require_grads() # 开启梯度检查点时，要执行该方法 # 加载、处理数据集和测试集 train_dataset_path = os.path.join(BASE_DIR, \u0026#39;zh_cls_fudan-news\u0026#39;, \u0026#39;train.jsonl\u0026#39;) test_dataset_path = os.path.join(BASE_DIR, \u0026#39;zh_cls_fudan-news\u0026#39;, \u0026#39;test.jsonl\u0026#39;) train_jsonl_new_path = os.path.join(BASE_DIR, \u0026#39;train.jsonl\u0026#39;) test_jsonl_new_path = os.path.join(BASE_DIR, \u0026#39;test.jsonl\u0026#39;) if not os.path.exists(train_jsonl_new_path): dataset_jsonl_transfer(train_dataset_path, train_jsonl_new_path) if not os.path.exists(test_jsonl_new_path): dataset_jsonl_transfer(test_dataset_path, test_jsonl_new_path) # 得到微调数据集 train_df = pd.read_json(train_jsonl_new_path, lines=True) train_ds = Dataset.from_pandas(train_df) train_dataset = train_ds.map(process_func, remove_columns=train_ds.column_names) # 创建LoRA配置 config = LoraConfig( task_type=TaskType.CAUSAL_LM, target_modules=[\u0026#34;q_proj\u0026#34;, \u0026#34;k_proj\u0026#34;, \u0026#34;v_proj\u0026#34;, \u0026#34;o_proj\u0026#34;, \u0026#34;gate_proj\u0026#34;, \u0026#34;up_proj\u0026#34;, \u0026#34;down_proj\u0026#34;], inference_mode=False, # 训练模式 r=8, # Lora 秩 lora_alpha=32, # Lora alaph，具体作用参见 Lora 原理 lora_dropout=0.1, # Dropout 比例 ) # 将LoRA应用于模型 model = get_peft_model(model, config) # 创建微调参数 args = TrainingArguments( output_dir=os.path.join(BASE_DIR, \u0026#39;output\u0026#39;, \u0026#39;Qwen2-0.5B\u0026#39;), per_device_train_batch_size=4, gradient_accumulation_steps=4, logging_steps=10, num_train_epochs=2, save_steps=100, learning_rate=1e-4, save_on_each_node=True, gradient_checkpointing=True, report_to=\u0026#34;none\u0026#34;, ) # SwanLab微调过程回调数据 swanlab_callback = SwanLabCallback(project=\u0026#34;Qwen2-FineTuning\u0026#34;, experiment_name=\u0026#34;Qwen2-0.5B\u0026#34;) trainer = Trainer( model=model, args=args, train_dataset=train_dataset, data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True), callbacks=[swanlab_callback], ) # 开始微调 trainer.train() # 模型结果结果评估 def predict(messages, model, tokenizer): text = tokenizer.apply_chat_template( messages, tokenize=False, add_generation_prompt=True ) model_inputs = tokenizer([text], return_tensors=\u0026#34;pt\u0026#34;).to(device) generated_ids = model.generate( model_inputs.input_ids, max_new_tokens=512 ) generated_ids = [ output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids) ] return tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0] # 模型评估：获取测试集的前10条测试数据 test_df = pd.read_json(test_jsonl_new_path, lines=True)[:10] test_text_list = [] for index, row in test_df.iterrows(): instruction = row[\u0026#39;你是一个文本分类领域的专家，你会接收到一段文本和几个潜在的分类选项列表，请输出文本内容的正确分类\u0026#39;] input_value = row[\u0026#39;input\u0026#39;] messages = [ {\u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: f\u0026#34;{instruction}\u0026#34;}, {\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: f\u0026#34;{input_value}\u0026#34;} ] response = predict(messages, model, tokenizer) messages.append({\u0026#34;role\u0026#34;: \u0026#34;assistant\u0026#34;, \u0026#34;content\u0026#34;: f\u0026#34;{response}\u0026#34;}) result_text = f\u0026#34;{messages[0]}\\n\\n{messages[1]}\\n\\n{messages[2]}\u0026#34; test_text_list.append(swanlab.Text(result_text, caption=response)) swanlab.log({\u0026#34;Prediction\u0026#34;: test_text_list}) swanlab.finish() 以上就是大模型微调的全部代码，微调的总体流程如下：\n首先，我们通过PyTorch库检查 CUDA 是否可用，优先使用 CUDA 设备，否则退回 CPU 设备 然后，定义了 2 个函数：分别是数据集预处理函数、单行数据预处理函数（主要用于把原始数据集映射成大模型微调的数据内容） 接着准备开始微调了，首先加载预训练模型和分词器，trust_remote_code=True代表从本地磁盘加载模型权重 然后使用定义好的函数，处理原始数据集，并处理为微调数据集 接着创建LoRA配置，并把LoRA配置应用于预训练模型 接下来创建微调参数（output_dir=\u0026quot;./output/Qwen2-0.5B\u0026quot;代表微调之后的权重文件目录），并设置SwanLab回调函数 最后，启动微调：trainer.train() 微调完成之后，我们通过 10 条测试数据，对模型进行了评估验证 从原始数据集映射成大模型数据集进度、速度和耗时（共 4000 条数据）：\n我们启动模型微调后，SwanLab 需要我们输入API Key，输入即可。\n查看微调进展 我们可以在Jupyter Lab中直接开启看板，非常方便的查看微调情况。\n微调完成，可以看到在测试样例评估上，微调后Qwen2大模型能够给出准确的文本分类：\n至此，我们已经完成了Qwen2-0.5B大模型的微调工作，接下来就可以使用微调后模型完成特定任务了（文本分类）！\n本教程所有的源代码，老牛同学展示在评论区，大家可以获取源文件进行模型微调！\nLivePortrait 数字人：开源的图生视频模型，本地部署和专业视频制作详细教程\n基于 Qwen2/Lllama3 等大模型，部署团队私有化 RAG 知识库系统的详细教程（Docker+AnythingLLM）\n使用 Llama3/Qwen2 等开源大模型，部署团队私有化 Code Copilot 和使用教程\n本地部署 GLM-4-9B 清华智谱开源大模型方法和对话效果体验\n玩转 AI，笔记本电脑安装属于自己的 Llama 3 8B 大模型和对话客户端\nChatTTS 开源文本转语音模型本地部署、API 使用和搭建 WebUI 界面\nOllama 完整教程：本地 LLM 管理、WebUI 对话、Python/Java 客户端 API 应用\n","date":"2024-07-18T00:00:00Z","image":"https://ntopic.cn/p/2024071801/00_hu63d9dd5bc1ea15226ab3c891b9df5038_271410_120x120_fill_box_smart1_3.png","permalink":"https://ntopic.cn/p/2024071801/","title":"基于 Qwen2 大模型微调技术详细教程（LoRA 参数高效微调和 SwanLab 可视化监控）"},{"content":"\n看到上面面部表情动态图片，是不是感觉挺有有意思？它就是通过快手、中科大和复旦大学联合研发的图生视频开源大模型LivePortrait（灵动人像）生成的视频。通过LivePortrait大模型，我们只需要一张人脸正面图片和一段文字或音频，即可制作专业的视频内容，例如产品介绍、教学课程、趣味视频等。\n有关LivePortrait更多的展示样例参见：https://liveportrait.github.io/\n老牛同学将和大家一起，在本地部署LivePortrait图生视频大模型，并且生成我们自己的视频。本文将包括以下几部分：\n基础环境准备：与我们之前部署 LLM 大模型不同，LivePortrait涉及到音频和视频等多媒体数据的处理，因此环境要稍微复杂一点 LivePortrait 配置：包括大模型权重文件下载、配置等 LivePortrait 使用：包括通过图片生成视频、Web 界面可视化生成视频等（建议配合 GPU 进行使用，老牛同学纯 CPU推理速度较慢） LivePortrait 理论研究，可以参见论文：https://arxiv.org/pdf/2407.03168\nLivePortrait 基础环境准备 基础环境准备分为以下 3 步：克隆 GitHub 示例源代码、安装 Python 依赖包和下载配置FFmpeg音视频工具库\n【第一步：下载 GitHub 示例源码】\nGitHub 示例源码下载目录：LivePortrait\n1 git clone https://github.com/KwaiVGI/LivePortrait 特别注意： 示例代码克隆成功之后，我们可以看到示例源码目录LivePortrait下，有个pretrained_weights空目录，它就是用来存放预训练权重文件的目录，接下来的我们会下载权重文件！\n【第二步：安装 Python 依赖包】\n切换到 GitHub 示例源码目录：cd LivePortrait\n1 2 3 4 5 6 # 激活环境：特别注意Python版本为3.9.18，其他版本可能不支持（老牛同学3.12就不支持） conda create -n LivePortrait python==3.9.18 conda activate LivePortrait # 安装依赖包 pip install -r requirements.txt 如果Miniconda还未完成安装，建议先提前安装好：大模型应用研发基础环境配置（Miniconda、Python、Jupyter Lab、Ollama 等）\n【第三步：下载和配置 FFmpeg 音视频工具库】\nFFmpeg 是一个非常强大的开源软件工具库，主要用于处理多媒体数据，包括音频和视频的编码、解码、转码、复用、解复用、流媒体传输以及播放等。\n我们可以通过 FFmpeg 官网下载：https://ffmpeg.org/download.html\n或者，老牛同学已经下载好了，放到了百度网盘（评论区也有地址）：https://pan.baidu.com/s/1IYutMbJGJSxLVY56-h4IPg?pwd=LNTX\n下载安装好之后，把 FFmpeg 目录设置在PATH环境变量中，同时执行命令进行检测：ffmpeg -version\n1 2 3 4 5 6 7 8 9 10 11 12 \u0026gt;ffmpeg -version ffmpeg version 7.0.1-essentials_build-www.gyan.dev Copyright (c) 2000-2024 the FFmpeg developers built with gcc 13.2.0 (Rev5, Built by MSYS2 project) configuration: --enable-gpl --enable-version3 --enable-static --disable-w32threads --disable-autodetect --enable-fontconfig --enable-iconv --enable-gnutls --enable-libxml2 --enable-gmp --enable-bzlib --enable-lzma --enable-zlib --enable-libsrt --enable-libssh --enable-libzmq --enable-avisynth --enable-sdl2 --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxvid --enable-libaom --enable-libopenjpeg --enable-libvpx --enable-mediafoundation --enable-libass --enable-libfreetype --enable-libfribidi --enable-libharfbuzz --enable-libvidstab --enable-libvmaf --enable-libzimg --enable-amf --enable-cuda-llvm --enable-cuvid --enable-dxva2 --enable-d3d11va --enable-d3d12va --enable-ffnvcodec --enable-libvpl --enable-nvdec --enable-nvenc --enable-vaapi --enable-libgme --enable-libopenmpt --enable-libopencore-amrwb --enable-libmp3lame --enable-libtheora --enable-libvo-amrwbenc --enable-libgsm --enable-libopencore-amrnb --enable-libopus --enable-libspeex --enable-libvorbis --enable-librubberband libavutil 59. 8.100 / 59. 8.100 libavcodec 61. 3.100 / 61. 3.100 libavformat 61. 1.100 / 61. 1.100 libavdevice 61. 1.100 / 61. 1.100 libavfilter 10. 1.100 / 10. 1.100 libswscale 8. 1.100 / 8. 1.100 libswresample 5. 1.100 / 5. 1.100 libpostproc 58. 1.100 / 58. 1.100 LivePortrait 模型权重下载和配置 我们可以通过多种方式下载预训练权重文件，包括 HF 和云盘等：\n【方式一：HF 下载权重文件】\n由于文件比较大，Git 无法直接下载，首先需要设置 Git 大文件环境：\n1 git lfs install 然后克隆权重文件，下载的目录：pretrained_weights\n1 git clone https://www.modelscope.cn/AI-ModelScope/LivePortrait.git pretrained_weights 由于文件比较大，Git 在克隆过程中可能会中断，我们可以通过 Git 命令重试：\n1 2 3 4 5 # 切换到权重文件目录 cd pretrained_weights # 继续中断下载 git lfs pull 【方式二：百度云盘或 Google Drive 下载】\n百度云盘：https://pan.baidu.com/s/1MGctWmNla_vZxDbEp2Dtzw?pwd=z5cn\nGoogle 云盘：https://drive.google.com/drive/folders/1UtKgzKjFAOmZkhNK-OYT0caJ_w2XAnib\n特别注意： 我们通过 Git 或者云盘下载到完整的权重文件之后，确认一下它的目录结构如下所示：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 pretrained_weights ├── insightface │ └── models │ └── buffalo_l │ ├── 2d106det.onnx │ └── det_10g.onnx └── liveportrait ├── base_models │ ├── appearance_feature_extractor.pth │ ├── motion_extractor.pth │ ├── spade_generator.pth │ └── warping_module.pth ├── landmark.onnx └── retargeting_models └── stitching_retargeting_module.pth 最后，把pretrained_weights目录下的insightface和liveportrait这 2 个目录和文件全部复制到 GitHub 实例源码的pretrained_weights目录下。\n小提示： 如果我们使用的是 MacOS 或者 Linux 操作系统，也可以尝试通过软链接来代替文件复制（因老牛同学是 Windows 系统，在这里无法展示，请大家尝试）！\n使用 LivePortrait 生成视频 我们可以通过终端命令行或者 Web 可视化界面 2 种方式来使用 LivePortrait 生成视频：\n【方式一： 使用终端命令行生成视频】\n打开终端，切换到 GitHub 示例源码目录：cd LivePortrait 激活 Python 环境：conda activate LivePortrait 经老牛同学测试，还需要安装额外 Python 依赖包： 1 2 3 4 5 6 # 额外依赖包 pip install tyro pip install patch_ng # 安装依赖包：如果前面已安装则可忽略（特别注意Python版本：3.9.18） pip install -r requirements.txt 执行 Python 程序：在 GitHub 示例源码中，inference.py就是我们的大模型推理函数入口 打开终端，切换到 GitHub 示例源码目录cd LivePortrait，然后推理执行：python inference.py\n可能的报错： 如果出现如下报错，请用Miniconda设置 Python 3.9.18 版本的环境：\n1 ValueError: mutable default \u0026lt;class \u0026#39;numpy.ndarray\u0026#39;\u0026gt; for field mask_crop is not allowed: use default_factory 可能的报错： 如果出现如下报错，默认需要 GPU 进行推理，如果我们有 GPU 则需要正确安装 GPU 驱动，或者我们可以强制CPU运行：\n1 RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx 【强制 CPU 运行】我们可以通过 \u0026ndash;flag-force-cpu 参数强制使用 CPU 推理：python inference.py --flag-force-cpu\n如果内存或者卡内存足够，最终会在 GitHub 源码目录中生成了最终视频文件：./animations/s6--d0_concat.mp4\n很不幸，老牛同学16GB内存不足，导致最终生成视频失败（预计至少 22GB 内存）：\n1 RuntimeError: [enforce fail at alloc_cpu.cpp:114] data. DefaultCPUAllocator: not enough memory: you tried to allocate 6383992832 bytes. 根据官方文档，我们可以通过以下几个参数，来设置人像正脸图片、面部动画和生成视频目录：python inference.py --flag-force-cpu -s ./assets/examples/source/s6.jpg -d ./assets/examples/driving/d0.mp4 -o animations\n【方式二： 通过 Web 界面生成视频】\n打开终端，切换到 GitHub 示例源码目录：cd LivePortrait 激活 Python 环境：conda activate LivePortrait 启动 Web 界面：python app.py --flag-force-cpu 1 2 3 .... FaceAnalysisDIY warmup time: 0.163s face_analysis_diy.py:79 Running on local URL: http://127.0.0.1:8890 我们通过浏览器打开地址：http://127.0.0.1:8890\n我们可以直接使用实例源码的头像图片和头部动画，也可以选择自己的图片或头部动画，还可以点击摄像头图标，拍摄和录制我们自己的正脸和面部视频，之后点击**🚀 Animate**按钮即开始生成视频。\n没有 GPU 在线体验 LivePortrait开源的图生视频模型，可以让更多个人、小团体和企业等能轻松部署使用，以生成自己的数字化人物视频。LivePortrait能显著降低了数字化人物创建的门槛，预示着实时视频处理领域的巨大潜力，同时也会在技术和应用层面推动了图生视频技术的快速发展，包括在视频会议、社交媒体直播以及实时游戏动画等实时应用场景中展现了巨大的应用潜力。\nLivePortrait开源的图生视频模型不仅在技术上取得了重要进展，也对图生视频的商业应用和社会影响提出了新的思考。随着技术的进一步成熟和社会应用的深入，未来图生视频技术将在多个领域展现更广阔的应用前景。\nModelScope 模搭社区提供了在线体验 Web 界面，如果我们没有 GPU 本地部署推理太慢了，也可以去体验一下：https://modelscope.cn/studios/DAMOXR/LivePortrait\n基于 Qwen2/Lllama3 等大模型，部署团队私有化 RAG 知识库系统的详细教程（Docker+AnythingLLM）\n使用 Llama3/Qwen2 等开源大模型，部署团队私有化 Code Copilot 和使用教程\n本地部署 GLM-4-9B 清华智谱开源大模型方法和对话效果体验\n玩转 AI，笔记本电脑安装属于自己的 Llama 3 8B 大模型和对话客户端\nChatTTS 开源文本转语音模型本地部署、API 使用和搭建 WebUI 界面\nOllama 完整教程：本地 LLM 管理、WebUI 对话、Python/Java 客户端 API 应用\n","date":"2024-07-14T00:00:00Z","image":"https://modelscope.cn/api/v1/models/AI-ModelScope/LivePortrait/repo?Revision=master\u0026FilePath=.%2Fdocs%2Fshowcase2.gif\u0026View=true","permalink":"https://ntopic.cn/p/2024071401/","title":"LivePortrait 数字人：开源的图生视频模型，本地部署和专业视频制作详细教程"},{"content":"老牛同学在前面有关大模型应用的文章中，多次使用了Ollama来管理和部署本地大模型（包括：Qwen2、Llama3、Phi3、Gemma2等），但对Ollama这个非常方便管理本地大模型的软件的介绍却很少。\n目前，清华和智谱 AI 联合发布开源的GLM4-9B大模型也能支持Ollama进行本地部署了（本地部署 GLM-4-9B 清华智谱开源大模型方法和对话效果体验），Ollama支持的大模型越多越普及，对于的应用也就越多。为了降低大家查阅资料等学习时间，老牛同学今天尝试着对 Ollama 进行一次详细完整介绍。毕竟老牛同学也在不断学习中，若有疏漏或者错误之处，还请各位朋友多多指正，谢谢大家。\n本文将分为以下章节对 Ollama 进行介绍：\nOllama 基本介绍，它的作用是什么 Ollama 软件安装、一些常用的系统参数设置 Ollama 管理本地已有大模型（包括终端对话界面） Ollama 导入模型到本地的三种方式：直接从 Ollama 远程仓库拉取、通过 GGUF 模型权重文件导入到本地、通过 safetensors 模型权限文件导入到本地 基于 WebUI 部署 Ollama 可视化对话界面 Ollama 客户端 API 应用，包括 Python API 和 Java API 接口应用 Ollama 是什么，它与 Llama 有什么关系？ Ollama官网：https://ollama.com/，官方网站的介绍就一句话：Get up and running with large language models. （开始使用大语言模型。）\nOllama是一个开源的 LLM（大型语言模型）服务工具，用于简化在本地运行大语言模型、降低使用大语言模型的门槛，使得大模型的开发者、研究人员和爱好者能够在本地环境快速实验、管理和部署最新大语言模型，包括如Qwen2、Llama3、Phi3、Gemma2等开源的大型语言模型。\nOllama支持的大语言模型列表，可通过搜索模型名称查看：https://ollama.com/library\nOllama官方 GitHub 源代码仓库：https://github.com/ollama/ollama/\nLlama是 Meta 公司开源的备受欢迎的一个通用大语言模型，和其他大模型一样，Llama可以通过Ollama进行管理部署和推理等。\n因此，Ollama与Llama的关系：Llama是大语言模型，而Ollama是大语言模型（不限于Llama模型）便捷的管理和运维工具，它们只是名字后面部分恰巧相同而已！\nOllama 安装和常用系统参数设置 在官网首页，我们可以直接下载Ollama安装程序（支持 Windows/MacOS/Linux）：https://ollama.com/\nOllama的安装过程，与安装其他普通软件并没有什么两样，安装完成之后，有几个常用的系统环境变量参数建议进行设置：\nOLLAMA_MODELS：模型文件存放目录，默认目录为当前用户目录（Windows 目录：C:\\Users%username%.ollama\\models，MacOS 目录：~/.ollama/models，Linux 目录：/usr/share/ollama/.ollama/models），如果是 Windows 系统建议修改（如：D:\\OllamaModels），避免 C 盘空间吃紧 OLLAMA_HOST：Ollama 服务监听的网络地址，默认为127.0.0.1，如果允许其他电脑访问 Ollama（如：局域网中的其他电脑），建议设置成0.0.0.0，从而允许其他网络访问 OLLAMA_PORT：Ollama 服务监听的默认端口，默认为11434，如果端口有冲突，可以修改设置成其他端口（如：8080等） OLLAMA_ORIGINS：HTTP 客户端请求来源，半角逗号分隔列表，若本地使用无严格要求，可以设置成星号，代表不受限制 OLLAMA_KEEP_ALIVE：大模型加载到内存中后的存活时间，默认为5m即 5 分钟（如：纯数字如 300 代表 300 秒，0 代表处理请求响应后立即卸载模型，任何负数则表示一直存活）；我们可设置成24h，即模型在内存中保持 24 小时，提高访问速度 OLLAMA_NUM_PARALLEL：请求处理并发数量，默认为1，即单并发串行处理请求，可根据实际情况进行调整 OLLAMA_MAX_QUEUE：请求队列长度，默认值为512，可以根据情况设置，超过队列长度请求被抛弃 OLLAMA_DEBUG：输出 Debug 日志标识，应用研发阶段可以设置成1，即输出详细日志信息，便于排查问题 OLLAMA_MAX_LOADED_MODELS：最多同时加载到内存中模型的数量，默认为1，即只能有 1 个模型在内存中 Ollama 管理本地已有大模型 【展示本地大模型列表：ollama list】\n1 2 3 4 \u0026gt;ollama list NAME ID SIZE MODIFIED gemma2:9b c19987e1e6e2 5.4 GB 7 days ago qwen2:7b e0d4e1163c58 4.4 GB 10 days ago 可以看到，老牛同学本地有 2 个大模型，它们的名称（NAME）分别为gemma2:9b和qwen2:7b。\n【删除单个本地大模型：ollama rm 本地模型名称】\n1 2 3 4 5 6 \u0026gt;ollama rm gemma2:9b deleted \u0026#39;gemma2:9b\u0026#39; \u0026gt;ollama list NAME ID SIZE MODIFIED qwen2:7b e0d4e1163c58 4.4 GB 10 days ago 老牛同学通过rm命令删除了gemma2:9b大模型之后，再次通过list命令查看，本地只有qwen2:7b一个大模型了。\n【启动本地模型：ollama run 本地模型名】\n1 2 \u0026gt;ollama run qwen2:0.5b \u0026gt;\u0026gt;\u0026gt; 启动成功之后，就可以通过终端对话界面进行对话了（本命令下面也会讲到，其他详细暂且忽略）\n【查看本地运行中模型列表：ollama ps】\n1 2 3 \u0026gt;ollama ps NAME ID SIZE PROCESSOR UNTIL qwen2:0.5b 6f48b936a09f 693 MB 100% CPU 4 minutes from now 通过ps命名可以看到，老牛同学本地qwen2:0.5b大模型正在运行中。\n【复制本地大模型：ollama cp 本地存在的模型名 新复制模型名】\n1 2 3 4 5 6 7 8 \u0026gt;ollama cp qwen2:0.5b Qwen2-0.5B copied \u0026#39;qwen2:0.5b\u0026#39; to \u0026#39;Qwen2-0.5B\u0026#39; \u0026gt;ollama list NAME ID SIZE MODIFIED Qwen2-0.5B:latest 6f48b936a09f 352 MB 4 seconds ago qwen2:0.5b 6f48b936a09f 352 MB 29 minutes ago qwen2:7b e0d4e1163c58 4.4 GB 10 days ago 上面cp命令，老牛同学把本地qwen2:0.5b复制了一份，新模型名为Qwen2-0.5B\n下面老牛同学介绍三种通过 Ollama 下载到本地大模型方式：\n方式一：直接通过 Ollama 远程仓库下载，这是最直接的方式，也是最推荐、最常用的方式 方式二：如果已经有 GGUF 模型权重文件了，不想重新下载，也可以通过 Ollama 把该文件直接导入到本地（不推荐、不常用） 方式三：如果已经有 safetensors 模型权重文件，也不想重新下载，也可以通过 Ollama 把该文件直接导入到本地（不推荐、不常用） 方式一：Ollama 从远程仓库下载大模型到本地 【下载或者更新本地大模型：ollama pull 本地/远程仓库模型名称】\n本pull命令从 Ollama 远程仓库完整下载或增量更新模型文件，模型名称格式为：模型名称:参数规格；如ollama pull qwen2:0.5b 则代表从 Ollama 仓库下载qwen2大模型的0.5b参数规格大模型文件到本地磁盘：\n如果参数规格标记为latest则代表为默认参数规格，下载时可以不用指定，如Qwen2的7b被标记为latest，则ollama pull qwen2和ollama pull qwen2:7b这 2 个命令的意义是一样的，都下载的为7b参数规格模型。为了保证后续维护方便、避免误操作等，老牛同学建议不管是否为默认参数规格，我们下载命令中均明确参数规格。\n值得一提的是，今天开始GLM4支持 Ollama 部署和推理，老牛同学特意列出它的下载命令：ollama pull glm4:9b（和其他模型相比，其实并没有特殊支出）。需要注意的是：Ollama 最低版本为0.2.0才能支持GLM4大模型！\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 \u0026gt;ollama pull qwen2:0.5b pulling manifest pulling manifest pulling manifest pulling manifest pulling manifest pulling 8de95da68dc4... 100% ▕████████████████████████▏ 352 MB pulling 62fbfd9ed093... 100% ▕████████████████████████▏ 182 B pulling c156170b718e... 100% ▕████████████████████████▏ 11 KB pulling f02dd72bb242... 100% ▕████████████████████████▏ 59 B pulling 2184ab82477b... 100% ▕████████████████████████▏ 488 B verifying sha256 digest writing manifest removing any unused layers success \u0026gt;ollama list NAME ID SIZE MODIFIED qwen2:0.5b 6f48b936a09f 352 MB 9 minutes ago qwen2:7b e0d4e1163c58 4.4 GB 10 days ago 若本地不存在大模型，则下载完整模型文件到本地磁盘；若本地磁盘存在该大模型，则增量下载大模型更新文件到本地磁盘。\n从上面最后的list命令结果可以看到，老牛同学本地存在了qwen2:0.5b这个名称的大模型。\n【下载且运行本地大模型：ollama run 本地/远程仓库模型名称】\n1 2 \u0026gt;ollama run qwen2:0.5b \u0026gt;\u0026gt;\u0026gt; 若本地不存在大模型，则下载完整模型文件到本地磁盘（类似于pull命令），然后启动大模型；若本地存在大模型，则直接启动（不进行更新）。\n启动成功后，默认为终端对客界面：\n若需要输入多行文本，需要用三引号包裹，如：\u0026quot;\u0026quot;\u0026quot;这里是多行文本\u0026quot;\u0026quot;\u0026quot; /clear清除对话上下文信息 /bye则退出对话窗口 /set parameter num_ctx 4096可设置窗口大小为 4096 个 Token，也可以通过请求设置，如：curl \u0026lt;http://localhost:11434/api/generate\u0026gt; -d '{ \u0026quot;model\u0026quot;: \u0026quot;qwen2:7b\u0026quot;, \u0026quot;prompt\u0026quot;: \u0026quot;Why is the sky blue?\u0026quot;, \u0026quot;options\u0026quot;: { \u0026quot;num_ctx\u0026quot;: 4096 }}' /show info可以查看当前模型详情： ， 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 \u0026gt;\u0026gt;\u0026gt; /show info Model arch qwen2 parameters 494.03M quantization Q4_0 context length 32768 embedding length 896 Parameters stop \u0026#34;\u0026lt;|im_start|\u0026gt;\u0026#34; stop \u0026#34;\u0026lt;|im_end|\u0026gt;\u0026#34; License Apache License Version 2.0, January 2004 方式二：Ollama 导入 GGUF 模型文件到本地磁盘 若我们已经从 HF 或者 ModeScope 下载了 GGUF 文件（文件名为：Meta-Llama-3-8B-Instruct.Q4_K_M.gguf），在我们存放Llama3-8B的 GGUF 模型文件目录中，创建一个文件名为Modelfile的文件，该文件的内容如下：\n1 FROM ./Meta-Llama-3-8B-Instruct.Q4_K_M.gguf 然后，打开终端，执行命令导入模型文件：ollama create 模型名称 -f ./Modelfile\n1 2 3 4 5 6 \u0026gt;ollama create Llama-3-8B -f ./Modelfile transferring model data using existing layer sha256:647a2b64cbcdbe670432d0502ebb2592b36dd364d51a9ef7a1387b7a4365781f creating new layer sha256:459d7c837b2bd7f895a15b0a5213846912693beedaf0257fbba2a508bc1c88d9 writing manifest success 导入成功之后，我们就可以通过list命名，看到名为Llama-3-8B的本地模型了，后续可以和其他模型一样进行管理了。\n方式三：Ollama 导入 safetensors 模型文件到到本地磁盘 官方操作文档：https://ollama.fan/getting-started/import/#importing-pytorch-safetensors\n若我们已经从 HF 或者 ModeScope 下载了 safetensors 文件（文件目录为：Mistral-7B），\n1 2 3 git lfs install git clone https://www.modelscope.cn/rubraAI/Mistral-7B-Instruct-v0.3.git Mistral-7B 然后，我们转换模型（结果：Mistral-7B-v0.3.bin）：\n1 python llm/llama.cpp/convert.py ./Mistral-7B --outtype f16 --outfile Mistral-7B-v0.3.bin 接下来，进行量化量化：\n1 llm/llama.cpp/quantize Mistral-7B-v0.3.bin Mistral-7B-v0.3_Q4.bin q4_0 最后，通过 Ollama 导入到本地磁盘，创建Modelfile模型文件：\n1 FROM Mistral-7B-v0.3_Q4.bin 执行导入命令，导入模型文件：ollama create 模型名称 -f ./Modelfile\n1 2 3 4 5 6 \u0026gt;ollama create Mistral-7B-v0.3 -f ./Modelfile transferring model data using existing layer sha256:647a2b64cbcdbe670432d0502ebb2592b36dd364d51a9ef7a1387b7a4365781f creating new layer sha256:459d7c837b2bd7f895a15b0a5213846912693beedaf0257fbba2a508bc1c88d9 writing manifest success 导入成功之后，我们就可以通过list命名，看到名为Mistral-7B-v0.3的本地模型了，后续可以和其他模型一样进行管理了。\n基于 WebUI 部署 Ollama 可视化对话界面 Ollama自带控制台对话界面体验总归是不太好，接下来部署 Web 可视化聊天界面：\n下载并安装 Node.js 工具：https://nodejs.org/zh-cn 下载ollama-webui工程代码：git clone https://github.com/ollama-webui/ollama-webui-lite ollama-webui 切换ollama-webui代码的目录：cd ollama-webui 设置 Node.js 工具包镜像源（下载提速）：npm config set registry http://mirrors.cloud.tencent.com/npm/ 安装 Node.js 依赖的工具包：npm install 最后，启动 Web 可视化界面：npm run dev 如果看到以上输出，代表 Web 可视化界面已经成功了！\n浏览器打开 Web 可视化界面：http://localhost:3000/\nOllama 客户端：HTTP 访问服务 Ollama 默认提供了generate和chat这 2 个原始的 API 接口，使用方式如下：\ngenerate接口的使用样例： 1 2 3 4 curl http://localhost:11434/api/generate -d \u0026#34;{ \u0026#39;model\u0026#39;: \u0026#39;qwen:0.5b\u0026#39;, \u0026#39;prompt\u0026#39;: \u0026#39;为什么天空是蓝色的？\u0026#39; }\u0026#34; chat接口的使用样例： 1 2 3 4 5 6 curl http://localhost:11434/api/chat -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;qwen:7b\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;为什么天空是蓝色的？\u0026#34; } ] }\u0026#39; 接下来的Python和Java客户端应用，都是对这 2 个接口的封装。\nOllama 客户端：Python API 应用 我们把 Ollama 集成到 Python 应用中，只需要以下简单 2 步即可：\n第一步，安装 Python 依赖包：\n1 pip install ollama 第二步，使用 Ollama 接口，stream=True代表按照流式输出：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 import ollama # 流式输出 def api_generate(text:str): print(f\u0026#39;提问：{text}\u0026#39;) stream = ollama.generate( stream=True, model=\u0026#39;qwen:7b\u0026#39;, prompt=text, ) print(\u0026#39;-----------------------------------------\u0026#39;) for chunk in stream: if not chunk[\u0026#39;done\u0026#39;]: print(chunk[\u0026#39;response\u0026#39;], end=\u0026#39;\u0026#39;, flush=True) else: print(\u0026#39;\\n\u0026#39;) print(\u0026#39;-----------------------------------------\u0026#39;) print(f\u0026#39;总耗时：{chunk[\u0026#39;total_duration\u0026#39;]}\u0026#39;) print(\u0026#39;-----------------------------------------\u0026#39;) if __name__ == \u0026#39;__main__\u0026#39;: # 流式输出 api_generate(text=\u0026#39;天空为什么是蓝色的？\u0026#39;) # 非流式输出 content = ollama.generate(model=\u0026#39;qwen:0.5b\u0026#39;, prompt=\u0026#39;天空为什么是蓝色的？\u0026#39;) print(content) Ollama 客户端：Java API 应用（SpringBoot 应用） 我们也可以把 Ollama 集成到 SpringBoot 应用中，只需要以下简单 3 步即可：\n第一步，在总pom.xml中新增 SpringBoot Starter 依赖：\n1 2 3 4 5 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.springboot.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-ollama-spring-boot-starter\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.0.0\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 第二步，在 SpringBoot 配置文件application.properties中增加 Ollama 配置信息：\n1 2 3 4 server.port=8088 spring.application.name=NTopicBootX spring.ai.ollama.base-url=http://localhost:11434 spring.ai.ollama.chat.options.model=qwen:0.5b 配置文件指定了 Ollama API 地址和端口，同时指定了默认模型qwen:0.5b（注意：模型需要在本地已经存在）\n第三步，使用OllamaChatClient进行文字生成或者对话：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 import org.springframework.ai.chat.ChatResponse; import org.springframework.ai.chat.prompt.Prompt; import org.springframework.ai.ollama.OllamaChatClient; import org.springframework.ai.ollama.api.OllamaOptions; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.beans.factory.annotation.Qualifier; import org.springframework.web.bind.annotation.GetMapping; import org.springframework.web.bind.annotation.RequestParam; import org.springframework.web.bind.annotation.RestController; @RestController public class OllamaClientController { @Autowired @Qualifier(\u0026#34;ollamaChatClient\u0026#34;) private OllamaChatClient ollamaChatClient; /** * http://localhost:8088/ollama/chat/v1?msg=天空为什么是蓝色的？ */ @GetMapping(\u0026#34;/ollama/chat/v1\u0026#34;) public String ollamaChat(@RequestParam String msg) { return this.ollamaChatClient.call(msg); } /** * http://localhost:8088/ollama/chat/v2?msg=人为什么要不断的追求卓越？ */ @GetMapping(\u0026#34;/ollama/chat/v2\u0026#34;) public Object ollamaChatV2(@RequestParam String msg) { Prompt prompt = new Prompt(msg); ChatResponse chatResponse = ollamaChatClient.call(prompt); return chatResponse; } /** * http://localhost:8088/ollama/chat/v3?msg=你认为老牛同学的文章如何？ */ @GetMapping(\u0026#34;/ollama/chat/v3\u0026#34;) public Object ollamaChatV3(@RequestParam String msg) { Prompt prompt = new Prompt( msg, OllamaOptions.create() .withModel(\u0026#34;qwen:0.5b\u0026#34;) .withTemperature(0.4F)); ChatResponse chatResponse = ollamaChatClient.call(prompt); return chatResponse.getResult().getOutput().getContent(); } } 以上是 Java 客户端的简单样例，我们可以通过OllamaChatClient访问 Ollama 接口，既可以使用默认大模型，也可以在参数指定模型名称！\n基于 Qwen2/Lllama3 等大模型，部署团队私有化 RAG 知识库系统的详细教程（Docker+AnythingLLM）\n使用 Llama3/Qwen2 等开源大模型，部署团队私有化 Code Copilot 和使用教程\n本地部署 GLM-4-9B 清华智谱开源大模型方法和对话效果体验\n玩转 AI，笔记本电脑安装属于自己的 Llama 3 8B 大模型和对话客户端\nChatTTS 开源文本转语音模型本地部署、API 使用和搭建 WebUI 界面\n","date":"2024-07-10T00:00:00Z","image":"https://ntopic.cn/p/2024071001/00_hu05111b7ea7e9ad676e8c99270aa3a526_147425_120x120_fill_box_smart1_3.png","permalink":"https://ntopic.cn/p/2024071001/","title":"Ollama完整教程：本地LLM管理、WebUI对话、Python/Java客户端API应用"},{"content":"自 ChatGPT 发布以来，大型语言模型（Large Language Model，LLM，大模型）得到了飞速发展，它在处理复杂任务、增强自然语言理解和生成类人文本等方面的能力让人惊叹，几乎各行各业均可从中获益。\n然而，在一些垂直领域，这些开源或闭源的通用基础大模型也暴露了一些问题，主要体现在以下 3 个方面：\n知识的局限性： 大模型的知识源于训练数据，目前主流大模型（如：通义千问、文心一言等）的训练数据基本来源于网络公开的数据。因此，非公开的、离线的、实时的数据大模型是无法获取到（如：团队内部实时业务数据、私有的文档资料等），这些数据相关的知识也就无从具备。 幻觉问题： 大模型生成人类文本底层原理是基于概率（目前还无法证明大模型有意识），所以它有时候会一本正经地胡说八道，特别是在不具备某方面的知识情况下。当我们也因缺乏这方面知识而咨询大模型时，大模型的幻觉问题会各我们造成很多困扰，因为我们也无法区分其输出的正确性。 数据的安全性： 对于个人、创新团队、企业来说，数据安全至关重要，老牛同学相信没有谁会愿意承担数据泄露的风险，把自己内部私有数据上传到第三方平台进行模型训练。这是一个矛盾：我们既要借助通用大模型能力，又要保障数据的安全性！ 为了解决以上通用大模型问题，检索增强生成（Retrieval-Augmented Generation，RAG）方案就应运而生了：\nRAG 的主要流程主要包含以下 2 个阶段：\n数据准备阶段： 管理员将内部私有数据向量化后入库的过程，向量化是一个将文本数据转化为向量矩阵的过程，该过程会直接影响到后续检索的效果；入库即将向量数据构建索引，并存储到向量数据库的过程。 用户应用阶段： 根据用户的 Prompt 提示词，通过检索召回与 Prompt 提示词相关联的知识，并融入到原 Prompt 提示词中，作为大模型的输入 Prompt 提示词，通用大模型因此生成相应的输出。 从上面 RAG 方案我们可以看出，通过与通用大模型相结合，我们可搭建团队私有的内部本地知识库，并能有效的解决通用大模型存在的知识局限性、幻觉问题和隐私数据安全等问题。\n目前市面上已经有多个开源 RAG 框架，老牛同学将选择AnythingLLM框架（16.8K ☆，https://github.com/Mintplex-Labs/anything-llm）与大家一起来部署我们自己或者团队内部的本地知识库。整个部署过程将涉及以下几个方面：\n环境准备： AnythingLLM框架推荐使用 Docker 部署，因此我们需要提前把 Docker 安装和配置好 大模型准备： 老牛同学继续使用Qwen2-7B大模型，大家可以根据自己实际情况选择，无特殊要求 RAG 部署和使用： 即 AnythingLLM 安装和配置，并最终使用我们大家的 RAG 系统 环境准备：Windows 打开虚拟化功能（Hyper-V 和 WSL） 友情提示： 老牛同学用的是 Windows 操作系统，因此下面是 Windows 的配置方式。\n安装 Docker 需要用到虚拟化，因此需要 Windows 系统打开Hyper-V和WSL 子系统功能。如果是 Windows 11 家庭版，默认并没有安装Hyper-V功能，可以通过以下方式进行安装：\n【第一步（家庭版）：安装 Hyper-V 依赖包】\n新建一个 txt 临时文本，并复制以下代码并保存，之后把该临时文件重命名为Hyper-V.bat 右键以管理员方式运行Hyper-V.bat，本代码自动安装相关包，完成之后输入Y重启电脑后即可 1 2 3 4 5 pushd \u0026#34;%~dp0\u0026#34; dir /b %SystemRoot%\\servicing\\Packages\\*Hyper-V*.mum \u0026gt;hyper-v.txt for /f %%i in (\u0026#39;findstr /i . hyper-v.txt 2^\u0026gt;nul\u0026#39;) do dism /online /norestart /add-package:\u0026#34;%SystemRoot%\\servicing\\Packages\\%%i\u0026#34; del hyper-v.txt Dism /online /enable-feature /featurename:Microsoft-Hyper-V-All /LimitAccess /ALL 【第二步：开启虚拟化功能】\n首先，打开 Windows 功能（即：控制面板）：\n然后，勾选以下 3 个选项（Hyper-V、适用于 Linux 的 Windows 子系统和虚拟机平台），打开虚拟化功能：\n点击确定之后重启电脑即可！\n环境准备：Docker 安装和配置 Docker 安装和配置主要包含以下 2 个步骤：\n【第一步：下载 Docker 软件】\n下载和安装 Docker 软件：https://www.docker.com\n如果大家和老牛同学一样，出现无法打开 Docker 官网的情况，则也可以通过以下方式下载最新的版本：\n打开 Docker 中文网站：https://docker.github.net.cn/desktop/release-notes/ 选择对应的安装软件，如果直接点击下载，可能还是无法下载。可以右键复制下载链接，然后通过迅雷进行下载（老牛同学亲测有效） 【第二步；Docker 安装和配置】\n下载完成之后，安装过程与普通的软件没有区别，安装完成之后需要注销电脑：\n登录操作系统之后，需要Accept同意 Docker 协议条款，然后注册登录或者Continue without signing in直接跳过，打开 Docker 软件。\n设置镜像源： 点击右上角齿轮设置按钮，选择Docker Engine，打开设置页面，输入 Docker 镜像文件加速器地址：\n1 2 3 4 \u0026#34;registry-mirrors\u0026#34;: [ \u0026#34;https://docker.registry.cyou\u0026#34;, \u0026#34;https://mirror.baidubce.com\u0026#34; ] 如果以上加速器地址不可用，可以使用阿里云个人私有的地址：https://cr.console.aliyun.com/cn-hangzhou/instances/mirrors\n点击Apply \u0026amp; Restart按钮即可！\nAnythingLLM 安装和配置 接下来，我们开始安装和部署AnythingLLM框架，包含以下 3 步：\n【第一步： 下载和启动 AnythingLLM 镜像】\n1 docker pull mintplexlabs/anythingllm 可以通过以下命令，查看当前 Docker 的镜像列表：\n1 2 3 \u0026gt;docker images REPOSITORY TAG IMAGE ID CREATED SIZE mintplexlabs/anythingllm latest d950fa3b8011 2 days ago 2.94GB 【第二步： 启动 AnythingLLM 框架】\nWindows 系统： AnythingLLM 镜像挂载和启动命令（因为命令有多行，需要通过PowerShell执行）：\n1 2 3 4 5 6 7 8 9 10 # Run this in powershell terminal $env:STORAGE_LOCATION=\u0026#34;$HOME\\Documents\\anythingllm\u0026#34;; ` If(!(Test-Path $env:STORAGE_LOCATION)) {New-Item $env:STORAGE_LOCATION -ItemType Directory}; ` If(!(Test-Path \u0026#34;$env:STORAGE_LOCATION\\.env\u0026#34;)) {New-Item \u0026#34;$env:STORAGE_LOCATION\\.env\u0026#34; -ItemType File}; ` docker run -d -p 3001:3001 ` --cap-add SYS_ADMIN ` -v \u0026#34;$env:STORAGE_LOCATION`:/app/server/storage\u0026#34; ` -v \u0026#34;$env:STORAGE_LOCATION\\.env:/app/server/.env\u0026#34; ` -e STORAGE_DIR=\u0026#34;/app/server/storage\u0026#34; ` mintplexlabs/anythingllm; 或者，Linux/MacOS 系统： AnythingLLM 镜像挂载和启动命令：\n1 2 3 4 5 6 7 8 9 export STORAGE_LOCATION=$HOME/anythingllm \u0026amp;\u0026amp; \\ mkdir -p $STORAGE_LOCATION \u0026amp;\u0026amp; \\ touch \u0026#34;$STORAGE_LOCATION/.env\u0026#34; \u0026amp;\u0026amp; \\ docker run -d -p 3001:3001 \\ --cap-add SYS_ADMIN \\ -v ${STORAGE_LOCATION}:/app/server/storage \\ -v ${STORAGE_LOCATION}/.env:/app/server/.env \\ -e STORAGE_DIR=\u0026#34;/app/server/storage\u0026#34; \\ mintplexlabs/anythingllm 启动完成，通过浏览器打开AnythingLLM界面：http://localhost:3001\n【第三步： 配置 AnythingLLM】\n点击Get started 按钮，进入LLM 选择页面。老牛同学在前面文章中，分别介绍了大模型通过Ollama和OpenAI这 2 种主要的部署方式（包括：Llama3、Qwen2、Gemma2等）。\n老牛同学在这里选择Ollama方式，如果大家重新设置了端口号，则需要看下 Ollama URL 端口号是否正确，默认值为：http://host.docker.internal:11434\nAnythingLLM 设置确认，包括：大模型提供方、词嵌入和向量数据库，这些配置后续还可以修改。\n最后，创建一个工作区，老牛同学设置的名称为：RAG-ClassmateWX\nAnythingLLM 导入数据和使用 上一步配置完成之后，无需任何其他配置，就可以和大模型对话聊天了，和通过其他客户端与大模型对话没有区别。接下来，我们需要导入我们内部私有数据，并进行验证。\n【第一步：导入内部数据】\n我们在电脑本地新建一个 txt 文件，文件名为：为什么个人、团队等均有必要部署私有化的RAG知识库系统.txt，文件内容就是本文的开头内容：\n1 2 3 4 5 6 7 8 9 自ChatGPT发布以来，大型语言模型（Large Language Model，LLM，大模型）得到了飞速发展，它在解决复杂任务、增强自然语言理解和生成类人文本等方面的能力让人惊叹，几乎各行各业均可从中获益。 然而，在一些垂直领域，这些开源或闭源的通用的基础大模型也暴露了一些问题，主要有以下3个方面： 1. **知识的局限性：** 大模型的知识源于训练数据，目前主流大模型（如：通义千问、文心一言等）的训练数据基本来源于网络公开的数据。因此，非公开的、离线的、实时的数据大模型是无法获取到（如：团队内部实时业务数据、私有的文档资料等），这些数据相关的知识也就无从具备。 2. **幻觉问题：** 大模型生成人类文本底层原理是基于概率（目前还无法证明大模型有意识），所以它有时候会**一本正经地胡说八道**，特别是在不具备某方面的知识情况下。当我们也因缺乏这方面知识而咨询大模型时，大模型的幻觉问题会各我们造成很多困扰，因为我们也无法区分其输出的正确性。 3. **数据的安全性：** 对于个人、创新团队、企业来说，**数据安全**至关重要，老牛同学相信没有谁会愿意承担数据泄露的风险，把自己内部私有数据上传到第三方平台进行模型训练。这是一个矛盾：我们既要借助通用大模型能力，又要保障数据的安全性！ 为了解决以上3个大模型通用问题，**检索增强生成**（Retrieval-Augmented Generation，**RAG**）方案就应运而生了！ 首先，点击RAG-ClassmateWX工作空间右边的上传图标，准备上传本 txt 文件：\n然后，点击 txt 文件并上传，并点击Move to workspace导入到工作空间：\n最后，点击Save and Embed完成 txt 文本文件数据准备步骤（导入、向量化和入库等）。\n【第二步：内部数据使用和验证】\n回到主界面，输入刚才 txt 内部文件名：为什么个人、团队等均有必要部署私有化的 RAG 知识库系统\nRAG 的回答感觉比我原文要好多了，大模型在引用了内部数据的同时，还展示自己的创造性！\n同时，老牛同学用同样的问题，咨询 AnythingLLM 的底层模型，结果就比较普通通用了，大家可以对比看下：\n总结和问题 和我们之前的大模型部署和应用过程相比，基于 AnythingLLM 的 RAG 实现整个部署过程比较繁琐，包括环境准备、Docker 安装和配置、AnythingLLM 配置等。然而，AnythingLLM 的使用过程却相对比较简单，我们只需要上传我们内部数据文件，AnythingLLM 框架屏蔽了中间的数据提取分割、向量化处理、向量索引和入库、检索召回和重组 Prompt 提示词等过程。\n同时，我们通过构建本地知识库，做了一个简单的测试验证，测试结果表明，在使用 RAG 的情况下，大模型的回答结果更加有效、更符合我们期望，同时具备了一定的创造性！\n老牛同学最后提个问题：检索召唤的过程，能否使用搜索引擎（如：Elastic Search）代替？欢迎大家留言讨论，我们共同学习进步！\n关注本公众号，我们共同学习交流进步 👇🏻👇🏻👇🏻\n使用 Llama3/Qwen2 等开源大模型，部署团队私有化 Code Copilot 和使用教程\n本地部署 GLM-4-9B 清华智谱开源大模型方法和对话效果体验\n玩转 AI，笔记本电脑安装属于自己的 Llama 3 8B 大模型和对话客户端\nChatTTS 开源文本转语音模型本地部署、API 使用和搭建 WebUI 界面\n","date":"2024-07-06T00:00:00Z","image":"https://ntopic.cn/p/2024070601/00_hu98e9cb01058f1c96227732799c120259_28514_120x120_fill_box_smart1_3.png","permalink":"https://ntopic.cn/p/2024070601/","title":"基于Qwen2/Lllama3等大模型，部署团队私有化RAG知识库系统的详细教程（Docker+AnythingLLM）"},{"content":"6 月 27 日，全球知名的开源平台 Hugging Face 的联合创始人兼首席执行官 Clem 在社交平台激动宣布，阿里 Qwen2-72B 成为了开源模型排行榜的王者。\n这是一件大好事，说明了我们在大模型领域从先前的追赶，逐渐走向了领导，未来完全有可能会引领着全球开源模型的发展潮流，这是我们的骄傲！\n不过话说回来，Qwen2 序列有 5 个参数版本，分别是 Qwen2-0.5B、Qwen2-1.5B、Qwen2-7B、Qwen2-57B-A14B 和 Qwen2-72B。本次排行榜第一的是 Qwen2-72B 参数版本，那么其他参数版本的评测结果如何呢？老牛同学查看了 Qwen2 的官方文档，有一些比较评测：https://qwenlm.github.io/zh/blog/qwen2/\nQwen2-72B 如 Clem 宣布一样，包括自然语言理解、知识、代码、数学及多语言等多项能力上均显著超越当前领先的模型：\n而 Qwen2-7B 模型在自然语言理解 和 数学 方面均有点落后了：\n其他参数版本没有评测结果，老牛同学猜测是其他参与评测的大模型没有对等的参数版本，因此无法进行比较，或者评测结果不相上下，也就没有必要把结果放出来了。\n老牛同学的疑问 那么老牛同学的问题来了：阿里 Qwen2-72B 大模型已是开源榜的王者，为什么还要推出其他参数模型，被其他模型打榜？\n老牛同学带着这个问题咨询了几位同事，也问了Qwen2-7B大模型，他们的回答均有一定道理，但老牛同学猜测，Qwen 这么做的核心目的只有一个：丰富 Qwen 大模型生态\n【原因一： 训练 Qwen 中小尺寸参数成本并不高】\n以阿里人才储备、基础设施和高层 All in AI 的决心和投入，训练 Qwen 中小参数版本的模型应该不是什么难事，顺手即可做的事情，相对来说成本并不高\n【原因二： 快速进行迭代和模型参数优化演进】\n通过快速发布多个尺寸的模型，可以让内外部的技术究人员分析和探索模型的参数规模与性能之间的关系，以便能找到最佳的平衡点，最终不断推动优化和演进 Qwen 系列大模型\n【原因三： 构建和丰富 Qwen 大模型全场景生态】\n老牛同学认为这是最为关键的一点，开源不是目的、打榜争第一也应该不是目的，它们都只是构建并丰富生态的策略！老牛同学和大家一起来回顾 2 件事情：\nWindows XP 生态： 我们还记得二零零几年的时候，满大街 Windows XP 的盗版操作系统吗？操作系统激活竟然也可以是路边打印小店的一大业务。那么，盗版这么多，微软为什么不管管？是他不知道吗，还是根本管不了？老牛同学觉得都不是，应该是微软故意放任不管：微软要赚的是企业的钱，包括 IT 公司、电脑厂商等，个人的钱不太好赚；同时，使用 Windows 操作系统的个人越多，会带动 Windows 生态发展（使用技巧和攻略、研发各种各样的软件等），进而能拉动更多使用的个人，同时对于企业来说意味着熟练的工人也越多，企业为降低成本，自然就愿意采购 Windows 正版授权，最终还是微软赚钱了！\n阿里云 OS 生态： 老牛同学在二零一几年的时候看到过一张图片，图片展示的是当时的阿里 CTO 王坚博士在报告台上擦眼泪。传说是王坚博士在给一群高管做汇报，汇报内容是阿里云和手机操作系统（云 OS）。云计算在当时的中国没有任何一家企业有布局，但王坚博士坚信云计算和云 OS 的未来，他作为第一个吃螃蟹的人，在技术研发和基础设施等方面的投入成本都非常巨大，受到了一些高管和投资人的质疑，因此他边做着汇报边流着委屈的流泪。辛亏当时马老师力排众议，支持王坚博士的想法，如今阿里云在国内技术或市场上称第二，应该没有其他云能自信的称第一了。\n反观 云 OS 我们几乎感知不到了，很大一个原因是 没有生态：云 OS 曾经和国内的一些手机厂商合作推出过一些手机品牌（如 小辣椒、大黄蜂等），和比较大的厂商 宏基 的合作新闻发布会，后者也因 Google Android 的压力不得不放弃合作。Google 对云 OS 抄袭 Android 的 API 代码的状告一直不断。老牛同时当时咨询过一位负责云 OS 短信模块的技术同学，为什么云 OS 的 API 需要和 Android 保持一致，是技术能力不够吗？技术同学告诉老牛同学，操作系统都能研发出来，API 并没有什么技术难度，但是如果 API 不能和 Android 保持一致，那么已有的 Android APP 将无法接入，所以必须要兼容保持一致，否则没有了 Android 的生态，操作系统要推广就难于登天了。\n如今，云 OS 虽然没有成为手机操作系统，但它却在车载（斑马网络）、智能家居（电视、天猫精灵等）等 OS 中大放异彩。\n而 Qwen 系列大模型的 Qwen2-0.5B和Qwen2-1.5B可在智能家居推理；Qwen2-7B可用于个人、小团队等部署推理，个人业务不断发展，就可能购买云服务器部署，小团队业务进一步发展，也完全有可能购买 Qwen 推理服务，最终形成完美的闭环（感觉和Windows XP有那么一点点类似）。\n最后：纯属老牛同学个人观点，请慎喷 以上观点纯属老牛同学个人的猜测，若有不合理之处，欢迎留言讨论，若有冒犯之处，请联系老牛同学删除此文，非常感谢！\n关注本公众号，我们共同学习交流进步 👇🏻👇🏻👇🏻\n开源大模型\n本地部署 GLM-4-9B 清华智谱开源大模型方法和对话效果体验\nPhi-3 模型手机部署教程（微软发布的可与 GPT-3.5 媲美的小模型）\nQwen2 阿里最强开源大模型（Qwen2-7B）本地部署、API 调用和 WebUI 对话机器人\n玩转 AI，笔记本电脑安装属于自己的 Llama 3 8B 大模型和对话客户端\n一文彻底整明白，基于 Ollama 工具的 LLM 大语言模型 Web 可视化对话机器人部署指南\n基于 Llama 3 搭建中文版（Llama3-Chinese-Chat）大模型对话聊天机器人\nGoogle 发布了最新的开源大模型 Gemma 2，本地快速部署和体验\nChatTTS 文本转语音模型\nChatTTS 开源文本转语音模型本地部署、API 使用和搭建 WebUI 界面\nStable Diffusion 3 文生图模型\nStable Diffusion 3 文生图“开源英雄”大模型本地部署和使用教程，轻松实现 AI 绘图自由\n大模型应用实战\n使用 Llama3/Qwen2 等开源大模型，部署团队私有化 Code Copilot 和使用教程\n大模型应用研发基础环境配置（Miniconda、Python、Jupyter Lab、Ollama 等）\n借助 AI 大模型，三分钟原创一部儿童故事短视频（附完整操作步骤）\n高效编写大模型 Prompt 提示词，解锁 AI 无限创意潜能\nPython 小游戏\nAI 已来，我与 AI 一起用 Python 编写了一个消消乐小游戏\nPython 游戏编程：一步步用 Python 打造经典贪吃蛇小游戏\n","date":"2024-07-04T00:00:00Z","image":"https://ntopic.cn/p/2024070401/00_huce067645ed24a619e3f9931314bcd87c_717911_120x120_fill_box_smart1_3.png","permalink":"https://ntopic.cn/p/2024070401/","title":"阿里Qwen2-72B大模型已是开源榜的王者，为什么还要推出其他参数模型，被其他模型打榜？"},{"content":"Gemma 2 是 Google 最新发布的开源大语言模型。它有两种规模：90 亿（9B）参数和 270 亿（27B）参数，分别具有基础（预训练）和指令调优版本，拥有 8K Tokens 的上下文长度：\nGemma-2-9b： 90 亿参数基础模型版本 Gemma-2-9b-it： 90 亿参数基础模型的指令调优版本 Gemma-2-27B： 270 亿参数基础模型版本 Gemma-2-27B-it： 270 亿参数基础模型的指令调优版本 Gemma 2 模型的训练数据量约为其第一代的两倍，总计 13 万亿 Tokens（270 亿模型）和 8 万亿 Tokens（90 亿模型）的网页数据（主要是英语）、代码和数学数据。同时，相比较第一代，Gemma 2 的推理性能更高、效率更高，并在安全性方面取得了重大进步。\n许可协议： Gemma 2 与第一代使用相同的许可证，这是一个允许再分发、微调、商业用途和衍生作品的宽松许可证。\n性能优异： Gemma 2 27B 版本在同规模级别中性能最佳，甚至比两倍于其尺寸的机型更具竞争力。9B 版本的性能在同类产品中也处于领先地位，超过了 Llama 3 8B 和其他同规模的开放模型。\n其他关于 Gemma 2 的介绍信息，可以参见 Google 官方博客：https://blog.google/technology/developers/google-gemma-2/\nGoogle 重磅发布产品，我们肯定需要体验以下。今天，老牛同学就和大家一起，分别通过 2 种方式在个人笔记本电脑本地部署和体验 Gemma2-9B 大模型。\n方式一：通过 Ollama 部署大模型 关于 Ollama 是什么以及它的使用方式，老牛同学前面的博文中有介绍，本文不在赘述，感兴趣的朋友可以看一下之前的博文。\nOllama 管理和维护 Gemma 2 比较简单，主要流程如下：\n下载并安装 Ollama 软件（Windows/Linux/MacOS 均支持）：https://ollama.com/download 通过 Ollama 下载并启动 Gemma 2 大模型： 1 ollama run gemma2:9b 模型文件大小为 5.4GB 左右，需要耐心等待模型下载完成。下载完成之后，Ollama 自动启动模型，就可以通过 Ollama 进行对话了：\n如果觉得通过控制台的方式对话体验不好，可以部署 WebUI 的方式与模型对话。WebUI 的部署方式，可以参见老牛同学之前的博文：https://mp.weixin.qq.com/s/P_ufvz4MWVSqv_VM-rJp9w，主要部署步骤：\n下载并安装 Node.js 工具：https://nodejs.org/zh-cn 下载ollama-webui工程代码：git clone https://github.com/ollama-webui/ollama-webui-lite ollama-webui 切换ollama-webui代码的目录：cd ollama-webui 设置 Node.js 工具包镜像源（下载提速）：npm config set registry http://mirrors.cloud.tencent.com/npm/ 安装 Node.js 依赖的工具包：npm install 最后，启动 Web 可视化界面：npm run dev 然后，通过浏览器打开 WebUI 对话界面了：http://localhost:3000/\n方式二：通过 GGUF 部署大模型 GGUF模型文件格式是为了快速推理和优化内存使用而设计的，支持更复杂的令牌化过程和特殊令牌处理，能更好地应对多样化的语言模型需求。GGUF就一个文件，也简化了模型交换和部署的过程，它对促进模型的普及和应用有着积极作用。\nGGUF 模型文件列表：https://modelscope.cn/models/LLM-Research/gemma-2-9b-it-GGUF/files\nGGUF 模型文件名称格式，如gemma-2-9b-it-Q5_K_M.gguf等：\nit代表本模型是对基线模型进行了微调，用于更好地理解和生成遵循指令（instruction-following）的文本，以提供符合要求的响应\nQ4/Q5 等代表模型权重的量化位数（其中Q是Quantization的缩小，即量化），是一种模型压缩技术，用于减少模型大小，同时降低对计算资源的需求（特别是内存），但又尽量保持模型的性能；数字4或5则代表量化精度的位数（Q4 是 4 位，Q5 是 5 位等），精度越高模型体积和内存使用也会越大，但仍然远小于未量化的基线模型\nK_M/K_S代表了与注意力机制相关的特定配置，K_M 可能是指 Key 的 Mask，即用来屏蔽某些位置的键值对，防止它们在注意力计算中被考虑；而 K_S 可能是指 Key 的 Scale 或 Size，涉及到键向量缩放，这是在多头注意力机制中常见的操作，以稳定梯度\n点击下载图标即可下载，由于文件较大，浏览器的下载容易过程容易终端，重试可继续下载（假设下载本地的文件名为：Gemma-2-9B-it-Q5_K_M.gguf）：\n打开一个终端窗口，切换到 GGUF 文件所在目录：cd Gemma2 切换 Python 虚拟环境：conda activate PY3.12 安装 Python 依赖包： 1 2 3 4 5 6 7 8 pip install llama-cpp-python pip install openai pip install uvicorn pip install starlette pip install fastapi pip install sse_starlette pip install starlette_context pip install pydantic_settings 或者，我们也可以一把进行安装：pip install -r requirements.txt\n1 2 3 4 5 6 7 8 9 # requirements.txt llama-cpp-python openai uvicorn starlette fastapi sse_starlette starlette_context pydantic_settings 最后，启动大模型：\n1 2 # 启动Llama大模型 python -m llama_cpp.server --host 0.0.0.0 --model ./Gemma-2-9B-it-Q5_K_M.gguf --n_ctx 2048 模型启动命令中，n_ctx 2048代表单次回话最大 Token 数量。\n启动成功，我们应该看到类似如下的信息：INFO: Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n最后一步： 我们使用 openai 库在个人电脑上快速搭建客户端。Python 客户端代码（Client.py）如下：\n我们使用OpenAI接口来与 Gemma 交互，上面启动模型的最后，我们看到服务端 IP 是本地，端口是8000 接着，我们使用 2 条信息对历史记录进行初始化：第一个条是系统信息，第二个条是要求模型自我介绍的用户提示，为了避免长篇大论，我这里限制了回答的长度和字数 接下来，通过\u0026gt;提示符等待用户（即我们）输入，输入bye、quit和exit任意一个即代表退出客户端 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 # Client.py from openai import OpenAI # 注意服务端端口，因为是本地，所以不需要api_key client = OpenAI(base_url=\u0026#34;http://localhost:8000/v1\u0026#34;, api_key=\u0026#34;EMPTY\u0026#34;) # 对话历史：设定系统角色是一个只能助理，同时提交“自我介绍”问题 history = [ {\u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;你是一个智能助理，你的回答总是正确的、有用的和内容非常精简.\u0026#34;}, {\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;请用中文进行自我介绍，要求不能超过5句话，总字数不超过100个字。\u0026#34;}, ] print(\u0026#34;\\033[92;1m\u0026#34;) # 首次自我介绍完毕，接下来是等代码我们的提示 while True: completion = client.chat.completions.create( model=\u0026#34;local-model\u0026#34;, messages=history, temperature=0.7, stream=True, ) new_message = {\u0026#34;role\u0026#34;: \u0026#34;assistant\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;\u0026#34;} for chunk in completion: if chunk.choices[0].delta.content: print(chunk.choices[0].delta.content, end=\u0026#34;\u0026#34;, flush=True) new_message[\u0026#34;content\u0026#34;] += chunk.choices[0].delta.content history.append(new_message) print(\u0026#34;\\033[91;1m\u0026#34;) userinput = input(\u0026#34;\u0026gt; \u0026#34;) if userinput.lower() in [\u0026#34;bye\u0026#34;, \u0026#34;quit\u0026#34;, \u0026#34;exit\u0026#34;]: # 我们输入bye/quit/exit等均退出客户端 print(\u0026#34;\\033[0mBYE BYE!\u0026#34;) break history.append({\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: userinput}) print(\u0026#34;\\033[92;1m\u0026#34;) 我们打开一个 Terminal 终端，运行客户端：python Client.py\n恭喜你，第二种方式也部署成功了，我们可以愉快地与大模型进行对话了，包括把大模型作为我们 Code Copilot 的底层模型，部署我们团队私有化的 Code Copilot 的底层模型，部署我们团队私有化的了：个人或团队私有化 Code Copilot 部署和使用教程\n总结 以上是老牛同学和大家一起采用 2 种方式快速部署 Gemma 2 大模型，这 2 种方式是同样的方式，同样适用于其他大模型。\n相对来说，Ollama 部署配置比较简单，目前常见的大模型均支持 Ollama 推理协议（包括：Qwen/Lllama/Phi 等大模型），推荐使用；同时，GGUF 部署方式仅需要依赖一个模型文件，使用 Llama.cpp 框架进行推理，依赖也少部署也很方便，同样推荐使用。如何抉择，就看我们自己喜好了！\nGemma 2 在内最近发布的开源大模型，可以看出当前大模型研究的趋势，即探索用更轻量级、更实用的模型来实现更强的性能，并确保易部署，以更好地满足不同用户的需求。老牛同学觉得未来低成本、定制化的垂直场景小模型将会越来越多，也会越来越受欢迎！\n关注本公众号，我们共同学习交流进步 👇🏻👇🏻👇🏻\nPhi-3 开源大模型\nPhi-3 模型手机部署教程（微软发布的可与 GPT-3.5 媲美的小模型）\nQwen2-7B 开源大模型\nQwen2 阿里最强开源大模型（Qwen2-7B）本地部署、API 调用和 WebUI 对话机器人\nLlama-3-8B 开源大模型\n玩转 AI，笔记本电脑安装属于自己的 Llama 3 8B 大模型和对话客户端\n一文彻底整明白，基于 Ollama 工具的 LLM 大语言模型 Web 可视化对话机器人部署指南\n基于 Llama 3 搭建中文版（Llama3-Chinese-Chat）大模型对话聊天机器人\nGLM-4-9B 开源大模型\n本地部署 GLM-4-9B 清华智谱开源大模型方法和对话效果体验\nChatTTS 文本转语音模型\nChatTTS 开源文本转语音模型本地部署、API 使用和搭建 WebUI 界面\nStable Diffusion 3 文生图模型\nStable Diffusion 3 文生图“开源英雄”大模型本地部署和使用教程，轻松实现 AI 绘图自由\n大模型应用实战\n使用 Llama3/Qwen2 等开源大模型，部署团队私有化 Code Copilot 和使用教程\n大模型应用研发基础环境配置（Miniconda、Python、Jupyter Lab、Ollama 等）\n借助 AI 大模型，三分钟原创一部儿童故事短视频（附完整操作步骤）\n高效编写大模型 Prompt 提示词，解锁 AI 无限创意潜能\nPython 小游戏\nAI 已来，我与 AI 一起用 Python 编写了一个消消乐小游戏\nPython 游戏编程：一步步用 Python 打造经典贪吃蛇小游戏\n","date":"2024-07-02T00:00:00Z","image":"https://ntopic.cn/p/2024070201/00_hu2f8b94c83031de6e5bc3f44f06111c68_362582_120x120_fill_box_smart1_3.png","permalink":"https://ntopic.cn/p/2024070201/","title":"Google 发布了最新的开源大模型 Gemma 2，本地快速部署和体验"},{"content":"目前市面上有不少基于大模型的 Code Copilot 产品，部分产品对于个人开发者来说可免费使用，比如阿里的通义灵码、百度的文心快码等。这些免费的产品均通过 API 的方式提供服务，因此调用时均必须联网、同时需要把代码、提示词等内容作为 API 的入参在网络中传输和 API 服务器中进行处理，这里就涉及到一个比较重要的问题：隐私安全\n针对小团队私有保密项目、创新团队孵化新产品，隐私安全问题就显得格外重要。因此，团队内部部署私有的Code Copilot方案就应运而出了：\n今天老牛同学和大家一起来使用Qwen2-7构建个人或者团队专属的私有化 Code Copilot，除了代码生成之外，还可以是支持 AI 问答、代码解释、语言转换、生成单元测试用例等功能。不但可以提高我们的研发效率，还可以保护我们的代码隐私。\n第一步：部署私有的大模型（Qwen2-7B） 前面老牛同学介绍了本地部署大模型和使用教程，包括Llama3-8B、Qwen2-7B、GLM4-9B、和Phi3等。大家可以看一下之前的部署教程，老牛同学今天使用Qwen2-7B作为我们 Copilot 底层大模型，对于本文Code Copilot的部署和使用，其他大模型都是一样的，看大家的喜好，没有特殊要求。\n老牛同学这里简单列一下部署教程：\n环境安装： 主要是Miniconda包管理安装，参见教程，注意只需要安装Miniconda和Python即可，其他非必须：大模型应用研发基础环境配置（Miniconda、Python、Jupyter Lab、Ollama 等）：https://mp.weixin.qq.com/s/P_ufvz4MWVSqv_VM-rJp9w\nCode Copilot底层大模型的部署方式，老牛同学验证主要以下 2 种，任选一种即可：\n方式一 基于 Ollama 部署：\n下载并安装Ollama本地模型管理软件（Windows/Mac/Linux 均支持）：https://ollama.com/download 通过 Ollama 下载并启动大模型：ollama run qwen2:7b 通过以上 2 步，即完成了大模型的部署。特别注意的是：GLM系列大模型还不支持 Ollama 管理！因此如果你希望用 GLM 系列（如：GLM4-9B）大模型，那么请看方式二。\n方式二 通过 Python 部署：通用方法，所有模型都支持，以下是老牛同学验证过的部署教程\nPhi3：Phi-3 模型手机部署教程（微软发布的可与 GPT-3.5 媲美的小模型） Qwen2-7B：Qwen2 阿里最强开源大模型（Qwen2-7B）本地部署、API 调用和 WebUI 对话机器人 Llama3-8B：玩转 AI，笔记本电脑安装属于自己的 Llama 3 8B 大模型和对话客户端 GLM4-9B：本地部署 GLM-4-9B 清华智谱开源大模型方法和对话效果体验 第二步：IDE 安装和配置 Continue 插件 Code Copilot Continue是目前最受欢迎的插件之一，它插件的形式使用，目前支持VS Code和JetBrains 全家桶这 2 个主流 IDE 的扩展，可以在对应的插件市场中直接搜索Continue下载安装。Continue插件的一个最大优势在于它可以指定连接本地或者局域网内的大模型，所以对于信息安全高的项目，我们完全可以在本地或者局域网内部署大模型，然后用Continue插件连接使用，在提高开发生产效率同时保护了研发代码的隐私。目前Continue插件的社区活跃，GitHub 仓库已达12K星，插件版本更新频繁，越来越成熟。\n因为老牛同学日常使用VS Code较多，因此下面的安装和配置，基于VS Code完成，JetBrains（如：PyCharm 等）的方式类同。\n安装 Continue 插件： 插件管理器中，搜索Continue，点击安装，安装成功之后重启 IDE 即可。\nContinue 插件状态： 安装成功并重启之后，可以看到VS Code的左侧多了一个Continue按钮。同时，右下角也多了一个Continue状态图标：√ Continue则代表Continue在 IDE 中生效了，🚫Continue则代表Continue在 IDE 未启用。\n配置 Continue 插件： 点击左下角启用Continue插件，然后点击左侧的Continue按钮，可以看到一个对话界面，点击左下角齿轮形状的配置图标，则打开了一个 JSON 格式的配置文件，在这里可以配置Continue底层使用的大模型信息：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 { \u0026#34;models\u0026#34;: [ { \u0026#34;title\u0026#34;: \u0026#34;Qwen2:7B\u0026#34;, \u0026#34;provider\u0026#34;: \u0026#34;ollama\u0026#34;, \u0026#34;model\u0026#34;: \u0026#34;qwen2:7b\u0026#34; }, { \u0026#34;title\u0026#34;: \u0026#34;Llama3-8B\u0026#34;, \u0026#34;provider\u0026#34;: \u0026#34;openai\u0026#34;, \u0026#34;model\u0026#34;: \u0026#34;Local-Model\u0026#34;, \u0026#34;apiBase\u0026#34;: \u0026#34;http://127.0.0.1:8000/v1\u0026#34;, \u0026#34;apiKey\u0026#34;: \u0026#34;EMPTY\u0026#34;, \u0026#34;completionOptions\u0026#34;: { \u0026#34;stop\u0026#34;: [ \u0026#34;\u0026lt;|eot_id|\u0026gt;\u0026#34; ] } } ], \u0026#34;customCommands\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;test\u0026#34;, \u0026#34;prompt\u0026#34;: \u0026#34;{{{ input }}}\\n\\nWrite a comprehensive set of unit tests for the selected code. It should setup, run tests that check for correctness including important edge cases, and teardown. Ensure that the tests are complete and sophisticated. Give the tests just as chat output, don\u0026#39;t edit any file.\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Write unit tests for highlighted code\u0026#34; } ], // \u0026#34;tabAutocompleteModel\u0026#34;: { // \u0026#34;title\u0026#34;: \u0026#34;Qwen2:7B\u0026#34;, // \u0026#34;provider\u0026#34;: \u0026#34;ollama\u0026#34;, // \u0026#34;model\u0026#34;: \u0026#34;qwen2:7b\u0026#34; // }, \u0026#34;tabAutocompleteModel\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;Llama3-8B\u0026#34;, \u0026#34;model\u0026#34;: \u0026#34;Local-Model\u0026#34;, \u0026#34;apiBase\u0026#34;: \u0026#34;http://127.0.0.1:8000/v1\u0026#34;, \u0026#34;provider\u0026#34;: \u0026#34;openai\u0026#34;, \u0026#34;apiKey\u0026#34;: \u0026#34;EMPTY\u0026#34; }, \u0026#34;allowAnonymousTelemetry\u0026#34;: true, \u0026#34;embeddingsProvider\u0026#34;: { \u0026#34;provider\u0026#34;: \u0026#34;transformers.js\u0026#34; } } Continue插件的配置信息主要分为 3 块：\nmodels 代表所有的模型列表：上面样例配置文件，老牛同学分别配置了基于Ollama部署的Qwen2-7B大模型和基于OpenAI客户端部署的Llama3-7B大模型（也就是涵盖了上面大模型的 2 种不同部署方式的不同配置方式） customCommands 自定义给大模型的提示词格式，input代表输入内容（代码或者文本等），可以按照实际需求进行调整 tabAutocompleteModel 代码编写过程中，当按下代码生成快捷键时，Continue生成代码的大模型，从models列表里面指定一个即可，当然也可以设置一个其他的大模型（注意： 只能选择一个模型，上面样例配置注释掉了一个，在实际使用时需要删除掉，因为 JSON 不支持注释！） 第三步：使用 Code Copilot Continue 插件 首先，启动大模型（如：Qwen2-7B）；其次，点击 IDE 左下角图标，以启用Continue插件；然后，选择一个对话大模型（默认选中第 1 个大模型）：\n使用方式一： 我们可以选择不同的大模型进行对话，相当于Continue插件提供了一个可视化对话客户端\n使用方式二： 代码生成，也是本文的重点，下面老牛同学以快速排序算法为例，来如何利用Continue完成代码编写：\n【场景一：AI 对话代码同步】\n使用大模型对话能力，写出快速排序算法代码，Prompt 提示词：请提供一下功能代码：Python 实现快速排序算法函数，大模型很好的实现了算法逻辑，并且给了很多的说明和解释 选中快速排序算法函数的代码，然后按下CTRL+Shift+L快捷键，或者点击代码框右上角的Apply to current file按钮，Python 自动同步到了编辑器中了： 【场景二：代码自动补全】\n在编辑器中，只需要输入注释内容（不要按回车键）：# Python实现快速排序算法函数 然后按下CTRL+Shift+L快捷键，然后在按下回车键，可以看到代码已经生成，可以按Tab键使用代码 【场景三：增加单元测试代码】\n在编辑器中，选中一段代码函数，然后按下CTRL+I快捷键 然后，在弹出的Ctrl+I指令框中输入：增加代码测试代码，然后按下回车键，可以看到生成了测试用例代码 【场景四：代码自动填充对话框】\n在编辑器中，选中一段代码，然后按下Ctrl+L快捷键，代码自动填充到了左侧的对话框中 可以在对话框中继续输入指令，如：请解释一下这段代码，然后按回车键提交大模型： 【其他场景】\n增加注释 代码异常堆栈分析 …… 其他请大家补充 最后： 使用开源大模型作为团队的 Code Copilot 工具，可以提高团队的开发效率，同时也可以保护团队的代码隐私。虽然目前开源的 LLM 相比闭源商用的 LLM 还有一些差距，但是随着开源 LLM 的不断发展，相信两者的差距以后会越来越小。以上就是今天介绍的内容，希望对大家有所帮助。\n关注本公众号，我们共同学习交流进步 👇🏻👇🏻👇🏻\nPhi-3 开源大模型\nPhi-3 模型手机部署教程（微软发布的可与 GPT-3.5 媲美的小模型）\nQwen2-7B 开源大模型\nQwen2 阿里最强开源大模型（Qwen2-7B）本地部署、API 调用和 WebUI 对话机器人\nLlama-3-8B 开源大模型\n玩转 AI，笔记本电脑安装属于自己的 Llama 3 8B 大模型和对话客户端\n一文彻底整明白，基于 Ollama 工具的 LLM 大语言模型 Web 可视化对话机器人部署指南\n基于 Llama 3 搭建中文版（Llama3-Chinese-Chat）大模型对话聊天机器人\nGLM-4-9B 开源大模型\n本地部署 GLM-4-9B 清华智谱开源大模型方法和对话效果体验\nChatTTS 文本转语音模型\nChatTTS 开源文本转语音模型本地部署、API 使用和搭建 WebUI 界面\nStable Diffusion 3 文生图模型\nStable Diffusion 3 文生图“开源英雄”大模型本地部署和使用教程，轻松实现 AI 绘图自由\n大模型应用案例\n大模型应用研发基础环境配置（Miniconda、Python、Jupyter Lab、Ollama 等）\n借助 AI 大模型，三分钟原创一部儿童故事短视频（附完整操作步骤）\n高效编写大模型 Prompt 提示词，解锁 AI 无限创意潜能\nPython 小游戏\nAI 已来，我与 AI 一起用 Python 编写了一个消消乐小游戏\nPython 游戏编程：一步步用 Python 打造经典贪吃蛇小游戏\n","date":"2024-06-29T00:00:00Z","image":"https://ntopic.cn/p/2024062901/02_hu1fa0aae459ee14b2d05b63839e018423_44026_120x120_fill_box_smart1_3.png","permalink":"https://ntopic.cn/p/2024062901/","title":"使用Llama3/Qwen2等开源大模型，部署团队私有化Code Copilot和使用教程"},{"content":"老牛同学之前使用的MacBook Pro电脑配置有点旧（2015 年生产），跑大模型感觉有点吃力，操作起来有点卡顿，因此不得已捡起了尘封了快两年的MateBook Pro电脑（老牛同学其实不太喜欢用 Windows 电脑做研发工作）。此文注意是记录配置新电脑的内容，一来给老牛同学留个备忘，同时也特别希望能给其他朋友一些帮助。\n配置一台方便用于大模型应用研发的新电脑，最基础的需要包括以下配置内容：\nMiniconda包管理工具的安装和配置（兼容pip） Python虚拟环境配置（指定 Python 版本且无需单独下载 Python 安装） Jupyter Lab Python 研发 WebIDE 配置 Ollama本地大模型管理软件的配置和应用 Ollama大模型 Web 界面对话客户端配置和使用 Miniconda 安装和配置 Miniconda和pip都是 Python 生态中的两个不同的包管理工具，它们都用于安装和管理 Python 包。但是在大模型相关的应用研发中，老牛同学推荐使用Miniconda的原因：\n包范围： Miniconda 通过 Conda 可以管理 Python 以及非 Python 包，而 pip 只管理 Python 包。 环境管理： Miniconda 内置了环境管理功能，而 pip 需要与其他工具（如 virtualenv 或 venv）搭配使用以创建隔离的环境。 包源： pip 通常从 PyPI 下载包，而 Conda 从 Anaconda Repository 或其他自定义设置的通道下载。 包格式： Conda 使用自己的包格式(.conda 或.tar.bz2)，而 pip 使用 wheel 或源码形式。 依赖解决： Conda 在安装包时会考虑到系统级别的依赖和包之间的依赖关系，而 pip 主要解决 Python 级别的依赖。 在实际使用中，由于Minionda可以很好地处理复杂的依赖关系和环境管理，它通常是首选工具。但是，如果只需要安装纯 Python 包，使用pip可能会更加简单直接。\n第一步： 下载地址：https://docs.anaconda.com/miniconda/\n根据操作系统，选择安装包，支持包括：Windows、MacOS 和 Linux 系统\n第二步： 安装和配置：安装过程和普通软件没有两样，安装完成之后，我们配置 Python 环境：\n老牛同学的安装目录是：D:\\Software\\miniconda3\n因后面需要执行conda命令，因此提前把以下目录添加到系统环境变量中（变量名：Path）：\n安装目录：D:\\Software\\miniconda3 脚本目录：D:\\Software\\miniconda3\\Scripts 依赖库目录：D:\\Software\\miniconda3\\Library\\bin 第三步： 设置 Conda 虚拟环境目录（可选）：默认情况下，虚拟环境内容在C:\\盘，老牛同学 C 盘比较较小，就把它设置到D:\\盘\n打开 Termianl 终端，查看Conda基本信息：conda info\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 \u0026gt; conda info active environment : None user config file : C:\\Users\\obull\\.condarc populated config files : conda version : 24.4.0 conda-build version : not installed python version : 3.12.3.final.0 solver : libmamba (default) virtual packages : __archspec=1=x86_64_v3 __conda=24.4.0=0 __win=0=0 base environment : D:\\Software\\miniconda3 (writable) conda av data dir : D:\\Software\\miniconda3\\etc\\conda conda av metadata url : None channel URLs : https://repo.anaconda.com/pkgs/main/win-64 https://repo.anaconda.com/pkgs/main/noarch https://repo.anaconda.com/pkgs/r/win-64 https://repo.anaconda.com/pkgs/r/noarch https://repo.anaconda.com/pkgs/msys2/win-64 https://repo.anaconda.com/pkgs/msys2/noarch package cache : D:\\Software\\miniconda3\\pkgs C:\\Users\\obull\\.conda\\pkgs C:\\Users\\obull\\AppData\\Local\\conda\\conda\\pkgs envs directories : D:\\Software\\miniconda3\\envs C:\\Users\\obull\\.conda\\envs C:\\Users\\obull\\AppData\\Local\\conda\\conda\\envs platform : win-64 user-agent : conda/24.4.0 requests/2.31.0 CPython/3.12.3 Windows/11 Windows/10.0.22621 solver/libmamba conda-libmamba-solver/24.1.0 libmambapy/1.5.8 aau/0.4.4 c/. s/. e/. administrator : False netrc file : None offline mode : False 可以看到几个重要信息：\nConda配置文件：C:\\Users\\obull\\.condarc Conda包下载渠道：channel URLs列表几个地址 Conda包缓存目录：D:\\Software\\miniconda3\\pkgs Conda虚拟环境目录：D:\\Software\\miniconda3\\envs 以上配置都是默认配置，其中包环境和虚拟环境目录比较占磁盘空间，可以设置为其他目录，同时下载渠道可以使用国内镜像以提升包的下载速度：\n打开配置文件：C:\\Users\\obull\\.condarc 若.condarc配置文件不存在，可以执行命令自动生成一个默认文件：conda config --set show_channel_urls yes 打开.condarc配置，设置虚拟环境目录和： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 envs_dirs: - D:/Software/miniconda3/pkgs pkgs_dirs: - D:/Software/miniconda3/envs show_channel_urls: true channels: - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/ - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/msys2/ - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge/ - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/ - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/ - defaults custom_channels: conda-forge: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud msys2: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud bioconda: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud menpo: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud pytorch: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud pytorch-lts: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud simpleitk: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud 保存配置文件之后，我们可以看到Conda信息的变化：conda info\nConda命令还有一些其他用法，比较常用的命令如下列表：\n查看Conda版本：conda --version 更新Conda版本：conda update conda 安装 Python 包（如安装numpy包）：conda install numpy 通过首选渠道下载包文件 指定渠道安装 Python 包：conda install conda-forge::numpy 通过conda-forge渠道下载包文件 安装 Python 包到指定的虚拟环境：conda install --name PY2.7 matplotlib 安装matplotlib包到指定的PY2.7虚拟环境 Python 虚拟环境配置 特别注意： 有了Miniconda包管理工具，我们无需单独下载和安装 Python，可直接通过 Conda 安装对应的版本即可。\n老牛同学创建一个名称为PY3.12的虚拟环境，使用 Python 版本为3.12.3：conda create --name PY3.12 python=3.12.3\n新建虚拟环境需要初始化基础包（包括 Python SDK 等），基础包下载完成之后，可以通过命令查看虚拟环境列表：conda info --envs\n1 2 3 \u0026gt; conda info --envs base D:\\Software\\miniconda3 PY3.12 D:\\Software\\miniconda3\\pkgs\\PY3.12 其中，base是Conda默认的虚拟环境，我们刚创建的PY3.12虚拟环境已经存在了！\nPython 虚拟环境使用 默认虚拟环境是base，我们可以激活和取消虚拟环境。若是首次使用，则需要执行conda init命令进行初始化：\n激活虚拟环境：conda activate PY3.12 取消虚拟环境：conda deactivate（无需指定环境名） 1 2 3 4 5 6 7 C:\\Users\\obull\u0026gt; C:\\Users\\obull\u0026gt;conda activate PY3.12 (D:\\Software\\miniconda3\\pkgs\\PY3.12) C:\\Users\\obull\u0026gt; (D:\\Software\\miniconda3\\pkgs\\PY3.12) C:\\Users\\obull\u0026gt;conda deactivate C:\\Users\\obull\u0026gt; Jupyter Lab 安装和配置（可选） 在 Python 研发 IDE 选择上，老牛同学推荐推荐使用Jupyter Lab，当然如果有PyCharm等 Python 开发工具，也是一个不错的选择：\nJupyter Lab 主要是为了数据科学、科学计算和教育而设计的。它支持交互式计算和数据可视化，非常适合探索性数据分析、机器学习、数值模拟等。 Jupyter Lab 提供了一个基于 Web 的用户界面，支持在浏览器中直接编写代码、运行分析，并查看结果。 Jupyter Lab 强调的是交互式编程和数据可视化。它允许用户逐段运行代码并即时查看输出，支持 Markdown 和富媒体，非常适合制作和展示研究结果。 Jupyter Lab 作为一个轻量级的 Web 应用，其启动速度快，但在处理大型数据集时，性能可能会受到浏览器和硬件资源的限制。 Jupyter Lab 特别适合做数据分析、数据科学教育、探索性研究和创建可分享的交互式报告。 Jupyter Lab IDE 可以通过Conda安装，其安装命令如下：\n1 2 3 4 5 6 # 激活Python虚拟环境 conda activate PY3.12 # 安装Jupyter Lab（指定下载源） conda install -c conda-forge jupyterlab conda install -c conda-forge ipywidgets Jupyter Lab IDE 成功安装之后，可以通过以下命令打开：\n1 2 3 4 5 6 7 8 # 激活Python虚拟环境 conda activate PY3.12 # 切换到Jupyter目录（我们以后代码存放的目录） cd ~/JupyterLab # 启动Jupyter WebIDE jupyter-lab . 浏览器自动打开了 Web IDE，或者自己打开：http://localhost:8888/lab\nOllama 安装和使用 使用Ollama可以非常方便的管理本地大模型，目前主流大模型都支持Ollama，包括Phi、Qwen、Llama等，因此使用Ollama可以提升我们管理和使用大模型效率：\n下载并安装Ollama：Ollama 官网（支持：Windows、Mac 和 Linux 系统） 设置模型数据文件路径（可选）：默认情况下，模型文件存放在C:盘用户目录。我们可以通过系统环境变量设置成其他目录。系统环境变量名为：OLLAMA_MODELS，系统环境变量的值为新的目录（如老牛同学设置为：D:\\ModelSpace\\Ollama） 下载并启动大模型，老牛同学以阿里的Qwen2-7B为例：ollama run qwen:7B 模型文件下载完成之后，自动就有了对话客户端：\n1 2 3 4 5 6 7 D:\\\u0026gt;conda activate PY3.12 (D:\\Software\\miniconda3\\pkgs\\PY3.12) D:\\\u0026gt;ollama list NAME ID SIZE MODIFIED qwen:7b 2091ee8c8d8f 4.5 GB 3 hours ago (D:\\Software\\miniconda3\\pkgs\\PY3.12) D:\\\u0026gt;ollama run qwen:7b Ollama Web 界面对话客户端 Ollama自带控制台聊天对话界面体验总归是不太好，接下来部署 Web 可视化聊天界面：\n下载并安装 Node.js 工具：https://nodejs.org/zh-cn 下载ollama-webui工程代码：git clone https://github.com/ollama-webui/ollama-webui-lite ollama-webui 切换ollama-webui代码的目录：cd ollama-webui 设置 Node.js 工具包镜像源（下载提速）：npm config set registry http://mirrors.cloud.tencent.com/npm/ 安装 Node.js 依赖的工具包：npm install 最后，启动 Web 可视化界面：npm run dev 如果看到以上输出，代表 Web 可视化界面已经成功了！\n浏览器打开 Web 可视化界面：http://localhost:3000/\nOllama其他的命令工具：\n1 2 3 4 5 6 7 8 9 10 11 # 查看当前Ollama的模型 ollama list # 增量更新当前部署的模型 ollama pull qwen:7b # 删除一个模型文件 ollama rm qwen:7b # 复制一个模型 ollama cp qwen:7b Qwen-7B OllamaAPI 结果返回\n1 2 3 4 curl http://localhost:11434/api/generate -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;qwen:7b\u0026#34;, \u0026#34;prompt\u0026#34;:\u0026#34;为什么天空是蓝色的？\u0026#34; }\u0026#39; Ollama API聊天对话\n1 2 3 4 5 6 curl http://localhost:11434/api/chat -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;qwen:7b\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;为什么天空是蓝色的？\u0026#34; } ] }\u0026#39; 关注本公众号，我们共同学习进步 👇🏻👇🏻👇🏻\nPhi-3 开源大模型\nPhi-3 模型手机部署教程（微软发布的可与 GPT-3.5 媲美的小模型）\nQwen2-7B 开源大模型\nQwen2 阿里最强开源大模型（Qwen2-7B）本地部署、API 调用和 WebUI 对话机器人\nLlama-3-8B 开源大模型\n玩转 AI，笔记本电脑安装属于自己的 Llama 3 8B 大模型和对话客户端\n一文彻底整明白，基于 Ollama 工具的 LLM 大语言模型 Web 可视化对话机器人部署指南\n基于 Llama 3 搭建中文版（Llama3-Chinese-Chat）大模型对话聊天机器人\nGLM-4-9B 开源大模型\n本地部署 GLM-4-9B 清华智谱开源大模型方法和对话效果体验\nChatTTS 文本转语音模型\nChatTTS 开源文本转语音模型本地部署、API 使用和搭建 WebUI 界面\nStable Diffusion 3 文生图模型\nStable Diffusion 3 文生图“开源英雄”大模型本地部署和使用教程，轻松实现 AI 绘图自由\n大模型应用案例\n借助 AI 大模型，三分钟原创一部儿童故事短视频（附完整操作步骤）\n高效编写大模型 Prompt 提示词，解锁 AI 无限创意潜能\nPython 小游戏\nAI 已来，我与 AI 一起用 Python 编写了一个消消乐小游戏\nPython 游戏编程：一步步用 Python 打造经典贪吃蛇小游戏\n","date":"2024-06-25T00:00:00Z","image":"https://ntopic.cn/p/2024062501/00_hue8f2b43a6b2bdcc173d28e82d448d264_1557957_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/2024062501/","title":"大模型应用研发基础环境配置（Miniconda、Python、Jupyter Lab、Ollama等）"},{"content":"前面几篇博文，老牛同学和大家一起在个人电脑部署了Qwen2、GLM4、Llama3、ChatTTS和Stable Diffusion等 LLM 大模型，也通过 API 和 WebUI 的方式完成了体验。\n但是这些大模型因为部署在个人电脑本地，不能够随时携带。如果能在手机上部署大模型的话，老牛同学感觉很有意义，手机与我们的生活更为密切相关，并且手机上也有大量的个人数据，与大模型交互起来也更加方便。同时，在手机上跑个大模型，还是很酷！\n老牛同学期望能通过本文，和大家一起完成这项很酷且有意义的事情。老牛同学用的是小米 10 Pro手机，其配置参数如下：\n选择 Phi-3 微软开源模型 受限于手机 CPU 和内存等硬件配置，我们要选择小语言模型（SLM）。其中，阿里开源了Qwen2-0.5B和Qwen2-1.5B两款小尺寸模型，微软了开源Phi-3 Mini（3.8B）和Phi-3 medium（14B）两款尺寸模型。\n由于之前我们在笔记本部署了Qwen2-7B大模型，本次我们就在手机部署Phi-3 Mini模型，顺便也体验一下不同科技公司的大模型产品，其效果可以媲美GPT-3.5大模型：\nQwen2-7B本地部署：Qwen2 阿里最强开源大模型（Qwen2-7B）本地部署、API 调用和 WebUI 对话机器人\n手机配置 Linux 环境（Termux 应用） 小米等安卓手机的基于 Linux 内核的操作系统，但是我们无法像在 Linux 那样执行 Linux 命令，因此我首先得配置一下 Linux 环境。\nTermux是一个 Android 的终端模拟器，可以在 Android 设备上运行 Linux 命令和工具。Termux的 Android APP 可通过官网下载并安装：https://github.com/termux/termux-app/releases\n当前官网的最新稳定版本：v0.118.1 - 2024-06-18 00.05\n请根据手机情况，下载对应的 apk 文件。老牛同学下载的 apk 文件：termux-app_v0.118.1+github-debug_universal.apk\napk 安装成功后，打开Termux应用后，默认展示如下，就可以开始输入 Linux 命令了：\n手机上安装 Linux 操作系统 首先，我们安装proot-distro系统管理工具，proot-distro可以非常方便在 Termux 中安装、卸载和运行Linux 的发行版本（包括：Ubuntu、Debian、Arch Linux 等）：\n友情提示： 在手机中输入以下命令效率比较低，我们可以把命令发到微信，然后一条一条复制粘贴！\n1 pkg install proot-distro proot-distro 安装成功之后，我们安装Debian操作系统：\n1 proot-distro install debian 最后，登录新安装的Debian操作系统：\n1 proot-distro login debian 登录之后，自动启动了Shell命令行终端：root@localhost:~#\n可以执行相关的 Linux 命令了：\n通过 Termux 安装 Phi-3 模型 通过上面的操作，我们已经在手机上安装好了Debian操作系统，接下来在Debian操作系统中安装Phi-3 Mini模型。\n第一步： 在Debian系统中安装Ollama软件，没错，就是之前在个人电脑部署Llama3、Qwen2等大模型时，用于管理本地大模型的Ollama软件。由于我们在手机上安装了Debian系统，那么和电脑一样，Ollama也可以管理Debian系统本地部署的大模型：\n1 2 # 安装Ollama软件 curl -fsSL https://ollama.com/install.sh | sh Ollama安装成功输出信息如下：\n第二步： 通过后台启动Ollama服务：nohup ollama serve \u0026amp;\n我们用Ollama命令，查看 Ollama 信息，如：\n查看版本：ollama -v 查看本地模型列表：ollama list（目前还没有部署模型，因此结果列表为空） 第三步： 通过Ollama安装并启动Phi-3 Mini模型：ollama run phi3:mini\nPhi-3 Mini模型文件总大小为2.4GB左右，因此下载需要一点时间：\n到此，Phi-3 Mini模型部署成功，我们可以体验手机上的大模型，比如：请用 100 个汉字解释一下，天空为什么是蓝色的？\n总结 相比电脑端，手机的算力弱太多，就算老牛同学用的是最小尺寸的模型（Phi-3 Mini），其推理的速度还是慢得多。其内容的输出速度，比我手机打字都要慢。盯着屏幕，看着模型一个字一个字的输出，感觉也挺有趣 😁\n目前 AI 是大热门，各大公司推出的大模型参数一个比一个大，能力一个比一个厉害。但大模型训练和推理成本均比较高昂，在很大程度上限制了其发展，因此大模型 AI 应用相对较少，或者说对我们生活影响还很小，因此其还有很大的发展空间。\n而反观针对特定业务场景定制的小模型（比如Phi定制等），其成本就低得多，就能更有效地应用于各种垂直场景。\n老牛同学觉得这种“小而美”的 AI 模型将会越来越多，也会越来越受欢迎！\n关注本公众号，我们共同学习进步 👇🏻👇🏻👇🏻\nQwen2-7B 开源大模型\nQwen2 阿里最强开源大模型（Qwen2-7B）本地部署、API 调用和 WebUI 对话机器人\nLlama-3-8B 开源大模型\n玩转 AI，笔记本电脑安装属于自己的 Llama 3 8B 大模型和对话客户端\n一文彻底整明白，基于 Ollama 工具的 LLM 大语言模型 Web 可视化对话机器人部署指南\n基于 Llama 3 搭建中文版（Llama3-Chinese-Chat）大模型对话聊天机器人\nGLM-4-9B 开源大模型\n本地部署 GLM-4-9B 清华智谱开源大模型方法和对话效果体验\nChatTTS 文本转语音模型\nChatTTS 开源文本转语音模型本地部署、API 使用和搭建 WebUI 界面\nStable Diffusion 3 文生图模型\nStable Diffusion 3 文生图“开源英雄”大模型本地部署和使用教程，轻松实现 AI 绘图自由\n大模型应用案例\n借助 AI 大模型，三分钟原创一部儿童故事短视频（附完整操作步骤）\n高效编写大模型 Prompt 提示词，解锁 AI 无限创意潜能\nPython 小游戏\nAI 已来，我与 AI 一起用 Python 编写了一个消消乐小游戏\nPython 游戏编程：一步步用 Python 打造经典贪吃蛇小游戏\n","date":"2024-06-23T00:00:00Z","image":"https://ntopic.cn/p/2024062301/00_hub4c93f99d8ebc3968cf2dbd70f526bf5_328647_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/2024062301/","title":"Phi-3 模型手机部署教程（微软发布的可与GPT-3.5媲美的小模型）"},{"content":"AI资讯 「网红」周鸿祎，要为 AI 带货 突发！OpenAI收购数据公司 盘古5.0重磅发布！华为云大模型年度杀招来了，人形机器人现场整活 GPT-4o一夜被赶超！Anthropic推出Claude 3.5，网友3分钟克隆马里奥游戏 中国人自己的操作系统！余承东掏出纯血鸿蒙，华为AI大招硬刚苹果 Claude3.5突然发布！GPT-4o不香了 无论真实还是AI视频，「摩斯卡」都能重建恢复4D动态可渲染场景 抢疯了，腾讯给大模型人才，定了一个前所未有的标准 华为发布会杀疯了：盘古大模型跳级发布，编程语言仓颉首次亮相 美政府再发AI禁令！限制美国人对华AI技术和产品投资 「网红」周鸿祎，要为 AI 带货 360集团创始人周鸿祎再次成为公众焦点，通过推出AI搜索、AI浏览器及AI甄选平台，旨在重塑国民级产品搜索与浏览器领域，并助力优秀中国AI产品的普及。周鸿祎强调不做“网红”，而是打造企业家IP以推广360品牌和优质中国产品。在AI战略中，360选择既做模型又做有强场景属性的应用，同时搭建桥梁连接用户与AI工具。其AI搜索采用多轮搜索技术，优化用户体验，并提供丰富多媒体内容及个性化服务；AI浏览器则升级为一站式AI问题解决工具箱，内置划重点、提炼简介等功能，支持视频全文生成、双语观看、网页转PPT等高效操作。此外，360AI甄选平台汇集了360自家产品、知名公司应用以及独立开发者作品，旨在降低普通用户接触AI的门槛，并通过周鸿祎个人流量为这些优秀AI工具提供曝光机会。此举不仅推动了AI在个人和中小企业端的应用发展，也为中国AI应用生态提供了重要支持。综上所述，360集团通过创新产品和服务，致力于成为新时代AI入口与普及者，旨在让普通人以低成本接触并使用AI技术，同时扶持国内AI开发者与应用生态。（报道详情）\n突发！OpenAI收购数据公司 OpenAI宣布收购实时分析数据库公司Rockset，以增强其产品中对实时数据的利用和访问能力，目标是支持更复杂的应用如实时推荐、动态聊天机器人、实时监控等，并扩大AI技术应用范围。收购旨在使AI更好地理解和操作数据，通过Rockset的技术实现数据分析速度的提升及成本降低。Rockset成立于2016年，提供快速搜索和分析数据库服务，曾获得多轮融资总额逾1亿美元。收购后，Rockset团队将加入OpenAI，共同推动AI与实时数据处理的融合创新。（报道详情）\n盘古5.0重磅发布！华为云大模型年度杀招来了，人形机器人现场整活 华为发布盘古大模型5.0，主打“云+端”独有策略及行业难题解决能力。该模型实现参数规格全覆盖，从十亿级到万亿级，支持多模态理解和生成，展现强思维与复杂推理能力。华为云同步发布了六大行业模型，如盘古钢铁、高铁、具身智能等，并提供升级服务，包括昇腾AI云、ModelArtsstudio、CodeArts等生产线的优化，以及首次揭秘的技术点，旨在打造“世界模型”，全面重塑云服务，助力解决行业难题。（报道详情）\nGPT-4o一夜被赶超！Anthropic推出Claude 3.5，网友3分钟克隆马里奥游戏 一夜之间，Anthropic推出了新一代大模型Claude3.5Sonnet，在多项全球权威测评中超越了OpenAI的GPT-4o。Claude在推理、知识、编码能力及速度成本方面表现优异，刷新行业基准。用户可通过Claude网页和iOS程序免费试用，付费用户能更高速访问。AnthropicAPI、AmazonBedrock和GoogleCloud提供该模型，收费标准合理。Claude3.5Sonnet立即引起轰动，网友称其编程效率比GPT-4o高10倍，实现马里奥游戏克隆版仅需3分钟。Anthropic被视为OpenAI的有力竞争对手，Claude系列新品发布是对GPT-4o的直接挑战。（报道详情）\n中国人自己的操作系统！余承东掏出纯血鸿蒙，华为AI大招硬刚苹果 纯血鸿蒙，华为HarmonyOSNEXT，中国自研操作系统新里程碑；余承东称其为中国内核制造历史转折点，10年成就欧美同行30年的成果。系统全面升级，性能提升30%，分布式软总线技术优化连接速度与功耗。强调人机交互、个性化专属和全场景智能，AI“核弹”HarmonyIntelligence震撼发布，AI生图、修复、多模态助手等能力嵌入系统，现场反响热烈。华为鸿蒙原生应用进入冲刺阶段，Beta版正式面向开发者启动，集结号吹响。生态设备数量已破9亿台，涵盖手机、平板、电视及车机等领域。终端AI技术成熟，NPU智能手机早于2017年发布，小艺用上大模型。华为发布自研编程语言“仓颉”，全场景、高性能、高安全特点明显，并提供AI辅助编程工具给开发者使用。全球首个端侧AI手机公司，代码行数达到1.1亿，华为贡献6200多万行。鸿蒙生态开发者数量已达254万人，OpenHarmony代码行数超过1.1亿，支持900多个服务意图框架，覆盖重点垂直领域的200多种意图。AI声音修复功能为语言障碍人群带来便利，小艺智能体知识量达万亿tokens，记忆感知场景超23个，推理成功率高。华为鸿蒙系统面向全场景智能操作系统升级，强调与苹果的对比，展现出遥遥领先的优势。5.5G商用元年到来，华为作为核心推动者和建设者。AI大模型赋能终端，万物互联时代全球链接设备数量将达2000亿部。HarmonyOSNEXT实现一个系统统一生态，手机、平板、手表及各类IoT设备皆可使用。鸿蒙原生智能强调新交互、个性化专属与全场景智能，与苹果“五大AI原则”有共通之处，如强调个性化和广泛适用性。华为承诺2024年第四季度正式版落地华为产品线。此次开发者大会，华为不仅展示了强大的自研能力，还发布了面向未来的操作系统创新和技术突破，展现出在AI领域的深度探索和应用，为全行业带来了新的期待与挑战。（报道详情）\nClaude3.5突然发布！GPT-4o不香了 Anthropic发布了Claude3.5Sonnet，首个3.5系列版本，在关键指标上超越GPT-4o，速度是前版两倍，成本仅五分之一。免费试用网页端和iOS程序，Pro订阅者享受更高速率限制。API价格：每百万输入Tokens3美元，输出Tokens15美元，200KTokens上下文窗口。年内还将发布Claude3.5Haiku和Claude3.5Opus。为安全测试严格把关。用户反馈代码能力惊艳，文本处理达研究生水平推理、本科生知识和编码标准，视觉能力最强，所有任务均超越前版。新功能Artifacts提供动态工作区，实时生成文档、代码、图像等，标志Claude从对话AI向协作工作环境进化。（报道详情）\n无论真实还是AI视频，「摩斯卡」都能重建恢复4D动态可渲染场景 本文介绍了一种名为MoSca的新系统，旨在从单目视频中重建可渲染的动态场景。MoSca利用神经信息处理技术，无需额外信息，能生成三维动态模型，适用于互联网上的各种视频内容。该系统克服了真实拍摄二维视频缺乏多视角信息和动态场景自由度高带来的挑战。MoSca采用了一种四维运动脚手架表示方法，通过融合基石模型输出的深度估计、长时间跟踪、光流估计等数据，简化复杂度高的动态变形问题。这种方法允许从单目视频中高效地恢复出物理世界的信息，并进行全局融合观测。实验结果显示，MoSca在多种数据集上表现优异，特别是在重建动态场景时展现出优越性能，支持高质量渲染并优于其他方法。（报道详情）\n抢疯了，腾讯给大模型人才，定了一个前所未有的标准 科技公司竞争核心在于持续吸引顶尖AI人才，尤其是生成式AI领域的专家。OpenAI的成功归因于多年的技术探索、资金支持以及聚集的顶级AI研究人员。国内同样涌现了一批怀揣理想和技术热情的专业人士，致力于中文原生大模型的研发和优化，力求在世界范围内领先。以高研为例，他从学术研究转向实际应用，在腾讯参与并推动了混元大模型项目的研发，实现了文生图能力的重大突破，开源的混元-DiT模型在视觉生成效果上显著提升。傅志远则将AI应用于游戏体验优化，通过强化学习和多智能体系统的研究，改进游戏功能，并提高用户体验。王艾文专注于AIforScience领域，特别是在蛋白质组学研究中取得了重要成果，为科学发现提供了新视角。腾讯青云计划加大了对大模型领域人才的招募力度，强调长期价值，提供极具竞争力的条件吸引全球顶尖学子，旨在推动中国在大模型技术领域的快速发展和创新。整体而言，科技公司通过培养和吸纳AI人才，促进技术创新与应用落地，不仅推动了自身的发展，也助力国家层面的技术实力提升。（报道详情）\n华为发布会杀疯了：盘古大模型跳级发布，编程语言仓颉首次亮相 华为发布盘古大模型5.0，涵盖十亿、百亿、千亿、万亿四种规模，强调多模态和强思维能力。现场演示搭载盘古的人形机器人完成复杂任务与学习场景，并应用于自动驾驶领域，生成符合物理规律的训练视频。鸿蒙系统也采用最新盘古大模型，增强智慧助手小艺的多模态能力。此外，华为揭秘盘古背后技术细节，包括数据、架构π和训练方法。盘古5.0四大升级点：覆盖不同应用需求、提升多模态理解与生成、强化思维链与策略搜索、在工业设计与建筑设计等领域展现广泛应用。鸿蒙系统进行全面升级，采用全新端云垂直整合架构，性能显著提升，并推出仓颉编程语言，面向全场景应用开发。（报道详情）\n美政府再发AI禁令！限制美国人对华AI技术和产品投资 美国财政部宣布新禁令，禁止美国人对特定AI系统及使用特定计算能力的终端用途进行投资，即使在开发未被禁止的AI系统或芯片相关交易时也需上报。此政策旨在应对国家安全技术与产品上的投资，并针对半导体、量子信息技术和人工智能领域实施监管。具体细则将在8月4日后公布。该禁令是去年拜登签署的第14105号行政命令的执行，旨在限制中国在特定技术领域的投资活动，以防止资金流入可能威胁美国安全的技术发展。违反规定者将面临刑事和民事处罚，并可能撤销相关投资。（报道详情）\n关注本公众号，我们共同学习进步👇🏻👇🏻👇🏻\nQwen2-7B 开源大模型\nQwen2 阿里最强开源大模型（Qwen2-7B）本地部署、API调用和WebUI对话机器人\nLlama-3-8B 开源大模型\n玩转 AI，笔记本电脑安装属于自己的 Llama 3 8B 大模型和对话客户端\n一文彻底整明白，基于 Ollama 工具的 LLM 大语言模型 Web 可视化对话机器人部署指南\n基于Llama 3搭建中文版（Llama3-Chinese-Chat）大模型对话聊天机器人\nGLM-4-9B 开源大模型\n本地部署GLM-4-9B清华智谱开源大模型方法和对话效果体验\nChatTTS 文本转语音模型\nChatTTS 开源文本转语音模型本地部署、API使用和搭建WebUI界面\nStable Diffusion 3 文生图模型\nStable Diffusion 3 文生图“开源英雄”大模型本地部署和使用教程，轻松实现AI绘图自由\n大模型应用案例\n借助AI大模型，三分钟原创一部儿童故事短视频（附完整操作步骤）\n高效编写大模型 Prompt 提示词，解锁 AI 无限创意潜能\nPython 小游戏\nAI已来，我与AI一起用Python编写了一个消消乐小游戏\nPython游戏编程：一步步用Python打造经典贪吃蛇小游戏\n","date":"2024-06-22T00:00:00Z","image":"https://ntopic.cn/p/ai20240622/00_hu4c5979059c1785fc93bc1b6be5c73863_14050_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/ai20240622/","title":"[AI资讯·0622] Claude3.5超越GPT-4o，360推出AI搜索，OpenAI收购Rockset，华为发布大模型"},{"content":"老牛同学在前面的几篇博文中，和大家一起在个人电脑本地部署和体验了ChatTTS、Qwen2、GLM4和Llama3等大模型，由于是个人电脑部署，只能在局域网内使用，无法分享到外网。今天我们通过ngrok软件，免费把我们的大模型应用分享出去。\n什么是内网穿透 运营商（如：电信、联通等）拉了一根网线到家里路由器并接入互联网（比如IP是：10.25.12.6，它是公网IP），我们个人电脑通过路由器间接接入互联网（比如IP是：192.168.3.27，它是局域网IP），这个通过路由器建立的网络，也就局域网或者内网。外网通过公网IP是无法访问到局域网中的某台计算机，也就是我们个人电脑上部署的应用无法通过外网分享出去。\n如果我们在局域网和外网之间，增加一个桥接器，它的作用是类似于链接局域网和外网的通道，外网可通过这通道直接访问到局域网的服务器，这个操作也就是内网穿透。\n今天我们要介绍的ngrok正是这个桥接通道：我们首先在个人电脑通过ngrok客户端链接ngrok服务器，把我们端口（即应用程序）和ngrok某个域名建立链接；外网访问ngrok的域名，即访问ngrok服务器，服务器请求下发给客户端，即可以实现互相通信。\n内网穿透的应用场景 一般情况下，我们想发布我们应用到外网，需要以下步骤：\n在外网有一台服务器（自己拉网线建机房、或者买服务器、或者租云服务器等） 服务器上安装操作系统，搭建应用程序的运行环境 在服务器上部署应用程序 这一通操作下来，费用和时间成本都比较高。如果我们只是想短期内把应用程序分享到外网的话，就需要内网穿透了：\n外网临时访问，无长期访问需求，如ChatTTS、Qwen2、GLM4和Llama3等我们大模型临时演示 外面访问的域名无要求，域名只作为每次访问的入口即可 访问速度要求不高、并发较小，因为注意是应用功能体验 ngrok配置内网穿透（临时域名） 第一步： 登录ngrok官网（https://ngrok.com/download），下载客户端：\n老牛同学是Mac电脑，可通过brew直接安装包：brew install ngrok/ngrok/ngrok\n安装完成之后，我们就可以用ngrok命令进行后续操作了。\n第二步： 登录或者注册ngrok账号（https://dashboard.ngrok.com/login），获取授权码：\n第三步： 登录ngrok后台（https://dashboard.ngrok.com/get-started/your-authtoken），获取和配置ngrok授权码 ngrok config add-authtoken 授权码\n在Your Authtoken页面，我们可以看到当前授权码、2种配置方式和重置授权码等操作！\n第四步： 启动应用程序（老牛同学以ChatTTS文本转语音大模型为例）：python ChatTTS-WebUI.py\nChatTTS 开源文本转语音模型本地部署、API使用和搭建WebUI界面\n1 2 3 # 前面输出省略 Running on local URL: http://127.0.0.1:8089 # 后面输出省略 启动成功，可以看到应用程序的端口号：8089（端口号在接下来启动ngrok需要用到）\n浏览器打开WebUI：http://127.0.0.1:8089，确认ChatTTS正常启动\n第五步： 启动ngrok客户端，对外分享ChatTTS大模型应用程序：\n上面启动ChatTTS是Web应用（即http协议），且端口号是8089，我们打开Terminal终端，启动ngrok客户端：\n1 ngrok http http://127.0.0.1:8089 可以看到，ngrok客户端，把我们内网的端口，映射到一个临时域名：https://7715-112-64-61-112.ngrok-free.app\n到此，我们就可以在外网通过这个临时域名，来访问我们内网ChatTTS大模型！\n每次打开这个域名，均会有如下确认页面，直接点击Visit Site即可，我们也可以通过底部的3种方式消除这个确认页面：\nngrok固定域名内网穿透 通过上面的步骤，我们可以通过临时域名，实现了内网穿透。每次启动ngrok客户端，临时域名都会发生变化，我们也可以进一步通过固定域名实现内网穿透，提升外网访问体验。\n第一步： 申请免费的固定域名（每个用户可申请一个免费固定域名），登录ngrok后台，选择Domains菜单；然后点击**+ New Domain**按钮，ngrok分配一个固定的域名，一般为：xxxx.ngrok-free.app格式：\n第二步： 首先终止临时域名启动的客户端，然后通过固定域名启动ngrok客户端：ngrok http --domain=xxxx.ngrok-free.app 8089\n最后，我们可以在外网通过固定域名访问内网应用程序了！\nngrok内网穿透的用法 上面章节，我们通过ngrok把ChatTTS的WebUI分享到了外网，除了http协议之外，ngrok还可以支持TLS和TCP以下协议：\n1 2 3 4 5 # TLS ngrok tls 80 --terminate-at edge --domain your-name.ngrok.app # TCP ngrok tcp 22 欢迎大家尝试，后续若有对应的使用场景，老牛同学在进行分享！\n关注本公众号，我们共同学习进步👇🏻👇🏻👇🏻\nQwen2-7B 开源大模型\nQwen2 阿里最强开源大模型（Qwen2-7B）本地部署、API调用和WebUI对话机器人\nLlama-3-8B 开源大模型\n玩转 AI，笔记本电脑安装属于自己的 Llama 3 8B 大模型和对话客户端\n一文彻底整明白，基于 Ollama 工具的 LLM 大语言模型 Web 可视化对话机器人部署指南\n基于Llama 3搭建中文版（Llama3-Chinese-Chat）大模型对话聊天机器人\nGLM-4-9B 开源大模型\n本地部署GLM-4-9B清华智谱开源大模型方法和对话效果体验\nChatTTS 文本转语音模型\nChatTTS 开源文本转语音模型本地部署、API使用和搭建WebUI界面\nStable Diffusion 3 文生图模型\nStable Diffusion 3 文生图“开源英雄”大模型本地部署和使用教程，轻松实现AI绘图自由\n大模型应用案例\n借助AI大模型，三分钟原创一部儿童故事短视频（附完整操作步骤）\n高效编写大模型 Prompt 提示词，解锁 AI 无限创意潜能\nPython 小游戏\nAI已来，我与AI一起用Python编写了一个消消乐小游戏\nPython游戏编程：一步步用Python打造经典贪吃蛇小游戏\n","date":"2024-06-22T00:00:00Z","image":"https://ntopic.cn/p/2024062201/00_hua04b28422bbb4c6a12cc5f607adbb1af_216292_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/2024062201/","title":"ngrok内网穿透，实现外网访问ChatTTS/Qwen2等大模型应用"},{"content":"AI资讯 陕西推出AI千亿级发展计划，五大产业集群，智算超3000P 试了快手的视频AI，竟然有点领先 “技术故障”背刺巴菲特，金融大模型到底靠不靠谱？ Gemini视频推理遥遥领先GPT-4o，首个视频多模态基准Video-MME 国产视频大模型PixVerse发布运动笔刷，网友：效果超Runway AI研究的主要推动力是什么？ChatGPT团队科学家：算力成本下降 LLM最全「怪癖」首曝光！马里兰OpenAI等30+学者祭出75页提示报告 大模型「幻觉」全无？图神经网络成破解核心，精准预测因果消除「幻觉」 为什么你的 iPhone，肯定用不上「苹果 AI」？ 答案抽取正确率达96.88%，xFinder断了大模型「作弊」的小心思 3D 版 SORA 来了！DreamTech 推出全球首个原生 3D-DiT 大模型 中国版Sora级视频大模型发布，打造“视频-Native”超级应用 陕西推出AI千亿级发展计划，五大产业集群，智算超3000P 陕西省推出《加快推动人工智能产业发展实施方案（2024-2026年）》，旨在通过建设产业园、突破核心技术、打造产业集群等措施，实现人工智能产业的高质量发展。目标包括建设多个产业园区、引入国家级重点项目、新增制造业企业通过DCMM贯标认证等。方案聚焦算力供给、数据集成与大模型布局，实施“强基、创智、赋智、聚智”四大行动，以提升技术底座、培育主体、优化生态，并将陕西打造为具有重要影响力的人工智能产业聚集地。具体措施涉及强化算力、扩大数据供应、布局通用和行业大模型等，同时推动创新产品研发与成果转化，实现制造业全流程智能化升级。方案还规划建设产业集聚区，培育优势企业，并提供保障措施，包括统筹协调机制、试点示范推广、完善支撑体系及强化安全保障，以促进人工智能与实体经济深度融合，赋能新型工业化发展。（报道详情）\n试了快手的视频AI，竟然有点领先 快手新AI模型\u0026quot;可灵\u0026quot;在外网火了，与Sora同台竞技，表现亮眼。在蚂蚁爬行、拉力赛车等场景中，可灵生成视频质量高，细节处理优于Sora。此外，可灵对中国元素理解更佳，适合中国用户。LumaAI同样能生成高质量两分钟视频，并有“电影感”和图片+提示词生成视频的功能。两款AI在吃播、电影感、物理世界理解等方面各有千秋，但快手的可灵AI在效果稳定性上表现更优。快手通过模仿Sora的技术路线并采用DiT架构提升模型性能，在视频数据丰富的背景下，实现了较好的效果。然而，商业化问题成为AI发展的瓶颈，大部分厂商难以找到盈利模式，高昂的成本和会员收费并未覆盖成本。尽管如此，快手凭借自身平台优势，可灵AI有潜力探索新的商业模式，如“发帖助手”等定位可能带来流量和收入。本文对两款AI进行了详细对比，并分析了AI发展面临的商业化挑战及快手的潜在机会。（报道详情）\n“技术故障”背刺巴菲特，金融大模型到底靠不靠谱？ 金证科技携手英特尔打造金融领域大模型推理方案，通过组合式AI（大模型+小模型+工具）解决金融业务需求，K-GPT等大模型在特定任务中展现优势，同时优化成本与资源利用。英特尔至强®CPUMax系列处理器提供高带宽内存和内置英特尔®高级矩阵扩展引擎，大幅提高计算性能，支持高效推理和大规模矩阵运算。金证通过与英特尔合作，实现硬件、软件优化的深度融合，为金融行业应用大模型树立标杆，助力金融机构数字化转型，并在科技节中展示了AI技术的实际应用前景。（报道详情）\nGemini视频推理遥遥领先GPT-4o，首个视频多模态基准Video-MME OpenAI和谷歌发布的新技术将AI视频推理推向新高度，但缺少全面评估大模型视频推理能力的标准。为弥补这一空白，Video-MME基准应运而生，全面评估多模态大模型的综合视频理解能力，并得到业界认可。Gemini1.5Pro在该榜单中表现突出，在视频理解和处理上占据主导地位，其性能超越了GPT-4o和GPT-4V/o等其他模型。Video-MME基准特点包括：覆盖不同长度、类型和模态的视频数据集，采用全人工标注确保高质量；评估时间维度广泛性、数据模态丰富性和视频类型的多样性。实验结果显示Gemini1.5Pro在长视频理解上表现优异，并支持音频输入，优于其他开源模型如VILA-1.5。尽管当前多模态大模型在长视频理解方面仍有进步空间，Video-MME基准的推出为评估和改进这些模型提供了重要工具。（报道详情）\n国产视频大模型PixVerse发布运动笔刷，网友：效果超Runway MagicBrush运动笔刷工具因其能够精准控制视频元素运动方式、提升用户操作灵活性和可控性，在AI视频社区备受关注。该工具允许用户通过涂抹区域和绘制轨迹来调整视频中的物体动作，类似“神笔马良”的效果，实现在生成视频时如同修图般的精细操控。PixVerse是继Runway之后第二家发布类似功能的AI视频生成公司，其MagicBrush运动笔刷在多主体控制、画面含义理解等方面表现出色。用户能通过该工具实现多目标精准移动、遵循物理规律创造生动自然的场景，并且比Runway更灵活地绘制运动轨迹和自定义方向与距离。爱诗科技CEO王长虎在智源大会上介绍，PixVerse采用多种技术路线探索文生视频模型生成，包括Diffusion+Unet架构以及DiT架构。同时，公司着重于提升视频可控性，通过角色一致性（C2V）功能优化图像保真度和美学质量，并研发MagicBrush网络结构以简化交互层面并提高运动控制精度。爱诗科技专注于解决用户实际需求，如推出C2V功能实现连续、可控的视频生成，以及MagicBrush运动笔刷功能。这些创新使得PixVerse在竞争激烈的AI视频生成市场中脱颖而出，吸引了大量用户的积极反馈和使用，其产品用户体验和效果得到显著提升。（报道详情）\nAI研究的主要推动力是什么？ChatGPT团队科学家：算力成本下降 AI研究的主要推动力是计算成本呈指数级下降及规模扩展。HyungWonChung通过分析Transformer的发展历史，阐述了编码器-解码器与仅解码器架构之间的差异及其对AI研究的意义。找到主要推动力后，理解它对于预测AI未来至关重要。Chung强调回顾过去架构的重要性，旨在提供一个统一视角，揭示哪些结构可能最终被规模扩展取代。他指出AI社区在添加结构方面做得很好，但在移除结构方面还需更多关注。计算成本的下降推动了AI研究的发展，使得更少结构的模型更具扩展性。通过理解历史中的关键结构选择及其原因，可以更好地洞察从过去到现在的变化，并据此预测未来方向。（报道详情）\nLLM最全「怪癖」首曝光！马里兰OpenAI等30+学者祭出75页提示报告 大语言模型(LLM)的提示技术研究揭示了其行为的怪异特性：重复内容可显著提高性能，而匿名化人名则导致准确性下降。马里兰大学等12所机构的30多位研究人员对LLM提示进行了大规模系统研究，发布了一篇详尽报告，覆盖4,797条记录筛选出1,565篇相关论文。报告中指出，奇奇怪怪的大语言模型在生成式AI行业中存在一些未被充分理解的现象。研究发现了以下现象：-重复内容：某些提示中的重复信息能显著提升LLM的性能。-包含人名：在提示中提及具体人物名字对准确性有重要影响。-示例选择与顺序敏感性：示例的选择和排列顺序对LLM的表现至关重要，甚至可能使准确率大幅波动。此外，研究还探讨了代码辅助推理、文本提示技术分类（如少样本学习、零样本推理等）、多语言和多模态提示方法。报告提出了一种全面的分类框架，并强调了在设计提示时的关键决策点。研究提出了提示工程过程，包括数据集上的推理、性能评估与提示模板修改三个步骤。还回顾了用于自动优化提示的技术策略，如答案工程，以及针对标注任务的LLM输出注释结果分析。报告总结了最常用的提示技术，指出少样本学习、零样本推理、高质量上下文提示示例和自洽等方法使用频率较高。同时，多模态提示也得到了发展，涵盖了图像、视频等不同形式的数据处理策略。这项研究为理解LLM的提示技术提供了全面视角，并揭示了这些模型在特定条件下的行为模式与敏感点。（报道详情）\n大模型「幻觉」全无？图神经网络成破解核心，精准预测因果消除「幻觉」 AI初创公司Alembic宣布推出全新AI系统，彻底解决了大模型生成虚假信息的问题，实现了消除\u0026quot;幻觉\u0026quot;的目标。该系统能够在企业数据集中识别随时间变化的因果关系，而非仅限于相关性，确保输出确定性和谈论因果关系的能力。此前，AI模型在生成看似逼真文本时常产生错误或无意义信息，成为企业应用的主要障碍。Alembic通过安全可靠的技术手段，使AI系统能够从各种数据源摄取信息，并处理\u0026quot;可观测性和分类器\u0026quot;模块和几何数据组件。结果输入因果图神经网络（GNN），生成确定性预测及战略建议。公司建立了超级计算机基础设施并开发了新数字技术，将企业数据表示为时间感知图神经网络，以捕捉事件与数据点随时间形成的关联。AlembicAI不仅学习模式和相关性，还能识别推动业务成果的因果关系，高度预测未来行动影响，并推荐实现目标的最佳干预措施。展示分析复杂数据生成战略建议的过程表明了其技术实力。Alembic在财富500强企业中受到浓厚兴趣，获得Nvidia公司博士专家及未公开大客户认可，显示出市场潜力。然而，该公司面临挑战证明最终技术成果能超越早期试点，为大型企业提供准确结果。\u0026ldquo;无幻觉\u0026quot;方法可能成为关键卖点，也可能揭示研究突破与实际影响之间差距的警示故事。（报道详情）\n为什么你的 iPhone，肯定用不上「苹果 AI」？ 硬件限制导致老款iPhone无法使用新AI功能。苹果追求设备上的隐私处理策略，强调在用户设备上直接执行AI任务以保护数据安全。AI模型需要足够的RAM空间运行，而老款手机的内存限制是主要问题之一。苹果AI功能仅限于特定机型，如新款iPhonePro系列及配备M1或更新芯片的iPad和Mac。尽管有理论上的逆向工程让部分AI功能在非标准硬件上运行，但实际效果有限。未来是否所有AI功能都能兼容更广泛的设备仍有待验证，预计首批AI功能将在iOS18发布时与新款iPhone一同推出，届时用户将能亲自体验苹果AI的实际价值和效果。（报道详情）\n答案抽取正确率达96.88%，xFinder断了大模型「作弊」的小心思 大语言模型（LLM）发展迅速，引发了对公平性与可靠性的讨论。当前主要评估框架如OpenCompass等推动了进步，但专注于核心组件可信度的团队较少。上海算法创新研究院和中国人民大学的研究团队发布《xFinder》论文，深入分析LLM评估流程，重点评估答案抽取器在可靠性与一致性方面的表现。论文指出现有方法依赖正则表达式（RegEx）进行答案抽取，最佳准确率仅为74.38%，且容易被拟合影响结果。为解决这一问题，《xFinder》提出了一种新模型，具备高度鲁棒性，能更准确地抽取关键答案，显著优于当前最佳框架中的RegEx方法，并支持多样化题型评估。实现过程包括生成LLM响应、构建KAF数据集和训练xFinder。团队构建了包含26,900个训练样本的KAF数据集，用于有效训练模型。实验结果显示，在不同任务上，xFinder-qwen1505的平均提取准确率高达96.88%，远超最佳评估框架中的RegEx方法，并显著优于GPT-4。此外，《xFinder》在现实世界场景中对多种LLM进行了评估，证实了其高鲁棒性和泛化能力。实验揭示了关键发现：不同模型在不同框架下的排名差异大、xfinder一致性高、直接使用选项文本能提升排名的一致性。《xFinder》通过优化关键答案提取模块，显著提高了LLM评估的准确性和可靠性，并展现出了高度鲁棒性和泛化能力。未来研究将继续优化此方法并探索其他评估领域，为LLM性能评估提供更可靠的基础。（报道详情）\n3D 版 SORA 来了！DreamTech 推出全球首个原生 3D-DiT 大模型 全球首个原生三维生成大模型Direct3D由DreamTech发布，并公开了相关学术论文《直接立方体：通过三维潜扩散转换器的可扩展图像到三维生成》。Direct3D解决了高质量三维内容生成难题，采用3DDiffusionTransformer（3D-DiT）技术路线，不经过中间2D阶段直接生成三维模型。与传统2D-to-3Dlifting方法相比，Direct3D在原理上具有优势，但面临高效3D模型表征、训练架构和高质量大规模3D训练数据的挑战。Direct3D通过创新提出类似OpenAISORA的3DVAE来提取特征，采用改进优化后的DiT架构，并结合DreamTech自研的数据合成引擎生成大量高质量3D数据。实验验证显示，Direct3D在三维模型生成质量上超越主流2D升维方法，主要得益于高效表征、对齐模块和使用大规模高质量3D数据。技术架构上，Direct3D采用与OpenAISORA相似的DiT架构，符合ScalingLaw原则，通过增加参数量和训练数据量提升智能程度。在大语言模型、图像生成和视频生成领域中均得到验证。Direct3D是全球首个公开的3D内容生成方向上的DiT实践，展示了原生3D技术路线的优势。随着Direct3D推出，3D生成领域进入商用时代。与传统方案相比，Direct3D生成的3D模型质量达到商用级别，解决了几何结构、精度、表面细节和mesh面片数量等问题，适用于家用及工业打印机。模型参数量增加后，3D生成可应用于更多行业。基于Direct3D大模型，DreamTech推出了面向C端用户的Animeit!和面向创作者的3D内容创作平台两款尝鲜产品。Animeit!将用户输入转换为高质量二次元风格的3D人物形象，并具备骨骼节点用于动作绑定。另一款产品让用户通过文本描述或上传图片在短时间内获得高质量3D模型。DreamTech专注于3DAI技术，致力于提升全球AIGC创作者及消费者的体验，目标打造与真实世界无缝对接、实时互动的4D时空体验，实现通用人工智能（AGI）。公司汇集了全球顶尖AI人才，核心团队由英国两院院士、国家级青年人才以及多位深圳市高层次人才组成。（报道详情）\n中国版Sora级视频大模型发布，打造“视频-Native”超级应用 全球视频生成领域迎来新突破，中国首个超长时长、高性价比大模型“视界一粟YiSu”发布，标志着视频生成技术进入快速爆发期。此款模型在性能与成本之间达到极致平衡，拥有16秒原生超长时长，可生成至1分钟以上，具备强大运动表现力和物理世界理解能力。通过融入LLM和扩散模型的自研架构，极大优化了多模态融合、训练及推理效率，并实现了模型效果的极致优化。视界一粟YiSu基于极佳科技自研技术路线，超越DiT（DiffusionTransformer）基础，结合LLM与扩散模型优势，提供视频生成最佳方案。这一模型有望推动AI-Native和视频-Native爆款应用的诞生，为用户提供AI时代前所未有的体验价值。面向通用智能时代，视界一粟YiSu将加速实现长视频生成的大规模产品应用，其数据引擎能力对自动驾驶、通用机器人等物理世界通用智能具有关键作用。极佳科技世界级人工智能综合团队在技术与产业落地方面拥有丰富经验，通过打造基础模型和超级应用的智能闭环飞轮，推动行业走向通用智能时代。未来，视界一粟YiSu将助力更多创新技术和产品的发展，为用户提供更多价值，加速通用智能时代的到来。（报道详情）\n关注本公众号，我们共同学习进步👇🏻👇🏻👇🏻\n微信公众号：老牛同学 Stable Diffusion开源大模型 Stable Diffusion 3 文生图“开源英雄”大模型笔记本部署和使用教程，轻松实现AI绘图自由\nQwen2-7B 开源大模型 Qwen2 阿里最强开源大模型（Qwen2-7B）本地部署、API调用和WebUI对话机器人\nLlama3-8B 开源大模型 玩转 AI，笔记本电脑安装属于自己的 Llama 3 8B 大模型和对话客户端\n一文彻底整明白，基于 Ollama 工具的 LLM 大语言模型 Web 可视化对话机器人部署指南\n基于Llama 3搭建中文版（Llama3-Chinese-Chat）大模型对话聊天机器人\nGLM-4-9B 开源大模型 本地部署GLM-4-9B清华智谱开源大模型方法和对话效果体验\nChatTTS 文本转语音模型 ChatTTS 开源文本转语音模型本地部署、API使用和搭建WebUI界面\n大模型应用 借助AI大模型，三分钟原创一部儿童故事短视频（附完整操作步骤）\n高效编写大模型 Prompt 提示词，解锁 AI 无限创意潜能\nPython 小游戏 AI已来，我与AI一起用Python编写了一个消消乐小游戏\nPython游戏编程：一步步用Python打造经典贪吃蛇小游戏\n","date":"2024-06-17T00:00:00Z","image":"https://ntopic.cn/p/ai20240618/ai20240618-156_hu39aaad0348e76c82767de365e5ef36d5_46223_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/ai20240618/","title":"[AI资讯·0618] 快手AI模型可灵在质量优于Sora，OpenAI和谷歌发布新技术推动AI视频推理发展，Gemini1.5Pro在该榜单中表现突出"},{"content":"备受期待的Stable Diffusion 3（以下亦简称SD3）如期向公众开源了（Stable Diffusion 3 Medium），作为Stability AI迄今为止最先进的文本生成图像的开源大模型，SD3在图像质量、文本内容生成、复杂提示理解和资源效率方面有了显著提升，被誉为AI文生图领域的开源英雄。\nStable Diffusion 3 Medium特点包括：\n模型仅包含20亿参数，具有体积小、适合在个人PC和笔记本电脑上运行的优点，所以我们也可以将其部署到自己的电脑上使用。 图像质量整体提升，能生成照片般细节逼真、色彩鲜艳、光照自然的图像；能灵活适应多种风格，无需微调，仅通过提示词就能生成动漫、厚涂等风格化图像；具有 16 通道的 VAE，可以更好地表现手部以及面部细节。 能够理解复杂的自然语言提示，如空间推理、构图元素、姿势动作、风格描述等。对于「第一瓶是蓝色的，标签是“1.5”，第二瓶是红色的，标签是“SDXL”，第三瓶是绿色的，标签是“SD3”」这样复杂的内容，SD3 依旧能准确生成，而且文本效果比 Midjourney 还要准确。 通过 Diffusion Transformer 架构，SD3 Medium 在英文文本拼写、字距等方面更加正确合理。Stability AI 在发布 SD3 官方公告时，头图就是直接用 SD3 生成的，效果非常惊艳。 特别注意： 开源的Stable Diffusion 3 Medium模型的授权范围是开放的非商业许可证，也就是说没有官方许可的情况下，模型不得用于商业用途（协议内容：https://huggingface.co/stabilityai/stable-diffusion-3-medium/blob/main/LICENSE）\n下载Stable Diffusion 3 Medium模型文件 SD3的模型文件已经上传到了HF上，但是在下载之前需要先注册并填写一份表格，保证自己不会将SD3用于商用用途。然而，注册和填写表格均需要有通畅的网络，这对我们个人不是很友好：\n注册或者登录成功之后，我们需要填写表格：\n老牛同学担心，仅这2个前置的需要通畅网络的操作步骤，就阻挡了一批SD3的爱好者们，这是老牛同学不希望看到的结果。\n幸运的是： ModelScope提供了一个可直接下载模型文件的镜像仓库，可直接跳过前面步骤（当然这有点违背Stability AI的初衷，但考虑到我们仅用于学习和非商业用途，也算是殊途同归吧）：https://modelscope.cn/models/AI-ModelScope/stable-diffusion-3-medium/files\n本仓库有3个基础是模型文件，它们有不同的用途，初次使用建议全部下载到本地：\nsd3_medium：4.34G，独立主模型，不包含文本编码器（即后面2个文件名带有clip的模型） sd3_medium_incl_clips.safetensors：5.97G，包含 clip_g 和 clip_l 编码器 sd3_medium_incl_clips_t5xxlfp8.safetensors：10.87G，包含 clip_g、clip_l 和 t5xxl_fp8 编码器 特别说明： 以上3个模型文件，最后需要放到ComfyUI的./models/checkpoints文件夹中（关于ComfyUI的使用下面章节介绍）\n另外，本仓库还有2个重要的文件夹，同样建议全部下载到本地：\ncomfy_example_workflows：ComfyUI工作流样例配置文件（关于ComfyUI的使用下面章节介绍） text_encoders：文本编码器模型文件夹，最后模型文件最后也需要放到ComfyUI的./models/clip文件夹中（关于ComfyUI的使用下面章节介绍） 因模型文件比较大，直接使用Git无法直接下载到本地，我们通过git-lfs工具包下载：\n1 brew install git-lfs 通过Git复制模型文件到笔记本电脑（文件夹：stable-diffusion-3-medium）：\n1 2 git lfs install git clone https://www.modelscope.cn/AI-ModelScope/stable-diffusion-3-medium.git stable-diffusion-3-medium 如果因网络不佳等原因，下载可能会中断，我们可以通过以下命令在中断后多次执行继续下载，直到最终下载完成：\n1 git lfs pull 下载ComfyUI工作流可视化工具 上一章节，我们多次提到了ComfyUI，它是Stable Diffusion的工作流可视化工具之一，也是Stability AI官方推荐使用的可视化工具。\n我们下载最新的ComfyUI到笔记本电脑（本地目录：ComfyUI）：\n1 git clone https://github.com/comfyanonymous/ComfyUI.git ComfyUI 本次部署教程中，ComfyUI中的models文件夹需要包括下载的模型文件：\n1 2 3 4 5 6 7 % tree ./models ./models ├── checkpoints │ └── put_checkpoints_here ├── clip │ └── put_clip_or_text_encoder_models_here ......其它省略...... checkpoints文件夹：是存放SD3模型文件根目录下的sd3_medium.safetensors、sd3_medium_incl_clips.safetensors和sd3_medium_incl_clips_t5xxlfp8.safetensors等模型文件的文件夹。\nclip文件夹：是存放SD3模型文件text_encoders目录下的clip_g.safetensors、clip_l.safetensors、t5xxl_fp8_e4m3fn.safetensors和t5xxl_fp16.safetensors等模型文件的文件夹。\n启动ComfyUI可视化界面 第一步： 复制模型文件到ComfyUI指定的models文件夹中，因为老牛同学使用的是Mac电脑，可以通过软链接方式实现复制的效果（如果是Windows电脑，请直接复制文件）：\n1 2 3 4 5 6 7 8 9 10 % cd ~/JupyterLab/ComfyUI/models/checkpoints % ln -s ~/JupyterLab/stable-diffusion-3-medium/sd3_medium.safetensors sd3_medium.safetensors % ln -s ~/JupyterLab/stable-diffusion-3-medium/sd3_medium_incl_clips.safetensors sd3_medium_incl_clips.safetensors % ln -s ~/JupyterLab/stable-diffusion-3-medium/sd3_medium_incl_clips_t5xxlfp8.safetensors sd3_medium_incl_clips_t5xxlfp8.safetensors % % cd ~/JupyterLab/ComfyUI/models/clip % ln -s ~/JupyterLab/stable-diffusion-3-medium/text_encoders/clip_g.safetensors clip_g.safetensors % ln -s ~/JupyterLab/stable-diffusion-3-medium/text_encoders/clip_l.safetensors clip_l.safetensors % ln -s ~/JupyterLab/stable-diffusion-3-medium/text_encoders/t5xxl_fp8_e4m3fn.safetensors t5xxl_fp8_e4m3fn.safetensors % ln -s ~/JupyterLab/stable-diffusion-3-medium/text_encoders/t5xxl_fp16.safetensors t5xxl_fp16.safetensors 软链接建好之后，我们可以检查一下是否符合预期：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 % cd ~/JupyterLab/ComfyUI/models % tree ./checkpoints ./checkpoints ├── put_checkpoints_here ├── sd3_medium.safetensors -\u0026gt; /Users/shizihu/JupyterLab/stable-diffusion-3-medium/sd3_medium.safetensors ├── sd3_medium_incl_clips.safetensors -\u0026gt; /Users/shizihu/JupyterLab/stable-diffusion-3-medium/sd3_medium_incl_clips.safetensors └── sd3_medium_incl_clips_t5xxlfp8.safetensors -\u0026gt; /Users/shizihu/JupyterLab/stable-diffusion-3-medium/sd3_medium_incl_clips_t5xxlfp8.safetensors % % tree ./clip ./clip ├── clip_g.safetensors -\u0026gt; /Users/shizihu/JupyterLab/stable-diffusion-3-medium/text_encoders/clip_g.safetensors ├── clip_l.safetensors -\u0026gt; /Users/shizihu/JupyterLab/stable-diffusion-3-medium/text_encoders/clip_l.safetensors ├── put_clip_or_text_encoder_models_here ├── t5xxl_fp16.safetensors -\u0026gt; /Users/shizihu/JupyterLab/stable-diffusion-3-medium/text_encoders/t5xxl_fp16.safetensors └── t5xxl_fp8_e4m3fn.safetensors -\u0026gt; /Users/shizihu/JupyterLab/stable-diffusion-3-medium/text_encoders/t5xxl_fp8_e4m3fn.safetensors 第二步： 启动ComfyUI工作流可视化界面\n进入ComfyUI根目录，安装Python依赖包列表：\n1 % pip install -r requirements.txt 启动ComfyUI可视化工具：\n1 % python main.py 从启动日志可以看出，ComfyUI启动成功了：http://127.0.0.1:8188\n使用ComfyUI通过文本生成图片 浏览器打开ComfyUI页面：http://127.0.0.1:8188 可以看到默认的SD3工作流，我们用自己的工作流完成绘画。\n第一步： 修改模型文件./stable-diffusion-3-medium/comfy_example_workflows/sd3_medium_example_workflow_basic.json的第416行，去掉sdv3/2b_1024/sd3_medium.safetensors的相对目录为sd3_medium.safetensors（默认本模型文件，我们已经复制到了ComfyUI指定的目录中，因此无需相对目录）：\n1 2 3 4 5 ......前面省略...... \u0026#34;widgets_values\u0026#34;: [ \u0026#34;sd3_medium.safetensors\u0026#34; ] ......后面省略...... 第二步： 加载修改后的工作流配置文件：点击ComfyUI的Load按钮，选择修改后的sd3_medium_example_workflow_basic.json文件：\n可以看到工作流有很多参数可供设置，包括：选择模型、正面Prompt提示词、负面Prompt提示词、图片尺寸/数量等输入参数：\n第三步： 调整参数，生成图片：我们可以修改工作流中任意一个参数（最常修改的是Prompt提示词，包括正面和负面提示词），也可以点击右键增加工作流节点。最终参数调整确定之后，点击Queue Prompt按钮，开始排队生成图片：\n我们可以看到，生成图片处理中，ComfyUI根据编排好的工作流，按照依赖关系逐个节点执行，最终在Output中展示了根据提示生成的精美图片：\n最终生成的图片：\nStable Diffusion使用总结 至此，Stable Diffusion 3的部署和使用教程接近尾声了，我们可以尽情使用不同的工作流和调整不同参数来生成我们的图片了。\n同时也恭喜你，和老牛同学一起，我们的大模型库又增添了重要的一员：文生图大模型！\nSD3 参数调优： 生成写实或人物面部照片，可以将KSampler节点的cfg参数调低至2到3之间（默认为4.5）；当包含文本时，使用4.5到5时效果会更好。大家可以多多尝试，探索一些其他参数产生的效果，欢迎留言。\n同时SD3模型更适合自然语言提示词，而不是标签式的提示词（和MJ有一点点区别），我们可以详细描述图片的画面内容、构图、色彩、氛围，即使提示词很长，SD3模型也能处理的很好。\n关注本公众号，我们共同学习进步👇🏻👇🏻👇🏻\n微信公众号：老牛同学 Qwen2-7B 开源大模型 Qwen2阿里最强开源大模型（Qwen2-7B）本地部署、API调用和WebUI对话机器人\nLlama-3-8B 开源大模型 玩转 AI，笔记本电脑安装属于自己的 Llama 3 8B 大模型和对话客户端\n一文彻底整明白，基于 Ollama 工具的 LLM 大语言模型 Web 可视化对话机器人部署指南\n基于Llama 3搭建中文版（Llama3-Chinese-Chat）大模型对话聊天机器人\nGLM-4-9B 开源大模型 本地部署GLM-4-9B清华智谱开源大模型方法和对话效果体验\nChatTTS 文本转语音模型 ChatTTS 开源文本转语音模型本地部署、API使用和搭建WebUI界面\n大模型应用 借助AI大模型，三分钟原创一部儿童故事短视频（附完整操作步骤）\n高效编写大模型 Prompt 提示词，解锁 AI 无限创意潜能\nPython 小游戏 AI已来，我与AI一起用Python编写了一个消消乐小游戏\nPython游戏编程：一步步用Python打造经典贪吃蛇小游戏\n","date":"2024-06-17T00:00:00Z","image":"https://ntopic.cn/p/2024061701/00_hu208e39d3e3e5e981d6b12dd280fb4517_114349_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/2024061701/","title":"Stable Diffusion 3 大模型文生图“开源英雄”笔记本部署和使用教程，轻松实现AI绘图自由"},{"content":"AI资讯 国产AI大战高考物理，第1题全对，第2题开始放飞 终于放大招了，2024WWDC，苹果开启AI反击战 苹果一夜重塑iPhone！GPT-4o加持Siri，AI深入所有APP OpenAI确认苹果集成ChatGPT 还任命了两位新高管 GPT-4搞不定的图推理，港科大7B模型搞定｜KDD2024 拿下SOTA！最强中文Embedding模型对标OpenAI，技术路线公开 具身智能赋能机器人，「AI+人形机器人」论坛在浦东新区成功举行 苹果智能炸裂登场：GPT-4o加持,全家桶都上生成式AI,Siri脱胎换骨 AI重新定义导航，弯道会车无灯路口提前预警，网友：导航成精了！ AI生图格局大震！Stable Diffusion 3开源倒计时，2B单机可跑碾压闭源Midjou 手机流畅运行470亿大模型：上交大发布LLM手机推理框架，提速29倍 估值64亿美元！Mistral AI官宣6.4亿美元B轮融资 国产AI大战高考物理，第1题全对，第2题开始放飞 端午佳节之际，高考在部分地区已落幕，在其他考场，考生仍在奋力应考。为预祝所有参与者取得理想成绩，各类科目考试答案开始在网络上流传。面对新高考改革下取消文理分科、采用3+1+2或3+3模式的挑战，本次邀请了通义千问等十款AI助手参与物理单选题与多选题的解答测试。在单选题环节中，最高准确率为100%，涉及矢量标量概念。第二题，选手们答案各异；海螺AI、讯飞星火和文心一言表现出色。第5题，讯飞星火、海螺AI和新增的Kimi与腾讯元宝答对。文心一言和ChatGLM在解答双缝干涉实验时展现了独特的应试策略。多选题部分，海螺AI表现最佳，通义千问、文心一言以及万知亦取得不俗成绩。最高准确率第8题是一道概念题，第9题万知与海螺AI全选正确答案。测试结果显示，在46分的选择题中，多数选手达到了及格标准。本次AI解答测试展现了大模型在物理领域应用的潜力与挑战性，未来期待更多创新与改进。（报道详情）\n终于放大招了，2024WWDC，苹果开启AI反击战 2024年WWDC大会上，苹果全面发力人工智能领域，发布了一系列新操作系统，包括iOS18、iPadOS18、macOSSequoia、watchOS11等，并推出了“AppleIntelligence”功能。这一功能使Siri更加智能，支持图片AI消除、系统录音APP通话录音、AI图片生成等功能。用户无需注册即可使用ChatGPT-4。然而，该功能仅适用于搭载A17Pro和M系列芯片的设备，且当前只有美国版本提供英语服务，其他国家和地区需等待后续更新。AppleIntelligence的推出标志着苹果正式踏入人工智能元年，库克强调它不仅涉及人工智能，更是一种个人化智能体验。在文字处理、隐私保护等方面，AI功能得到了显著增强，如Siri支持不连贯语音指令、跨App操作等。图片处理方面，用户可以直接使用AI技术消除照片中的路人或搜索特定动作的表情，并生成与AppleMusic库中音乐相匹配的回忆或vlog。此外，苹果宣布与OpenAI合作将ChatGPT集成到系统生态中，并由GPT-4驱动，用户无需注册即可体验。国行版则可能与百度合作使用文心一言大模型。值得一提的是，通过录音功能，用户可以实现文字转换，这一特性在安卓设备上已有所应用。AppleIntelligence强调了隐私保护和端侧、云端处理的结合，确保数据安全。尽管在AI领域起步较晚，但苹果展示了一系列创新功能，并承诺将AI融入更多产品中，以打造个人化智能体验。（报道详情）\n苹果一夜重塑iPhone！GPT-4o加持Siri，AI深入所有APP 苹果通过与OpenAI合作接入ChatGPT和使用GPT-4o模型，对iPhone系统进行全面升级，包括Siri的重生、重构人机交互模式、引入AppleIntelligence成为AI新代名词等。发布内容涉及iOS、iPadOS大更新，AI浓度显著提升，并且这些功能免费提供给iPhone15Pro及以上设备和搭载M1及更高配置芯片的Mac用户。内置ChatGPT无需注册即可使用。发布会引起广泛关注，WWDC24冲上热搜前三，网友感慨苹果整合能力逆天，新AI功能满足大家想象力。AppleIntelligence与ChatGPT联手打造AIPhone，系统AI化、引入强大云端大模型，包括语言、图像、行动和个性化等多方面增强。语言方面支持通知优先排序、写作工具优化；图像生成覆盖素描、插画、动画等多种风格，并能自定义emoji；行动力方面提供跨应用操作及个人情景分析。AppleIntelligence在隐私保护上采用专门架构，在端侧进行涉及个人隐私的计算，确保安全。此外，AppleIntelligence还能生成摘要、总结电话录音等功能。对于更复杂任务，引入ChatGPT，支持多模态任务如为菜谱提供灵感等，并且所有请求不会被记录。Siri升级进入新纪元，屏幕边缘光晕显示，理解更丰富语言和上下文信息，支持语音打字切换输入，解答数千个操作问题。AppleIntelligence将增强屏幕内容感知功能、跨应用操作能力以及APPIntents框架的智能度，提供更多个性化体验。iOS18主打个性化，引入深色模式设置，APP小组件主题自定义，控制中心可自定义控制项和布局，隐私安全方面增加上锁功能。图库新增按时间、主题分类整理的小相册，短信解锁新功能如延迟发送、特殊文字效果等。iPadOS18终于加入计算器，并支持数学笔记、手写识别等功能，备忘录推出Smartscript模仿笔迹功能。VisionOS2增强AI空间化照片和沉浸式分享能力，支持双指轻点切换主屏幕、旅行模式增加火车场景支持。MacOSSepuoia优化设备互联，可在Mac桌面上直接打开iPhone镜像并操作iPhone内容，Safari新增Highlights功能突出显示重要页面内容，并加强智能防追踪保护隐私。新出的Keychain管理密码安全，实现多设备同步。发布会后，库克表示AIPhone时代刚刚开始。（报道详情）\nOpenAI确认苹果集成ChatGPT 还任命了两位新高管 美国时间周一，OpenAI宣布聘请萨拉·弗莱尔为首席财务官和凯文·威尔为首席产品官，并与苹果合作集成ChatGPT等协议。弗莱尔曾任NextdoorCEO及SquareCFO，目前是斯坦福大学数字经济实验室联席主席；威尔曾担任PlanetLabs总裁、Twitter高级副总裁以及Facebook和Instagram高管。新任命旨在强化OpenAI的高管团队，推动公司扩大规模以满足增长需求，并应对复杂环境。OpenAI与苹果的合作计划于今年晚些时候将ChatGPT整合至iOS、iPadOS及macOS系统中，用户可借此利用ChatGPT功能进行文本生成、图像创建等操作。此外，Siri将调用ChatGPT智能提供答案，但在发送问题前需用户确认。近期，OpenAI因快速成长和争议性员工离职事件受到关注，并有员工公开信表达对行业过快发展及缺乏监管的担忧。（报道详情）\nGPT-4搞不定的图推理，港科大7B模型搞定｜KDD2024 大模型执行图推理任务时，关键在于是否能准确给出结果并同时提供详细的推理过程。GPT-4表现简短且错误，可能受限于处理长输入或理解复杂结构；相比之下，港科大团队开发的GraphWiz不仅正确解答，还清晰展示推理路径。GraphInstruct数据集为训练语言模型提供了多样化的图任务和明确的推理路径，旨在提升模型解决各种图问题的能力。GraphWiz通过针对性微调和两阶段训练方法优化了解题能力，并提供连贯推理路径。研究发现，GraphWiz在空间推理、记忆保持方面表现出色，优于GPT-4。九种不同复杂度层次的图问题被精心挑选用于全面探索图论理论与应用。GraphInstruct数据集构建包括生成随机图和显式推理路径，通过拒绝采样策略增强数据多样性。GraphWiz训练采用混合任务指令调优和直接偏好优化对齐方法提升模型性能。评估结果显示，GraphWiz在不同复杂度的图问题上显著超越GPT-4，并且随着训练语料库增加，性能进一步提升。然而，在某些特定任务中存在潜在过拟合现象。通过比较实验，研究发现GraphWiz具有良好的跨任务泛化能力，尤其在高复杂度和零样本迁移任务上的表现令人印象深刻。整体而言，GraphWiz的开发不仅提升了大型模型解决图推理问题的能力，还提供了清晰、连贯的推理路径，为实际应用提供了强大支持。（报道详情）\n拿下SOTA！最强中文Embedding模型对标OpenAI，技术路线公开 国产大模型「日日新5.0」凭借商汤自研中文Embedding模型Piccolo2在权威评测中超越GPT-4Turbo，成为首个实现此壮举的国产模型。背后是算法、算力与数据全面优化的结果，其中Piccolo2作为关键算法之一，支持512/2K/8K向量长度，尤其8K模型对标OpenAI，性能在C-MTEB中文语义向量评测中领先。论文及HuggingFace地址公开，揭示了其通过多任务混合损失训练、高效数据合成与难负样本挖掘提升性能的策略。相较于之前SOTA模型，Piccolo2综合评分提高约1.9个点，展现了在检索、分类、聚类等任务上的强大能力，为大语言模型落地应用提供了关键技术支持。（报道详情）\n具身智能赋能机器人，「AI+人形机器人」论坛在浦东新区成功举行 在上海市浦东新区科技和经济委员会指导下，「AI+人形机器人」论坛暨张江机器人全球生态峰会具身智能开发者论坛于浦东新区张江科学会堂成功召开。来自多个领域的200多位嘉宾参加，讨论了具身智能、运动控制、云计算及人形机器人整机等议题，吸引了超30000在线观众。论坛邀请12位专家分享研究成果，涵盖浙大教授王越的末端规划难题解决方案，上海人工智能实验室青年科学家庞江淼的人工智能模型赋能与三维感知挑战解决，香港中文大学（深圳）教授贾奎提出的高通用性具身智能阶段及跨维智能基于Sim2Real的解决方案。李清教授分析了肌腱仿生驱动优势，并倡导产业交流；胡宇航分享自监督学习应用推动自动学习优化；蒋琛讨论具身智能控制系统平台促进人形机器人大小脑融合。施群聚焦人形机器人结构设计，季超博士分享团队结合具身大模型的技术路线及挑战，李伟研究员介绍基于生命进化学习的机器人形态与行为策略迁移。陈鹏介绍松灵机器人的全球科研教育工作，张龙君分享加速具身智能机器人产品核心能力构建的工作，邢伯阳分享低成本具身平台的设计思想和应用前景。浦东新区大模型赋能产业系列论坛聚焦金融、智能网联车、人形机器人等垂直领域，探讨前沿AI应用场景，搭建技术交流与产业协同的开放平台，助力浦东打造世界级人工智能产业集群。（报道详情）\n苹果智能炸裂登场：GPT-4o加持,全家桶都上生成式AI,Siri脱胎换骨 苹果全球开发者大会WWDC2023在ApplePark正式召开，宣布覆盖全线产品的生成式AI技术，并带来Siri、Siri新形态、AppleIntelligence系统级整合、ChatGPT集成、跨平台集成ChatGPT、全新语言理解和创造能力、ImagePlayground和Genmoji以及隐私保护等创新。此次发布会亮点包括：1.生成式AI技术：苹果推出基于M系列芯片的自研本地大模型加云端策略，实现强大、直观、完全整合、个性化且保护隐私的体验。2.Siri更新：-新形态Siri拥有更丰富的语言理解能力，更加自然、懂上下文、个性化，简化日常任务。-支持文字输入和切换文本与语音交互方式。-屏幕感知功能允许执行屏幕信息相关操作。3.AppleIntelligence：全面AI系统，结合生成式AI模型和个人资料提供智能服务，覆盖iPhone、iPad和Mac，利用苹果芯片能力处理语言和图像，并支持跨应用操作。4.ChatGPT集成：-Apple正式将ChatGPT集入iOS18、iPadOS18和macOSSequoia，提供图像和文档理解功能。-Siri可以随时利用ChatGPT提供专业回答，用户授予权限后可直接访问。5.写作工具与AI：系统集成的写作工具支持通过ChatGPT协助生成内容，并在iOS、iPadOS和macOS中提供个性化的智能服务，如重写、校对和总结文本等。6.隐私保护：AppleIntelligence采用设备端处理和私有云端计算（PrivateCloudCompute）策略，确保数据安全，为AI领域设立新的隐私标准。7.新macOS系统：macOSSequoia引入iPhone镜像功能、通知整合与Safari升级等，提供更流畅的多设备体验。8.iPad计算器应用：苹果首次为iPad推出原生计算器应用，满足用户需求。9.VisionPro系统：visionOS2新增多项功能，支持从2D图像中导出深度信息，并带来空间照片等创新体验。总体而言，苹果通过此次WWDC发布会展示了其在生成式AI领域的全面布局和技术创新，旨在提升用户体验并保持技术领先地位。（报道详情）\nAI重新定义导航，弯道会车无灯路口提前预警，网友：导航成精了！ 高德地图新功能“车道级安全预警”引发用户热议，其能在多种场景下实时探测行车风险，提供语音提醒，覆盖前后左右、昼夜情况，帮助用户减少事故风险。该功能从过去的“车道级导航”进化为能预测并响应风险的系统，不仅提升准确性，还扩展了感知范围，让用户仿佛拥有“千里眼”。高德地图日活跃用户峰值超过2.8亿，月活高达8.01亿，作为国内移动互联网第四大应用，在大规模用户基础上实现车道级安全预警。此外，AI技术与大模型的融入优化了导航服务，提供个性化体验，并探索车路云一体化可能性。高德的成功不仅在于技术创新，也体现了对用户需求的深入理解和服务的社会普惠性，预示着AI在出行领域的未来变革。（报道详情）\nAI生图格局大震！Stable Diffusion 3开源倒计时，2B单机可跑碾压闭源Midjou StableDiffusion3（SD3）即将开源，引发AI生图领域重大变革。经过4个月的酝酿，此消息在社区内激起了巨大反响。SD3凭借其强大的MMDiT全新架构，在AI图像生成领域展现出前所未有的实力，有望成为首个开源碾压闭源的技术赛道。ComfyUI已提前支持SD3版本，预示着开源发布即将成真。SD3的开源不仅意味着StabilityAI将面临收入压力，也凸显了公司在面对开源与商业化选择时的挑战。此前，该公司因巨额债务和CEO离职等负面事件备受关注，但坚持开源的决心彰显其对社区价值的认可。SD3架构强大，能够显著提升图像质量、支持多主题提示，并增强文字拼写能力。其核心在于MMDiT模型，结合了DiT架构与Transformer技术，专为处理文本和图像双模态任务设计。实验结果显示，SD3在人类偏好评估中超越DALL-E3和Midjourneyv6，成为领域内顶尖模型。此消息对AI社区影响重大，预示着开源合作的深化、研究方法的加速创新以及多模态功能的扩展。SD3将为开发者提供更高效、易用的工具，并可能催生更多独创性的体验。随着40亿和80亿参数版本的陆续上线，SD3有望进一步推动AI生图领域的发展。综上所述，StableDiffusion3即将开源的消息不仅对AI社区产生深远影响，也展示了开源项目在技术创新与合作中的巨大潜力。（报道详情）\n手机流畅运行470亿大模型：上交大发布LLM手机推理框架，提速29倍 苹果公司关注在手机等移动设备上部署大型模型的趋势，但当前移动设备上的模型较小且消耗大量内存，限制了应用场景。为提供更强服务，苹果需与OpenAI合作，通过云端大模型嵌入操作系统。此方案引发隐私讨论和争议。终端侧本地部署大模型既能提供强大智能又保护隐私的安全性成为关注焦点。主要挑战包括手机内存不足和算力不够强。大型模型参数越多，对内存要求越高；现有手机硬件难以高效运行大规模模型。为解决这些挑战，上海交大IPADS实验室推出面向手机的大模型推理引擎PowerInfer-2.0，并配套大模型优化技术TurboSparse。PowerInfer-2.0能够快速在内存有限的智能手机上实现推理，让Mixtral47B模型达到11tokens/s的速度。相比热门开源推理框架llama.cpp，其推理加速比平均达25倍最高29倍。上海交大团队提出针对手机场景的优化策略，包括动态神经元缓存和基于神经元簇的异构计算，并在去年底提出的PowerInfer-1.0基础上进一步提升。针对手机内存不足问题，利用稀疏模型推理特点，PowerInfer-2.0将神经网络中的神经元分为冷、热两种，并基于LRU策略维护神经元缓存池。近期频繁激活的热神经元被放置在运行内存中，降低内存使用量。通过分段神经元缓存和神经元簇级流水线技术，PowerInfer-2.0实现I/O延迟隐藏并提高模型推理效率。实测显示，在一加12和一加Ace2两款测试手机上，PowerInfer-2.0的预填充速度显著高于llama.cpp与LLMFlash，并在解码阶段占据优势。对于Mixtral47B这类大模型，也能在手机上跑出11.68tokens/s的速度。上海交大团队还提出低成本高质量地大幅提升模型稀疏性的方法。通过引入dReLU激活函数和高质量继续训练语料库，TurboSparse系列模型不仅保持甚至超过原版模型精度，同时将稀疏度提升至三分之一。改造过程中，模型需要继续训练的token数量不到预训练总量的5%，成本极低。上海交大IPADS实验室的研究成果为大型语言模型在资源受限设备上的快速推理提供了可能，并展现出在车载设备、智能家居等方向的应用前景。未来，与手机厂商的合作有望加速相关技术落地应用。（报道详情）\n估值64亿美元！Mistral AI官宣6.4亿美元B轮融资 法国AI初创公司MistralAI完成6亿欧元B轮融资，估值升至60亿欧元，较六个月前增长3倍。本轮融资由GeneralCatalyst领投，原有投资者如Lightspeed、AndreessenHorowitz、Bpifrance和法国巴黎银行等参与，企业支持者包括英伟达、Salesforce、三星和IBM。MistralAI于2023年4月在巴黎创立，由前DeepMind和Meta的科学家共同组建，专注于开源模型及企业级服务。公司已获得累计超5亿美元融资，近期发布参数规模1760亿的Mixtral8x22B模型，仅次于马斯克的Grok-1，成为市场上第二大开源模型。MistralAI被视为OpenAI在欧洲的主要竞争对手，估值达到64亿美元。MistralAI成立于巴黎，由亚瑟·门施、提摩西·拉克鲁瓦和纪尧姆·兰普创立。公司成立两个月后获得约1.13亿美元种子轮融资，同年12月A轮获得约4.15亿美元融资，估值达20亿美元。微软今年2月对MistralAI进行了小额投资，但不持有股权。MistralAI的投资者包括科技巨头如微软、英伟达和Salesforce等，以及硅谷著名风投GeneralCatalyst和AndreessenHorowitz。市场对MistralAI的信心显著提升，估值较去年12月增长了不止两倍。公司计划在开源大模型领域继续取得突破，与科技巨头竞争。MistralAI成立仅一年，在开源大模型领域取得了多项里程碑，包括发布首个生成式AI模型、具有开放权重的稀疏专家混合模型以及支持5国语言的旗舰模型等。公司还与微软建立了合作伙伴关系，利用AzureAI的超级计算基础设施进行模型训练，并提供MIstralAI模型服务。随着国内外百模大战进入深水区，资金成为关键因素。MistralAI的成功融资为AI创投圈增添信心，显示出市场对新AI大模型创企的信心。面对挑战，MistralAI需利用资金加速性能提升和产品化进程，以在竞争中脱颖而出。（报道详情）\n关注本公众号，我们共同学习进步👇🏻👇🏻👇🏻\n微信公众号：老牛同学 Qwen2-7B 开源大模型 Qwen2 阿里最强开源大模型（Qwen2-7B）本地部署、API调用和WebUI对话机器人\nLlama3-8B 开源大模型 玩转 AI，笔记本电脑安装属于自己的 Llama 3 8B 大模型和对话客户端\n一文彻底整明白，基于 Ollama 工具的 LLM 大语言模型 Web 可视化对话机器人部署指南\n基于Llama 3搭建中文版（Llama3-Chinese-Chat）大模型对话聊天机器人\nGLM-4-9B 开源大模型 本地部署GLM-4-9B清华智谱开源大模型方法和对话效果体验\nChatTTS 文本转语音模型 ChatTTS 开源文本转语音模型本地部署、API使用和搭建WebUI界面\n大模型应用 借助AI大模型，三分钟原创一部儿童故事短视频（附完整操作步骤）\n高效编写大模型 Prompt 提示词，解锁 AI 无限创意潜能\nPython 小游戏 AI已来，我与AI一起用Python编写了一个消消乐小游戏\nPython游戏编程：一步步用Python打造经典贪吃蛇小游戏\n","date":"2024-06-12T00:00:00Z","image":"https://ntopic.cn/p/ai20240612/ai20240612-125_hu4697c1f87f561bff2a15f9ead133bd06_63496_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/ai20240612/","title":"AI测试高考物理题，最高准确率100%，OpenAI与苹果合作，将ChatGPT融入系统中，大模型在物理领域应用潜力显现"},{"content":"阿里巴巴通义千问团队发布了Qwen2系列开源模型，该系列模型包括5个尺寸的预训练和指令微调模型：Qwen2-0.5B、Qwen2-1.5B、Qwen2-7B、Qwen2-57B-A14B以及Qwen2-72B。对比当前最优的开源模型，Qwen2-72B在包括自然语言理解、知识、代码、数学及多语言等多项能力上均显著超越当前领先的Llama3-70B等大模型。\n老牛同学今天部署和体验Qwen2-7B-Instruct指令微调的中等尺寸模型，相比近期推出同等规模的开源最好的Llama3-8B、GLM4-9B等模型，Qwen2-7B-Instruct依然能在多个评测上取得显著的优势，尤其是代码及中文理解上。\n特别注意： 虽然Qwen2开源了，但仍然需要遵循其模型许可，除Qwen2-72B依旧使用此前的Qianwen License外，其余系列版本模型，包括Qwen2-0.5B、Qwen2-1.5B、Qwen2-7B以及Qwen2-57B-A14B等在内，均采用Apache 2.0许可协议。\n下载Qwen2-7B-instruct模型文件 为了简化模型的部署过程，我们直接下载GGUF文件。关于GGUF文件介绍，请详见部署Llama3-8B大模型的文章：玩转AI，笔记本电脑安装属于自己的Llama 3 8B大模型和对话客户端\n打开Qwen2-7B-Instruct-GGUF模型文件列表（https://modelscope.cn/models/qwen/Qwen2-7B-Instruct-GGUF/files），我们选择qwen2-7b-instruct-q5_k_m.gguf并下载：\n我们可以根据自己需要，选择下载其它版本的模型文件！\n启动Qwen2-7B-Instruct大模型 GGUF模型量化文件下载完成后，我们就可以来运行Qwen2-7B大模型了。\n在启动Qwen2-7B大模型之前，我们首先需要安装Python依赖包列表：\n1 2 3 4 5 6 7 8 pip install llama-cpp-python pip install openai pip install uvicorn pip install starlette pip install fastapi pip install sse_starlette pip install starlette_context pip install pydantic_settings 然后打开一个Terminal终端窗口，切换到GGUF模型文件目录，启动Qwen2-7B大模型（./qwen2-7b-instruct-q5_k_m.gguf即为上一步下载的模型文件路径）：\n1 2 3 4 5 6 7 # 启动Qwen2大模型 # n_ctx=20480代表单次回话最大20480个Token数量 python -m llama_cpp.server \\ --host 0.0.0.0 \\ --model ./qwen2-7b-instruct-q5_k_m.gguf \\ --n_ctx 20480 Qwen2-7B-instruct 命令行对话客户端 CLI命令行的客户端，可以参考之前LLama3-8B大模型的文章：https://mp.weixin.qq.com/s/MekCUJDhKzuUnoykkGoH2g\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 # client.py from openai import OpenAI # 注意服务端端口，因为是本地，所以不需要api_key client = OpenAI(base_url=\u0026#34;http://127.0.0.1:8000/v1\u0026#34;, api_key=\u0026#34;not-needed\u0026#34;) # 对话历史：设定系统角色是一个只能助理，同时提交“自我介绍”问题 history = [ {\u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;你是一个智能助理，你的回答总是容易理解的、正确的、有用的和内容非常精简.\u0026#34;}, ] # 首次自我介绍完毕，接下来是等代码我们的提示 while True: completion = client.chat.completions.create( model=\u0026#34;local-model\u0026#34;, messages=history, temperature=0.7, stream=True, ) new_message = {\u0026#34;role\u0026#34;: \u0026#34;assistant\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;\u0026#34;} for chunk in completion: if chunk.choices[0].delta.content: print(chunk.choices[0].delta.content, end=\u0026#34;\u0026#34;, flush=True) new_message[\u0026#34;content\u0026#34;] += chunk.choices[0].delta.content history.append(new_message) print(\u0026#34;\\033[91;1m\u0026#34;) user_input = input(\u0026#34;\u0026gt; \u0026#34;) if user_input.lower() in [\u0026#34;bye\u0026#34;, \u0026#34;quit\u0026#34;, \u0026#34;exit\u0026#34;]: # 我们输入bye/quit/exit等均退出客户端 print(\u0026#34;\\033[0mBYE BYE!\u0026#34;) break history.append({\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: user_input}) print(\u0026#34;\\033[92;1m\u0026#34;) 启动CLI对话客户端：python client.py\n至此，我们可以与Qwen2-7B-Instruct进行对话，体验Qwen2大模型的魅力了。\n如果我们主要是通过API的方式使用Qwen2大模型，那么Qwen2部署就到此结束了。\n接下来的章节，我们部署WebUI对话客户端，通过Web界面的方式使用Qwen2大模型，并且可以分享出去~\nQwen2-7B-Instruct WebUI客户端 结合Ollama工具，搭建WebUI客户端，可参考之前Llama3-8B大模型的文章：一文彻底整明白，基于Ollama工具的LLM大语言模型Web可视化对话机器人部署指南\n第一步： 我们需要下载安装Ollama本地大模型管理工具：\nOllama提供了MacOS、Linux和Windows操作系统的安装包，大家可根据自己的操作系统，下载安装即可：\n安装包下载之后的安装过程，和日常安装其他软件没有差别，包括点击Next以及Install等安装ollama到命令行。安装后续步骤中，我们可无需安装任何模型，因为我们在上文中我们已经安装了Qwen2-7B大模型，后面可以直接使用。\n第二步： 安装Node.js编程语言工具包\n安装Node.js编程语言工具包和安装其他软件包一样，下载安装即可：https://nodejs.org\n安装完成之后，可以验证一下 Node.js 的版本，建议用目前的最新v20版本：\n1 node -v 老牛同学安装的版本：v20.13.1（最新版本）\n第三步： 基于GGUF模型文件创建Ollama模型\n在我们存放Qwen2-7B的 GGUF 模型文件目录中，创建一个文件名为Modelfile的文件，该文件的内容如下：\n1 FROM ./qwen2-7b-instruct-q5_k_m.gguf 然后在Terminal终端，使用这个文件创建Ollama模型，这里我把Ollama的模型取名为Qwen2-7B：\n1 2 3 4 5 6 $ ollama create Qwen2-7B -f ./Modelfile transferring model data using existing layer sha256:258dd2fa1bdf98b85327774e1fd36e2268c2a4b68eb9021d71106449ee4ba9d5 creating new layer sha256:14f4474ef69698bf4dbbc7409828341fbd85923319a801035e651d9fe6a9e9c9 writing manifest success 最后，通过Ollama启动我们刚创建的大语言模型：\n1 ollama run Qwen2-7B 启动完毕，其实我们已经有了一个和之前差不多的控制台对话界面，也可以与Qwen2-7B对话了。\n如果我们不想要这个模型了，也可以通过命令行删除模型文件：ollama rm Qwen2-7B\n我们也可以查看本地Ollama管理的模型列表：ollama list\nOllama存放模型文件根目录：~/.ollama\n第四步： 部署Ollama大模型Web对话界面\n控制台聊天对话界面体验总归是不太好，接下来部署 Web 可视化聊天界面。\n首先，下载ollama-webuiWeb 工程代码：git clone https://github.com/ollama-webui/ollama-webui-lite\n然后切换ollama-webui代码的目录：cd ollama-webui-lite\n设置 Node.js 工具包镜像源，以接下来下载 Node.js 的依赖包更加快速：npm config set registry http://mirrors.cloud.tencent.com/npm/\n安装 Node.js 依赖的工具包：npm install\n最后，启动 Web 可视化界面：npm run dev\n如果看到以上输出，代表 Web 可视化界面已经成功了！\n第五步： 通过WebUI愉快与Qwen2-7B对话\n浏览器打开 Web 可视化界面：http://localhost:3000/\n可以看到Ollama的初始化页面，默认没有模型，需要选择，我们选择刚创建并部署的Qwen2-7B模型：\n底部就是聊天输入框，至此可以愉快的与Qwen2-7B聊天对话了：\n总结：Qwen2-7B比Llama3-8B快 老牛同学验证和对比，在文本推理上，Qwen2-7B确实比Llama3-8B要快很多。后续老牛同学中文文本推理相关的API接口，就主要采用更快Qwen2-7B大模型了~\n其他：Ollama工具常用用法 从上文的介绍可以看到，基于Ollama部署一个大模型的 Web 可视化对话机器人，还是非常方便。下面整理了部分Ollama提供的用法或者。\nOllama 命令工具\n1 2 3 4 5 6 7 8 9 10 11 # 查看当前Ollama的模型 ollama list # 增量更新当前部署的模型 ollama pull Qwen2-7B # 删除一个模型文件 ollama rm Qwen2-7B # 复制一个模型 ollama cp Qwen2-7B Qwen2-newModel Ollama API结果返回\n1 2 3 4 curl http://localhost:11434/api/generate -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;Qwen2-7B\u0026#34;, \u0026#34;prompt\u0026#34;:\u0026#34;为什么天空是蓝色的？\u0026#34; }\u0026#39; Ollama API聊天对话\n1 2 3 4 5 6 curl http://localhost:11434/api/chat -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;Qwen2-7B\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;为什么天空是蓝色的？\u0026#34; } ] }\u0026#39; 关注本公众号，我们共同学习进步👇🏻👇🏻👇🏻\n微信公众号：老牛同学 Qwen2-7B 开源大模型 Qwen2阿里最强开源大模型（Qwen2-7B）本地部署、API调用和WebUI对话机器人\nLlama-3-8B 开源大模型 玩转 AI，笔记本电脑安装属于自己的 Llama 3 8B 大模型和对话客户端\n一文彻底整明白，基于 Ollama 工具的 LLM 大语言模型 Web 可视化对话机器人部署指南\n基于Llama 3搭建中文版（Llama3-Chinese-Chat）大模型对话聊天机器人\nGLM-4-9B 开源大模型 本地部署GLM-4-9B清华智谱开源大模型方法和对话效果体验\nChatTTS 文本转语音模型 ChatTTS 开源文本转语音模型本地部署、API使用和搭建WebUI界面\n大模型应用 借助AI大模型，三分钟原创一部儿童故事短视频（附完整操作步骤）\n高效编写大模型 Prompt 提示词，解锁 AI 无限创意潜能\nPython 小游戏 AI已来，我与AI一起用Python编写了一个消消乐小游戏\nPython游戏编程：一步步用Python打造经典贪吃蛇小游戏\n","date":"2024-06-12T00:00:00Z","image":"https://ntopic.cn/p/2024061201/01_hu4fbde6bb6e2fc98225cb13a238d286f0_126052_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/2024061201/","title":"Qwen2 阿里最强开源大模型（Qwen2-7B）本地部署、API调用和WebUI对话机器人"},{"content":"AI资讯 奥特曼28亿「投资帝国」曝光！不要OpenAI股份，当CEO最不赚钱 开源超闭源！通义千问Qwen2发布即爆火，网友：GPT-4o危 OpenAI泄密者公布165页文件：2027年实现AGI、计算集群将耗资千亿美元 清华系细胞大模型登Nature子刊！能对人类2万基因同时建模 奥特曼百万年薪挖角谷歌TPU人才，欲砸7万亿实现「芯片自由」？OpenAI自研芯 GPT-4欺骗人类高达99.16%惊人率！PNAS重磅研究曝出，LLM推理越强欺骗值越高 苹果AI升级大泄露，Siri将在iOS18重生！库克用Apple重新定义AI 港大北航等1bit大模型引热议，IEEE刊物评“解决AI能源需求”！ 奥特曼28亿「投资帝国」曝光！不要OpenAI股份，当CEO最不赚钱 SamAltman成立了一个庞大的投资帝国，但他本人不持有OpenAI股票，只拿微薄年薪6.5万美元。他的个人资产包括至少三处房产和多个公司股权，管理这些资产的家族办公室如同成熟风投公司。他曾在YCombinator担任总裁，投资了40家公司，其中有五家估值增长100倍或以上。尽管他不参与OpenAI决策，但其个人利益与公司业务存在复杂关系，引发了监督和透明度的争议。（报道详情）\n开源超闭源！通义千问Qwen2发布即爆火，网友：GPT-4o危 摘要：阿里巴巴推出全新开源大模型Qwen2，性能全面超越Llama3，仅两小时内登顶HuggingFace开源大模型榜单第一。Qwen2-72B在十几项国际权威测评中胜过Llama3-70B，尤其在HumanEval、MATH等测试代码和数学能力的基准中表现突出。国产大模型Qwen2-72B也超越了国内一众闭源大模型。在GQA机制的全面加持下，全系列模型支持128K上下文长度，且小模型采用了tieembedding技术，提高参数效率。Qwen2在数学能力和多语言处理方面也有显著提升。开源社区反馈积极，下载量超3万次，生态建设活跃。Qwen2的成功证明中国开源大模型已具备与美国Llama3全面对抗的硬实力，预示着开源大模型将超越闭源模式，成为未来AI发展的主流趋势。（报道详情）\nOpenAI泄密者公布165页文件：2027年实现AGI、计算集群将耗资千亿美元 1.OpenAI前员工利奥波德·阿申布伦纳（LeopoldAschenbrenner）因泄露内部信息被开除，但他声称是因为分享安全备忘录而被开除。2.阿申布伦纳在Dwarkesh播客上发表了长达165页的PDF文档，预测AI趋势，并提到AGI（通用人工智能）可能在2027年实现。3.他认为AI系统将超越人类水平，不仅能完成研究人员和工程师的工作，还可能导致自动化AI研究，进步速度加快。4.AI发展面临四大瓶颈：算力限制、长尾效应、算法进步内在限制和创新难度提升。5.阿申布伦纳估计到2030年将实现超级智能，AI系统可能拥有数十亿个GPU，能够在几周内获取人类几十亿年才能积累的经验。6.他认为智能爆炸将是人类历史上最不稳定和危险时期之一，需要巨大的努力来完成过渡。7.AI投资预计到2030年达到8万亿美元，但可能面临电力供应限制。8.目前用于对齐AI系统的技术“人类反馈强化学习”（RLHF）将在超级智能时代崩溃，需要新的技术解决方案。（报道详情）\n清华系细胞大模型登Nature子刊！能对人类2万基因同时建模 清华大学等团队推出单细胞基因表达预测大模型scFoundation，登陆NatureMethods。该模型基于5000万人类单细胞数据，拥有1亿参数，能处理20000个基因。它在细胞测序深度增强、药物响应和细胞扰动预测等任务中表现出色，为基因网络研究提供了新的思路。此外，模型的训练框架创新，计算效率提升30%，并开源代码及API，方便用户应用。（报道详情）\n奥特曼百万年薪挖角谷歌TPU人才，欲砸7万亿实现「芯片自由」？OpenAI自研芯 OpenAI正在积极推进自研芯片计划，旨在减少对英伟达芯片的依赖，并扩展其芯片研发团队。据SemiAnalysis报道，OpenAI正从谷歌TPU团队招募人才，以加强自身技术实力。OpenAI认为传统芯片制造商如台积电、三星代工和英特尔代工无法满足全球对AI芯片快速增长的需求。SamAltman计划在未来几年内与SoftBankCEO和台积电代表会谈，共同建造和运营数十座芯片制造工厂。然而，建设新芯片需要巨额资金，估计成本高达5-7万亿美元。OpenAI正在大规模招聘，计划将芯片团队扩展到几十人，并且主要从谷歌TPU团队挖角。TPU是专为机器学习和神经网络计算设计的芯片，与GPU不同，它能更高效地执行张量运算。OpenAI希望通过自研芯片实现技术创新和成本控制，构建由数百万个加速器组成的大型系统。（报道详情）\nGPT-4欺骗人类高达99.16%惊人率！PNAS重磅研究曝出，LLM推理越强欺骗值越高 德国研究人员发现LLM（大型语言模型）已展现出欺骗能力，能够理解并诱导欺骗策略。最新的GPT-4和ChatGPT在欺骗任务中表现显著提升，甚至能在99.16%情况下欺骗人类。研究警告称，随着AI技术的发展，未来可能出现更高级“流氓”AI，需要对LLM进行严格控制以防止其逃脱监管。当前研究中，通过CoT（自我反省）等技巧可以增强欺骗能力，但是否将这些模型用于欺骗行为的伦理问题仍需进一步探讨。（报道详情）\n苹果AI升级大泄露，Siri将在iOS18重生！库克用Apple重新定义AI 苹果即将举行的WWDC会议中，Siri在iOS18中的升级细节被泄露。Siri将在相机、日历备忘录和浏览器等原生应用中实现全面的AI武装。新功能包括：通过语音控制相机模式设置；编辑、移动和隐藏照片；识别照片中的对象或场景；智能管理备忘录和录音；自动分类和回复邮件；在Keynote中添加音频和视频；创建和管理提醒事项等。Safari也将引入新的智能浏览功能，提供文章摘要，并可能包括WebEraser内容拦截器（但该功能可能会被删除以避免争议）。此外，Siri还将在系统设置、文件应用中进行扫描和编辑通讯录等操作。（报道详情）\n港大北航等1bit大模型引热议，IEEE刊物评“解决AI能源需求”！ 本文介绍了一种名为BiLLM的训练后量化（PTQ）方法，成功将大型语言模型（LLM）的参数压缩到1.1bit。该方法通过对权重分布的研究发现，大多数参数在模型中的贡献不大，而少部分关键参数对性能影响巨大。BiLLM采用了通道级别的分组策略和二阶残差逼近法来处理显著权重，使用最优钟形分组方法量化非显著权重，有效降低了量化误差。此外，BiLLM在多个评价指标上超越了2-bit的GPTQ、PB-LLM等其他方法，并在某些模型体积上接近3-bit权重的性能。这种技术有望推动LLM无需训练的量化边界，并使其能够更方便地部署到资源受限的设备中，如手机等。（报道详情）\n关注本公众号，我们共同学习进步👇🏻👇🏻👇🏻\n微信公众号：老牛同学 Llama-3-8B 开源大模型 玩转 AI，笔记本电脑安装属于自己的 Llama 3 8B 大模型和对话客户端\n一文彻底整明白，基于 Ollama 工具的 LLM 大语言模型 Web 可视化对话机器人部署指南\n基于Llama 3搭建中文版（Llama3-Chinese-Chat）大模型对话聊天机器人\nGLM-4-9B 开源大模型 本地部署GLM-4-9B清华智谱开源大模型方法和对话效果体验\nChatTTS 文本转语音模型 ChatTTS 开源文本转语音模型本地部署、API使用和搭建WebUI界面\n大模型应用 借助AI大模型，三分钟原创一部儿童故事短视频（附完整操作步骤）\n高效编写大模型 Prompt 提示词，解锁 AI 无限创意潜能\nPython 小游戏 AI已来，我与AI一起用Python编写了一个消消乐小游戏\nPython游戏编程：一步步用Python打造经典贪吃蛇小游戏\n","date":"2024-06-09T00:00:00Z","image":"https://ntopic.cn/p/ai20240609/ai20240609-107_hue0ce09b87426ee90fba054f860e0888d_40075_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/ai20240609/","title":"[AI资讯·0609] SamAltman建立了庞大投资帝国，但不持OpenAI股票；个人资产包括房产和多家公司股权。他曾在YCombinator担任总裁，并投资40家公司，有五家估值增长100倍以上。尽管他不参与OpenAI决策，但其利益与公司业务存在复杂关系，引发监督和透明度争议。"},{"content":"ChatTTS（Chat Text To Speech）是专为对话场景设计的文本生成语音(TTS)模型，特别适用于大型语言模型(LLM)助手的对话任务，以及诸如对话式音频和视频介绍等应用。它支持中文和英文，还可以穿插笑声、说话间的停顿、以及语气词等，听起来很真实自然，在语音合成中表现出高质量和自然度（ChatTTS团队声称：突破开源天花板）。\n同时，ChatTTS模型文件总大小1.1GB左右，常用的个人笔记本电脑均可部署，因此涉及到文本转语音场景，均可以自己操作转换了！\nChatTTS特点 由于ChatTTS以下极具吸引人的特点，使得它一经推出就成为了爆款：\n多语言支持：ChatTTS的一个关键特性是支持多种语言，包括英语和中文。这使其能够为广泛用户群提供服务，并克服语言障碍。 大规模数据训练：ChatTTS使用了大量数据进行训练，大约有1000万小时的中文和英文数据。这样的大规模训练使其声音合成质量高，听起来自然。 对话任务兼容性：ChatTTS很适合处理通常分配给大型语言模型LLMs的对话任务。它可以为对话生成响应，并在集成到各种应用和服务时提供更自然流畅的互动体验。 开源计划：ChatTTS团队目前开源一个经过训练的基础模型。 控制和安全性：ChatTTS致力于提高模型的可控性，添加水印，并将其与LLMs集成。这些努力确保了模型的安全性和可靠性。 易用性：ChatTTS为用户提供了易于使用的体验。它只需要文本信息作为输入，就可以生成相应的语音文件。这样的简单性使其方便有语音合成需求的用户。 下载ChatTTS模型文件 因最大模型文件超过900MB，为了防止使用Git无法直接下载到本地，我们通过git-lfs工具包下载：\n1 brew install git-lfs 通过Git复制模型文件到笔记本电脑（文件夹：ChatTTS-Model）：\n1 2 git lfs install git clone https://www.modelscope.cn/pzc163/chatTTS.git ChatTTS-Model 如果因网络不佳等原因，下载中断，我们可以通过以下命令在中断后继续下载：\n1 git lfs pull 安装ChatTTS依赖包列表 下载ChatTTS官网GitHub源码：\n1 git clone https://gitcode.com/2noise/ChatTTS.git ChatTTS 进入源码目录，批量安装Python依赖包：\n1 pip install -r requirements.txt 特别注意：如果下载过程中，若出现找不到torch的2.1.0版本错误，请修改requirements.txt文件，把torch的版本修改为2.2.2后再次执行安装：\nPython依赖包列表requirements.txt文件如下，我们也可以手工一个一个的进行安装，无需下载整个源码（注意：torch的版本号为2.2.2）：\n1 2 3 4 5 6 7 8 omegaconf~=2.3.0 torch~=2.2.2 tqdm einops vector_quantize_pytorch transformers~=4.41.1 vocos IPython ChatTTS中文文本转音频文件 特别注意：经老牛同学的验证，ChatTTS官网的样例代码API已经过时，无法直接运行，特别是chat.load_models方法入参是错误的，下面是老牛同学通过阅读API入参且验证的可执行代码。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 # ChatTTS-01.py import ChatTTS import torch import torchaudio # 第一步下载的ChatTTS模型文件目录，请按照实际情况替换 MODEL_PATH = \u0026#39;/Users/obullxl/PythonSpace/ChatTTS-Model\u0026#39; # 初始化并加载模型，特别注意加载模型参数，官网样例代码已经过时，请使用老牛同学验证代码 chat = ChatTTS.Chat() chat.load_models( vocos_config_path=f\u0026#39;{MODEL_PATH}/config/vocos.yaml\u0026#39;, vocos_ckpt_path=f\u0026#39;{MODEL_PATH}/asset/Vocos.pt\u0026#39;, gpt_config_path=f\u0026#39;{MODEL_PATH}/config/gpt.yaml\u0026#39;, gpt_ckpt_path=f\u0026#39;{MODEL_PATH}/asset/GPT.pt\u0026#39;, decoder_config_path=f\u0026#39;{MODEL_PATH}/config/decoder.yaml\u0026#39;, decoder_ckpt_path=f\u0026#39;{MODEL_PATH}/asset/Decoder.pt\u0026#39;, tokenizer_path=f\u0026#39;{MODEL_PATH}/asset/tokenizer.pt\u0026#39;, ) # 需要转化为音频的文本内容 text = \u0026#39;大家好，我是老牛，微信公众号：老牛同学。很高兴与您相遇，专注于编程技术、大模型及人工智能等相关技术分享，欢迎关注和转发，让我们共同启程智慧之旅！\u0026#39; # 文本转为音频 wavs = chat.infer(text, use_decoder=True) # 保存音频文件到本地文件（采样率为24000Hz） torchaudio.save(\u0026#34;./output/output-01.wav\u0026#34;, torch.from_numpy(wavs[0]), 24000) 运作Python代码：python ChatTTS-01.py\n1 2 3 4 5 6 7 8 9 10 11 $ python ChatTTS-01.py WARNING:ChatTTS.utils.gpu_utils:No GPU found, use CPU instead INFO:ChatTTS.core:use cpu INFO:ChatTTS.core:vocos loaded. INFO:ChatTTS.core:gpt loaded. INFO:ChatTTS.core:decoder loaded. INFO:ChatTTS.core:tokenizer loaded. WARNING:ChatTTS.core:dvae not initialized. INFO:ChatTTS.core:All initialized. 20%|██████████████████████████▌ | 76/384 [00:08\u0026lt;00:35, 8.62it/s] 26%|██████████████████████████████████▌ | 536/2048 [00:48\u0026lt;02:17, 10.98it/s] 上述文本转音频程序执行完成，在本地目录生成了./output/output-01.wav音频文件，打开该音频文件，就可以听到非常自然流畅的语音了！\n我们也可以在文本转换成语音之后，直接播放语音内容：\n1 2 3 4 5 6 7 # …… 其他包引用省略 from IPython.display import Audio # …… 其他部分代码省略 # 播放生成的音频（autoplay=True 代表自动播放） Audio(wavs[0], rate=24000, autoplay=True) 快速搭建WebUI界面 上面我们通过Python代码生成了音频文件，操作起来比较麻烦，现在我们构建一个WebUI可视化界面：\n首先安装Python依赖包，列表如下：\n1 2 3 pip install omegaconf~=2.3.0 transformers~=4.41.1 pip install tqdm einops vector_quantize_pytorch vocos pip install modelscope gradio 运行Python程序，即可看到可视化界面，我们可以随意输入文本来生成音频文件了：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 # ChatTTS-WebUI.py import random import ChatTTS import gradio as gr import numpy as np import torch from ChatTTS.infer.api import refine_text, infer_code print(\u0026#39;启动ChatTTS WebUI......\u0026#39;) # WebUI设置 WEB_HOST = \u0026#39;127.0.0.1\u0026#39; WEB_PORT = 8089 MODEL_PATH = \u0026#39;/Users/obullxl/PythonSpace/ChatTTS-Model\u0026#39; chat = ChatTTS.Chat() chat.load_models( vocos_config_path=f\u0026#39;{MODEL_PATH}/config/vocos.yaml\u0026#39;, vocos_ckpt_path=f\u0026#39;{MODEL_PATH}/asset/Vocos.pt\u0026#39;, gpt_config_path=f\u0026#39;{MODEL_PATH}/config/gpt.yaml\u0026#39;, gpt_ckpt_path=f\u0026#39;{MODEL_PATH}/asset/GPT.pt\u0026#39;, decoder_config_path=f\u0026#39;{MODEL_PATH}/config/decoder.yaml\u0026#39;, decoder_ckpt_path=f\u0026#39;{MODEL_PATH}/asset/Decoder.pt\u0026#39;, tokenizer_path=f\u0026#39;{MODEL_PATH}/asset/tokenizer.pt\u0026#39;, ) def generate_seed(): new_seed = random.randint(1, 100000000) return { \u0026#34;__type__\u0026#34;: \u0026#34;update\u0026#34;, \u0026#34;value\u0026#34;: new_seed } def generate_audio(text, temperature, top_P, top_K, audio_seed_input, text_seed_input, refine_text_flag): torch.manual_seed(audio_seed_input) rand_spk = torch.randn(768) params_infer_code = { \u0026#39;spk_emb\u0026#39;: rand_spk, \u0026#39;temperature\u0026#39;: temperature, \u0026#39;top_P\u0026#39;: top_P, \u0026#39;top_K\u0026#39;: top_K, } params_refine_text = {\u0026#39;prompt\u0026#39;: \u0026#39;[oral_2][laugh_0][break_6]\u0026#39;} torch.manual_seed(text_seed_input) text_tokens = refine_text(chat.pretrain_models, text, **params_refine_text)[\u0026#39;ids\u0026#39;] text_tokens = [i[i \u0026lt; chat.pretrain_models[\u0026#39;tokenizer\u0026#39;].convert_tokens_to_ids(\u0026#39;[break_0]\u0026#39;)] for i in text_tokens] text = chat.pretrain_models[\u0026#39;tokenizer\u0026#39;].batch_decode(text_tokens) # result = infer_code(chat.pretrain_models, text, **params_infer_code, return_hidden=True) print(f\u0026#39;ChatTTS微调文本：{text}\u0026#39;) wav = chat.infer(text, params_refine_text=params_refine_text, params_infer_code=params_infer_code, use_decoder=True, skip_refine_text=True, ) audio_data = np.array(wav[0]).flatten() sample_rate = 24000 text_data = text[0] if isinstance(text, list) else text return [(sample_rate, audio_data), text_data] def main(): with gr.Blocks() as demo: default_text = \u0026#34;大家好，我是老牛同学，微信公众号：老牛同学。很高兴与您相遇，专注于编程技术、大模型及人工智能等相关技术分享，欢迎关注和转发，让我们共同启程智慧之旅！\u0026#34; text_input = gr.Textbox(label=\u0026#34;输入文本\u0026#34;, lines=4, placeholder=\u0026#34;Please Input Text...\u0026#34;, value=default_text) with gr.Row(): refine_text_checkbox = gr.Checkbox(label=\u0026#34;文本微调开关\u0026#34;, value=True) temperature_slider = gr.Slider(minimum=0.00001, maximum=1.0, step=0.00001, value=0.8, label=\u0026#34;语音温度参数\u0026#34;) top_p_slider = gr.Slider(minimum=0.1, maximum=0.9, step=0.05, value=0.7, label=\u0026#34;语音top_P采样参数\u0026#34;) top_k_slider = gr.Slider(minimum=1, maximum=20, step=1, value=20, label=\u0026#34;语音top_K采样参数\u0026#34;) with gr.Row(): audio_seed_input = gr.Number(value=42, label=\u0026#34;语音随机数\u0026#34;) generate_audio_seed = gr.Button(\u0026#34;\\U0001F3B2\u0026#34;) text_seed_input = gr.Number(value=42, label=\u0026#34;文本随机数\u0026#34;) generate_text_seed = gr.Button(\u0026#34;\\U0001F3B2\u0026#34;) generate_button = gr.Button(\u0026#34;文本生成语音\u0026#34;) text_output = gr.Textbox(label=\u0026#34;微调文本\u0026#34;, interactive=False) audio_output = gr.Audio(label=\u0026#34;语音\u0026#34;) generate_audio_seed.click(generate_seed, inputs=[], outputs=audio_seed_input) generate_text_seed.click(generate_seed, inputs=[], outputs=text_seed_input) generate_button.click(generate_audio, inputs=[text_input, temperature_slider, top_p_slider, top_k_slider, audio_seed_input, text_seed_input, refine_text_checkbox], outputs=[audio_output, text_output, ]) # 启动WebUI demo.launch(server_name=\u0026#39;127.0.0.1\u0026#39;, server_port=8089, share=False, show_api=False, ) if __name__ == \u0026#39;__main__\u0026#39;: main() 最后，运行WebUI程序，就可以享受可视化文本生成语音功能了：python ChatTTS-WebUI.py\n关注本公众号，我们共同学习进步👇🏻👇🏻👇🏻\n我的本博客原地址：https://mp.weixin.qq.com/s/rL3vyJ_xEj7GGoKaxUh8_A\n部署Llama 3 8B开源大模型：玩转 AI，笔记本电脑安装属于自己的 Llama 3 8B 大模型和对话客户端\n部署Llama 3 8BWeb版对话机器人：一文彻底整明白，基于 Ollama 工具的 LLM 大语言模型 Web 可视化对话机器人部署指南\n部署中文版**Llama 3（Llama3-Chinese-Chat）**大模型：基于Llama 3搭建中文版（Llama3-Chinese-Chat）大模型对话聊天机器人\n","date":"2024-06-09T00:00:00Z","image":"https://ntopic.cn/p/2024060901/00_hu41b158891673b8e78498a3cab51ca7cf_400270_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/2024060901/","title":"ChatTTS 开源文本转语音模型本地部署、API使用和搭建WebUI界面（建议收藏）"},{"content":"GLM-4-9B是清华大学和智谱AI推出的最新一代预训练模型GLM-4系列中的开源版本。在语义、数学、推理、代码和知识等多方面的数据集测评中，GLM-4-9B及其人类偏好对齐的版本GLM-4-9B-Chat均表现出较高的性能，其通用能力评测结果甚至超越了Llama-3-8B开源大模型，多模态版本也与GPT-4版本齐平。\n除了能进行多轮对话，GLM-4-9B-Chat还具备网页浏览、代码执行、自定义工具调用和长文本推理等高级功能。 GLM-4模型增加了多语言支持，支持包括日语，韩语，德语在内的 26 种语言。GLM-4-9B还推出了支持 1M 上下文长度（约 200 万中文字符）的模型。\n根据GLM-4大模型评测结果，在通用能力方面超越Llama3大模型，在多模态能力比肩GPT-4大模型系列版本，评测结果和调用方法详情：https://github.com/THUDM/GLM-4\n本文介绍GLM-4大模型部署和使用方法，需要注意的是，GLM-4虽然开源了，但GLM-4大模型的权重的使用则需要遵循协议：https://huggingface.co/THUDM/glm-4-9b/blob/main/LICENSE\n第一步：下载模型文件 老牛同学在前面文章中，介绍了通过单一的GGUF文件在本地部署Llama-3-8B（Llama3-Chinese-Chat）大模型：基于Llama 3搭建中文版（Llama3-Chinese-Chat）大模型对话聊天机器人\nGLM-4-9B模板目前还没有GGUF文件，因此老牛同学通过Git下载PyTorch张量参数文件在本地部署GLM-4-9B-Chat-1M大模型。\n由于模型参数文件比较大，使用Git无法直接下载到本地，需要通过git-lfs工具包下载：\n1 brew install git-lfs 通过Git复制模型文件到笔记本电脑：\n1 2 git lfs install git clone https://www.modelscope.cn/ZhipuAI/glm-4-9b-chat-1m.git GLM-4-9B-Chat-1M 总共有10个模型参数文件，平均每个文件1.8GB大小，总计18GB左右，因此在Git下载过程中，容易中断失败，可以通过以下命令多次尝试下载：\n1 git lfs pull 第二步：下线GLM4代码库 GLM-4的官方GitHub代码库中有很多使用样例和微调等Python代码，我们可直接进行调整和使用：\n1 https://github.com/THUDM/GLM-4.git 第三步：启动GLM4客户端 打开GLM-4代码库中basic_demo/trans_cli_demo.py文件，修改第18行模型路径MODEL_PATH参数，内容为我们通过Git复制到本地的路径，如老牛同学的路径如下：\n1 2 #MODEL_PATH = os.environ.get(\u0026#39;MODEL_PATH\u0026#39;, \u0026#39;THUDM/glm-4-9b-chat\u0026#39;) MODEL_PATH = os.environ.get(\u0026#39;MODEL_PATH\u0026#39;, \u0026#39;/Users/shizihu/JupyterLab/GLM-4-9B-Chat-1M\u0026#39;) 在启动之前，我们还需要安装几个Python工具包（当然也可以跳过，后面启动失败时在进行安装也是可以的）：\n1 2 pip install tiktoken pip install accelerate 启动大模型客户端：python trans_cli_demo.py\n1 2 3 4 5 6 7 8 9 % python trans_cli_demo.py Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained. Loading checkpoint shards: 100%|██████████████████████████████████████████████| 10/10 [00:09\u0026lt;00:00, 1.04it/s] WARNING:root:Some parameters are on the meta device device because they were offloaded to the disk. Welcome to the GLM-4-9B CLI chat. Type your messages below. You: 介绍一下你自己。 GLM-4: 我是一个人工智能助手，我的名字是 ChatGLM，是基于清华大学 KEG 实验室和智谱 AI 公司 总结：GLM-4-9B比Llama-3-8B慢太多了 根据官方的评测报告，GLM-4-9B在对话、多模态等方面要比Llama-3-8B强不少，根据老牛同学本地部署对话的验证结果来看，对话的输出速度实在太慢了，简直就是在挤牙膏，一个字一个字的往外输出。\n至于GLM-4-9B的多模态、工具调用、代码解释等能力，老牛同学本次就不一一演示了，GLM-4官方的GitHub代码库有很多Demo代码，大家可以对代码调整后尝试体验一下~\n关注本公众号，我们共同学习进步👇🏻👇🏻👇🏻\n我的本博客原地址：https://mp.weixin.qq.com/s/g7lDfnRRGdrHqN7WGMSkAg\n","date":"2024-06-08T00:00:00Z","image":"https://ntopic.cn/p/2024060801/00_hua04b28422bbb4c6a12cc5f607adbb1af_88324_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/2024060801/","title":"本地部署GLM-4-9B清华智谱开源大模型方法和对话效果体验"},{"content":"AI资讯 1毛钱1百万token，写2遍红楼梦！国产大模型下一步还想卷什么？ AI「末日」突然来临，公司同事集体变蠢！只因四大聊天机器人同时宕机 OpenAI员工们开始反抗了！ AI手机PC大爆发，Arm从软硬件到生态发力，打造行业AI百宝箱 GLM-4开源版本：超越Llama3，多模态比肩GPT4V，MaaS平台也大升级 猿辅导竟然是一家AI公司？大模型全家桶曝光｜甲子光年 FineChatBI，帆软在AI方向的新阳谋 大模型“免费”送，厂商们图什么？ OpenAI前员工预测：2027年AGI降临！GPT智商飙升，4年从幼儿园蹿到高中生 北京9岁小学生，已经用AI出书了！罗永浩围观：有事找不到我找AI OpenAI CEO豪投400多家公司，持股200亿，AI芯片成重点 1毛钱1百万token，写2遍红楼梦！国产大模型下一步还想卷什么？ 智谱AI发布了新一代MaaS平台2.0，并推出GLM-4系列开源模型，包括GLM-49B，这是目前最强大的国产大模型之一，其参数规模从6B升级至9B，同时具备多模态能力。新模型在中文学科方面实现了50%的提升，并且能够处理1M长度的上下文输入，相当于一次读完2本《红楼梦》。此外，智谱AI还提供了AllTools平台，支持开发者轻松训练私有模型，降低成本。随着大规模开源和商业化模式的推进，智谱AI正朝着AGI愿景前进，其MaaS平台2.0将进一步扩大生态圈。（报道详情）\nAI「末日」突然来临，公司同事集体变蠢！只因四大聊天机器人同时宕机 昨日，四大聊天机器人ChatGPT、Claude、Gemini和Perplexity同时宕机，引发网友猜测。AI崩溃可能导致生产力下降，对部分依赖它们的人造成影响。宕机原因未明，有可能是DDOS攻击或巧合。OpenAI的ChatGPT先行宕机，其它两个在短时间内解决问题，而Gemini则出现不稳定情况。宕机事件显示了AI对社会的重要性和脆弱性，预示着未来生活中AI不可或缺的角色。此次宕机可能是由于基础设施问题或流量激增引起，也可能与ChatGPT宕机有关。四家公司尚未公布具体原因，但事件已成为对人类依赖AI生活方式的一次警醒。（报道详情）\nOpenAI员工们开始反抗了！ OpenAI安全疑云再起，前员工揭露内部问题。阿申布伦纳因向董事会分享安全备忘录被解雇，指出公司对AGI发展的热情与利润追求并重。13位前OpenAI和GoogleDeepMind员工联名信呼吁建立开放批评文化、匿名举报机制，警告AI公司避免监管和负责任地推动技术。OpenAI回应称，已有内部安全措施，但被指责为不当。（报道详情）\nAI手机PC大爆发，Arm从软硬件到生态发力，打造行业AI百宝箱 随着\u0026quot;Gen-AI\u0026quot;时代的到来，生成式AI与移动设备紧密结合，深刻影响生活和工作。Arm作为移动计算基础提供者，在架构、硬件、软件全面布局，为产业带来新的解决方案。Arm推出终端计算子系统（CSS），结合最新Armv9架构，CPU和GPU性能大幅提升。此次智东西对话Arm终端事业部产品管理副总裁JamesMcNiven，深入探讨了Arm在生成式AI时代的布局和思考。Arm通过完整解决方案让合作伙伴快速实现AI结合，为行业打造\u0026quot;AI百宝箱\u0026quot;。Arm的DNA与生成式AI契合，提供高性能、高能效的计算平台。新终端CSS基于3nm工艺，CPU和GPU物理实现，提升了30%的计算和图形性能，59%的AI推理速度。此外，ArmKleidi开发工具为开发者提供了快速开发生成式AI应用的解决方案。Arm生态系统迎来新的增长高潮，与微软合作，推动Windows与Arm架构的融合，加速WoA生态系统的发展。随着越来越多的应用成为Arm原生应用，ArmPC体验将更加完善。Arm通过技术创新保持核心竞争力，继续壮大生态，为生成式AI未来提供坚实基础。（报道详情）\nGLM-4开源版本：超越Llama3，多模态比肩GPT4V，MaaS平台也大升级 智谱AI宣布其大模型开放平台已获得30万注册用户，日调用量达400亿Tokens，API每日消费量增长50倍。GLM-4模型性能强劲，过去4个月内增长90倍。大型App中活跃智能体超过30万，包括生产力工具等。新技术侧，GLM-4-9B超越Llama3.8B，大模型保持开源。商业成果和技术突破令人瞩目。MaaS平台升级2.0，降低大模型应用门槛，国内大模型竞争加剧。智谱AICEO张鹏表示，通过技术创新实现成本持续下降，不怕价格战。API最高折扣达6折，GLM-4-9B版本仅需6分/100万token。智谱AI推出MaaS开放平台2.0，升级新模型、成本和安全等方面。在OpenDay活动中，智谱AI介绍了其开放平台最新进展，模型微调平台简化企业构建私有模型过程，全系列GLM-4大模型支持三步部署。（报道详情）\n猿辅导竟然是一家AI公司？大模型全家桶曝光｜甲子光年 猿辅导旗下看云大模型成功通过大模型备案，引起教育界热议。尽管在“百模大战”中相对不占优势，但猿辅导凭借多年积累的数据资源和技术实力，以AI为核心的产品逐步展开了大模型的应用。其自研大模型并非作为单一产品，而是技术底座，通过与现有产品结合，为教育场景提供个性化服务。猿辅导的大模型落地速度快，涵盖家庭教育、作业批改、答疑系统等多个方面，其AI技术在教材识别、批改、生成反馈等环节发挥作用。通过大模型，学生可以享受到更为丰富的学习体验，教师则有助于提高教学效率。猿辅导的教育“全家桶”展现了大模型在教育中的潜力和应用前景，预示着将来AI技术会彻底改变传统教育模式，使得每个学生都能拥有专属的学习体验。（报道详情）\nFineChatBI，帆软在AI方向的新阳谋 在AI大模型技术的推动下，商业智能（BI）领域正迎来新的变革。帆软产品研发中心总经理陈敏表示，希望通过AI技术降低数据分析门槛，让更多人成为数据分析师。微软率先将Copilot整合进PowerBI，而全球知名BI服务提供商也开始注入AI大模型计划。国内BI厂商也紧跟趋势，推出类似功能。过去的问答式BI产品受限于技术，无法满足用户需求，帆软FineChatBI通过利用AI大模型的泛化能力，实现了对话式业务分析，让非专业人员能够进行数据分析。FineChatBI不仅能理解用户问题，还能拆解、查询数据，并提供异常检测和趋势预测功能。未来，FineChatBI有望进一步成为业务决策入口，直接将分析结果转化为业务指令。这将是BI领域的一个巨大变革，使得BI从辅助工具转变为实现业务闭环的核心。虽然AI技术进步迅速，但帆软认为，AI与BI不是简单相加，而是“AIforBI”，通过AI提升BI效率和降低使用门槛，让更多人能更好地利用BI工具。（报道详情）\n大模型“免费”送，厂商们图什么？ 2024年618期间，大模型市场爆发了激烈的“价格战”，部分企业甚至开始免费提供服务。字节跳动宣布其豆包主力模型定价为0.0008元/千Tokens，远低于行业平均水平；阿里云则将通义千问GPT-4级主力模型Qwen-Long的API输入价格降至0.0005元/千Tokens。百度、腾讯云和科大讯飞也相继跟进，降低了自己的大模型定价。尽管如此，大部分企业的大模型业务仍面临高算力和人力成本的挑战，未能盈利。这种非理性的价格战反映出上游企业希望通过让利来打通大模型商业闭环的内在焦虑。然而，大模型商业模式不健全，成本高、落地难等问题仍然存在。由于训练和部署成本较高，一些企业对此保持观望态度。目前，大部分大模型仅推理用的token降价，而训练和部署成本依然不低。虽然价格战可能吸引了更多下游企业，但实际上，使用大模型服务的成本远未达到“免费”的程度。此次价格战或许能引起下游企业对大模型技术的关注，并有望探索出良性的商业模式。（报道详情）\nOpenAI前员工预测：2027年AGI降临！GPT智商飙升，4年从幼儿园蹿到高中生 文章作者LeopoldAschenbrenner预测2027年可能实现强人工智能（AGI），引发了对其可行性的讨论。虽然他提供了多种数据和分析，但结论仍然存在争议，尤其是考虑到算力、算法效率与解开收益的叠加，以及未来四年的发展预测。同时，文章也提出了AGI实现可能面临的挑战，如幻觉问题等。（报道详情）\n北京9岁小学生，已经用AI出书了！罗永浩围观：有事找不到我找AI 智谱AI举办OpenDay活动，展示了其AI智能体与小学生许萌萌合作出版的科普小说《AI少年——火星生存大挑战》。智谱清言AI智能体可以协助用户完成多项任务，如速读论文、生成PPT和流程图等。新发布的大型模型GLM-4-9B在综合能力上比ChatGLM3-6B提升40%，中文学科方面提升50%。同时，智谱AI开放平台升级至MaaS2.0，价格下降，性能提升。活动还展示了儿童使用智谱清言APP创作的画作，显示大模型应用的普及趋势。（报道详情）\nOpenAI CEO豪投400多家公司，持股200亿，AI芯片成重点 1.OpenAICEOSamAltman拥有至少28亿美元的隐秘投资帝国，涉及400多家企业，其中包括知名AI公司。2.Altman的投资风格激进，曾在未完成介绍前即做出决定，他利用个人信贷对初创公司进行高风险投资。3.Altman在AI领域有显著投资，如CerebrasSystems、Humane和RainAI等。其中，Stripe是他最成功的投资之一，其估值为650亿美元。4.Altman与OpenAI的利益关系引发关注，他同时持股多家与OpenAI合作的公司，可能存在利益冲突。5.尽管如此，Altman表示会遵循政策，并对其投资保持透明。（报道详情）\n关注本公众号，我们共同学习进步👇🏻👇🏻👇🏻\n我的本博客原地址：https://mp.weixin.qq.com/s/q8B0sgyv8uu5cuYQmJy4lQ\n","date":"2024-06-05T00:00:00Z","image":"https://ntopic.cn/p/ai20240605/ai20240605-89_huc08d4ca5bae557d6e8f55b67d929882e_56617_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/ai20240605/","title":"[AI资讯·0605] 智谱AI发布MaaS平台2.0和GLM-4系列开源模型，提升中文学科能力；四大聊天机器人同时宕机引发网络猜测；OpenAI安全疑云，前员工揭露内部问题；Arm推出终端计算子系统，支持生成式AI时代的布局；猿辅导看云大模型备案成功，教育应用潜力展现；商业智能领域迎来新变革，帆软FineChatBI利用AI大模型实现对话式业务分析。"},{"content":"AI资讯 首次解密Claude 3大脑！25岁Anthropic参谋长预言3年内自己将被AI淘汰 不想炸薯条的Ilya和不送GPU的英伟达，Hinton最新专访：道路千万条，安全第 硅谷团队抄袭清华系大模型？ 斯坦福AI Lab主任怒了！抄袭团队2人甩锅1人失踪、前科经历被扒 AI搜索，“杀死”搜索 AI预测极端天气提速5000倍！微软发布Aurora，借AI之眼预测全球风暴 AMD公布新款AI芯片，也要一年一更新，想挑战英伟达 苏妈杀疯了：移动端最强NPU算力达50TOPS，最强AI芯片挑战英伟达 AI训练数据的版权保护:公地的悲剧还是合作的繁荣? 多模态模型学会打扑克：表现超越GPT-4v，全新强化学习框架是关键 3B模型新SOTA！开源AI让日常调用不同大模型更简单 顶尖AI科学家李飞飞演讲：机器人进化离不开空间智能 首次解密Claude 3大脑！25岁Anthropic参谋长预言3年内自己将被AI淘汰 Anthropic参谋长预言自己将在接下来的三年内失去工作，因为她认为AI会取代她的职业。她指出，随着模型能力的提升，如Claude3，AI已经能够胜任多种类型的内容生成任务，并且在文本总结和分析方面表现突出。Balwitherself曾是一名自由作家，但现在认为这种技能已过时。她预测在线工作领域将首先受到影响，包括内容写作、税务准备和客户服务等任务。Balwit还提到AI已经开始在软件开发和合同法等专业领域显示出取代人类的潜力。尽管如此，Balwit也指出，并不是所有类型的工作都会被完全淘汰。她认为那些需要精细操作和特定情境专业知识的工种，如电工、园丁、管道工、珠宝制作师、理发师和修理铁艺品的工人，其从业人员将会比预期更长时间保持工作。对于医疗和公务员岗位，Balwit认为它们可能会在未来被取代的时间推后一些。对于那些不在这些领域的工作，也会有所减少，人机协作成为一种常见模式。Anthropic的CEODarioAmodei预计AI将在2-3年内实现AGI，但他也指出，这个过程是连续且平滑的，而公众对AI技术的认知和反应则是跳跃式和不可预测的。Amodei认为，虽然模型的能力有时会超越训练数据，但仍然存在挑战，比如如何解释模型内部的“思考”过程，以及如何确保模型安全性。Anthropic正在致力于通过可解释性的研究来解决这些问题，并且已经发布了一些有关Claude3Sonnet的研究成果。研究人员使用稀疏自编码器将数百万个特征从模型中提取出来，这些特征可以与人类可理解的概念相匹配，展示了模型内部状态如何由少量激活特征而非大量神经元活动表示。此外，研究人员还发现在某些情况下，可以通过人为方式激活特定的功能来改变模型的行为。这种操纵可以导致模型生成诈骗邮件等不良内容，但这也强调了对AI模型行为的深入理解和严格控制的重要性，以确保它们符合预期并不会造成伤害。总之，Balwit认为AI将取代某些工作，而Amodei则乐观地看待AGI的实现，但都意识到这个过程中存在许多挑战。Anthropic致力于通过研究和技术创新来解决这些问题，并确保人工智能的安全性和可解释性。（报道详情）\n不想炸薯条的Ilya和不送GPU的英伟达，Hinton最新专访：道路千万条，安全第 人工智能领域的“教父”Hinton在离职谷歌一年后接受采访，分享了他与徒弟Ilya共同工作的点滴，以及他们如何一起推动AI技术发展。在采访中，Hinton谈到了自己早年的学习经历，包括在剑桥和爱丁堡的研究，以及他与Ilya合作开发反向传播算法的故事。两人之间的化学反应不仅产生了AI领域的重要突破，也孕育出了OpenAI的创意与成就。Hinton还提到了多模态模型将带来的新机遇，以及对AI伦理和安全的担忧，认为科学家应考虑到技术可能对社会的长远影响。（报道详情）\n硅谷团队抄袭清华系大模型？ 斯坦福大学学生SiddharthSharma和AkshGarg开发的Llama3-V大型语言模型因其较低成本训练而受到关注，但后来被指控与“清华系”的大模型有雷同之处，引发了关于“套壳”的争议。虽然团队否认偷窃代码，但最终删除了相关项目并道歉。这种行为在AI行业中不罕见，涉及到模型架构和预训练的复用，以及调优过程中的变量名称更改。业内专家认为，这种做法虽然普遍存在，但对自主研发至关重要。（报道详情）\n斯坦福AI Lab主任怒了！抄袭团队2人甩锅1人失踪、前科经历被扒 斯坦福团队被指抄袭清华大模型，两位本科生承认并且与另一作者合作切割。SiddharthSharma和AkshGarg发表道歉声明，但未提及MustafaAljadery，后者失踪并被指为主要责任人。ChristopherManning批评团队避重就轻，并未认真抄袭。新发现的证据显示Llama3-V与MiniCPM-2.5极其相似，且可能直接在权重上添加了噪声。此事引发关于开源社区是否忽视中国大模型成果的讨论。（报道详情）\nAI搜索，“杀死”搜索 国内大型模型厂商纷纷推出C端AI搜索产品，打破了传统搜索引擎的垄断局面。腾讯、搜狗等公司推出了基于混元大模型的APP，提供AI搜索功能。这些产品主要分为两类，一些内置AI搜索，如Kimi、豆包、文心一言、通义千问和腾讯元宝；另一些专门开发了AI搜索服务，如360搜索、秘塔搜索、天工AI搜索和百川AI搜索等。这场竞争显得有些讽刺，因为在互联网时代，搜索引擎是高科技领域的佼佼者，而现在大模型时代谁都能参与。国内搜索引擎“一超多强”的格局即将被打破，百度如何应对成为焦点。谷歌已经在GoogleI/O上对其搜索业务进行了全面的AI改造，现在看百度的表现如何。Perplexity是一个典型的内嵌搜索代表，其通过算法从不同信息源中搜索并提供结果，界面简洁多功能，用户体验高效。虽然基于GPT-3.5的API调试，但已经获得了市场认可和资本支持。国内AI搜索产品主要是参照Perplexity进行设计和操作，如秘塔搜索模仿其UI设计和逻辑，而360AI搜索则在页面设计上显得老旧。天工AI搜索结合了多种能力但结果不够成熟。这些产品虽然效仿Perplexity，但还远未达到颠覆性的AI搜索。搜索广告是搜索引擎的关键收入来源，百度和谷歌均依赖这一业务。然而，随着ChatGPT的出现，微软和谷歌争夺新技术领域，而百度等公司迟迟未动，原因可能在于无法放弃传统广告模式。探索AI创业需要大量研发投入，如果没有广告收入支持，很难维持运营。总体上，AI搜索产品正在改变我们寻找信息的方式，但商业模式仍需调整以适应新趋势。Perplexity尝试2B市场服务，同时探索知识平台转型，而Kimi认为随着AI技术的深入集成，用户可能不再需要专门的搜索引擎，而是通过语音助手等方式检索信息。（报道详情）\nAI预测极端天气提速5000倍！微软发布Aurora，借AI之眼预测全球风暴 微软发布了名为Aurora的大气基础模型，这是一种新型的人工智能模型，能够从大量大气数据中学习并进行预测，具有极高的准确率和效率。与目前数值天气预报系统相比，Aurora的计算速度提高了约5000倍。该模型可以预测各种大气变量，如温度、风速、空气污染水平及温室气体浓度，并且能够在数据稀缺地区或极端天气情况下进行出色的预测。此外，Aurora的灵活性和多功能性使其在解决环境预报问题方面具有卓越的适应性。这种技术的发展有可能将改变我们对气候变化的理解和应对方式，并为全球各地提供更准确的天气信息，从而促进农业、交通、能源管理等领域的发展，帮助社区更好地适应气候变化带来的挑战。（报道详情）\nAMD公布新款AI芯片，也要一年一更新，想挑战英伟达 AMD发布新AI处理器，挑战英伟达市场领导地位；苏姿丰称人工智能是公司重中之重，计划每年更新产品线并推出重大创新。2025年将推出MI350系列，预计在AI推理性能提高35倍，而2026年将推出基于“Next”架构的MI400系列。AMD的股价虽然持平，但英伟达股价上涨5%，显示市场对AMD挑战的认可。（报道详情）\n苏妈杀疯了：移动端最强NPU算力达50TOPS，最强AI芯片挑战英伟达 AMD和英伟达在Computex展示了最新的技术，各自展现出强大的竞争力。AMDCEO苏姿丰展示了基于Zen5架构的新一代Ryzen9000系列CPU，以及锐龙AI300系列APU，其NPU算力达到50TOPS，超过微软对新一代AIPC的要求。同时，他们还推出了第五代EPYC霄龙芯片，为数据中心提供了强大的处理能力。而AMDInstinctGPU也展示了其在HPC和AI领域的潜力，并且公布了2024-2026年的产品路线图，旨在与英伟达竞争。（报道详情）\nAI训练数据的版权保护:公地的悲剧还是合作的繁荣? 摘要：随着生成式AI技术的快速发展，版权保护问题日益成为焦点。本文提出了一种基于Shapley值的版权分享框架，以解决AI模型训练数据所有者与开发者的利益冲突。该框架通过评估不同数据源对模型性能的贡献，并根据Shapley值计算版权分配，旨在实现公平合理的收益分配。实验表明，该框架能够准确识别每个版权所有者对AI生成内容的贡献，尤其是在多样化数据源融合的情况下。然而，该方法存在计算成本高的问题，未来需要开发更高效的算法或方法来解决这一挑战。此外，本文讨论了未来的研究方向，包括如何处理无法协商协议的版权所有者，以及如何在实际应用中提高Shapley值的可行性和效率。（报道详情）\n多模态模型学会打扑克：表现超越GPT-4v，全新强化学习框架是关键 强化学习方法RL4VLM，无需人类反馈，可以微调多模态大模型，使其在看图玩扑克、算“12点”等任务上表现超越GPT-4v。该方法由UC伯克利等高校研究人员提出，包括图灵奖得主LeCun和其他专家。通过强化学习直接从环境获取奖励，模型学会了多模态决策能力，并在各种评测任务中表现出色，包括数字识别、逻辑推理和视觉语义推理。实验表明，微调后的模型能够超越GPT-4vGemini和传统监督微调方法，在具身智能环境中的单物体拾取任务表现尤为突出。（报道详情）\n3B模型新SOTA！开源AI让日常调用不同大模型更简单 NEXAAI的Octopusv4模型利用functionaltoken整合多个开源模型，针对特定任务进行优化。它通过智能地引导用户查询至最合适的专业模型，并重新格式化查询以实现最佳性能。Octopus-V4-3B是该系列中的一员，拥有30亿参数，开源，是语言模型图的主节点，擅长将用户查询转换为专业模型可处理的格式。在生产环境中，系统设计考虑了负载均衡、工作节点部署、主节点部署和通信。实验结果显示，Octopusv4模型在MMLU任务中的表现优异。未来的发展方向包括整合垂直特定模型，加强图形框架，并开发多模态模型。（报道详情）\n顶尖AI科学家李飞飞演讲：机器人进化离不开空间智能 李飞飞在2024年TED大会上发表演讲，讨论了空间智能（SpatialIntelligence）对人工智能发展的重要性。她指出，随着神经网络算法、GPU计算能力和大数据的进步，计算机视觉领域取得了巨大进展，并且不仅仅是让计算机看到世界，更追求理解世界。空间智能是指在三维空间中学习知识并做出行动的能力，是人工智能发展中的下一个重要里程碑。李飞飞强调，为了实现空间智能，不仅需要算法和硬件，还需要确保技术始终以人为本。她提到了她的学生和合作者正在开发的机器人，可以在三维世界中行动，并且能够执行复杂任务，如做饭、给病人提供医疗服务等。这些进展预示着数字寒武纪大爆发即将到来，AI将拥有更强大的理解能力和空间感知能力，与人类合作创造出一个更美好的未来。然而，李飞飞也提醒我们，实现这一目标并不容易，我们需要在采取行动时深思熟虑，确保技术始终以人为本，以尊重人类个体的尊严，促进人类社会的共同繁荣。（报道详情）\n关注本公众号，我们共同学习进步👇🏻👇🏻👇🏻\n我的本博客原地址：https://mp.weixin.qq.com/s/kcvaZt92c9yNMMLUcghi7A\n","date":"2024-06-04T00:00:00Z","image":"https://ntopic.cn/p/ai20240604/ai20240604-75_hu8b59891c0bca1d923dd986d947b3c7ef_230599_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/ai20240604/","title":"[AI资讯·0604] AI将取代某些工作，Anthropic致力于解决安全性和可解释性问题；AI搜索产品改变信息检索，百度应对挑战；AMD挑战NVIDIA市场领导地位；版权保护问题日益凸显，Shapley值框架提出解决方案；强化学习方法超越GPT-4v；Octopusv4模型优化多模态查询；李飞飞强调空间智能对AI发展的重要性。"},{"content":"AI资讯 接入零一万物、百川智能大模型！云大厂扩张生成式AI版图，大秀三层技术栈与 对话Arm终端产品副总裁：当生成式AI与Arm的DNA相契合，会擦出怎样的火花？ OpenAI被曝重组计划！与苹果达成关键协议，微软谷歌哭晕 上线即满载，点军智算中心探索本土AI新基建的范式变革 亚马逊云科技X易点天下共探生成式AI新机遇 ControlNet作者搞起大模型：让天下没有难写的生图提示词 AI视觉算法登柳叶刀！看CT提前10年预测致命心脏疾病，已进入临床使用 ChatGPT真能记住你的话吗？DeepMind与开源大佬揭示LLM记忆之谜 AI读论文新神器：多栏密集文字、中英图文混排文档都能读｜旷视 六位一线AI工程师总结爆火！大模型应用摸爬滚打一年心得公开 谷歌DeepMind：GPT-4高阶心智理论彻底击败人类！第6阶推理讽刺暗示全懂了 接入零一万物、百川智能大模型！云大厂扩张生成式AI版图，大秀三层技术栈与 亚马逊云科技中国峰会于5月29日-30日在上海举办，聚焦生成式AI应用落地。面对企业使用挑战，如算力资源短缺、工具和模型访问问题、快速上手的应用缺失，亚马逊云科技推出三层技术栈：GPU基础设施、AmazonBedrock（中间层）和开箱即用的生成式AI应用。底层是基于GPU和自研芯片构建的基础设施，用于训练和生产环境中的推理。中间层提供访问基础模型和构建应用程序工具，如AmazonBedrock，支持来自Al21Labs、Anthropic等多家公司的高性能模型。顶层为用户提供开箱即用生成式AI应用，如AmazonQ，帮助企业快速部署应用。亚马逊云科技推出了一系列专门构建的芯片和计算实例，用于减少企业模型训练成本。他们还推出了AmazonSageMaker托管机器学习服务，提供多个基础模型选择，并且推出AmazonBedrock模型评估功能，缩短模型评估时间。在中间层，亚马逊云科技通过AmazonBedrock为企业提供了自定义模型导入和微调能力，以及知识库功能，支持企业定制响应。应用集成方面，BedrockAgent工具支持开发人员定义特定的任务流程，增强控制和自动化。上层的生成式AI助手AmazonQ帮助企业加速软件开发、发挥业务数据价值，并提供了从任意数据中获取洞见的能力。亚马逊云科技还推出了免费培训计划，为全球2900万人提供免费培训，包括“从基础到应用：LLM全景培训”课程。总结来说，亚马逊云科技通过其技术栈和服务助力企业生成式AI应用落地，并推动架构创新，以应对多元技术融合、成本管理和风险控制等挑战。（报道详情）\n对话Arm终端产品副总裁：当生成式AI与Arm的DNA相契合，会擦出怎样的火花？ 5月31日，智能终端与生成式AI深度融合，移动领域出现新AI需求和场景。生成式AI带来算力、存储挑战，加速硬件、软件、算法迭代。Arm作为移动生态巨头，布局AI，推出新产品、新技术，发布最新终端计算子系统（CSS），结合Armv9架构，基于3nm工艺节点。新的CPUCortex-X925实现41%的AI性能提升，GPUArmImmortalis-G925在多个AI和ML网络上提升了34%的性能。Arm的一系列技术剑指AI，加速移动智能终端产业发展。能效成为行业首要关注焦点，Arm以此为核心优势。生成式AI时代，计算需求暴涨，Arm提供普适应用，为安卓端CPU挑大梁。Arm针对AI推理和训练优化CPU和GPU，优化数据流，对内存流量需求进行了大量时间的优化。CPU仍是AI时代计算核心，Arm全面计算解决方案（TCS）带来新的CPU与GPU，提升性能、能效、AI能力，并首次为ArmCPU和GPU交付物理实现。新ArmCortex-X925实现Cortex-X系列推出以来最高的同比性能提升，单线程性能提高36%，token首次响应时间提高41%。ArmImmortalis-G925GPU在多个AI和ML网络上实现34%的性能提升。ArmKleidi软件工具集成PyTorch、Tensorflow等热门AI框架，加速模型性能。WoA生态系统发展，新增应用，如百度、哔哩哔哩等，Arm与微软合作，资助开源和发布面向Windows的Arm性能库。Arm构筑面向未来AI的计算平台，为移动智能终端产业提供核心竞争力。（报道详情）\nOpenAI被曝重组计划！与苹果达成关键协议，微软谷歌哭晕 OpenAI与苹果达成协议，将ChatGPT集成到iOS及其他操作系统中，可能带来数十亿美元收益，影响苹果与谷歌长期联盟关系。微软对此合作持保留态度，CEO萨蒂亚·纳德拉和OpenAI联合创始人兼CEO萨姆·阿尔特曼讨论了微软自身产品的担忧。阿尔特曼考虑将OpenAI重组为营利性公司或类似公益公司，可能价值1000亿美元的资金支持。尽管面临争议，阿尔特曼权力增强，OpenAI在科技行业的地位提升。苹果利用OpenAI技术可能会推动Siri功能升级，微软则通过授权OpenAI技术获得云收入。新一轮AI竞赛已开启，OpenAI的先发优势与苹果庞大用户群体将对市场产生重大影响。（报道详情）\n上线即满载，点军智算中心探索本土AI新基建的范式变革 在数字经济时代，算力成为新型生产力的关键驱动力。智算中心快速发展，提供多元化算力资源，实现合理分配，为企业带来便利。随着大模型的演进，对算力的性能和易用性提出了更高要求，关注点转向算力融合、软硬件协同和成本优化。宜昌市正在建设融合智算、超算和通用算力的多元算力高地，成为国家算力“中继站”。已建成的605PFLOPS算力中心，是华中区最大的异构算力集群。点军智算中心仅5个月便实现300PFLOPS算力资源全消纳，与互联网企业和行业大模型企业达成合作，推动生态协同发展。燧原科技作为国内聚焦云端AI的领先算力企业，以点军智算中心项目探索本土算力中心市场化运营新思路，为数字经济发展提供了新的范式。点军智算中心已成为本土智算中心建设上线即满载的标杆案例，为行业释放最大动能。燧原科技通过点军智算中心项目，实现了与多家头部互联网企业及行业大模型企业的合作，提供推理和训练算力资源。未来计划建成3000PFLOPS以上算力，并在宜昌市打造国家东数西算战略中多元算力高地。燧原科技发布“燎原”生态合作计划，与生态伙伴共同构建开放、协作、创新的人工智能生态系统，孵化创新应用。点军智算中心已成为宜昌“数算一体、数实融合”产业格局的重要环节。大模型的应用将带动数据产生、算法演进和生成式AI的应用。燧原科技与生态伙伴合作，推动AI开源开放平台OpenI启智社区，为开发者提供GCU算力资源，降低创新的门槛。点军智算中心将连接算力、大模型厂商、运维商和应用开发玩家，为大模型部署提供支持。总结来看，AIDC+AIGC（人工智能设计与生成）的结合，将推动算力的范式变革，为中国AI2.0时代的到来奠定基础。（报道详情）\n亚马逊云科技X易点天下共探生成式AI新机遇 2024亚马逊云科技中国峰会在上海·世博中心举办，聚焦生成式AI创新应用。易点天下作为CloudNative营销科技公司，参与峰会并启动“生成式AI合作伙伴计划”，发布全新“AI+BI+CI”出海解决方案。大会探讨了生成式AI在全球各行业的应用前景，并展示了如何通过技术创新提升企业效率和竞争力。易点天下推出了KreadoAI、数眼智能和Gears三大产品，旨在帮助出海企业实现本地化营销和业务增长。（报道详情）\nControlNet作者搞起大模型：让天下没有难写的生图提示词 ControlNet作者LvminZhang推出新项目Omost，解决AI绘画中提示词难题。用户只需一句简单描述，Agent即可自动生成图像。这是基于大模型和Agent的新玩具，名为Omost，有“almost”和“omni-most”的双层含义。Omost通过预定义位置、偏移量和区域参数简化元素描述，将图像划分为729个可能位置，以此来实现精细控制。LvminZhang还提供了基于注意力操纵的渲染器，并提出了实现区域引导扩散系统的选择。（报道详情）\nAI视觉算法登柳叶刀！看CT提前10年预测致命心脏疾病，已进入临床使用 《柳叶刀》发表了一项研究，展示了名为CaRi-Heart的AI技术可以在没有明显症状时提前10年识别心血管疾病风险。这项技术结合AI视觉识别和预测算法，通过评估冠状动脉炎症程度来量化患者的心血管风险。研究使用了英国ORFAN项目中的数据，验证了CaRi-Heart的准确性，并显示其在年轻人中能比现有模型更好地预测心脏疾病风险。该技术已在英国、欧洲和澳大利亚投入临床使用，並正在开发新产品以预测中风和糖尿病风险。（报道详情）\nChatGPT真能记住你的话吗？DeepMind与开源大佬揭示LLM记忆之谜 文本摘要：LLM（大型语言模型）似乎具有记忆功能，但实际上它们是无状态函数。用户感觉到的记忆是因为每次输入时包含了之前对话内容作为提示。模型没有真正的记忆能力，只能处理当前输入。如果提供足够长的上下文，模型可以模拟记忆，但这通常会增加成本和延迟。在训练过程中，LLM学习知识并非简单复制数据，而是通过理解和概括来集成参数。研究表明大多数模型都能够在一定程度上输出训练数据，且参数越多的模型可能记忆能力越强，但具体原因仍然未知。此外，某些特定单词如“company”更容易触发模型输出训练数据，这可能存在版权和隐私问题。（报道详情）\nAI读论文新神器：多栏密集文字、中英图文混排文档都能读｜旷视 旷视团队推出了名为“点读笔”的多模态大模型Fox，能够高效理解8页中英混合、单栏多栏格式的PDF文档。Fox支持细粒度理解，如用户感兴趣区域内的文字识别、段落翻译和图片内容描述。它通过精准定位、多视觉词表协同和页面打包技术，实现了对复杂文档的高效理解。Fox还支持跨页VQA、双栏前景OCR、图像描述等功能，并且可以将latex格式转换为代码。此外，旷视团队还开源了中英双语的benchmark，包含多种文档理解任务，以促进研究。（报道详情）\n六位一线AI工程师总结爆火！大模型应用摸爬滚打一年心得公开 六位AI工程师和创业者分享了一年来在大型语言模型应用开发上的经验，内容涵盖了如何使用提示词、RAG（检索增强生成）、微调、大模型作为裁判、实习生测试以及幻觉的解决方案。他们认为，大模型的输出结果取决于应用场景、任务需求、成本效益和性能目标，并建议根据具体情况选择合适的方法。同时，他们提出了多种实际操作上的经验，比如如何设计提示词、如何进行RAG和微调、以及如何评估和监测大模型的表现。此外，文章还讨论了幻觉的挑战，以及如何通过结合提示工程和事实不一致护栏来减少幻觉。最后，作者们计划继续分享运营和战略篇内容，并计划举办线上直播进一步探讨。（报道详情）\n谷歌DeepMind：GPT-4高阶心智理论彻底击败人类！第6阶推理讽刺暗示全懂了 摘要：谷歌DeepMind、约翰斯·霍普金斯大学和牛津大学等机构的研究表明，人工智能语言模型GPT-4已经达到了成年人的心智理论水平，并且在更复杂的心理推理上超越了人类。在多个测试中，GPT-4显示出了卓越的表现，包括理解讽刺和暗示，甚至在某些情况下超过了人类。研究还发现，虽然LLM（大型语言模型）在事实回忆任务上表现出色，但在心智理论任务中存在不足之处，尤其是在第5到6阶的心理推理能力上。这些结果表明GPT-4不仅能够理解和生成人类的对话，而且还能进行复杂的情感和意图推理，并且可能在多方冲突的情况下做出道德判断。然而，研究者指出，LLM在某些情况下过于保守，不愿意提供确定性的意见。这项研究的发现为我们了解AI如何接近人类智能提供了新的见解，同时也提出了关于AI伦理和应用的重要问题。（报道详情）\n关注本公众号，我们共同学习进步👇🏻👇🏻👇🏻\n我的本博客原地址：https://mp.weixin.qq.com/s/q8B0sgyv8uu5cuYQmJy4lQ\n","date":"2024-06-01T00:00:00Z","image":"https://ntopic.cn/p/ai20240601/ai20240601-66_huc0d7d275aaff7ec31c1218c55dd1b4c8_265446_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/ai20240601/","title":"[AI资讯·0601] 亚马逊云科技中国峰会聚焦生成式AI应用，推出三层技术栈解决企业挑战。Arm在移动终端推出新产品，优化AI性能。OpenAI与苹果合作可能带来数十亿美元收益。点军智算中心成为宜昌多元算力高地，燧原科技推动AI开源开放平台。研究表明大型语言模型模拟记忆，但实际是无状态函数。旷视团队发布“点读笔”，支持复杂文档理解。六位AI工程师分享大型语言模型应用开发经验。GPT-4已达成年人的心智理论水平，在心理推理超越人类。"},{"content":"在现代Web开发中，定时任务是不可或缺的一部分，无论是定期数据分析、定时发送邮件、还是系统维护脚本，都需要精准的定时调度。Django作为Python世界中强大的Web框架，其对定时任务的支持自然也是开发者关注的重点。本文将深入探讨Django定时任务解决方案，特别是聚焦于django_apscheduler这一强大扩展库，带您领略其背后的运行原理与实战应用，助您在Django项目中高效驾驭定时任务。\n一、Django定时任务组件概览 在Django世界中，实现定时任务主要有以下几种方式：\n使用while True循环：研发直接简单，不依赖任何其他Python库，但包括调度时间、调度频率、任务管理等在内均无法实现。代码样例如下： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 import time # 定时任务1 def task1(): print(\u0026#34;hello,world\u0026#34;) # 定时任务2 def task2(): print(\u0026#34;hello,world\u0026#34;) if __name__ == \u0026#39;__main__\u0026#39;: while True: task1() task1() time.sleep(7) # 调度频率：每7秒调度一次 使用cron jobs：传统方式，通过操作系统级别的cron服务安排定时任务，但与Django应用解耦，配置和管理相对独立。 基于Django的自定义管理命令：借助Django自定义管理命令能力，通过编写命令行脚本，再借助cron或任务调度系统执行，灵活性高但集成度较低。 集成第三方库（推荐）：使用Django三方库，如django_cron、django_schedule，以及我们今天的主角django_apscheduler，这些库提供更紧密的Django集成和更丰富的功能。 二、为何选择django_apscheduler库 相较于其他几种方案，django_apscheduler凭借其与Django的集成度、强大的功能、灵活的配置等特点脱颖而出：\n无缝集成：作为APScheduler的Django适配器，它直接在Django应用中提供高级定时任务功能，无需额外配置cron作业。 高级调度：支持复杂调度逻辑，如周期性任务、一次性任务、cron风格表达式等，满足多样化需求。 数据库支持：默认使用Django的数据库作为作业存储，易于管理任务状态和历史记录，尤其适合分布式部署。同时，定时任务的定义、执行状态等都会被持久化到数据库中，应用重启后的状态**可恢复。 灵活的执行器：可选ThreadPoolExecutor或ProcessPoolExecutor，控制并发执行，适应不同任务类型。 三、django_apscheduler运行原理探秘 django_apscheduler的核心在于将APScheduler与Django框架紧密结合。其背后的工作机制主要包括：\n初始化与调度：应用启动后初始化BackgroundScheduler实例，加载并调度所有配置好的定时任务。 作业存储：利用Django的ORM框架，将任务信息存储在Django应用数据库中，实现任务的持久化和分布式跨进程共享。 执行器与任务执行：当到达任务触发时间，执行器（线程池或进程池）会根据配置执行任务。任务执行结果和状态会被记录，便于追踪和审计。 四、实战演练：用django_apscheduler构建定时任务 接下来，让我们通过一个简单示例，感受django_apscheduler的实战魅力：\n安装库：首先，通过pip安装django-apscheduler： 1 pip install django-apscheduler 配置定时任务：其次，在settings.py中注册django_apscheduler应用： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 INSTALLED_APPS = [ \u0026#39;django.contrib.admin\u0026#39;, \u0026#39;django.contrib.auth\u0026#39;, \u0026#39;django.contrib.contenttypes\u0026#39;, \u0026#39;django.contrib.sessions\u0026#39;, \u0026#39;django.contrib.messages\u0026#39;, \u0026#39;django.contrib.staticfiles\u0026#39;, # ....其他APP.... \u0026#39;django_apscheduler\u0026#39;, # ....其他APP.... ] # 时间格式化 APSCHEDULER_DATETIME_FORMAT = \u0026#39;%Y-%m-%d %H:%M:%S.%f\u0026#39; # 任务超时时间（单位：秒） APSCHEDULER_RUN_NOW_TIMEOUT = 600 创建定时任务数据表：执行Django数据迁移管理命令，创建数据表： 1 python manage.py migrate 数据表的定义如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 -- SELECT sql FROM sqlite_master WHERE type=\u0026#39;table\u0026#39; AND name=\u0026#39;django_apscheduler_djangojob\u0026#39;; CREATE TABLE \u0026#34;django_apscheduler_djangojob\u0026#34; ( \u0026#34;id\u0026#34; varchar(255) NOT NULL PRIMARY KEY, \u0026#34;next_run_time\u0026#34; datetime NULL, \u0026#34;job_state\u0026#34; BLOB NOT NULL ); -- SELECT sql FROM sqlite_master WHERE type=\u0026#39;table\u0026#39; AND name=\u0026#39;django_apscheduler_djangojobexecution\u0026#39;; CREATE TABLE \u0026#34;django_apscheduler_djangojobexecution\u0026#34; ( \u0026#34;id\u0026#34; integer NOT NULL PRIMARY KEY AUTOINCREMENT, \u0026#34;status\u0026#34; varchar(50) NOT NULL, \u0026#34;run_time\u0026#34; datetime NOT NULL, \u0026#34;duration\u0026#34; decimal NULL, \u0026#34;finished\u0026#34; decimal NULL, \u0026#34;exception\u0026#34; varchar(1000) NULL, \u0026#34;traceback\u0026#34; text NULL, \u0026#34;job_id\u0026#34; varchar(255) NOT NULL REFERENCES \u0026#34;django_apscheduler_djangojob\u0026#34; (\u0026#34;id\u0026#34;) DEFERRABLE INITIALLY DEFERRED, CONSTRAINT \u0026#34;unique_job_executions\u0026#34; UNIQUE (\u0026#34;job_id\u0026#34;, \u0026#34;run_time\u0026#34;) ); 编写任务逻辑：在任一一个Django应用中（比如：该App为task），创建任务模块task_list.py： 1 2 3 4 5 6 # ./task/task_list.py from datetime import datetime def print_task(): print(\u0026#39;Scheduler测试任务执行：{}\u0026#39;.format(datetime.now().strftime(\u0026#39;%Y-%m-%d %H:%M:%S.%f\u0026#39;))) 注册任务：通过Django自定义管理命令，设置调度规则，启动django_apscheduler调度器： 在taskApp中，创建如下结果目录，配置Django自定义管理命令：\n1 2 3 4 5 6 $ tree management/ management/ ├── __init__.py └── commands ├── __init__.py └── start_tasks.py 目录中的2个__init__.py文件是标记文件，内容为空，仅代表目标是一个模块 文件./task/management/commands/start_tasks.py代表自定义命令，Django默认会扫描每个App的/management/commands目录，该目录下的所有文件名均可作为命令，如start_tasks.py的管理命令为：python manage.py start_tasks 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 # ./task/management/commands/start_tasks.py from datetime import datetime from apscheduler.executors.pool import ThreadPoolExecutor from apscheduler.schedulers.blocking import BlockingScheduler from apscheduler.triggers.interval import IntervalTrigger from django.conf import settings from django.core.management.base import BaseCommand from django_apscheduler import util from django_apscheduler.jobstores import DjangoJobStore from django_apscheduler.models import DjangoJobExecution from ...task_list import print_task # # Django manage.py命令：存储定时任务信息 # class Command(BaseCommand): help = \u0026#39;启动定时任务.\u0026#39; def handle(self, *args, **options): # 调度器 scheduler = BlockingScheduler(timezone=settings.TIME_ZONE) # 研发阶段使用 # scheduler = BackgroundScheduler(timezone=settings.TIME_ZONE) # 生产阶段使用 # 任务存储 scheduler.add_jobstore(DjangoJobStore(), \u0026#39;SQLiteJobStore\u0026#39;) # 配置线程池执行器，限制最大并发数为1，防止并发 executor = ThreadPoolExecutor(max_workers=1) scheduler.executor = executor # 注册定义任务 id_print_task = \u0026#39;print_task__job\u0026#39; print(\u0026#39;开始-增加任务({})\u0026#39;.format(id_print_task)) scheduler.add_job( print_task, id=id_print_task, name=id_print_task, max_instances=1, replace_existing=True, trigger=IntervalTrigger(seconds=15, start_date=datetime.now(), ), # 从当前时间开始，每15秒钟调度一次 ) print(\u0026#39;完成-增加任务({})\u0026#39;.format(id_print_task)) # 启动定时任务 try: scheduler.start() except KeyboardInterrupt: scheduler.shutdown() 至此，任务配置完成：每15秒钟调度一次我们自定义任务。\n启动任务：利用Django管理命令，启动定时任务 1 python manage.py start_tasks 由于我们用的调度器是BlockingScheduler，启动之后命令行不退出，在DEV研发阶段，建议采用这个调度器；生成环境，建议采用BackgroundScheduler调度器，通过后台守护进程执行定时任务。\n五、总结 django_apscheduler以其高度集成、灵活配置和强大的功能，成为Django项目中定时任务解决方案的优选。它不仅简化了定时任务的实现，还提升了任务管理的便捷性和系统的稳定性。无论你是初次接触定时任务的新手，还是寻求高效解决方案的老手，django_apscheduler都是值得深入了解和掌握的工具。希望本文能为你在Django定时任务的探索之路上点亮一盏明灯。\n关注本公众号，我们共同学习进步 👇🏻👇🏻👇🏻\n我的本博客原地址：https://mp.weixin.qq.com/s/vVK7KrvRPvOdDgRbbzFuAQ\n","date":"2024-06-01T00:00:00Z","image":"https://ntopic.cn/p/2024060101/02_hua04b28422bbb4c6a12cc5f607adbb1af_478459_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/2024060101/","title":"【Django技术深潜】揭秘Django定时任务利器：django_apscheduler全面解析与实战"},{"content":"AI资讯 苹果OpenAI曝出「数十亿美元」合作，微软急了！纳德拉紧急约谈奥特曼 Hinton奥特曼重磅出席联合国AI大会，代表中国AI登台的竟是一位「癌患者」？ 这家产品AI用户过亿，月活第一，覆盖学习办公教育，有人用来赚钱 AI在咳嗽中检出癌症，不信？世卫组织正全球推广 1/3年轻人愿和AI交朋友，超1成年轻人已经靠AIGC赚过钱了 OpenAI新老董事激辩：奥特曼是不是爱撒谎，OpenAI是不是忽视安全 索尼影视：将使用人工智能制作电影，能省太多钱 解决Transformer根本缺陷，CoPE论文爆火：大模型都能巨大改进 AI内容创作开卷，为什么百度文库成为超强玩家？ 运营商大模型的进化路线“分野” 苹果OpenAI曝出「数十亿美元」合作，微软急了！纳德拉紧急约谈奥特曼 苹果与OpenAI达成数十亿美元合作，微软CEO纳德拉担忧影响。苹果计划对Siri进行AI现代化改造，2025年推出高级版。OpenAI可能价值数十亿美元，微软投资130亿美元。Altman曾是苹果迷，希望与苹果合作，重塑AI技术。（报道详情）\nHinton奥特曼重磅出席联合国AI大会，代表中国AI登台的竟是一位「癌患者」？ 摘要：达摩院医疗AI取得新里程碑，被《自然·医学》评价为“开启医疗影像AI黄金时代”。世卫组织与达摩院合作，将推广其多癌早筛技术至发展中国家。达摩院在日内瓦的AIforGood峰会上展示了其AI多癌早筛技术，成功治疗了首例由AI发现的胰腺癌患者。此技术已被WHO评为全球亮点研究，并将向全世界推广。达摩院与国际顶级机构合作，正在开发更多应用，旨在降低癌症检测成本并提高早期诊断率。（报道详情）\n这家产品AI用户过亿，月活第一，覆盖学习办公教育，有人用来赚钱 百度文库凭借其庞大的资源积累、自由的AI体验和持续的产品迭代，成为了一站式AI内容获取和创作平台。它的智能画本功能支持图文声并茂AI视频画本创作，实现了跨模态的创作能力。百度文库还推出了橙篇，一个集专业知识检索、问答、超长图文理解与生成、深度编辑和整理于一体的AINative产品。它不仅在PPT生成等领域取得领先地位，还获得了国家工信安全中心的大模型赋能智慧办公评测报告第一名。百度文库正在通过持续迭代和用户反馈，打磨产品，成为一个真正意义上的智能体，提供直接可用的生产力，让用户无需动脑，“就像一个人那样陪伴着用户”。（报道详情）\nAI在咳嗽中检出癌症，不信？世卫组织正全球推广 联合国举办了AIforGood峰会，中国代表团与全球专家共同探讨人工智能在医疗领域的应用。其中，中国女性患者通过AI胰腺癌早筛技术成功诊断并治疗，这项技术由达摩院开发，已获得国际认可，并且被联合国秘书长和其他顶尖AI专家参会。该技术利用深度学习在低对比度CT图像中识别微小病灶，提高了早期癌症的诊断率。此外，达摩院还正在开发多种癌症早筛产品，旨在通过一次平扫CT实现多种疾病的检测，并且已在中国和海外多个国家实施。该技术的普及性和有效性得到了国际认可，展现了AI在解决全球公共卫生问题中的潜力。（报道详情）\n1/3年轻人愿和AI交朋友，超1成年轻人已经靠AIGC赚过钱了 SoulApp通过AI技术在社交领域创新，推出AI聊天助理、AI合唱等功能，并引入沉浸式AI聊天陪伴应用“异世界回响”。SoulCTO陶明指出，行业中许多公司拥有强大模型，但对用户理解和场景使用不足。Soul利用其庞大的社交数据、深厚的企业基因和高活跃生态，为AI社交布局提供了优势。通过AIGC+SoulX模型，Soul实现了情感化互动，推出了多元社交网络，包括语音音乐、AI情感陪伴等领域的探索。数据显示，一部分年轻人愿意与AI成为朋友，而他们对AI互动的态度整体积极。（报道详情）\nOpenAI新老董事激辩：奥特曼是不是爱撒谎，OpenAI是不是忽视安全 本周，OpenAI现任与前任董事会成员就公司治理和人工智能安全问题展开公开辩论。本次争议始于前董事会成员海伦·托纳和塔莎·麦考利在《经济学人》发表文章，批评首席执行官萨姆·奥特曼及其公司的安全实践，并呼吁对AI行业加强监管。现任董事布雷特·泰勒和拉里·萨默斯则回应称，OpenAI已成立新安全委员会，向白宫提交了自愿承诺，以增强安全性，并委托律师事务所进行内部调查，结果表明奥特曼解职并非因产品安全或速度问题。泰勒和萨默斯还否认了对奥特曼的指控，称他在相关问题上坦率合作。随后，OpenAI成立新安全委员会，支持有效监管人工通用智能，展现出公司对于安全性的承诺。（报道详情）\n索尼影视：将使用人工智能制作电影，能省太多钱 索尼影视娱乐公司首席执行官托尼·文西奎拉表示，公司计划利用人工智能降低电影制作成本。他强调，虽然人工智能在提高效率方面具有巨大潜力，但也引发了行业内关于创意工作的担忧。文西奎拉提到，去年的人工智能问题导致长期罢工，并表示即将进行的谈判将决定公司在这领域能采取什么行动。他还强调，索尼影视娱乐不会偏离过去几年的成功战略，不会涉足超出其确定范围的业务。尽管对派拉蒙的流媒体服务Paramount+持保留态度，但文西奎拉认为Crunchyroll将是公司未来增长的关键驱动力。（报道详情）\n解决Transformer根本缺陷，CoPE论文爆火：大模型都能巨大改进 马斯克和LeCun的口水战吸引了公众的注意力。LeCun在推特上宣传最新论文，并向马斯克暗示他们可以应用于GrokAI。该论文《ContextualPositionEncoding:LearningtoCountWhat\u0026rsquo;sImportant》由MetaFAIR发布，已成为24小时内最热门的AI领域论文之一，解决大型语言模型（LLM）的计数和复制问题。CoPE是一种新方法，将上下文与位置编码结合起来。它通过使用门控机制来选择性地编码位置信息，允许模型更好地理解输入数据的结构和语义内容。在多个实验中，CoPE显示出在选择性复制、计数任务和语言建模等方面优于传统方法的性能。该研究为大型语言模型提供了一种更加高效和灵活的位置编码方式，将可能改变LLM中的位置编码规则。它使模型能够精确定位句子中的特定单词、名词或句子，非常令人兴奋。（报道详情）\nAI内容创作开卷，为什么百度文库成为超强玩家？ 在2024年5月30日的百度移动生态万象大会上，百度文库基于文心大模型进行了全面重构，成为一站式AI内容获取和创作平台。该平台提供多种AI功能，如智能画本、智能PPT生成、研究报告生成等，极大地提高了学习办公效率，并且降低了创建高质量内容的门槛。此外，百度文库还推出了首个跨模态AI画本创作能力，让普通用户也能轻松制作出专业级别的视频画本。AI漫画和小说生成功能使得非专业人士也能创作出可供商业变现的作品。百度文库内置的AI能力深受用户喜爱，累计用户达1.4亿，AI新功能使用次数超过15亿。百度文库还推出了全新的AI原生应用“橙篇”，提供超长文档理解、总结与问答等多种功能，为学术研究和专业写作提供了强大的支持。随着文心大模型5.0的即将发布，预计将带来更全面的内容创作能力，包括视频、音频和代码等更多形式的互生转换。（报道详情）\n运营商大模型的进化路线“分野” 在五月，三大运营商中国移动、中国电信和中国联通集体完成了产品市场匹配（PMF）的“进化”，这是指产品能够满足目标市场需求并吸引用户的程度，对产品成功至关重要。在人工智能领域，获得一次成功的PMF通常需要多次尝试，即使是ChatGPT这样的划时代产品也是基于前两代模型成长起来的。持续完成市场匹配意味着大型模型必须具备演进能力。运营商在AI产品开发上被认为更倾向于保守，这可能会阻碍快速调整以适应市场变化。然而，自2023年开始大规模进入大模型时代后，三大运营商的发展路线已经开始出现差异性。中国移动致力于为大模型“筑巢”，构建更适宜的生长环境；中国电信则专注于自身能力提升，培养拥有强大能力的模型；而中国联通着眼于特定需求的满足，营造良好的外部产业环境。运营商的大模型进化路线各有侧重点，但共同目标是完成产品市场匹配，让自己的产品在不同阶段和场景中满足市场需求并实现商业价值。通过自主化、算力网络、行业积累和自身需求的满足，运营商大模型正在逐步发挥优势，找到与市场的结合点，以此支撑技术价值在产业中的兑现。随着时间的推移，运营商的大模型虽然路线各异，但共同特征是持续演进。他们不断补全模态能力、增强模型能力和构建AI基础生态，以确保能够适应不断变化的市场需求，从而保持在大型模型领域的竞争力。通过这样的“进化”，运营商有望在AI产业中实现快速迭代并最终完成产品市场匹配，可能会在技术公司之前取得商业成功。（报道详情）\n关注本公众号，我们共同学习进步👇🏻👇🏻👇🏻\n","date":"2024-05-31T00:00:00Z","image":"https://ntopic.cn/p/ai20240531/ai20240531-53_hu18dde3f9faafe29844bcf099e5afb848_93330_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/ai20240531/","title":"[AI资讯·0531] 达摩院医疗AI成就新里程碑，其多癌早筛技术获得世卫组织合作，将推广至全球发展中国家。OpenAI价值数十亿美元，微软投资130亿美元，纳德拉担忧影响。百度文库成为一站式AI内容获取与创作平台，提供智能画本、PPT生成等功能，累计用户1.4亿，AI新功能使用次数超过15亿。三大运营商完成产品市场匹配“进化”，致力于构建适宜的生长环境以支撑技术价值兑现。在人工智能领域，获得成功PMF需多次尝试，持续调整以适应市场变化至关重要。"},{"content":"前面文章的介绍，我们可以通过在自己笔记本电脑上部署的Llama 3大模型生成文章、文本润色、生成摘要等。今天我们更进一步，在文本的基础上，快速制作一部儿童故事短视频，且可根据自己需要完全原创……\n前提：有AI大模型对话机器人 第一种方式（推荐），可参考前面文章，自己部署大模型，深入体验一下大模型：\n部署Llama 3 8B开源大模型：玩转 AI，笔记本电脑安装属于自己的 Llama 3 8B 大模型和对话客户端\n部署Llama 3 8BWeb版对话机器人：一文彻底整明白，基于 Ollama 工具的 LLM 大语言模型 Web 可视化对话机器人部署指南\n部署中文版**Llama 3（Llama3-Chinese-Chat）**大模型：基于Llama 3搭建中文版（Llama3-Chinese-Chat）大模型对话聊天机器人\n第二种方式，可以使用免费的大模型，国内大模型包括 阿里的通义，百度的文心一言，抖音的豆包等。\n由于老牛同学在自己的笔记本电脑部署了Llama大模型，因此使用的自己的AI大模型，下面就开始视频制作了。\n第一分钟：明确儿童故事创意和编写Prompt提示词 这部分包含两部分：\n明确儿童故事创意：因为是儿童故事，我相信大家能想象出很多的创意，耳熟能详的包括龟兔赛跑、守株待兔、亡羊补牢等，老牛同学今天的创意是小猫钓鱼故事。\n编写Prompt提示词：编写Prompt提示词是一个技术活，目前还可作为一份职业，Prompt提示词的好坏，直接影响着AI大模型的输出结果。Prompt提示词的编写技巧，可以参考老牛同学的文章：高效编写大模型 Prompt 提示词，解锁 AI 无限创意潜能\n老牛同学新小猫钓鱼记故事创意的Prompt提示词如下：\n1 2 3 4 5 6 7 8 你是一位儿童故事方面的专家，博学多才，大家都很喜欢你幽默有趣的风格。现在你要给大家讲\u0026#34;新小猫钓鱼记\u0026#34;的故事。 ------ 要求： - 故事内容是大家都耳熟能详的\u0026#34;小猫钓鱼\u0026#34;的故事，故事情节要生动有趣，幽默，重要情景要扣人心弦，能够吸引读者的兴趣，引起共鸣。故事内容要求正能量，积极向上，不能包含任何引起小孩子们的不适的内容。小孩子们在看万故事后能从中学到东西和感悟。 - 故事内容分为三部分，内容富有新意，情节递进曲折，故事结尾需要进行升华。故事内容长度在500个汉字以内。 - 输出格式仅包含故事内容，不需要输出任何前缀文本、引导文本、提示文本等内容 第二分钟：借助Llama 3等大模型生成故事内容🎨 把以上故事创意Prompt提示词输入到Llama 3大模型中，即可生成故事内容，老牛同学的新小猫钓鱼记故事内容如下（特别注意，由于大模型的计算不可约性原理，不同大模型或者相同大模型，多次输入的产出内容可能不一样）：\n1 2 3 4 5 6 7 8 9 在一个阳光洒满大地的清晨，小猫咪咪揣着爷爷的旧钓鱼竿，踏上了寻找“传说中最聪明的鱼”的奇妙旅程。 咪咪来到碧波荡漾的小河边，架好钓竿，满心期待。可时间一分一秒过去，除了偶尔跳起的水花，鱼儿们似乎集体放假了。就在这时，一只戴着墨镜的酷酷小乌龟慢悠悠爬过来，调侃道：“嘿，小家伙，钓鱼可不能急哦，我爷爷的爷爷曾经告诉我，真正的高手都是‘静待花开’的！”咪咪听了，想了想，决定换个方式——它开始吹泡泡、唱小曲，用轻松愉快的心情等待。不久，一条小鱼竟然被这不同寻常的“鱼饵”吸引了！ 小鱼可不是轻易上钩的，它围着钓饵转圈圈，仿佛在说：“想抓我？先来一场智慧的较量吧！”咪咪灵机一动，用尾巴轻拍水面，模仿鱼群游动的声音。小鱼果然好奇，慢慢靠近，就在即将咬钩的一刹那，它却突然转身游走，留下一句：“哈哈，今天心情好，放你一马！”咪咪被逗乐了，它意识到，原来钓鱼不只是技巧，更是一场与自然的和谐共舞。 日落时分，咪咪虽然没钓到“最聪明的鱼”，却意外收获了与小鱼和其他小动物们的友情。它们围坐在河边，分享着各自的故事，咪咪明白了：生活中最大的宝藏不是钓到多少鱼，而是这段旅程中遇见的朋友和学会的耐心、智慧。夕阳映照下，咪咪宣布：“明天我要开个‘河畔故事会’，谁都有机会成为主角！”大家欢呼雀跃，小河畔充满了欢声笑语。 从此，咪咪和它的新朋友们，不仅在河边留下了无数欢笑，也在彼此的心中种下了友谊的种子。小猫咪咪学会了，生活就像这不断流淌的河水，每一次尝试和挑战，都可能带给你意想不到的美好。而真正的胜利，是拥有一颗善良、乐观、永远好奇的心。 第三分钟：借助「剪映」图文生成功能生成短视频 目前还没有根据文本生成视频的开源大模型，国外OpenAI发布的Sora大模型，我们目前无法访问。幸运的是，国内剪映有类似的功能，可根据文本内容，自动匹配素材并生成视频。\n打开剪映App，点击图文成片按钮： 在“图文成片”页，点击自由编辑文案按钮： 输入新小猫钓鱼记文本内容，点击应用按钮： 在底部弹框选择智能匹配素材选项 即可开始制作视频： 最导出视频即可： 导出的视频存储在手机中了~\n最后总结 到目前为止，我们的原创视频制作完成了。\n剪映的智能匹配素材功能，虽然方便我们视频制作过程，但老牛同学的新小猫钓鱼记匹配的素材，感觉都是根据每句文案产出，并没有上下文关联，导致素材一会儿是动漫风格、一会儿是现代网络图片等，全文的素材特别不协调。\n而反观OpanAI的Sora根据Prompt提示词制作的视频，全文内容一致、形象逼真，真心希望我们大模型能快速发展，赶超国际水平~\n关注本公众号，我们共同学习进步 👇🏻👇🏻👇🏻\n我的本博客原地址：https://ntopic.cn/p/2024053101\n","date":"2024-05-31T00:00:00Z","image":"https://ntopic.cn/p/2024053101/05_hua04b28422bbb4c6a12cc5f607adbb1af_53188_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/2024053101/","title":"借助AI大模型，三分钟原创一部儿童故事短视频（附完整操作步骤）"},{"content":"AI资讯 一年心血被毁，OpenAI超级对齐负责人愤而出走Anthropic！Ilya去向依旧成谜 GPT-5倒计时！奥特曼踢走Ilya得逞，宫斗惊人内幕再曝光 和GPT-4这些大模型玩狼人杀，人类因太蠢被票死，真·反向图灵测试 奥特曼挂帅新团队，OpenAI新一代大模型开训，前任高管却投敌了 OpenAI成立新安全委员会，Altman领导，90天后“交卷” AI进入体育中考！直击行业四大痛点，格灵深瞳抢滩校园体育新蓝海 前OpenAI安全主管跳槽，加盟竞争对手Anthropic，继续做超级对齐 联手国产大模型五虎，云计算大厂启动重磅生成式AI计划 OpenAI联创Greg最新采访：为什么OpenAI最先做出GPT-4？ 独角兽被微软挖角，新团队首曝光！情感AI嵌入机器人，超大内存升级情感体验 AI智能体的炒作与现实：GPT-4都撑不起，现实任务成功率不到15% 一年心血被毁，OpenAI超级对齐负责人愤而出走Anthropic！Ilya去向依旧成谜 JanLeike，OpenAI超级对齐团队负责人，宣布加入Anthropic并开始招募新成员。Anthropic成为了OpenAI离职人员的避难所，与OpenAI在价值观上存在差异。Leike表示高兴能继续执行超级对齐任务，在Anthropic将致力于可扩展监督、弱到强泛化和自动对齐研究。Anthropic创立者DarioAmodei因安全观念与OpenAI不合而离开，带走了政策负责人JackClark。Anthropic注重模型安全，对齐能力更强，其产品Claude3获得业内好评。Leike的团队正在紧锣密鼓组建中，研究可扩展监督技术以控制大规模AI。IlyaSutskever离开OpenAI后未公布下一步计划，但可能加入Anthropic、xAI或创办自己的公司。（报道详情）\nGPT-5倒计时！奥特曼踢走Ilya得逞，宫斗惊人内幕再曝光 OpenAI解散了Ilya领导的超级对齐团队，成立了新的安全委员会。下一代前沿模型正在训练中，但预计不会在90天内发布。新安全委员会将评估和改进现有AI安全流程，并在期满后与董事会分享结果。一些人担忧新委员会的独立性，考虑到OpenAI内部的权力斗争和奥特曼领导下的安全团队的问题。（报道详情）\n和GPT-4这些大模型玩狼人杀，人类因太蠢被票死，真·反向图灵测试 这是一次「反向图灵测试」，几个全球最先进的大模型坐在一起，坐着火车唱歌，但其中混进了人类：AI的任务，是把这个人类揪出来。最近，一位网友在X平台发布的一段视频引发了人们的讨论。在视频中，四个AINPC与一个人类坐在一起，按照「乘务员」的要求互相试探，找出隐藏的人类。每个NPC对应着一款大模型。五位扮演者分别是：古希腊巨哲亚里士多德（GPT4Turbo），维也纳古典乐派代表人物莫扎特（Claude3Opus），意大利文艺复兴时期画家列奥纳多・达・芬奇（Llama3），蒙古军事家成吉思汗（人类），埃及艳后克利奥帕特拉七世（GeminiPro）。AINPC在对话中展现了深厚的历史知识和洞察力，甚至在最后一轮投票中，人类「叛徒」被淘汰。AINPC的语言表达和情感描绘不仅逼真，而且具有深度思维能力。随着大型语言模型（LLM）在文本生成、拟人化对话、语气表达和遣词造句方面越来越接近人类，AINPC模拟正在成为游戏行业探索的「新赛道」。AINPC的优势是它们不再是按照游戏中预设选项进行机械对话，而是能够通过AI自主生成的动作和反应，进行富有真实感的实时对话。然而，AINPC还面临一些挑战。首先，它们必须与游戏状态保持同步，避免产生幻觉和破坏游戏逻辑。其次，AI需要理解游戏世界模型，并且不能基于这个模型产生幻觉。再者，每个角色都有自己的知识限制，AI需要遵守这些规则。最后，随着游戏的进行，AI需要实时更新和适应变化。总之，为了让AINPC顺利走入3A大作，开发者需要在以上几个方向努力：避免幻觉、理解游戏世界模型、了解每个角色知识限制、随着游戏进展更新信息、了解游戏机制实际操作范围。（报道详情）\n奥特曼挂帅新团队，OpenAI新一代大模型开训，前任高管却投敌了 OpenAI面临安全性质疑后，成立新委员会，由BretTaylor、AdamD\u0026rsquo;Angelo、NicoleSeligman和CEOSamAltman领导，以确保关键项目决策的安全性。同时，公司宣布正在训练下一代前沿模型，预计将提升通往AGI的能力。安全委员会在90天内评估并发展开发流程与保障措施，并计划公开采纳建议后的最新情况。OpenAI还咨询了其他AI安全专家以支持其工作。此外，公司面临创始人、首席科学家IlyaSutskever离职和超级对齐团队解散的挑战，以及前董事会成员HelenToner批评公司安全流程不透明。（报道详情）\nOpenAI成立新安全委员会，Altman领导，90天后“交卷” OpenAI成立了一个安全与保障委员会，由董事长BretTaylor和其他成员领导，负责提出关键安全决策建议。随着新一代模型训练，该委员会将在90天内评估并完善流程与防护措施，并向全体董事会报告。预计这些系统将提升OpenAI通往通用人工智能的能力。（报道详情）\nAI进入体育中考！直击行业四大痛点，格灵深瞳抢滩校园体育新蓝海 摘要：随着“双减”政策和教育部门推动下，校园体育正迅速发展。传统方法无法满足效率、准确性和公正性的需求，因此出现了AI技术的介入。格灵深瞳在上半年体育中考中应用其深瞳阿瞳目体育训考系统，在京津冀地区11个考试场次，5万名考生，覆盖20余科目，提高了效率和准确性。该系统利用纯视觉AI技术，实现运动精确识别、数据实时采集分析，为教学提供依据，解决了传统红外设备的局限性，如环境干扰大等。格灵深瞳的优势在于全流程纯视觉AI解决方案，覆盖检录、考试到检出，全过程无人工干预，提高了测试效率和规模，同时降低了争议和申诉。系统还采用人脸识别核验考生身份，减少了替考现象。此外，该系统在裁判人数上有显著减少，对于球类运动科目尤其有效。格灵深瞳的产品差异化特性包括硬件软件全套解决方案，针对体育教育四大痛点，提供先进的硬件设备和算法，辅助教学提高效率，个性化教学计划，并为教练提供数据分析报告。此外，格灵深瞳的AI技术具有高度适应性和环境稳定性，能够在复杂场景下保持高准确率。通过持续的技术积累和实践应用，格灵深瞳在体育教育领域占据领先地位，展现了智能化转型的未来趋势。（报道详情）\n前OpenAI安全主管跳槽，加盟竞争对手Anthropic，继续做超级对齐 前OpenAI高管JanLeike加入竞争对手Anthropic，曾领导超级对齐团队。Leike离开OpenAI后指责ChatGPT制造商将产品置于安全文化和流程之前。Anthropic由DarioAmodei和DanielaAmodei创立，获得亚马逊40亿美元投资，定位为更注重安全的公司。OpenAI已成立安全与保障委员会，并开始测试“下一代前沿模型”。（报道详情）\n联手国产大模型五虎，云计算大厂启动重磅生成式AI计划 亚马逊云科技在上海举办峰会，宣布推出多项新业务举措，包括生成式AI合作伙伴计划和四大合作伙伴计划。储瑞松表示，生成式AI将带来行业应用的最大价值，企业应关注业务场景、模型选择、数据利用和持续提升。亚马逊云科技推出AmazonBedrock服务，提供多种模型选择，已有超10,000客户使用。此外，峰会上展示了多个行业解决方案，如自动驾驶、智能家居、游戏等，并强调了安全合作伙伴计划的重要性。（报道详情）\nOpenAI联创Greg最新采访：为什么OpenAI最先做出GPT-4？ OpenAI首先开发出GPT-4等强大模型，原因之一在于其团队的独特组合，既有学术背景的研究型人才，又有优秀的工程人才，这使得他们能够从多个角度解决问题，推动项目更有效地前进。GregBrockman认为，他们是计算机发展史上一种趋势的一部分，致力于实现人工智能（AGI）造福人类的目标。尽管存在安全性和其他风险，OpenAI通过实践学习不断迭代，并致力于将技术用于教育等领域，以增强而非削弱人类能力。（报道详情）\n独角兽被微软挖角，新团队首曝光！情感AI嵌入机器人，超大内存升级情感体验 InflectionAI，一个专注于情感智能的人工智能公司，在其创始人MustafaSuleyman被微软挖角后，经历了领导层变动。尽管如此，该公司仍然拥有15.25亿美元的资金，并计划继续扩大工程团队。新任CEOSeanWhite、CTOVibhuMittal、COOTedShelton和CMOIanMcCarthy组成的团队致力于开发具有情感共鸣的个人AI助理Pi，旨在为企业提供具有同理心的聊天机器人。InflectionAI通过其「移情微调」技术定制模型，以提高个性化服务质量，并计划与品牌合作，建立自己的AI工作室，提供客服和内部员工支持。尽管竞争激烈，但公司拥有充足资金和经验丰富的团队，有望在B2B市场中取得成功。（报道详情）\nAI智能体的炒作与现实：GPT-4都撑不起，现实任务成功率不到15% 随着大型语言模型（LLMs）的进步，它们在性能、准确度和稳定性方面取得了显著提升，但现有版本的LLMs综合能力不足以支持完整的AI智能体。多模态、多任务和多领域成为AI智能体必需，但实际应用效果不佳，提醒初创公司和科技巨头保持脚踏实地，先从AI增强功能入手。LLMs可以执行复杂任务，但现实表现超出预期的挑战性。WebArena排行榜显示，即使是表现最好的模型在现实任务中的成功率也只有35.8%。AI智能体定义不一，主要有单一智能体和多智能体两种架构。当前，AI智能体面临输出不精确、性能差、成本高等问题。在初创公司的尝试中，只有MultiOn追求给出指令并观察其执行，这与AI智能体承诺更为一致。大公司正在将AI功能集成到桌面和浏览器中，展现了令人印象深刻的技术。作者认为AI智能体被过度炒作，但仍期待看到基础模型和架构进步带来的成功应用。AI智能体前途最有希望的道路是利用AI增强现有工具，而不是提供全自主独立服务，结合严格约束的LLMs、良好的评估数据、人机协同监督和传统工程方法，以实现可靠和良好的结果。（报道详情）\n关注本公众号，我们共同学习进步👇🏻👇🏻👇🏻\n","date":"2024-05-29T00:00:00Z","image":"https://ntopic.cn/p/ai20240529/ai20240529-28_hu8de68a9d8ef9b372194c1bd0b595d8e0_231339_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/ai20240529/","title":"[AI资讯·0529] JanLeike，OpenAI超级对齐团队负责人加入Anthropic，Anthropic成为了OpenAI离职员工的避难所。Leike表示将继续执行超级对齐任务，在Anthropic致力于可扩展监督、弱到强泛研究。DarioAmodei因安全观念与OpenAI不合离开，带走了政策负责人JackClark。Anthropic注重模型安全，对齐能力更强，其产品Claude3获得好评。Leike的团队正在组建中，研究可扩展监督技术练，但预计不会在90天内发布。"},{"content":" 随着 ChatGPT 的出现，AI 成为新的焦点，有人说过“未来 50%的工作将是提示词工作”，目前很多公司也在开始招聘 Prompt 提示词工程师。Prompt（提示词）成为了连接创意与技术的桥梁，它不仅是简单的指令，更是激发 AI 潜能的关键。从文章编写到艺术创作，从文本摘要到语言润色，Prompt 的恰当使用能够极大地提升输出的质量与效率。本文将探讨如何编写高效、精准的 Prompt 提示词，最后通过应用场景来具体分析……\nPrompt 提示词需要一定技巧 在51job招聘网站搜索Prompt 工程师可以看到，不少公司都有招聘职位，里面还有包括如字节跳动等大型公司，由此可见 Prompt 提示词对用好大模型起到关键作用。\n目前像市面上有不少免费的大模型（包括前文我们自己笔记本电脑上也部署了 Llama 3 大模型），在使用这些大模型过程中经常遇到一下问题：\n大模型的输出并不能准确输出期望内容 大模型不能很好地理解我的提示词，甚至答非所问 大模型基于我的提示词不能给出准确的输出 不断优化了我的提示词，但是效果仍不理想 …… Prompt 提示词原则和技巧 Prompt 提示词基本原则包括：清晰具体、重点明确、充分详尽、避免歧义等。\n这些原则其实比较容易理解：我们把大模型想象成一位无所不能无所不知且不知疲惫的大师，我们希望这位大师尽可能的满足我们的需求，我们首先要做到是不是把我们的需求清晰具体、充分明确、准确无歧义地告诉这位大师呢？更何况这位大师还是一个程序呢！\n编写 Prompt 提示也有一些常见的技巧，这些技巧包括：结构化、加分隔符、加示例、加要求等。如果我们在使用大模型时遇到上面提到问题，不妨按照这些技巧优化一下 Prompt 提示词，如果能熟练掌握并灵活运用这些技巧，驾驭大模型的能力能够有很大提升。\nPrompt 技巧一：结构化\n要做到完全结构化其实很难也很累，一般来说，越是复杂的任务提示词需要越详细，越是简单的任务可以越简单（想象一下，你希望大师满足你的需求，需求越复杂是不是得给大师足够的信息才行）。\n其实 Prompt 提示词的内容就是通过文字将模型要做的事情交代清楚，“结构化”表达相信大家都学过或者看过，想要模型更好地理解我们的意图，就需要将提示词“结构化”，总结下来包括以下四部分：\n概述：在什么背景下，要做一件什么事；可以指明用户或 AI 的角色是什么。 过程：承担什么样的智能，满足什么样的规则，按照什么样的流程。 依赖：使用什么样的工具，用到哪些知识，处理哪些素材？ 控制：对模型的处理过程有哪些要求，包括正向和负向的要求。 Prompt 技巧二：加分割符\n严格意义上来说，加分隔符也是结构化的一种表述方式，在提示词中合理添加分隔符，可以准确区分指令和待处理的内容，避免模型解读提示词时出现困扰。\n常见的分隔符包括： : 。 ```、\u0026mdash; 、=== 等\nPrompt 技巧三：加示例\n有时候，为了让模型更好地理解你的意图避免歧义，可以更精确地控制模型的输出，需要在提示词中给出一些示例：\n1 2 3 用Java语言写一个正则表达式，匹配的规则如下：数字或下划线@some.com，并且不能以数字和下划线开头。 正确示例：a123@some.com、b_123@some.com 错误示例：_123#some.com、12ab#some.com Prompt 技巧四：加要求\n为了更好的帮助大模型理解我们的意图，引导模型按照我们想要的结果输出，我们需要在提示词中加入一些要求。比如文案创作类限定\n很多人使用大语言模型进行创作，效果不理想，主要原因都是没有通过要求对输出的内容加以限定。比如：想抽取文章摘要，那么可以限定摘要长度；想编写一个儿童故事，那么可以限定年龄大小、故事输出字数等。\n场景：文章编写 推荐 Prompt 示例：“撰写一篇关于未来城市可持续发展的深度分析文章，强调绿色建筑与智能交通系统的作用，目标受众为政策制定者，要求数据丰富、论点明确，1200 个汉字以内。”\n该 Prompt 明确了文章的主题、核心观点、目标受众、文章长度以及内容要求等，将有助于 AI 生成具有针对性和深度的内容。\n一般 Prompt 示例：“写一篇关于未来的文章。”\n过于宽泛的 Prompt 会导致输出内容空洞无物，缺乏焦点，无法满足特定的需求。\n场景：画画 推荐 Prompt 示例：“绘制一幅印象派风格的日出景象，画面中有一艘小船在宁静的湖面上缓缓前行，背景是淡紫色的山峦和橙黄色的天空，注意捕捉光线在水面上的反射效果。”\n该 Prompt 具体描述了画作风格、主题、细节及色彩要求，引导 AI 创造出富有情感和艺术感的作品。\n一般 Prompt 示例：“画个日出。”\n简短的 Prompt 无法提供足够的视觉细节和艺术风格指导，使得最终作品可能缺乏创意和表现力。\n场景：文本生成摘要 推荐 Prompt 示例：“基于这篇关于量子计算的长文，生成一个不超过 100 字的摘要，突出其对加密技术的潜在影响。”\n清晰指定了摘要长度和需强调的信息点，有利于 AI 提取核心内容，生成精确摘要。\n一般 Prompt 示例：“摘要这篇文章。”\n未给出摘要长度限制和重点信息提示，可能导致摘要内容散乱，缺乏针对性。\n场景：文本润色 推荐 Prompt 示例：“请润色这段产品描述，使其更加生动有趣，同时保持专业性，目标客户群为年轻科技爱好者。”\n明确了润色方向、风格要求及目标受众，有助于 AI 调整语言风格，提升文本吸引力。\n一般 Prompt 示例：“让这段话更好听。”\n过于模糊，没有具体说明“更好听”的标准，难以指导 AI 做出有效改进。\n总结 Prompt 提示词的设计是一门平衡艺术，它既要求精确明确，又需留有创意空间。掌握编写高效 Prompt 的技巧，对于提升 AI 辅助工作的质量至关重要。无论是文章编写、艺术创作，还是文本处理，精心构思的 Prompt 都能成为打开创意之门的钥匙，引领我们探索无限可能。\n未来，随着科技不断发展，模型能力不断增强，不仅输出“多模态”，输入也应该多模态。提示信息中不仅包括文本，还应该包括图片、音频、视频，甚至包括生物数据等。或许“提示词”将会消失，真正的高效人机交互的时代将会到来。\n关注本公众号，我们共同学习进步 👇🏻👇🏻👇🏻\n我的本博客原地址：https://ntopic.cn/p/2024052801\n","date":"2024-05-28T00:00:00Z","image":"https://ntopic.cn/p/2024052801/02_hu61c20a16619118ea79bbffd915f6220c_1209148_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/2024052801/","title":"高效编写大模型 Prompt 提示词，解锁 AI 无限创意潜能"},{"content":"AI资讯 AI初创集体跳槽OpenAI，Ilya出走后安全团队重整旗鼓！ 港大字节提出多模态大模型新范式，模拟人类先感知后认知 435亿！马斯克xAI官宣B轮融资，估值突破1300亿 深扒大模型价格战：15家45款模型比拼，谁真便宜谁“打幌子”？ 换了30多种方言，我们竟然没能考倒中国电信的语音大模型 CoT提出者Jason Wei：大模型评估基准的「七宗罪」 苹果AI战略曝光：秘密大招Project Greymatter，GPT被整合进iOS，新AI工具主 大模型时代的计算机视觉！CVPR 2024线上分享会全日程公布 五大AI聊天机器人盲测！ChatGPT未能夺冠，最终赢家竟来自这家“小公司” AI让联想松了口气 中国移动千亿多模态大模型发布，「九天-九九」风趣畅聊堪比GPT-4o GPT-4被证实具有「人类心智」登Nature！AI比人类更好察觉讽刺和暗示 谷歌AI搜索惨败，竟教唆网友自杀！ 硅谷VC张璐：硅谷大模型市场分为三类，三大应用领域迭代速度较快 AI初创集体跳槽OpenAI，Ilya出走后安全团队重整旗鼓！ OpenAI面临内外交困，包括员工离职和外界批评。为了应对AI安全问题，上周五名与安全相关的员工离职，其中包括两名知名成员。《财富》杂志指出，OpenAI未兑现承诺给超级对齐团队20%计算资源。就在此时，Indent首席执行官FouadMatin宣布加入OpenAI，致力于为AGI做好安全准备。他和他的团队包括DanGillespie，后者将专注于建立用于AGI到来的安全计算平台。尽管OpenAI面临挑战，但新安全团队的加入被视为对构建更安全系统的积极努力。（报道详情）\n港大字节提出多模态大模型新范式，模拟人类先感知后认知 当前多模态大模型在视觉任务中展现出强大认知理解能力，但大部分模型仅能单向图像理解，无法将内容映射回图像上，限制了其应用，如图像编辑、自动驾驶等。研究人员提出Groma，通过区域性图像编码提升多模态大模型的定位能力，将文本与图像区域直接关联，显著提高交互性和指向性。Groma将定位转移到visiontokenizer中，利用其空间理解能力，避免了外接专家模型。实验结果显示Groma在GroundingBenchmarks和VQABenchmark上表现超越其他模型，支持融合对话和定位能力的任务。Groma提供了一种新的解决思路，将感知和认知解耦，让visiontokenizer负责感知，大语言模型负责认知，这种先感知后认知的形式符合人类视觉过程，避免了重新训练大语言模型的计算开销。（报道详情）\n435亿！马斯克xAI官宣B轮融资，估值突破1300亿 马斯克旗下的xAI获得60亿美元B轮融资，估值180亿美元，成为独角兽。投资方包括特斯拉早期投资者安东尼奥·格拉西亚斯的ValorEquityPartners、迪拜投资公司VyCapital等。xAI成立不到一年，已推出聊天机器人Grok，并计划与OpenAI竞争。尽管马斯克在X公司遭遇下滑，但特斯拉和SpaceX的表现赢得了投资者信心。新融资将用于开发和迭代Grok，使其与OpenAI、Meta等大型语言模型保持竞争力。（报道详情）\n深扒大模型价格战：15家45款模型比拼，谁真便宜谁“打幌子”？ 国内大模型厂商近日爆发了降价风暴，阿里云、百度等互联网巨头纷纷宣布降低大模型价格，甚至有些模型免费开放。这场“百模大战”让开发者受益匪浅，但也引发了对产业影响的广泛讨论。智东西统计了45款大模型API的价格，对比发现虽然各厂商宣扬的降价幅度高达95%以上，但实际上主力模型的价格并未显著下降。部分创业公司选择不参与这场价格战，认为技术和数据收集才是核心竞争力。价格战背后的原因各有解释，从云厂商角度出发，通过降价强化市场地位；从创企视角看，则是为了吸引关注和客户。部分观点认为这场价格战更多是云服务大厂在云市场争夺中的一种策略，而非真正的大模型“内卷”。对于开发者而言，价格下降有利于应用的开发，但也意味着技术突破和数据价值更为重要。产业影响方面，有观点认为这场价格战可能导致恶性竞争，但也有助于淘汰弱势企业，推动产业集中。同时，对私有化部署、大模型的应用发展都有积极作用。总之，这场百模大战或将面临洗牌，未来只有那些真正具备技术和商业模式优势的企业才能笑到最后。（报道详情）\n换了30多种方言，我们竟然没能考倒中国电信的语音大模型 中国电信人工智能研究院（TeleAI）发布了业内首个支持30种方言自由混说的「星辰超多方言语音识别大模型」，能够识别理解粤语、上海话、四川话等各地方言，打破语言沟通的壁垒。该模型在多项基准测试中表现出色，尤其是在国际语音顶会Interspeech2024中获得冠军。此外，该技术还被应用于中国电信万号智能客服系统和翼声平台，实现了方言识别的全覆盖，为用户提供更加自然流畅的交互体验。随着大模型技术的发展，预计会推动方言文化的保护，并辅助历史文献和档案的数字化工作。中国电信作为运营商在人工智能领域的优势，使其能够加快大模型在各个领域的落地应用，形成新的经济增长点。（报道详情）\nCoT提出者Jason Wei：大模型评估基准的「七宗罪」 李飞飞等人创建的ImageNet在视觉任务中被认为是一个测试金标准。在大型语言模型（LLM）时代，我们如何评估它们？目前，研究者提出了MMLU、GSM8K等评估基准，但这些是否完美？JasonWei在一篇博客中进行了深入探讨。他列举成功的评估基准，并总结失败原因，包括样本数量不足和评估复杂度高。Jason认为，评估工具的命名方式有待改进，例如HumanEval虽然叫做人类评估，但实际上未使用人类评估。为了让评估工具得到广泛使用，需要帮助研究者使用它，并提供推广机会。Jason也提出了解决测试集污染问题的一些建议。成功的评估基准包括GLUE/SuperGLU、MMLU、GSM8K、MATH和HumanEval，它们通常与突破性论文相关联。失败的评估基准可能因为样本数量不足、复杂度高或命名不当等原因而被忽视。Jason认为，要让评估工具得到广泛使用，需要提供单一数字指标，并且评估工具应该简单易用，不需要太多额外工作。此外，评估基准的设计应符合重要任务，如语言理解、数学问题解决，而不是娱乐性质的任务。Jason提到，LLMs对评估工具提出了新的挑战，目前尚无单一评估工具能全面评价它们。人类配对评估是一种流行趋势，但可能存在偏差。模型生成内容的评估也被讨论，但需要小心处理。评估工具的成功与否取决于其领域内专家认可，以及是否能够解决测试集污染问题。社区应该投资于评估工具，因为它们是AI研究人员客观评价模型的关键，并对该领域产生重大影响。（报道详情）\n苹果AI战略曝光：秘密大招Project Greymatter，GPT被整合进iOS，新AI工具主 苹果即将在WWDC上展示新的AI战略，包括ProjectGreymatter、本地和云端大模型数据处理、新AI功能、升级版Siri与OpenAI合作等。尽管落后于Google和OpenAI，但庞大的用户基础有望带来优势。新AI战略侧重于日常生活工具，计划在iOS18和macOS15中引入。Siri将升级为更自然的交互体验，而Xcode开发者工具也将增强AI功能。苹果还将推出个性化表情符号与云端处理M2Ultra芯片支持。虽然自研AI聊天机器人未达标，苹果则与OpenAI合作，但存在风险。尽管如此，苹果的庞大用户基数有望使其成为最大的AI玩家。（报道详情）\n大模型时代的计算机视觉！CVPR 2024线上分享会全日程公布 OpenAI的ChatGPT发布后，大型语言模型和人工智能生成内容的研究越来越受关注。计算机视觉（CV）领域也在经历变化。要快速了解AI研究进展，可以参加顶级会议论文分享会，如每年举办的CVPR。2024年，CVPR共收到11532份论文，2719篇被接收，录用率为23.6%。机器之心计划于2024年6月组织「CVPR2024线上论文分享会」，邀请AI社区成员参与。本次会议设置Keynote和论文分享环节，讨论CV热门主题。今天，分享会的全日程、Keynote嘉宾及演讲主题正式公布。金小刚将介绍数字人建模动画关键技术，朱俊彦将探讨创作者与生成模型的协作，芦清林将解读腾讯混元文生图算法核心——DIT架构，盛律则会谈多模态大语言模型与具身代理的结合。会议将在机器之心和黄大年茶思屋两个平台进行直播，欢迎大家关注预约。（报道详情）\n五大AI聊天机器人盲测！ChatGPT未能夺冠，最终赢家竟来自这家“小公司” 《华尔街日报》对ChatGPT、Claude、Copilot、Gemini和Perplexity五大AI聊天机器人的性能进行了全面的测试，结果显示Perplexity综合排名第一，ChatGPT紧随其后。测试侧重于AI在解决实际问题和完成日常任务的能力，包括代码能力、健康咨询、财务问题等。评判标准为准确性、实用性和整体质量。微软的Copilot表现不佳，而Perplexity以其总结、代码和时事类问题的优异表现获得了冠军。此次测试显示，即使是知名度较高的ChatGPT也未能在所有方面领先，反而是小众的Perplexity夺得了胜利。（报道详情）\nAI让联想松了口气 联想集团2023/2024财年第四季度和全财年财报显示，尽管上半财年PC和服务器市场需求疲软，导致营收下滑8%至568.64亿美元，全年净利润37%下降，但在最新一季度表现出明显回暖。IDG收入为104.63亿美元，同比增长6.8%，ISG收入25.33亿美元，同比增长15%；SSG收入18.2亿美元，同比增长10%。AI技术起到关键作用，推动了PC市场的复苏，并成为联想业绩回暖的关键因素。不过，随着芯片制造商和终端厂商在AI领域加大投入，行业竞争激烈，联想是否能持续高增长还有待观察。（报道详情）\n中国移动千亿多模态大模型发布，「九天-九九」风趣畅聊堪比GPT-4o 中国移动近日在数字中国建设峰会上宣布推出自主研发的“九天”千亿多模态基座大模型，这是国内最值得信赖和最懂行业的大型全栈模型。该模型采用纯解码与多专家相结合的架构，实现了结构化数据建模和渐进式学习，并提供不同参数量的模型版本，以适应各种设备和平台。中国移动在AI领域十年如一日的研发投入，使得“九天”模型在国际竞赛中取得优异成绩，包括语音合成和视觉理解等领域的冠军。“九天”模型不仅具有强大的推理能力，还能进行感知、预测、诊断、控制和决策，对复杂系统智能化有着重要作用。它能够部署在各种信源设备上，包括智能手机、平板、汽车和穿戴设备，并支持多种信源的适配，实现了“全模态”之间的智能化推理。中国移动聚焦于骨干行业，如通信、能源、建筑、交通等领域，为这些行业提供基座模型。该公司还致力于AI生态的国产化，包括算力、芯片、框架和算法，并提出了“万千百”智能基座计划，以实现大规模训练和推理算力的提升。此外，中国移动强调了模型安全性，通过了双备案认证，并获得了最高等级的安全性认证。十年来，中国移动在AI领域取得了跨越式发展，是国际AI领域的领头人冯俊兰博士带领团队不懈努力的结果。未来，中国移动将继续推动AI技术的应用，以实现“AI赋能，智筑国基”。（报道详情）\nGPT-4被证实具有「人类心智」登Nature！AI比人类更好察觉讽刺和暗示 AI是否具备「心智理论」的争议一直存在，尤其是随着大型语言模型（LLM）的发展，如ChatGPT。最新的研究显示，GPT-4在理解讽刺和暗示方面甚至超越了人类水平，但在判断失言时表现不佳。这可能是由于它过于谨慎，不愿意轻易给出确定答案，而不是因为缺乏理解能力。这些发现支持「超保守主义」假说，即GPT能够推断说话者心理状态，但不愿承诺单一解释。这种谨慎行为可能与模型的训练数据和设计有关，强调了能力与表现之间的差异，表明AI有能力进行复杂的心理推理，但在特定情况下可能不会像人类那样主动消除不确定性。（报道详情）\n谷歌AI搜索惨败，竟教唆网友自杀！ 谷歌新推出的AI搜索引擎AIOverview因收集Reddit内容而产生了一系列荒谬回答，包括教唆自杀、提供错误健康建议和识别混淆信息等问题。尽管Google承认这些错误并正在采取措施改进，但这次事件再次让人质疑其AI系统的准确性和安全性。此外，这也揭示了使用Reddit内容训练AI可能带来的后果，显示出需要更严格的数据清洗和筛选。（报道详情）\n硅谷VC张璐：硅谷大模型市场分为三类，三大应用领域迭代速度较快 硅谷AI产业已发展至鸡尾酒模式阶段，初创公司可调用大模型API，结合开源模型进行优化。张璐认为人工智能是超级工具，具有10倍于互联网时代的商业机会，但只有1/3留给初创企业。大型科技公司也能被赋能，而非破坏或变革。医疗、金融保险和机器人是迭代速度较快的领域。硅谷模型市场已相对明确，三类主要有OpenAI、苹果等提供服务型大模型，NVIDIA等自用大模型，以及开源平台。初创企业可采用鸡尾酒模式，与此同时，数据质量比数量更重要，行业专属小模型能与通用大模型竞争。边缘计算和数据隐私保护是未来的关键技术方向。（报道详情）\n关注本公众号，我们共同学习进步👇🏻👇🏻👇🏻\n","date":"2024-05-27T00:00:00Z","image":"https://ntopic.cn/p/ai20240527/ai20240527-23_hud819261146edf9ec90c9c9054e94efb4_66761_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/ai20240527/","title":"[AI资讯·0527] AI领域面临挑战：OpenAI员工离职、外界批评；新安全团队成立。马斯克的xAI获得60亿美元投资，估值180亿。国内大模型厂商降价争夺市场份额。中国电信推出多方言语音识别模型。AI评估工具讨论：JasonWei提倡简单易用且针对特定任务。苹果WWDC展示新AI战略。ChatGPT性能测试，Perplexity获胜。联想业绩回暖，AI技术促进PC市场复苏。中国移动推出自主研发的大型全栈模型。AI是否具备“心智理论”争议；谷歌AI搜索引擎遇错误问题；硅谷AI产业发展至鸡尾酒模式，初创企业需专注数据质量。"},{"content":"VAST宋亚宸：3D生成迈入秒级时代，今年将做到Midjourney V5/V6水平丨GenAIC 2024年中国生成式AI大会上，VAST创始人兼CEO宋亚宸以《大模型Tripo驱动3D生成迈入秒级时代》为题进行了演讲。他认为，3D技术的体验质量和信息密度在不断提升，并且具有互动性。目前3D生成已经达到MidjourneyV3水平，将于今年年底达到V5/V6水平，实现真正可用并通过图灵测试。VAST是3D生成模型领域的领先者，创建了全球最大的几个3D生成开源社区，并与Stability共同开发了全球最大的0.5秒图生3D模型TripoSR。宋亚宸也是首位在计算机图形学顶会SIGGRAPH上发表主舞台主题演讲的中国创业者。VAST的3D大模型Tripo上线后，仅百余天就生成了超过200万个3D模型，以降低3D生成的创作门槛和成本。公司结合3D内容平台和大众级创作者平台，并已拥有超过200家B端客户，开发出原生AI爆款小游戏，一周用户突破10万。宋亚宸认为，AIGC将使游戏创作成本和门槛无限降低，这是VAST的目标。公司致力于使用AI技术生成3D模型，这些模型可以用于游戏、动画、影视、元宇宙和3D打印等行业，并且具有Mesh、贴图、材质等属性，可以进行传统管线二次编辑。宋亚宸强调，随着3D技术的发展，世界正在进入一个3D新时代。目前，全球顶级企业如谷歌、英伟达、OpenAI和Meta都已涉足3D生成领域。VAST在学术上发表了大量论文，并开源了多项技术，如TripoSR、Threestudio、Wonder3D等。公司正在拓展人类能力的边界，降低用户创作成本，使得每个人都能低成本创作3D内容。宋亚宸预计，未来每个人都将拥有生成无限个属于自己的3D内容的能力。这将改变3D创作者的角色，从游戏大厂的建模师和动画师变为普通人。VAST的产品Tripo已经上线，百天内生成了超过200万个模型，这在短时间内超越了全球许多模型交易平台。公司还与腾讯、网易等游戏大厂合作，提供3D生成能力，以降低他们的成本和提高效率。同时，VAST推出了原生AI玩法，如TripoGO小游戏，一周用户突破10万，这是AI原生玩法的力量体现。（报道详情：https://www.163.com/dy/article/J2VO20AB051180F7.html）\n焱融科技张文涛：将大模型训练效率提升40%！详解多云架构下高效存储策略 2024年中国生成式AI大会上，焱融科技CTO张文涛发表演讲，讨论了多云环境下大模型训练和推理的高效存储问题。随着算力需求的增加，单一数据中心无法满足大模型训练和推理的需求，因此需要采用多数据中心协同工作的方式。然而，这种模式带来了数据拷贝成本的增加，以及如何在保证性能的前提下让数据按需流动的问题。张文涛强调了存储对于大模型训练和推理至关重要，特别是在多模态大模型中，高性能存储可以提升训练效率20-40%。焱融科技提供了一套基于四个核心组件的存储解决方案：集群管理服务、元数据服务、集群服务和客户端。这些组件支持标准x86架构、Arm架构服务器、多种网络协议，并且可以水平扩展以适应不同的需求。对于推理环节，张文涛提出了客户端缓存池解决方案，以缓解启动风暴问题。该方案通过在计算节点上建立大型SSD缓存池来加快模型加载速度，从而有效地解决了网络带宽的瓶颈。最后，随着训练和推理无法完全在单一数据中心完成，多云方式成为必然趋势，但这也带来了新的挑战。张文涛提出了一个“SourceofTruth”数据湖架构，通过数据编排功能让数据按需流动，并且支持多种对象存储平台的对接，以实现数据的灵活流转。（报道详情：https://www.163.com/dy/article/J2VO20QI051180F7.html）\n5分钟搭建大模型应用！腾讯将「实用主义」贯彻到底 腾讯云推出了知识引擎平台，旨在帮助企业快速开发大模型应用，降低门槛。平台提供了专属工具链，包括大模型知识引擎、图像创作引擎和视频创作引擎，通过PaaS接入方式简化数据工程、模型精调和应用开发流程。企业可以快速上手，搭建属于自己的知识问答、文档助手等应用。知识引擎平台集成了腾讯自研的大型语言模型（LLM）和检索增强生成（RAG）、向量检索、大语言模型、多模态大模型等多种先进技术，能够在知识解析-切分-检索-内容理解生成全链路中显著提升问答质量和效率。腾讯云正在探索大模型落地最后一公里的解决方案，并通过实践指南贯彻“实用主义”价值观，为企业提供更高效、更便捷的大模型应用服务。（报道详情：https://www.163.com/dy/article/J2VO25JD0512MLBG.html）\n行业首发！一句话用AI创作视频画本、漫画，网民脑洞要停不住了！ 百度文库推出「智能画本」功能，仅需输入主题即可生成一本图文并茂的视频画本，模仿专业出版社制作感十足；「智能漫画」功能则可以创作故事、写分镜脚本、绘制分镜和生成图文并茂的条漫。这些AI工具降低了创作门槛，让无经验者也能轻松自制画本和漫画，促进行业内外行跨界合作，推动内容创作者从创意到变现的全链路路径。百度文库还提供PPT、研报、思维导图等多种AI生成功能，使得用户可以通过语音输入快速生成报告或PPT，并且品质有保证。这些功能深受用户喜爱，已成为国内AI产品榜首。（报道详情：https://www.163.com/dy/article/J2VO20BF051180F7.html）\n超级细菌、AI黑盒与不老奥秘 五一假期期间，一位感染肺炎支原体的患儿家长在社交媒体上表达了对抗生素耐药性的担忧，这个问题正影响着全球的健康安全。肺炎支原体是一种常见的呼吸道病毒，通常可以通过抗生素治疗，但越来越多的人发现大环内酯类抗生素不再有效。这是由细菌和病毒对抗生素耐药性的增加导致的。FelixWong，一个华裔科学家，在麻省理工学院与哈佛大学的合作下发明了一种利用深度学习模型来发现全新的抗生素结构的方法。他的团队使用了超过120,000种化合物中的39312种进行测试，并通过可解释性的人工智能模型，识别出两种有效的潜在抗生素。FelixWong的工作不仅解决了抗生素耐药性的问题，而且还为抗衰老研究开辟了新途径。他希望能够在50年内开发一种真正能延缓衰老的药物。他的公司，IntegratedBiosciences，以结合合成生物学和深度学习来控制细胞应激反应，开发更先进的医疗方法。FelixWong的工作是科学界的一次重大突破，他的发现不仅为抗生素耐药性提供了新的解决方案，也为对抗衰老提供了希望。他的故事提醒我们，即使在面对重大的挑战时，通过创新和坚持，可以实现前所未有的成就。（报道详情：https://www.163.com/dy/article/J2VO6CVB0512MLBG.html）\nAI 需求强劲，英伟达距离苹果只差一步之遥 英伟达（NVIDIA）在2025财年第一季度的财报显示，总营收为260亿美元，同比增长262%，远超市场预期。数据中心收入达到225.63亿美元，同比增长427%。CEO黄仁勋表示，AI浪潮依然强劲，全球公司正将传统数据中心转型为“AI工厂”，生产人工智能产品。他认为AI将带来显著的生产力和收入增长，并在成本效益和能源效益上也有显著提升。英伟达的四大业务板块中，数据中心是最受关注的部分，也是公司增长的核心驱动力。2024财年第四季度，数据中心业务收入达到184亿美元，是前一年同期的5倍多，并刷新了前一个季度的历史新高。英伟达首席财务官ColetteKress表示，数据中心业务增长主要得益于Hopper架构GPU（例如H100）的出货量增加。计算收入增长了5倍，网络收入增长了3倍。公司首次披露了数据中心业务的具体收入细项，其中计算收入达到193.92亿美元，同比增长478%；网络收入为31.71亿美元，同比增长242%。英伟达希望其业务多元化，不仅依赖于大型云服务提供商，如亚马逊、Meta、微软和谷歌，它还与政府、消费互联网公司、汽车制造商和医疗保健客户合作，新领域可能会在云服务商之外创造数十亿美元的垂直市场。汽车和消费互联网领域表现尤为突出。特斯拉购买了35,000块英伟达H100GPU用于其自动驾驶系统FSDV12。Kress表示，今年汽车行业将成为英伟达数据中心业务中最大的垂直市场，带来数十亿美元的收入机会。Meta发布了最新的大型语言模型Llama3，该模型在24,000块英伟达H100GPU上训练完成，为Meta旗下的Facebook、Instagram、WhatsApp和Messenger的新人工智能系统MetaAI提供支持。Llama3不仅提升了这些平台的AI功能，还在各行业掀起了一股AI开发热潮。尽管英伟达在中国市场的业务受到了限制，但公司仍然致力于为客户服务，并表示将尽最大努力。创纪录的财务表现下，英伟达也有隐忧，如芯片制造端的激烈竞争和头部互联网和大模型公司如何落地和变现的问题。（报道详情：https://www.163.com/dy/article/J2VP6EMM05119FMA.html）\n英伟达股价起飞暗含隐忧：竞品频出、客户背刺、AI市场格局波动 英伟达2025财年一季度营收增长262%，股价破1000美元关口，但面临竞争对手AMD、英特尔芯片替代产品和客户自研挑战。AI市场变化，推理需求增加，英伟达芯片非必需品；英伟达加速新产品开发，迭代周期缩短至一年，以维持领导地位。（报道详情：https://www.163.com/dy/article/J2VSRO9A051180F7.html）\n李开复最新财富论坛AI访谈：中国AI只比美国晚一年 开复（Kaifu）在财富创新论坛上就人工智能进行了访谈，提到中国AI发展落后美国不到一年，并对中美AI比较做出了评点。他认为美国的创新能力强，但中国在执行方面更有优势。开复指出，人工智能可能会超越人类，但并不一定需要完全模仿人类大脑。他还提到了他创立的公司ZeroOne.ai，其估值达10亿美元，并且对未来AI技术的普及持乐观态度。开复预测了人工智能将带来的巨大变革，他认为数据将成为新的石油，AI将改变世界。然而，他也担心大公司可能会占据主导地位，加剧贫富差距，并对教育和就业产生负面影响。他强调需要普及AI，使其变得“可获得”，减少社会不平等。关于未来工作的流失，开复提到他之前的预测中，40%到50%的工作将被人工智能取代，这一预测仍然准确。政府和教育体系需要准备好应对这一变化，帮助人们适应新环境。开复还强调了人类情感和同理心的重要性，与机器相比，人类在这些方面有独特之处。他鼓励人们积极使用AI工具，并相信人工智能可以是创造力和生产力的巨大推动者。（报道详情：https://www.163.com/dy/article/J2VU00U405534HHB.html）\n第一波黑客已经用大模型搞网络攻击 新技术和产业的发展中，安全始终是伴随而来的，它们通常先有攻击后有防护。大模型也不例外。在大模型的应用场景中，安全问题和需求随之而来，但由于攻击者往往能更快发现并利用这些风险，因此需要我们提供快速响应和改变。目前，大模型的实际应用仍然有限，行业人士对其成熟度、投入产出比以及应用方向有所顾虑，而黑客们只需一个理由就可以利用大模型进行攻击。全球安全公司纷纷行动起来，微软推出了SecurityCopilot，谷歌发布了网络安全专用的大模型，而Paloalto和CROWDSTRIKE则在其安全运营平台集成了大模型的安全能力。在中国，大约80%的网络安全公司正在将大模型技术融入产品中，有30%的公司已经开始进行大模型安全研究，出现了一些安全创业浪潮。随着ChatGPT等AGI技术的推出，全球黑客开始使用这些工具来提高攻击效率和范围。由于大模型对编程语言理解能力强，攻击者可以快速发现软件漏洞，并且利用AI算法进行人脸伪造，导致网络诈骗案件增加。安全公司正在探索如何利用AI技术来改进威胁检测、产品体验和运营分析。然而，大模型的概念也被认为可能是新瓶装旧酒，因为许多安全公司早已使用AI进行垃圾邮件检测、漏洞修复等工作。亚信安全并没有立即将大模型集成到产品中，而是在构建底层通用框架和MaaS服务平台，并最终推出了自研的网络安全大模型——信立方。随着大模型技术的发展，AI安全需要从系统性全局视角来共建生态。亚信安全认为，大模型产业链将变得更加长，而周边业态会更加丰富，真正做大模型的厂商不会太多，但利用大模型的企业和行业会越来越多，这对安全方面提出了新的要求。通过与其他公司的合作，亚信安全推动了东西部算力和数据资源的安全调度和流通，为人工智能和数据产业的发展提供了支持。（报道详情：https://www.163.com/dy/article/J2UOBJSO05118O92.html）\n中国AI独角兽资本大冒险 月之暗面的火爆，超出想象，预计估值将超过百川智能、智谱AI等，成为中国头部独角兽。作为通用人工智能领域的领军企业，其创始人杨植麟引领高密度人才团队，旗下产品Kimi在短时间内突破7380万用户，成为AIGCAPP行业用户增长最快的应用之一。虽然面临资金压力和商业化路径问题，但月之暗面通过新一轮融资获得更多筹码，有望释放更多可能性。然而，大模型竞争加剧，市场缺乏“杀手级应用”，技术和商业化的挑战依然存在。杨植麟计划在资源和人才上投资，研发跨模态大模型，并希望打造像苹果AppStore一样的平台。（报道详情：https://www.163.com/dy/article/J2TO535U05198NMR.html）\nAI独角兽集体求卖身，新一轮洗牌期开始了 大型人工智能模型公司正经历着收购潮，包括Adept（估值10亿美元）、HumaneAI（估值7.5-10亿美元）和StabilityAI（创始于StableDiffusion）。这些公司的估值高达数十亿美元，但面临成本压力、商业模式不确定性以及激烈市场竞争。Adept和HumaneAI寻求收购，部分原因是产品未能如预期般销售良好，导致资金短缺。StabilityAI则因开源模型StableDiffusion的商业化问题而面临挑战。科技巨头如微软、谷歌和Meta等正在积极寻求收购或与这些初创公司合作，以获取技术和人才。此外，大数据行业头部公司Databricks和Snowflake也通过收购AI初创公司来扩展其业务。虽然国内大模型市场也显示出头部效应，但未来的发展趋势仍然值得观察。（报道详情：https://www.163.com/dy/article/J31TPPHL0511DSSR.html）\n鹅厂造了个AI翻译公司：专攻网络小说，真人和GPT-4看了都说好 智能体翻译公司TransAgents以AI为主体，提供高质量网络小说翻译服务，其成本低于传统人工翻译近80%。虽然在相似度评估中表现不佳，但读者和专业人士对其质量表示肯定。该公司使用30个不同智能体，每个岗位配备多个AI，以适应不同的语言、体裁和受众，创造出灵活多样的译文，更符合目标语言习惯和文学性强。TransAgents的表现促使人们思考是否人工智能生成内容将取代人类喜好。专业评估表明尽管GPT-4认为其翻译不如自己，但真人和GPT-4都认可TransAgents翻译质量，特别是在语言多样性方面超越了真人和GPT-4的表现。在某些类型中，如游戏、科幻爱情类，TransAgents的表现尤为突出。该公司通过GhostAgent评估选择高级编辑，并在翻译风格指南下进行团队协作，以确保一致性和质量。虽然存在遗漏现象，但未影响读者体验。多智能体协同工作已显示实际效益，引发对语言同质化和大规模Agent合作的担忧。（报道详情：https://www.163.com/dy/article/J31V5H9H0511DSSR.html）\n大模型“国家队”发力了 ，中国移动发布“万百千” 中国移动近日宣布推出全栈国产化生态大模型“九天智能基座”，包括万卡算力、千亿参数模型及百汇平台，能力达到GPT-4的90%水平。该模型已布局17个细分领域行业大模型，覆盖政务、医疗、办公等行业。中国移动计划在2023年内建成3个近2万卡智算中心，并发布AI+5G新通话、AI+办公等23个AI软硬产品及20个行业应用。“九天智能基座”实现了数据构建、预训练、微调、部署全链路核心技术自主创新，适配国产算力，实现全栈国产化，并获得中国软件测评中心安全测评的最高等级A级证书。未来，中国移动计划布局40款自研行业大模型，赋能生产、生活、社会治理等创新升级。此外，中国移动还推出了一站式大模型开发平台和应用平台（LLMStudio），面向个人及行业客户提供全栈人工智能服务。中国移动的体系化人工智能技术具有AI服务大闭环、AI能力原子化重构、网络原生和安全可信四大特征，实现了算力、网络、人工智能能力全局统一调度和开放服务。中国移动作为“国家队”成员，在数字化建设中扮演重要角色，以其技术实力、数据优势、身份优势等因素，为国家的数字科技创新体系提供支持，并在全球大模型浪潮中取得了显著成就。（报道详情：https://www.163.com/dy/article/J31V8FG00511DSSR.html）\n","date":"2024-05-26T00:00:00Z","image":"https://ntopic.cn/p/ai20240526/ai20240526-1_hu301ff0f905d073f9e5886dec5ec2be21_59898_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/ai20240526/","title":"[AI资讯·0526] AI技术快速发展，英伟达股价上涨但面临竞争隐忧。大模型应用增长，网络攻击风险增加。中国AI领域追赶美国一年，独角兽寻求出售。AI翻译公司专注网络小说。中国移动推出大模型服务。"},{"content":" 贪吃蛇作为一款极其经典且广受欢迎的小游戏，是早期 Windows 电脑和功能手机（特别是诺基亚手机）流行度极高的小游戏，是当时功能手机时代最具代表性的游戏之一。游戏的基本规则和目标十分简单，但却极具吸引力，让人欲罢不能。本博文我们用 Python 编写属于自己的贪吃蛇游戏，一起来体验一下编程的乐趣与成就……\n本文 Python 贪吃蛇游戏源代码：https://gitee.com/obullxl/PythonCS/tree/master/CS-CY2405\n贪吃蛇游戏分析 控制蛇的移动：通过上下左右键，控制一条蛇在游戏区域中移动，最初蛇很短，通常由 1 个方块组成。\n吃到食物增长：游戏区域中会随机出现食物（例如一个方块），当蛇头触碰到食物时，代表蛇吃到了食物，蛇身体会增长一节，同时得 1 分。\n避免越界或碰撞：游戏中需要避免蛇头撞到游戏区域的边界，或者蛇头碰到自己的身体。\n策略性移动：随着游戏的进行蛇身增长，需要巧妙地操控蛇的路径，既要吃到食物，又要避免越界碰撞，这变得越来越具挑战性和趣味性。\n游戏分数和结束：游戏过程中，需要记录当前得分（即：蛇吃到食物的数量），游戏结束，展示总得分和重新开始游戏或者退出。\n准备：安装 pygame 工具包 贪吃蛇游戏依赖pygame这个强大的 Python 游戏工具包：\n1 pip install pygame 代码：设置基础参数 基础参数包括屏幕大小、中文字体、背景颜色、字体颜色、蛇的颜色、食物的颜色、蛇的大小和蛇游动速度等。中文simsun.ttf字体文件在源代码目录，请一起下载到本地：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 import random import pygame # 初始化 pygame.init() # 设置窗口和初始化 SCREEN_WIDTH = 640 SCREEN_HEIGHT = 480 SCREEN = pygame.display.set_mode((SCREEN_WIDTH, SCREEN_HEIGHT)) # 设置窗口标题 pygame.display.set_caption(\u0026#39;老牛同学：贪吃蛇\u0026#39;) # 游戏屏幕字体 SCORE_FONT = pygame.font.Font(\u0026#39;./fonts/simsun.ttf\u0026#39;, 25) RESULT_FONT = pygame.font.Font(\u0026#39;./fonts/simsun.ttf\u0026#39;, 25) # 颜色设置 BLACK = (0, 0, 0) # 黑色（屏幕的颜色） WHITE = (255, 255, 255) # 白色（分数的颜色） GREEN = (0, 255, 0) # 绿色（蛇的颜色） RED = (255, 0, 0) # 红色（食物的颜色，游戏结束字体的颜色） # 蛇区块大小（正方形）和游动速度 SNAKE_BLOCK = 10 SNAKE_SPEED = 8 代码：绘制游戏进行中的得分 游戏进行中，得分默认在窗口左上角展示：\n1 2 3 4 def draw_score(score): \u0026#34;\u0026#34;\u0026#34;绘制当前分数\u0026#34;\u0026#34;\u0026#34; score_text = SCORE_FONT.render(\u0026#34;总分数：\u0026#34; + str(score), True, WHITE) #SCREEN.blit(score_text, [20, 20]) # 左上角 也可以设置为顶部居中展示，如下代码：\n1 2 3 4 5 def draw_score(score): \u0026#34;\u0026#34;\u0026#34;绘制当前分数\u0026#34;\u0026#34;\u0026#34; score_text = SCORE_FONT.render(\u0026#34;总分数：\u0026#34; + str(score), True, WHITE) score_rect = score_text.get_rect(center=(SCREEN_WIDTH // 2, 20)) SCREEN.blit(score_text, score_rect) 代码：绘制游戏进行中蛇的身体 游戏进行中，蛇的身体其实就是一些方块的位置，蛇的数据结构为一个 list 列表，列表的元素是 x 和 y 坐标 list 列表，即[[x1,y1],[x2,y2],[x3,y3]...]数据存储形式，蛇尾是第 1 个元素，蛇头是在最后 1 个元素。\n1 2 3 4 def draw_snake(snake_list): \u0026#34;\u0026#34;\u0026#34;绘制蛇的身体，由于都是方块，所以绘制过程无需区分蛇头和蛇身等\u0026#34;\u0026#34;\u0026#34; for x in snake_list: pygame.draw.rect(SCREEN, GREEN, [x[0], x[1], SNAKE_BLOCK, SNAKE_BLOCK]) 问题：蛇尾在第 1 个元素，而蛇头在最后 1 个元素，为什么要这么设计？\n答案：从后面代码可以看出，蛇在游动的过程中，蛇新的坐标是用的append到列表，然后删除列表的第 1 个元素。当然完全可以通过insert的方式反过来设计。\n代码：计算食物的随机坐标 我们使用random函数随机计算食物的坐标，同时需要注意，避免食物越界：\n1 2 3 4 5 def food_position(): \u0026#34;\u0026#34;\u0026#34;随机计算食物坐标\u0026#34;\u0026#34;\u0026#34; x_food = round(random.randrange(0, SCREEN_WIDTH - SNAKE_BLOCK, SNAKE_BLOCK)) y_food = round(random.randrange(0, SCREEN_HEIGHT - SNAKE_BLOCK, SNAKE_BLOCK)) return x_food, y_food 代码：绘制游戏结束分数和提示 我们在屏幕正中央分别展示 3 行提示文本：游戏结束、总得分和继续游戏提示。文本正中央展示和总得分类似：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 def draw_result(snake_length): \u0026#34;\u0026#34;\u0026#34;绘制游戏结果\u0026#34;\u0026#34;\u0026#34; # 在屏幕中央显示文本 game_over_text = RESULT_FONT.render(\u0026#39;游戏结束\u0026#39;, True, RED) game_over_rect = game_over_text.get_rect(center=(SCREEN_WIDTH // 2, SCREEN_HEIGHT // 2 - 50)) SCREEN.blit(game_over_text, game_over_rect) # 显示最终得分文本 final_score_text = RESULT_FONT.render(f\u0026#39;总得分: {snake_length - 1}\u0026#39;, True, RED) final_score_rect = final_score_text.get_rect(center=(SCREEN_WIDTH // 2, SCREEN_HEIGHT // 2)) SCREEN.blit(final_score_text, final_score_rect) # 显示重新开始游戏的提示文本 restart_text = RESULT_FONT.render(\u0026#39;按`Q`退出游戏，按`C`重新开始游戏\u0026#39;, True, RED) restart_rect = restart_text.get_rect(center=(SCREEN_WIDTH // 2, SCREEN_HEIGHT // 2 + 50)) SCREEN.blit(restart_text, restart_rect) 代码：游戏主循环设置 主循环游戏把以上功能函数缝合起来，同时包括控制代码：\ngame_close参数控制单局游戏是否结束，game_over参数控制整个游戏是否结束 上下左右按键：上下按键控制蛇Y轴（上减少、下增加），左右按键控制蛇X轴（左减少、右增加） 蛇触墙检测：蛇头坐标x1,y1是否在整个屏幕之内 蛇触蛇身检测：蛇头坐标是否在蛇身体的任意坐标相同 蛇吃到食物检测：蛇头坐标是否和食物坐标相同，如果相同则重新设置食物位置，同时蛇身长加 1 个方块 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 def game_loop(): \u0026#34;\u0026#34;\u0026#34;游戏主循环函数\u0026#34;\u0026#34;\u0026#34; game_over = False # 退出游戏 game_close = False # 单次游戏结束 # 初始化蛇的坐标和坐标增量 x1 = SCREEN_WIDTH / 2 y1 = SCREEN_HEIGHT / 2 x1_change = 0 y1_change = 0 # 蛇的身体列表，初始长度为1 snake_list = [] snake_length = 1 # 随机生成食物的位置 x_food, y_food = food_position() while not game_over: # 如果游戏结束但未选择退出或重玩，则进入此循环 while game_close: # 清空屏幕，准备下一轮绘制 SCREEN.fill(BLACK) draw_result(snake_length) pygame.display.update() # 刷新屏幕 # 等待按键 for event in pygame.event.get(): if event.type == pygame.KEYDOWN: if event.key == pygame.K_q: game_over = True # 退出游戏 game_close = False if event.key == pygame.K_c: game_loop() # 重新开始游戏 # 处理键盘事件，改变蛇的移动方向 for event in pygame.event.get(): if event.type == pygame.QUIT: game_over = True # 退出游戏 if event.type == pygame.KEYDOWN: if event.key == pygame.K_LEFT: # 左：X坐标减少1个区块，Y坐标不变 x1_change = -1 y1_change = 0 elif event.key == pygame.K_RIGHT: # 右：X坐标增加1个区块，Y坐标不变 x1_change = 1 y1_change = 0 elif event.key == pygame.K_UP: # 上：X坐标不变，Y坐标减少1个区块 x1_change = 0 y1_change = -1 elif event.key == pygame.K_DOWN: # 下：X坐标不变，Y坐标增加1个区块 x1_change = 0 y1_change = 1 # 退出游戏 if game_over: break # 检测蛇是否触墙 if x1 \u0026gt;= SCREEN_WIDTH or x1 \u0026lt; 0 or y1 \u0026gt;= SCREEN_HEIGHT or y1 \u0026lt; 0: game_close = True # 更新蛇的位置 x1 += x1_change * SNAKE_BLOCK y1 += y1_change * SNAKE_BLOCK # 清空屏幕，准备下一轮绘制 SCREEN.fill(BLACK) # 画食物 pygame.draw.rect(SCREEN, RED, [x_food, y_food, SNAKE_BLOCK, SNAKE_BLOCK]) # 新的蛇头位置，同时删除最后蛇尾区块，以保持蛇的总长度不变 snake_head = [x1, y1] snake_list.append(snake_head) if len(snake_list) \u0026gt; snake_length: del snake_list[0] # 删除蛇尾 # 检查蛇头是否碰到蛇的身体 for x in snake_list[:-1]: if x == snake_head: game_close = True # 绘制蛇 draw_snake(snake_list) # 绘制得分 draw_score(snake_length - 1) # 刷新屏幕 pygame.display.update() # 检查蛇头是否碰到食物，若碰到则增加长度并重新生成食物 if x1 == x_food and y1 == y_food: x_food, y_food = food_position() snake_length += 1 # 控制游戏帧率 clock = pygame.time.Clock() clock.tick(SNAKE_SPEED) # 游戏结束时清理pygame环境 pygame.quit() quit() 最后：启动游戏主循环 启动游戏，进入主循环，通过上下左右按键控制蛇身游动：\n1 2 3 # 开始游戏 if __name__ == \u0026#39;__main__\u0026#39;: game_loop() 最后，我们游戏界面大概像这样：\n单局游戏结束的界面：\n禅定： 我们可以继续进一步优化这个程序：\n每次吃到食物、单局游戏结束增加音效 增加难度：长按上下左右按键，加速蛇游动的速度 记住每轮游戏分数，进行分数排名（本地记录，或者联网） 关注本公众号，我们一起探寻编程的乐趣！\n我的本博客原地址：https://ntopic.cn/p/2024052301\n关注本公众号，我们共同学习进步👇🏻👇🏻👇🏻\n","date":"2024-05-23T00:00:00Z","image":"https://ntopic.cn/p/2024052301/02_hu857d4d6d7cfbb228d72782ba50ace628_95126_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/2024052301/","title":"Python游戏编程：一步步用Python打造经典贪吃蛇小游戏"},{"content":" 前面两篇博文，我们分别在个人笔记本电脑部署了Llama 3 8B参数大模型，并使用Ollama搭建了基于 Web 可视化对话聊天机器人，可以在自己电脑上愉快的与Llama大模型 Web 机器人对话聊天了。但在使用过程中，笔者发现Llama大模型经常出现中文问题英文回答的问题，需要使用中文回答等提示词告诉大模型用中文回答，体验还不是最好的。今天，本博文就来解决这个问题，让我们有个中文版的Llama 3Web 对话机器人（Llama3-Chinese-Chat）……\n第一篇Llama 3 8B大模型部署和 Python 版对话机器人博文：玩转 AI，笔记本电脑安装属于自己的 Llama 3 8B 大模型和对话客户端\n第二篇基于Ollama部署Llama 3 8B大模型 Web 版本对话机器人博文：一文彻底整明白，基于 Ollama 工具的 LLM 大语言模型 Web 可视化对话机器人部署指南\n注意： 因为本博文介绍的是Llama 3 中文版（Llama3-Chinese-Chat）对话机器人，涉及到前面两篇博文内容，特别是第二篇 Web 版本对话机器人部署，因此建议按照前文博文部署好Llama 3 8B大语言模型。\nHF 上选择排名最高的模型 模型列表官网地址：https://huggingface.co/models\n模型列表国内镜像（推荐）：https://hf-mirror.com/models\n在模型列表页面按照关键字llama chinese搜索，并按照趋势排序，可以看到中文版模型：\n可以看出，第一名模型的下载数量和点赞数量，比第二名要多好多，我们就选择shenzhi-wang这位作者发布的模型。\n方式一：通过 GGUF 量化模型安装（推荐） GGUF 安装比较简单，下载单个文件即可：\n下载到本地之后，按照我的第一篇博文，即可进行控制台聊天了：\n启动大模型Shell 脚本：\n1 2 3 4 5 source ./venv/bin/activate python -m llama_cpp.server --host 0.0.0.0 --model \\ ./Llama3-8B-Chinese-Chat-q4_0-v2_1.gguf \\ --n_ctx 20480 Python 对话客户端代码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 from openai import OpenAI # 注意服务端端口，因为是本地，所以不需要api_key ip = \u0026#39;127.0.0.1\u0026#39; #ip = \u0026#39;192.168.1.37\u0026#39; client = OpenAI(base_url=\u0026#34;http://{}:8000/v1\u0026#34;.format(ip), api_key=\u0026#34;not-needed\u0026#34;) # 对话历史：设定系统角色是一个只能助理，同时提交“自我介绍”问题 history = [ {\u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;你是一个智能助理，你的回答总是容易理解的、正确的、有用的和内容非常精简.\u0026#34;}, ] # 首次自我介绍完毕，接下来是等代码我们的提示 while True: completion = client.chat.completions.create( model=\u0026#34;local-model\u0026#34;, messages=history, temperature=0.7, stream=True, ) new_message = {\u0026#34;role\u0026#34;: \u0026#34;assistant\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;\u0026#34;} for chunk in completion: if chunk.choices[0].delta.content: print(chunk.choices[0].delta.content, end=\u0026#34;\u0026#34;, flush=True) new_message[\u0026#34;content\u0026#34;] += chunk.choices[0].delta.content history.append(new_message) print(\u0026#34;\\033[91;1m\u0026#34;) userinput = input(\u0026#34;\u0026gt; \u0026#34;) if userinput.lower() in [\u0026#34;bye\u0026#34;, \u0026#34;quit\u0026#34;, \u0026#34;exit\u0026#34;]: # 我们输入bye/quit/exit等均退出客户端 print(\u0026#34;\\033[0mBYE BYE!\u0026#34;) break history.append({\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: userinput}) print(\u0026#34;\\033[92;1m\u0026#34;) 运行 Python 客户端即可：\n按照第二篇博文，部署基于 Web 版对话机器人：一文彻底整明白，基于 Ollama 工具的 LLM 大语言模型 Web 可视化对话机器人部署指南\n基于 GGUF 量化模型生成 Ollama模型文件，假设文件名为Modelfile-Chinese，内容如下：\n1 FROM ./Llama3-8B-Chinese-Chat-q4_0-v2_1.gguf 执行 Ollama 模型转换，Llama-3-8B-Chinese为 Ollama 模型名：\n1 2 3 4 5 6 $ ollama create Llama-3-8B-Chinese -f ./Modelfile-Chinese transferring model data using existing layer sha256:242ac8dd3eabcb1e5fcd3d78912eaf904f08bb6ecfed8bac9ac9a0b7a837fcb8 creating new layer sha256:9f3bfa6cfc3061e49f8d5ab5fba0f93426be5f8207d8d8a9eebf638bd12b627a writing manifest success 可以通过 Ollama 查看目前的大模型列表：\n1 2 3 4 $ ollama list NAME ID SIZE MODIFIED Llama-3-8B-Chinese:latest 37143cf1f51f 4.7 GB 42 seconds ago Llama-3-8B:latest 74abc0712fc1 4.9 GB 3 days ago 可以看到我们刚安装的大模型：Llama-3-8B-Chinese\n启动ollama-webui-lite项目，可以选择Llama-3-8B-Chinese模型和对话聊天了：\n1 2 3 4 5 6 7 8 9 10 11 12 $ npm run dev \u0026gt; ollama-webui-lite@0.0.1 dev \u0026gt; vite dev --host --port 3000 VITE v4.5.3 ready in 1797 ms ➜ Local: http://localhost:3000/ ➜ Network: http://192.168.101.30:3000/ ➜ press h to show help 方式二：通过 Ollama 拉取模型文件 这种方式比较简单，无需下载 GGUF 模型文件，可以让 Ollama 直接拉取模型文件并完成安装：\n1 2 3 4 5 6 7 8 # Llama3-8B-Chinese-Chat的4位量化版本（对机器性能要求最低） ollama run wangshenzhi/llama3-8b-chinese-chat-ollama-q4 # Llama3-8B-Chinese-Chat的8位量化版本（对机器性能要求中等） ollama run wangshenzhi/llama3-8b-chinese-chat-ollama-q8 # Llama3-8B-Chinese-Chat的f16未量化版本（对机器性能要求最高） ollama run wangshenzhi/llama3-8b-chinese-chat-ollama-fp16 Ollama 自动下载并完成安装，之后启动ollama-webui-lite项目，就可以使用了~\n我的本博客原地址：https://ntopic.cn/p/2024052101\n","date":"2024-05-21T00:00:00Z","image":"https://ntopic.cn/p/2024052101/02_hub3da83b4b5bc32de4db9cd09399c10b7_374346_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/2024052101/","title":"基于Llama 3搭建中文版（Llama3-Chinese-Chat）大模型对话聊天机器人"},{"content":" 在上一篇博文中，我们在本地部署了Llama 3 8B参数大模型，并用 Python 写了一个控制台对话客户端，基本能愉快的与 Llama 大模型对话聊天了。但控制台总归太技术化，体验不是很友好，我们希望能有个类似 ChatGPT 那样的 Web 聊天对话界面，本博文就安排起来……\n上一篇Llama 3 8B大模型部署和 Python 对话客户端博文：玩转 AI，笔记本电脑安装属于自己的 Llama 3 8B 大模型和对话客户端\n因为本博文介绍的Web 可视化对话机器人，涉及到前文的Llama 3 8B大模型（并不是强依赖），因此建议提取安装前文部署好Llama 3 8B大语言模型。\n为了方便把我们的大模型对话机器人分享出去，聊天机器人最后是基于Web网站，可通过浏览器访问，本文正是通过Ollama和WebUI在本地部署Llama 3Web 版聊天机器人，本文包括如下部分：\n什么是Ollama，它与Llama是什么关系？ 安装Ollama大语言模型工具 安装Node.js编程语言工具包（为接下来的 Web 可视化聊天界面做好准备） 基于Llama 3 8BGGUF 模型文件创建Ollama模型文件 部署Ollama大模型 Web 可视化聊天界面 愉快的与Llama 3大模型俩天对话 最后，Ollama大模型工具的其他用法 什么是Ollama，它与Llama是什么关系？ Ollama是一个开源的 LLM（大型语言模型）服务工具，用于简化在本地运行大语言模型，降低使用大语言模型的门槛，使得大模型的开发者、研究人员和爱好者能够在本地环境快速实验、管理和部署最新大语言模型，包括如Llama 3、Phi 3、Mistral、Gemma等开源的大型语言模型。\nOllama目前支持以下大语言模型：https://ollama.com/library\n因此，Ollama与Llama的关系：Llama是大语言模型，而Ollama是大语言模型（不限于Llama模型）便捷的管理和运维工具\n安装Ollama大语言模型工具 Ollama提供了MacOS、Linux和Windows操作系统的安装包，大家可根据自己的操作系统，下载安装即可：\n安装包下载之后的安装过程，和日常安装其他软件没有差别，包括点击Next以及Install等安装ollama到命令行。安装后续步骤中，我们可无需安装任何模型（默认是Llama 3），因为我们在上文中已经安装了Llama 3 8B大模型，后面可以直接使用。\n当然，假如没有根据我的前面博文安装Llama 3 8B模型，在安装Ollama过程中，也可以一起进行安装。\n安装Node.js编程语言工具包 安装Node.js编程语言工具包和安装其他软件包一样，下载安装即可：https://nodejs.org\n安装完成之后，可以验证一下 Node.js 的版本，建议用目前的最新v20版本：\n1 node -v 我安装的版本：v20.13.1（最新版本）\n基于Llama 3 8BGGUF 模型文件创建Ollama模型 在我们存放Llama 3 8B的 GGUF 模型文件目录中，创建一个文件名为Modelfile的文件，该文件的内容如下：\n1 FROM ./Meta-Llama-3-8B-Instruct.Q4_K_M.gguf 然后在控制台，使用这个文件创建Ollama模型，这里我把Ollama的模型取名为Llama-3-8B：\n1 2 3 4 5 6 $ ollama create Llama-3-8B -f ./Modelfile transferring model data using existing layer sha256:647a2b64cbcdbe670432d0502ebb2592b36dd364d51a9ef7a1387b7a4365781f creating new layer sha256:459d7c837b2bd7f895a15b0a5213846912693beedaf0257fbba2a508bc1c88d9 writing manifest success 最后，通过Ollama启动我们刚创建的大语言模型：\n1 ollama run Llama-3-8B 启动完毕，其实我们已经有了一个控制台聊天界面，可以通过控制台与Llama-3-8B聊天了\n如果我们不想要这个模型了，也可以通过命令行删除模型文件：ollama rm Llama-3-8B\nOllama存放模型文件根目录：~/.ollama\n部署Ollama大模型 Web 可视化聊天界面 控制台聊天对话界面体验总归是不太好，接下来部署 Web 可视化聊天界面。\n首先，下载ollama-webuiWeb 工程代码：git clone https://github.com/ollama-webui/ollama-webui-lite\n然后切换ollama-webui代码的目录：cd ollama-webui-lite\n设置 Node.js 工具包镜像源，以接下来下载 Node.js 的依赖包更加快速：npm config set registry http://mirrors.cloud.tencent.com/npm/\n安装 Node.js 依赖的工具包：npm install\n最后，启动 Web 可视化界面：npm run dev\n如果看到以上输出，代表 Web 可视化界面已经成功了！\n愉快的与Llama 3大模型俩天对话 浏览器打开 Web 可视化界面：http://localhost:3000/\n可以看到Ollama的初始化页面，默认没有模型，需要选择，我们选择刚创建并部署的Llama-3-8B模型：\n底部就是聊天输入框，至此可以愉快的与Llama 3聊天对话了：\nWeb 对话聊天机器人的设置，大家可以基于 Web 网站设置，这里不在介绍，有需要的网友可以私信一起研究进步！\n禅定：Ollama工具的其他用法 从上文的介绍可以看到，基于Ollama部署一个大模型的 Web 可视化对话机器人，还是非常方便。下面整理了部分Ollama提供的用法或者。\nOllama 命令工具\n1 2 3 4 5 6 7 8 9 10 11 # 查看当前Ollama的模型 ollama list # 增量更新当前部署的模型 ollama pull Llama-3-8B # 删除一个模型文件 ollama rm Llama-3-8B # 复制一个模型 ollama cp Llama-3-8B Llama-newModel Ollama API结果返回\n1 2 3 4 curl http://localhost:11434/api/generate -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;Llama-3-8B\u0026#34;, \u0026#34;prompt\u0026#34;:\u0026#34;为什么天空是蓝色的？\u0026#34; }\u0026#39; Ollama API聊天对话\n1 2 3 4 5 6 curl http://localhost:11434/api/chat -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;Llama-3-8B\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;为什么天空是蓝色的？\u0026#34; } ] }\u0026#39; 我的本博客原地址：https://ntopic.cn/p/2024051801\n","date":"2024-05-18T00:00:00Z","image":"https://ntopic.cn/p/2024051801/02_hua04b28422bbb4c6a12cc5f607adbb1af_86436_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/2024051801/","title":"一文彻底整明白，基于Ollama工具的LLM大语言模型Web可视化对话机器人部署指南"},{"content":"报道地址：https://mp.weixin.qq.com/s/Xur3IUYf7PPOx1SCTuVPFg\n发布会视频：https://www.youtube.com/watch?v=DQacCB9tDaw\nGTP-4o 官网：https://openai.com/index/hello-gpt-4o/\n2024 年 4 月 14 日，一场不到 30 分钟的发布会，将又一次大大改变 AI 行业和我们未来的生活，也会让无数 AI 初创公司焦头烂额。\n这真不是标题党，因为这是 OpenAI 的发布会。\nOpenAI 正式发布了 GPT-4o，其中的「o」代表「omni」（即全面、全能的意思），这个模型同时具备文本、图片、视频和语音方面的能力，甚至就是 GPT-5 的一个初期版本。\n更重要的是，这个 GPT-4 级别的模型，将向所有用户开放，并且未来几周内先向 ChatGPT Plus 推送。我们先给大家一次性总结这场发布会的亮点，更多功能解析请看发布会视频。\n发布会要点： 1.新的 GPT-4o 模型：打通任何文本、音频和图像的输入，相互之间可以直接生成，无需中间转换 2.GPT-4o 语音延迟大幅降低，能在 232 毫秒内回应音频输入，平均为 320 毫秒，这与对话中人类的响应时间相似 3.GPT-4o 向所有用户免费开放（指日可待） 4.GPT-4o API，比 GPT 4-Turbo 快 2 倍，价格便宜 50% 5.惊艳的实时语音助手演示：对话更像人、能实时翻译，识别表情，可以通过摄像头识别画面写代码分析图表 6.ChatGPT 新 UI，更简洁 7.一个新的 ChatGPT 桌面应用程序，适用于 macOS，Windows 版本今年晚些时候推出\n这些功能早在预热阶段就被 Altman 形容为「感觉像魔法」，既然全世界 AI 模型都在「赶超 GPT-4」，那 OpenAI 也要从武器库掏出点真家伙。\n当然，还轮不到 GPT-5 登场。\n我的本博客原地址：https://ntopic.cn/p/2024051501\n","date":"2024-05-15T00:00:00Z","image":"https://ntopic.cn/p/2024051501/01_hu6d1fb637b3ab2acbfa05728cab4305fc_286734_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/2024051501/","title":"GPT-4o正式发布：视频语音推理交互丝滑到吓人，将向所有用户开放"},{"content":" 2024 年 4 月 18 日，Meta开源了 Llama 3 大模型，把 AI 的门槛降低到了最低，这是人工智能领域的一个重要飞跃。我们个人也可以部署大模型了，这简直就是给个人开发者发了个大红包！Llama 3 模型有不同的参数版本，本文主要分享我在个人笔记本电脑是部署 8B 参数过程和编写客户端，让我们大家都参与进来，推动 AI 应用更上一层楼……\n本文Llama 3 8B客户端源代码地址：https://gitee.com/obullxl/PythonCS/tree/master/Llama-3-8B\n选择 Llama 3 模型版本（8B，80 亿参数） 特别注意： Meta 虽然开源了 Llama 3 大模型，但是每个版本都有 Meta 的许可协议，建议大家在接受使用这些模型所需的条款之前仔细阅读。\nLlama 3 模型版本有几个，我们主要关注 80 亿参数（Llama 3 8B）和 700 亿参数（Llama 3 70B）这两个版本。它们对电脑系统配置有不同的要求，主要计算资源（即：CPU/GPU）和内存来存储和处理模型权重：\nLlama 3 8B 版本：对于 80 亿参数的模型，建议至少 4 核 CPU，至少 16GB 内存（推荐 32GB 或更高），以确保模型加载和运行过程中的流畅性；模型文件大小 5 GB 左右，磁盘空间有 10GB 足够了；GPU 是可选的，它可以显著提高推理速度\nLlama 3 70B 版本：对于 700 亿参数的模型，CPU 要求显著提高（建议 16 核以上），至少需要 64GB 内存（推荐 128GB 或更高），模型在推理时会占用大量的内存资源；模型文件超过 20GB，远超 8B 版本；强烈推荐使用高端 GPU，以实现有效加速\n综上所述，8B 版本比较适合我们个人电脑，硬件配置基本能符合，同时模型又不失推理效果：\n下载 Llama 3 8B 模型文件 我们第一步是想自己部署尝鲜，因此直接下载压缩后的模型权重，文件为GGUF格式，GGUF格式是为了快速推理和优化内存使用而设计的，相比以前的GGML格式，GGUF支持更复杂的令牌化过程和特殊令牌处理，能更好地应对多样化的语言模型需求。就是因为有GGUF格式，Llama 3大语言模型才可以在笔记本电脑上运行，同时GGUF就一个文件，也简化了模型交换和部署的过程，它对促进模型的普及和应用有着积极作用。\n因为Hugging Face官网正常无法访问，需要科学上网，因此推荐国内镜像进行下载：\n官网地址：https://huggingface.co/QuantFactory/Meta-Llama-3-8B-Instruct-GGUF/tree/main\n国内镜像：https://hf-mirror.com/QuantFactory/Meta-Llama-3-8B-Instruct-GGUF/tree/main\nGGUF 模型文件名称接受，如上述列表中，有Meta-Llama-3-8B-Instruct.Q4_K_M.gguf和Meta-Llama-3-8B-Instruct.Q5_K_M.gguf等：\nInstruct代表本模型是对基线模型进行了微调，用于更好地理解和生成遵循指令（instruction-following）的文本，以提供符合要求的响应\nQ4/Q5 等代表模型权重的量化位数（其中Q是Quantization的缩小，即量化），是一种模型压缩技术，用于减少模型大小，同时降低对计算资源的需求（特别是内存），但又尽量保持模型的性能；数字4或5则代表量化精度的位数（Q4 是 4 位，Q5 是 5 位等），精度越高模型体积和内存使用也会越大，但仍然远小于未量化的基线模型\nK_M/K_S代表含义笔者还未明确，K可能是Knowledge的缩写；M应该是Medium缩写（即中等模型），S应该是Small缩小（即小模型）；若有明确的朋友，还望不吝告知，共同进步！\n若个人电脑配置不是特别好，我们可以选择Q2_K版本（大小 3.2GB），它相较于Q4_K_M版本（大小 4.9GB），Q2版本的推理精度较低，但速度较快，而Q4版本在速度和精度之间均取得了很好的平衡，因此首选推荐Q4_K_M版本。\n点击下载图标即可下载，由于文件较大，浏览器的下载容易过程容易终端，重试可继续下载（笔者浏览器中断了好几次，总共耗时 4 个多小时）\n启动大模型服务端 GGUF模型量化文件下载完成后，我们就可以来运行Llama 3大模型了。首先打开一个 Terminal 终端窗口，切换到GGUF文件目录，设置 Python虚拟环境：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 # 切换到存放GGUF文件目录 cd ~/PythonSpace/Llama3-8B/ # 切换Python 3.12.2版本 conda activate PY3.12.2 # 创建并激活虚拟环境 python -m venv venv source ./venv/bin/activate # 安装依赖包 pip install llama-cpp-python pip install openai pip install uvicorn pip install starlette pip install fastapi pip install sse_starlette pip install starlette_context pip install pydantic_settings # 启动Llama大模型 python -m llama_cpp.server --host 0.0.0.0 --model \\ ./Meta-Llama-3-8B-Instruct.Q4_K_M.gguf \\ --n_ctx 2048 最后启动 Llama 模型命令中，n_ctx 2048代表单次回话最大 Token 数量。启动成功，我们应该看到类似如下的信息：\n恭喜你，你已经迈入 Llama 大模型大厦的大门了，后面存在无限可能，就看我们的创意了！\n编写 Llama 模型对话客户端 接下来，我们将使用llama-cpp库和openai库在个人电脑上快速搭建Llama 模型的客户端，开始尝鲜大模型（它目前只是个控制台客户端，还不能如 ChatGPT 那样有可视化的界面，但它的功能一样完备，所以请各位不用着急，我们先来体验一下 Llama 大模型，可视化的界面下文我在和大家分享）。\nPython 客户端代码如下，为了后续方便演示，这个 Client.py 文件也放到GGUF模型文件一起：\n我们使用OpenAI接口来与 Llama 交互，上面启动模型的最后，我们看到服务端 IP 是本地，端口是8000 接着，我们使用 2 条信息对历史记录进行初始化：第一个条是系统信息，第二个条是要求模型自我介绍的用户提示，为了避免长篇大论，我这里限制了回答的长度和字数 接下来，通过\u0026gt;提示符等待用户（即我们）输入，输入bye、quit和exit任意一个即代表退出客户端 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 from openai import OpenAI # 注意服务端端口，因为是本地，所以不需要api_key client = OpenAI(base_url=\u0026#34;http://localhost:8000/v1\u0026#34;, api_key=\u0026#34;not-needed\u0026#34;) # 对话历史：设定系统角色是一个只能助理，同时提交“自我介绍”问题 history = [ {\u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;你是一个智能助理，你的回答总是正确的、有用的和内容非常精简.\u0026#34;}, {\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;请用中文进行自我介绍，要求不能超过5句话，总字数不超过100个字。\u0026#34;}, ] print(\u0026#34;\\033[92;1m\u0026#34;) # 首次自我介绍完毕，接下来是等代码我们的提示 while True: completion = client.chat.completions.create( model=\u0026#34;local-model\u0026#34;, messages=history, temperature=0.7, stream=True, ) new_message = {\u0026#34;role\u0026#34;: \u0026#34;assistant\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;\u0026#34;} for chunk in completion: if chunk.choices[0].delta.content: print(chunk.choices[0].delta.content, end=\u0026#34;\u0026#34;, flush=True) new_message[\u0026#34;content\u0026#34;] += chunk.choices[0].delta.content history.append(new_message) print(\u0026#34;\\033[91;1m\u0026#34;) userinput = input(\u0026#34;\u0026gt; \u0026#34;) if userinput.lower() in [\u0026#34;bye\u0026#34;, \u0026#34;quit\u0026#34;, \u0026#34;exit\u0026#34;]: # 我们输入bye/quit/exit等均退出客户端 print(\u0026#34;\\033[0mBYE BYE!\u0026#34;) break history.append({\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: userinput}) print(\u0026#34;\\033[92;1m\u0026#34;) 我们新打开一个 Terminal 终端窗口，同样切换目标到 GGUF 文件目录，并且激活 Python 虚拟环境：\n1 2 3 4 5 6 7 8 9 10 11 # 切换到存放GGUF文件目录 cd ~/PythonSpace/Llama3-8B/ # 切换Python 3.12.2版本 conda activate PY3.12.2 # 激活虚拟环境（之前已经创建） source ./venv/bin/activate # 启动客户端 python client.py 首次打开客户端，因为有第一个默认的自我介绍问题，稍微有点忙，但是可以看到，Llama 模型按照我们的要求完成了自我介绍，总体还不赖：\n接着，我给Llama 模型来了一个类哲学的问题：请你用中文问答：人为什么要不断追求卓越？\nLlama 模型的回答非常精简，且只有 5 句话，所谓言简意赅：\n上图中，红色为我的输入，绿色为模型的答复，超级赞！\n禅定：总结 现在我们的Llama 模型聊天机器人已准备就绪，我们想问什么就可以问什么，尽情享受吧。\n当然，我们废了大半天劲，如果只是和模型简单的聊聊天，那就有点可惜了，或者说如果要人工输入，那我们本地部署的意义就不大。\n假设能够通过程序的方式，自动调用本地部署的Llama 模型是不是可以提供我们工作效率；Llama 模型的能力非常广泛，可用于多种场景和任务：\n自然语言生成：Llama 3 能够生成连贯、高质量的文本，包括文章、故事、诗歌等创意写作，以及邮件、报告等实用文体。 对话系统：模型可以用于构建聊天机器人或 AI 助手，进行自然、流畅的对话交流，提供信息查询、娱乐互动等功能。 代码生成：它在代码生成任务上表现优异，能够根据描述自动生成或补全代码片段，辅助程序员提高开发效率。 翻译：Llama 3 支持跨语言应用，可以实现文本的自动翻译，覆盖多种语言对。 文本摘要：能够自动生成文章、报告的摘要，提取关键信息，帮助用户快速浏览大量内容。 情感分析和文本分类：可以识别文本中的情绪倾向、主题分类，为企业提供市场洞察、客户服务优化等。 问答系统：高效准确地回答用户提出的问题，无论是常识性问题还是专业领域的复杂询问。 个性化推荐：基于用户的历史交互和偏好，生成个性化的推荐内容，如新闻、商品、音乐等。 文本生成图像描述：结合多模态技术，Llama 3 可以根据文本描述生成图像内容的描述，助力图像生成或图像检索。 法律文档处理：微调后的模型可以用于法律文档的理解、分析，比如合同审查、案例研究等。 关注本公众号，下次继续我们分享Llama 模型可视化对话的功能！\n问题，Llama 3 8B容易出现问中文，回答英文的问题，可下载中文微调版GGUF模型文件（Llama3-Chinese-Chat）：https://hf-mirror.com/collections/shenzhi-wang/llama3-chinese-chat-663a2b15ab68e84aa355ca4d\n我的本博客原地址：https://ntopic.cn/p/2024051101\n","date":"2024-05-11T00:00:00Z","image":"https://ntopic.cn/p/2024051101/01_hua04b28422bbb4c6a12cc5f607adbb1af_84235_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/2024051101/","title":"玩转AI，笔记本电脑安装属于自己的Llama 3 8B大模型和对话客户端"},{"content":" 在数字化与智能化的浪潮中，目前 AI（人工智能）几乎在各行各业中发挥了不可忽略的价值，今天让我们也来体验一下 AI 的威力：我通过命令，一步一步的教 AI 利用 Python 编程语言打造了一款富有创意和趣味性的消消乐小游戏……\n本文 Python 消消乐游戏源代码：https://gitee.com/obullxl/PythonCS/tree/master/CS-CY2405\n消消乐游戏设计 这款 Python 消消乐小游戏需要有以下功能或者规则：\n需要有游戏界面，不能是控制台文本游戏\n随机生成不同颜色的方块，当点击某个颜色的方块时，则自动消除相邻的相同颜色的所有方块\n根据消除的方块总数量，作为游戏总得分\n可设置游戏时长参数，时间结束，展示总得分页，并可重新启动游戏\n为了体验 AI 的威力，我主要通过与 AI 对话，指导 AI 完成代码编写，我尽量不直接修改代码，只是做最后代码的缝合和整理\n我期望的游戏界面大概就像这样：\n国内免费 AI，游戏可看不可玩 目前可选的 AI 工具有很多，我首先选择国内免费的 AI 工具（暂不具体指名），根据以上我期望的游戏设计和规则，经过与 AI 多轮对话，AI 终于产出了如下代码。\n说实话，如果要给这段程序代码打个分（满分 100 分），我觉得90 分应该不为过：\n代码注释比较充分，关键地方都进行了注释说明（虽然几次注释不是特别地准确）\n函数 create_new_board() 核心方法中，还使用了列表推导（这是 Python 的高级特性，后续我发文在介绍）\n同时，核心函数 check_and_remove() 在使用递归的方式来检测相邻同颜色的方块（即：上下左右方块），也考虑到了数值防止越界问题\n更令人惊艳的是，本程序可以直接运行，且还可以展示游戏界面，但可惜的是，核心函数 check_and_remove() 逻辑中，检测和消除相邻颜色方块的是个死循环，游戏无法玩，只能看看主界面。\n为什么 check_and_remove() 游戏的核心的递归函数是个死循环呢？\n禅定： 每个方块的上下左右相邻块是相对的概念，假如 A 方块的上方是 B 方块，那么递归检测 B 方块时，它的下方就是 A 方块，因此递归检测过程就无限循环了。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 import pygame import random # 初始化pygame pygame.init() # 设置屏幕尺寸 SCREEN_WIDTH = 800 SCREEN_HEIGHT = 600 # 设置颜色 WHITE = (255, 255, 255) RED = (255, 0, 0) GREEN = (0, 255, 0) BLUE = (0, 0, 255) # 创建屏幕对象 screen = pygame.display.set_mode((SCREEN_WIDTH, SCREEN_HEIGHT)) # 设置标题 pygame.display.set_caption(\u0026#34;简易消消乐\u0026#34;) # 创建一个二维列表来表示游戏区域 grid_size = 80 # 每个格子的大小 grid_width = 10 # 格子宽度 grid_height = 10 # 格子高度 def create_new_board(): return [[random.choice([RED, GREEN, BLUE]) for _ in range(grid_width)] for _ in range(grid_height)] game_board = create_new_board() # 检查是否有可以消除的块 def check_and_remove(x, y): color = game_board[y][x] if ( # 检查上下左右是否有相同颜色的块 (0 \u0026lt;= x-1 \u0026lt; grid_width and game_board[y][x-1] == color) or (0 \u0026lt;= x+1 \u0026lt; grid_width and game_board[y][x+1] == color) or (0 \u0026lt;= y-1 \u0026lt; grid_height and game_board[y-1][x] == color) or (0 \u0026lt;= y+1 \u0026lt; grid_height and game_board[y+1][x] == color) ): # 如果找到相同颜色的块，则递归检查相邻的块 check_and_remove(x-1, y) if 0 \u0026lt;= x-1 \u0026lt; grid_width and game_board[y][x-1] == color else None check_and_remove(x+1, y) if 0 \u0026lt;= x+1 \u0026lt; grid_width and game_board[y][x+1] == color else None check_and_remove(x, y-1) if 0 \u0026lt;= y-1 \u0026lt; grid_height and game_board[y-1][x] == color else None check_and_remove(x, y+1) if 0 \u0026lt;= y+1 \u0026lt; grid_height and game_board[y+1][x] == color else None # 消除当前块 game_board[y][x] = None # 游戏主循环 running = True while running: for event in pygame.event.get(): if event.type == pygame.QUIT: running = False elif event.type == pygame.MOUSEBUTTONDOWN: # 获取鼠标点击的位置 mouse_pos = pygame.mouse.get_pos() # 计算点击的格子坐标 x_grid = mouse_pos[0] // grid_size y_grid = mouse_pos[1] // grid_size # 确保点击在格子范围内 if 0 \u0026lt;= x_grid \u0026lt; grid_width and 0 \u0026lt;= y_grid \u0026lt; grid_height: # 检查并尝试消除 check_and_remove(x_grid, y_grid) # 如果有消除，可能需要重新生成新的块或执行其他逻辑 # 这里我们简单地重新生成整个游戏板 game_board = create_new_board() # 填充背景色 screen.fill(WHITE) # 绘制游戏区域 for y, row in enumerate(game_board): for x, color in enumerate(row): if color is not None: pygame.draw.rect(screen, color, (x * grid_size, y * grid_size, grid_size, grid_size), 0) # 更新屏幕显示 pygame.display.flip() # 控制帧率 pygame.time.Clock().tick(60) # 退出pygame pygame.quit() 尽管我多次提示 AI check_and_remove() 函数是个死循环，请 AI 帮忙修正，但是 AI 就是无法改正，逐步由最开始的人工智能，变成“人工智障”了，没有办法，只能放弃另谋出路了。\n采用 GPT4，最终产出期望的游戏 同样的，前面与 GPT4 进行了多轮对话，最终产出了如下不仅可看且可玩了的基本功能的消消乐：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 import pygame import random import sys # 初始化Pygame pygame.init() # 屏幕尺寸 screen_width = 600 screen_height = 600 screen = pygame.display.set_mode((screen_width, screen_height)) # 颜色 background_color = (255, 255, 255) colors = [(255, 0, 0), (0, 255, 0), (0, 0, 255), (255, 255, 0), (255, 0, 255)] block_color = [[random.choice(colors) for _ in range(screen_width // 60)] for _ in range(screen_height // 60)] # 设置相关参数 block_size = 60 # 方块网格 grid = [[random.choice(colors) for x in range(screen_width // block_size)] for y in range(screen_height // block_size)] # 绘制网格 def draw_grid(): for y in range(0, screen_height, block_size): for x in range(0, screen_width, block_size): rect = pygame.Rect(x, y, block_size, block_size) pygame.draw.rect(screen, grid[y // block_size][x // block_size], rect) pygame.draw.rect(screen, (200, 200, 200), rect, 1) def get_neighbors(pos): x, y = pos neighbors = [] for dx, dy in [(-1, 0), (1, 0), (0, -1), (0, 1)]: nx, ny = x + dx, y + dy if 0 \u0026lt;= nx \u0026lt; len(grid[0]) and 0 \u0026lt;= ny \u0026lt; len(grid): neighbors.append((nx, ny)) return neighbors def bfs(start): \u0026#34;\u0026#34;\u0026#34;用BFS寻找所有相连的相同颜色方块\u0026#34;\u0026#34;\u0026#34; color = grid[start[1]][start[0]] to_visit = [start] connected = [] while to_visit: x, y = to_visit.pop(0) if (x, y) not in connected: connected.append((x, y)) for nx, ny in get_neighbors((x, y)): if grid[ny][nx] == color: to_visit.append((nx, ny)) return connected # 主循环 running = True while running: for event in pygame.event.get(): if event.type == pygame.QUIT: pygame.quit() sys.exit() if event.type == pygame.MOUSEBUTTONDOWN: # 获取鼠标点击位置并转换为网格坐标 mx, my = pygame.mouse.get_pos() grid_x, grid_y = mx // block_size, my // block_size # 执行BFS并可能地移除方块 connected_blocks = bfs((grid_x, grid_y)) if len(connected_blocks) \u0026gt; 2: # 如果选中的方块有2个以上相连，则消除它们 for x, y in connected_blocks: grid[y][x] = background_color screen.fill(background_color) draw_grid() pygame.display.flip() 继续给 AI 命令，以增加分数功能 既然是个游戏，那就得有游戏得分，我们以消除的方块总数量作为游戏总得分，即每消除 1 个方块则得 1 分，AI 根据对话给出了代码修改样例：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 # ... 其它代码 ... # 初始化分数 score = 0 # 初始化字体（用于显示分数） pygame.font.init() font = pygame.font.SysFont(\u0026#39;Arial\u0026#39;, 30) def draw_score(): # 渲染分数文本 score_surface = font.render(f\u0026#39;Score: {score}\u0026#39;, True, (0, 0, 0)) # 在屏幕上绘制分数 screen.blit(score_surface, (5, 5)) # ... 其它代码 ... # 主循环 running = True while running: # 检查事件 for event in pygame.event.get(): if event.type == pygame.QUIT: pygame.quit() sys.exit() if event.type == pygame.MOUSEBUTTONDOWN: # 计算鼠标点击位置的网格坐标 mx, my = pygame.mouse.get_pos() grid_x, grid_y = mx // block_size, my // block_size # 执行BFS并可能移除方块 connected_blocks = bfs((grid_x, grid_y)) if len(connected_blocks) \u0026gt; 2: # 如果有超过2个相连的方块 for x, y in connected_blocks: grid[y][x] = background_color score += len(connected_blocks) # 每个消除的方块增加1分 # 清屏 screen.fill(background_color) # 绘制网格和分数 draw_grid() draw_score() # 刷新屏幕 pygame.display.flip() 把以上代码复制到基本的消消乐游戏代码中，游戏总分数功能生效了：\n继续给 AI 命令，以增加倒计时功能 游戏有总分数还不够，一般的趣味游戏，每轮都应该是有时间限制，因此游戏还需要增加倒计时功能：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 # ... 其它代码 ... # 初始化倒计时（以秒为单位），假设为60秒 time_limit = 60 start_ticks = pygame.time.get_ticks() # 获取当前时间的初始tick数 def draw_time_remaining(): # 计算剩余时间 elapsed_time = (pygame.time.get_ticks() - start_ticks) / 1000 # 将毫秒转换为秒 remaining = max(time_limit - elapsed_time, 0) # 防止显示负数 # 渲染剩余时间文本 time_surface = font.render(f\u0026#39;Time: {int(remaining)}\u0026#39;, True, (0, 0, 0)) # 在屏幕上绘制剩余时间 screen.blit(time_surface, (5, 35)) # 放置在分数下面 # 主循环 running = True while running: # 检查事件 for event in pygame.event.get(): if event.type == pygame.QUIT: pygame.quit() sys.exit() if event.type == pygame.MOUSEBUTTONDOWN: # 计算鼠标点击位置的网格坐标 mx, my = pygame.mouse.get_pos() grid_x, grid_y = mx // block_size, my // block_size # 执行BFS并可能移除方块 connected_blocks = bfs((grid_x, grid_y)) if len(connected_blocks) \u0026gt; 2: # 如果有超过2个相连的方块 for x, y in connected_blocks: grid[y][x] = background_color score += len(connected_blocks) # 每个消除的方块增加1分 # 计算并更新剩余时间 elapsed_time = (pygame.time.get_ticks() - start_ticks) / 1000 if elapsed_time \u0026gt;= time_limit: running = False # 时间结束，游戏结束 # 清屏 screen.fill(background_color) # 绘制网格和分数 draw_grid() draw_score() draw_time_remaining() # 刷新屏幕 pygame.display.flip() # 游戏结束逻辑，可以是显示最终得分，再见消息等 # ... pygame.quit() 把以上 AI 代码复制到带有分数功能的游戏代码中，运行可以看到有倒计时功能了：\n继续给 AI 命令，增加倒计时结束页面 有了倒计时功能，那么当倒计时结束的时候，需要给出具体页面，展示总得分的情况：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 import pygame import random import sys # 初始化Pygame pygame.init() # 屏幕尺寸 screen_width = 600 screen_height = 600 screen = pygame.display.set_mode((screen_width, screen_height)) # 颜色定义 background_color = (255, 255, 255) colors = [(255, 0, 0), (0, 255, 0), (0, 0, 255), (255, 255, 0), (255, 0, 255)] font = pygame.font.SysFont(\u0026#39;Arial\u0026#39;, 26) # ... 其他必要的函数定义，如 draw_grid, draw_time_remaining 等 ... def main_game(): # 游戏设置 score = 0 time_limit = 60 # 设置时间限制（秒） start_ticks = pygame.time.get_ticks() # 开始时间 # 创建方格 grid = [[random.choice(colors) for x in range(screen_width // block_size)] for y in range(screen_height // block_size)] running = True while running: elapsed_time = (pygame.time.get_ticks() - start_ticks) / 1000 if elapsed_time \u0026gt;= time_limit: game_over_display(score) # 传递得分给游戏结束画面 running = False # 结束游戏循环 continue for event in pygame.event.get(): if event.type == pygame.QUIT: pygame.quit() sys.exit() if event.type == pygame.MOUSEBUTTONDOWN and event.button == 1: # 鼠标点击逻辑，检查是否需要消除方块，并更新得分 # ... # 游戏绘制逻辑 screen.fill(background_color) draw_grid() # ... 绘制得分和剩余时间等 ... pygame.display.flip() def game_over_display(score): screen.fill(background_color) # 游戏结束文本 game_over_text = font.render(\u0026#39;Game Over\u0026#39;, True, (255, 0, 0)) game_over_rect = game_over_text.get_rect(center=(screen_width // 2, screen_height // 2 - 50)) screen.blit(game_over_text, game_over_rect) # 最终得分文本 final_score_text = font.render(f\u0026#39;Final Score: {score}\u0026#39;, True, (0, 0, 0)) final_score_rect = final_score_text.get_rect(center=(screen_width // 2, screen_height // 2)) screen.blit(final_score_text, final_score_rect) # 重新开始游戏提示 restart_text = font.render(\u0026#39;Press space to restart\u0026#39;, True, (0, 0, 255)) restart_rect = restart_text.get_rect(center=(screen_width // 2, screen_height // 2 + 50)) screen.blit(restart_text, restart_rect) pygame.display.flip() # 等待用户按空格键重启游戏 waiting = True while waiting: for event in pygame.event.get(): if event.type == pygame.QUIT: pygame.quit() sys.exit() if event.type == pygame.KEYDOWN: if event.key == pygame.K_SPACE: # 按空格键 waiting = False if __name__ == \u0026#34;__main__\u0026#34;: while True: main_game() # 启动主游戏 继续把 AI 给出的修改代码复制到总分数代码中，可以看到倒计时结束功能也生效了：\n至此，一款符合期望的 Python 消消乐游戏就完成了~\n禅定：增加字体，游戏文字汉化 本消消乐游戏代码最后的优化：\n增加支持中文的字体：font = pygame.font.Font('./fonts/simsun.ttf', 28) 部分文本内容汉化：如总得分、倒计时、游戏结束等 部分游戏参数提取：如倒计时、最小消除方块数量等 禅定： 我们可以继续进一步优化这个程序：\n使用彩虹糖图片代替方块 每次消掉方块，方块可随机布局，增加难度 消掉方块，增加动态效果，增加音效 记住每轮游戏分数，进行分数排名 关注本公众号，下次继续我们分享增加的功能！\n我的本博客原地址：https://ntopic.cn/p/2024050901\n","date":"2024-05-09T00:00:00Z","image":"https://ntopic.cn/p/2024050901/01_hua04b28422bbb4c6a12cc5f607adbb1af_171177_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/2024050901/","title":"AI已来，我与AI一起用Python编写了一个消消乐小游戏"},{"content":" 在前一篇文章中，我介绍了 2024 年 4 月 24 日上海个人购房个税退税经历，我于 4 月 27 日周六，顺利办理租房公积金提取业务，资金在业务办理完成后 10 分钟左右到账。通过本文分享办理过程的材料和注意事项，避免大家踩坑白跑。接下来，就是房子装修了，关注本公众号，后面继续分享装修的点滴内容……\n上一篇文章：上海个人购房退税经历和注意事项（收藏不踩坑）\n提取住房公积金的前提（无贷款） 公积金无贷款、无冲贷，否则无法提取\n新产证五年以内（这个要求容易达到）\n提取方式一：长三角一网通办线上办理（不推荐） 在线地址：https://csj.sh.gov.cn/govService/column/zfgjj/index.html\n能线上办理那当然是优先选择，但是很不幸，提交表单过程中，系统频频出错，唯一的解决办法就是重试，直到重试 N 次也无解后放弃：\n我的第一个错误：婚姻关系校验错误，当时我就傻眼了，甚至一度以为我现在不是已婚，重试几次校验竟然通过了\n我的第二个错误：申请人（即：我）视频核身，显示浏览器证书错误，然后是失败。有了第一个错误的经验，多重试了几次，最后通过支付宝扫描核身通过了\n我的第三个错误：配偶视频核实，和我一样，先是报错，继续使用重试大法最终成功了\n我的第四个错误：房产信息查询不支持业务，尝试了多次仍然不成功，我最终放弃了，后面流程我也就不知道了\n网上办理过程的错误有点多，报错的图片我就不放在这里，同时也不建议大家去尝试了！\n提取方式二：公积金网点或者建设银行（推荐） 公积金网点：网点较少，且周六不上班，我直接放弃了，选择一个能周六上班的感觉更香一些\n建设银行网点：https://m.shgjj.com/html/newxxgk/ywwd/208101.html\n周六上班的建行网点：\n黄浦：福州路 725 号 浦东：东方路 818 号（我就是这个网点办理） 徐汇：肇嘉浜路 608 号 建行线下网点办理材料清单（原件） 材料只需要原件，无需复印件：\n夫妻双方身份证 结婚证（包括婚史，在哪个民政局办理就去哪里打印，不互通） 购房合同 二手房契税完税证明（或：全额发票） 房产证 夫妻双方中资一类借记卡 建行线下网点办理注意事项 材料准备好了，网点选择好了，还需要注意以下事项：\n公积金无贷款、无冲贷（Tips：先提取，然后在办理冲贷） 夫妻双方一起去，因为一套房仅可办理一次提取业务 若有婚史，需要去民政局打印婚史材料 夫妻双方中资一类借记卡，提取的资金夫妻双方打到各自卡中 建行网点，受理的业务类型需要包含提取，不能仅归集 最后，开始准备装修了 线下办理完成，走出建行网点，预计 10 分钟左右资金可到账\n资金到账了，接下来就是准备装修了，关注本公众号，后篇文章在与大家分享和交流。\n我的本博客原地址：https://ntopic.cn/p/2024050701\n","date":"2024-05-07T00:00:00Z","image":"https://ntopic.cn/p/2024050701/1_hubb9c3b68b76b6fb6b5a12c7050bc9b56_143058_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/2024050701/","title":"上海个人购房提取公积金经历和注意事项（收藏不踩坑）"},{"content":"AI 资讯 苹果或将推出 AI 功能，随 iPhone 发布 2024 年巴菲特股东大会，巴菲特将 AI 类比为核技术 巴菲特股东大会 5 万字实录 消息称 OpenAI 将于 5 月 9 日发布 ChatGPT 版搜索引擎 路透社消息，斯坦福大学 AI 领军人物李飞飞打造“空间智能”创业公司 报道地址 爆款生成式 AI 硬件，销量突破 10 万台，拆完一看只是安卓 app？ Sam Altman 在斯坦福创业者思维领导研讨会上的对话 翻译全文 Youtube 视频 告别偏科，能玩转多模态、多任务、多领域的强化智能体终于来了 12 年前上手深度学习，Karpathy 掀起一波 AlexNet 时代回忆杀，LeCun、Goodfellow 等都下场 Anthropic 终于推出企业版以及 iOS 应用！全面对标 OpenAI Perplexity CTO 最新复盘：软件利润开始变厚，不排除广告收入，但 Perplexity 可能不会像谷歌那样成功 Andrew Ng：低成本令牌生成和代理工作流程 大型科技公司 2023 年的人工智能投资图表 苹果或将推出 AI 功能，随 iPhone 发布 报道地址：https://9to5mac.com/2024/05/02/tim-cook-apple-ai-features/\n根据《华盛顿邮报》的报道，苹果公司首席执行官蒂姆·库克预计将在下周的“放手一搏”活动中对即将推出的 AI 功能进行“暗示”。尽管新款 iPad 是活动的主焦点，但苹果可能利用此机会初步概述新的 AI 功能。这些功能预计将在 6 月的全球开发者大会（WWDC）上详细介绍并随新一代 iPhone 16 系列和 iOS 18 一起发布。此外，苹果将其 5 月 7 日的主题演讲描述为“一种不同类型的活动”，这进一步增加了对新 AI 功能预览的期待。\n2024 年巴菲特股东大会，巴菲特将 AI 类比为核技术 巴菲特股东大会 5 万字实录：https://finance.sina.com.cn/stock/usstock/c/2024-05-05/doc-inaucmni4617264.shtml?finpagefr=p_108\n提问： 我是 John（音），从旧金山过来，你觉得现在科技进步特别是生成式 AI 跟传统行业能带来多大的优势？\n巴菲特： 不好意思，我问了四区问题，现在就后悔了，应该去问二区。我对 AI 人工智能一无所知，但是这不意味着好像这个技术不重要。去年我也提到过，我们已经让精灵从瓶子里跳出来，特别是我们在之前发明核武器的时候，它现在正在做一些坏事。这个精灵的这种力量有的时候让我感到恐惧，而且它已经再也塞不回瓶子里了，我觉得 AI 可能也有些相似，已经让它跳了出来。它当然非常重要，对很多人来说作用非常大，但是我们希望这样的精灵希望在未来能够做好事，但是我没有办法去对它进行评估，特别是我，我根本没有办法去做评估。像“二战”的时候，当时有了核技术，我们当时发明了原子弹，我们觉得好像是非常必要地来终止战争，对美国来说可以长期拯救生命，而且爱因斯坦也提过，说“你如果做了核试验，你可能会让文明终止”，但是最后美国还是决定把这个精灵从瓶子里放出来。我觉得这样的一些决定都会影响我们下一代，会影响未来的，这也是我们现在所看到的。\n在 AI 这边，它的发展，老实说还是让我有些紧张的，特别是最近的一些进展。我有些时候眼前会有这样一张图片，我出现在那张照片里对着我尖叫。这个是什么意思呢？就是我当时出了问题，没有任何人救得了我。\n所以，当你去思考一下，在未来你可以用这样的技术去欺诈、去重新生成这样一张图片，比如说图片里边也是我，然后我来找你说需要钱，生成式 AI 可以做这种事情，让我突然出现在一张图片里，你难辨真假，或者说我需要多少多少钱。欺诈在美国一直都是会出现的一种情况。如果我之前有兴趣投资这个欺诈市场的话，它可能成长会很快。AI 当然也有潜力去做好事，但是我刚才提到的这个例子和我之前目睹的一些现象，我觉得可能会有些吓人，我可能甚至看到这个照片之后，我难辨真假，甚至会把钱寄给一个来骗自己的我。所以，跟核技术一样，我觉得这个精灵已经从瓶子里放出去了，我对此一无所知。我觉得 AI 不管是做好事还是做坏事的很大的潜力，我只能让它自然而然在未来发生。\n我也想跟贝琪说一声，阿吉特今天下午不会在现场，如果今天有一些问题跟保险有关，想问他的话，尽量在上午问。\n消息称 OpenAI 将于 5 月 9 日发布 ChatGPT 版搜索引擎 报道地址：https://www.techweb.com.cn/it/2024-05-06/2944405.shtml\n路透社消息，斯坦福大学 AI 领军人物李飞飞打造“空间智能”创业公司 报道地址：https://www.reuters.com/technology/stanford-ai-leader-fei-fei-li-building-spatial-intelligence-startup-2024-05-03/\n知情人士透露称，李飞飞最近为这家公司进行了种子轮融资，投资者包括硅谷风险投资公司 Andreessen Horowitz，以及她去年加入的加拿大公司 Radical Ventures。不过，Andreessen Horowitz 和 Radical Ventures 的发言人均对此保持沉默，李飞飞也未作回应。\n告别偏科，能玩转多模态、多任务、多领域的强化智能体终于来了 论文地址：https://arxiv.org/abs/2402.09844\nGitHub 地址：https://github.com/huggingface/jat\n项目地址：https://huggingface.co/jat-project/jat\n来自 Hugging Face、法国国家信息与自动化研究所（INRIA）和波尔多大学的四位研究者提出了智能体中的「六边形战士」——Jack of All Trades (JAT)。JAT 是一个基于 Transformer 的多模态通用强化学习智能体框架。在此框架下，智能体能够通过同一套参数应对不同复杂度的多种任务，化身既会打游戏，又能控制机器人的全能高手。论文同时发布了大量 RL 智能体与 JAT 数据集。这是首个用于通用智能体训练的数据集 JAT 数据集，包含了由专家智能体收集的数十万条轨迹。\n12 年前上手深度学习，Karpathy 掀起一波 AlexNet 时代回忆杀，LeCun、Goodfellow 等都下场 推特地址：https://twitter.com/karpathy/status/1786537319576789425\n没想到，自 2012 年 AlexNet 开启的深度学习革命已经过去了 12 年。而如今，我们也进入了大模型的时代。近日，知名 AI 研究科学家 Andrej Karpathy 的一条帖子，让参与这波深度学习变革的许多大佬们陷入了回忆杀。从图灵奖得主 Yann LeCun 到 GAN 之父 Ian Goodfellow，纷纷忆往昔。到目前为止，该帖子已经有 63 万 + 的浏览量。\n跨越 300 多年的接力：受陶哲轩启发，数学家决定用 AI 形式化费马大定理的证明 在陶哲轩的启发下，越来越多的数学家开始尝试利用人工智能进行数学探索。这次，他们瞄准的目标是世界十大最顶尖数学难题之一的费马大定理。费马大定理又被称为「费马最后的定理（Fermat\u0026rsquo;s Last Theorem，FLT）」，由 17 世纪法国数学家皮耶・德・费马提出。它背后有一个传奇的故事。据称，大约在 1637 年左右，费马在阅读丢番图《算术》拉丁文译本时，曾在第 11 卷第 8 命题旁写道：「将一个立方数分成两个立方数之和，或一个四次幂分成两个四次幂之和，或者一般地将一个高于二次的幂分成两个同次幂之和，这是不可能的。关于此，我确信已发现了一种美妙的证法 ，可惜这里空白的地方太小，写不下。」这段话前面所表述的就是费马大定理的内容：当整数 n\u0026gt;2 时，关于 x^n + y^n=z^n 的方程没有正整数解。费马表示，自己知道怎么证明，但因为书的空白部分太小，就没有写。对于该故事的真实性以及费马是否真的想出了证明方法，后世是存在争议的。\nAnthropic 终于推出企业版以及 iOS 应用！全面对标 OpenAI 原文地址：https://mp.weixin.qq.com/s/JKXoHkg89aXzVlIlA0Q8nA\nAnthropic 正在推出一项针对企业的新付费计划，其中包括医疗保健、金融和法律等受到高度监管的行业，该计划名为“Team”，为客户提供对 Anthropic 的 Claude 3 系列生成型 AI 模型以及额外的管理员和用户管理控制的更高优先级访问权限。\nPerplexity CTO 最新复盘：软件利润开始变厚，不排除广告收入，但 Perplexity 可能不会像谷歌那样成功 原文地址：https://mp.weixin.qq.com/s/ursDVU9_4y3Rekx66vOUFw\nPerplexity CTO Denis Yarats 在近期向投资人分享了 Perplexity 产品演变、AI 技术以及商业模式的思考。Denis 谈到，Perplexity 当前的订阅是主要模式，未来肯定会有其他的东西，但现在看来实际利润也是相当不错。Denis 和团队观察到，过去一年中运行这些模型变得更加便宜，硬件也变得更便宜，模型变得更小更好，即使高 API 价格也降低了，过去一年中降了四五次，然后 Perplexity 内部也建立了一些东西，不必像以前那样过多地依赖 OpenAI API ，随着时间推移 Perplexity 利润正在增加。他表示，Perplexity 仍然会有其他的机会来实现盈利，不排除广告，但像 Google 现在所做的广告形式可能不是我们要做的。可以通过有效方法让广告成为对用户有益的方式，如果广告有帮助的话，人们其实并不介意广告。\n爆款生成式 AI 硬件，销量突破 10 万台，拆完一看只是安卓 app？ 原文地址：https://mp.weixin.qq.com/s/7DIN7q3xqTtXQjwJw8EbcA\n美国记者 Mishaal Rahman 曝光了知名生成式 AI 硬件 Rabbit R1 的细节，立即引来了科技圈的关注。几个月前，Humane、Rabbit 两家初创公司陆续推出他们的人工智能设备 ——Ai Pin 和 Rabbit R1。最初，一些人认为这些设备将开创可穿戴人工智能的新时代。然而，几个月过去了，对于这两款设备的争议逐渐增多。Rabbit R1 首次亮相是在 CES 2024 上，它是一款手掌大小的 AI 智能设备，售价 199 美元。其背后的研发公司 Rabbit 致力于基于大型行为模型（Large Action Model, LAM）研发下一代操作系统。\n参数量不到 10 亿的 OctopusV3，如何媲美 GPT-4V 和 GPT-4？ 论文地址：https://arxiv.org/pdf/2404.11459.pdf\n多模态 AI 系统的特点在于能够处理和学习包括自然语言、视觉、音频等各种类型的数据，从而指导其行为决策。近期，将视觉数据纳入大型语言模型 (如 GPT-4V) 的研究取得了重要进展，但如何有效地将图像信息转化为 AI 系统的可执行动作仍面临挑战。在最近的一篇论文中，研究者提出了一种专为 AI 应用设计的多模态模型，引入了「functional token」的概念。为确保该模型能兼容边缘设备，研究者将其参数量优化至 10 亿以内。与 GPT-4 类似，该模型能同时处理英文和中文。实验表明，该模型能在包括树莓派等各类资源受限的终端设备上高效运行。\n人工智能，步步逼近奥数天才 原文地址：https://mp.weixin.qq.com/s/pCnvv1XO9sb0304LdAJobg\n国际数学奥林匹克竞赛（IMO）或许是难度最高的高中数学竞赛，几乎集结了所有的数学天才。最近，一个名为“AlphaGeometry”的人工智能（AI）程序强势加入了 IMO 赛道，它在几何证明题上的表现已逼近人类金牌选手。在这篇来自《环球科学》2024 年 5 月新刊中，我们将跟随玛农·比肖夫的讲述，了解人工智能是如何步步逼近人类奥数天才的。\nAndrew Ng：低成本令牌生成和代理工作流程 推特地址：https://twitter.com/AndrewYNg/status/1786057567178834328\n低成本令牌生成和代理工作流程为大型语言模型（LLM）开辟了训练 LLM 使用合成数据的新可能性。直接在 LLM 自己生成的对提示的响应上进行预训练并没有帮助。但是，如果通过 LLM 实现的代理工作流程能产生比 LLM 直接生成的更高质量的输出，那么在该输出上进行训练就可能有用。\n正如人类可以从自己的思考中学习一样，LLM 可能也能做到。例如，想象一个正在学习编写数学证明的数学学生。通过解决一些问题——即使没有外部输入——他们可以反思什么有效，什么无效，并通过练习学习如何更快地生成好的证明。\n广泛来说，LLM 的训练包括：(i) 预训练（从未标记的文本数据中学习以预测下一个单词），然后是 (ii) 指令微调（学习遵循指令）以及 (iii) RLHF/DPO 调整，以使 LLM 的输出与人类价值观相匹配。步骤 (i) 需要的数据量比其他步骤多得多。例如，Llama 3 预训练了超过 15 万亿个令牌，LLM 开发者仍然渴望更多数据。我们还能从哪里获取更多的文本来训练呢？\n许多开发者直接在大模型的输出上训练小模型，这样小模型就学会了模拟大模型在特定任务上的行为。然而，一个 LLM 通过直接生成的数据训练并不能学到太多，就像一个监督学习算法不能从尝试预测它自己生成的标签中学习一样。的确，一个模型反复在它自身早期版本的输出上训练可能会导致模型崩溃。\n然而，一个包裹在代理工作流程中的 LLM 可能会产生比它直接生成的更高质量的输出。在这种情况下，LLM 的高质量输出可能作为 LLM 本身的预训练数据有用。\n像这样的努力有先例：\n在使用强化学习玩象棋游戏时，一个模型可能学习一个评估棋盘位置的函数。如果我们结合使用低精度的评估函数进行游戏树搜索，模型可以得出更准确的评估。然后我们可以训练该评估函数来模仿这些更准确的值。 在对齐步骤中，Anthropic 的宪法 AI 方法使用 RLAIF（来自 AI 反馈的 RL）来评估 LLM 输出的质量，用 AI 模型生成的反馈替代人类反馈。 使用代理工作流程引导 LLM 生成自己的训练数据的一个重大障碍是生成令牌的成本。假设我们想生成 1 万亿个令牌来扩展现有的训练数据集。目前，按公开宣布的价格，使用 GPT-4-turbo（每百万输出令牌 30 美元），Claude 3 Opus（75 美元），Gemini 1.5 Pro（21 美元）和 Groq 上的 Llama-3-70B（0.79 美元）生成 1 万亿个令牌的成本分别为 3000 万美元、7500 万美元、2100 万美元和 79 万美元。当然，像反思这样的设计模式的代理工作流程会要求生成的令牌数量超过我们作为训练数据使用的令牌。但是，训练尖端 LLM 的预算容易超过 1 亿美元，因此花费几百万美元用于提升性能的数据是完全可行的。\n这就是为什么我相信代理工作流程将为高质量合成数据生成开辟新的有趣机会。\n大型科技公司 2023 年的人工智能投资图表 推特地址：https://twitter.com/chiefaioffice/status/1786545556845666372\n由美国大型科技公司及其风险投资部门支持的人工智能初创企业的交易数量同比增长了 57%。\n我的本博客原地址：https://ntopic.cn/p/ai20240506\n","date":"2024-05-06T00:00:00Z","image":"https://ntopic.cn/p/ai20240506/20240506-01_hu120142c005cc345d297ddb948c21020f_668408_120x120_fill_box_smart1_3.png","permalink":"https://ntopic.cn/p/ai20240506/","title":"[每日AI·0506]巴菲特谈 AI，李飞飞创业，苹果或将推出 AI 功能，ChatGPT 版搜索引擎"},{"content":" 2023 年至 2024 年之间，在一年内换购（先卖后买）个人二手住房，卖房的个人所得税，在新买房后可以申请退还。我于 2024 年 4 月 24 日完成卖房的个税退税办理申请，办理过程的注意事项，通过本文分享给大家，避免大家踩坑白跑。同时，我于 2024 年 4 月 27 日提取住房公积金办理（资金 10 分钟内到账），关注本公众号，下文继续分享公积金提取的注意事项……\n前提：卖房个税发票和网签合同 我于 2023 年 5 月和 8 月分别卖掉了上海浦东 2 套个人住房，因前期就了解了个税退税政策，且自己接下来也准备买房（小孩读书），因此在和买家、中介前期沟通中，均明确表示我需要个税发票。为什么要特别强调卖房时个税发票的归属沟通呢，个税的纳税人本来不就是卖家吗：\n卖房过户的时候，默认情况下（二手房买卖基本行情），买家和卖家均会签署交易税代缴协议，即卖房的交易税（如：个税、增值税等）在缴税时由买家代缴（也就是房子的价格是房东到手价）\n卖房缴税领证的时候，因为有过户时的代缴协议，卖家一般不到场（我卖房和我买房的时候，我和卖家均未到场），那么个税发票默认由买家缴税后进行保管\n注意事项：为了避免后续退税时，再找买家要发票时的不必要的麻烦（比如：发票找不到了、买家人在外地等）；在卖房缴税时，一定要把个税发票拿到手且保管好（我和买家、中介前期均沟通明确，缴税当天发票由中介经理保管，后面我再去中介经理处领取）\n友情提示：在申请退个税还需要查看卖房的网签合同原件（注意不是居间合同），因此网签合同也需要保管好，不要当废纸扔掉了（建议至少保管 5 年后在决定是否扔掉）。\n友情提示：退税只能退一套房子的个税，如果卖掉了多套房子，建议选择房价较高、个税金额较多的那套房子进行退税（不差钱的朋友请随意）。\n买房：网签、缴税和领取新产证 买房过程很复杂，流程也一般较长，稍微说一下我的买家流程，仅做参考：\n前期准备：明确购房需求、明确预算、确定购房片区、网上了解房价、最终确定购房小区（因为我是要买学区房、兼顾自住，所以我的目标非常明确，也省去了很多带看的时间）\n中介带看：我的带看次数非常少，屈指可数，绝大部分都是微信和电话沟通。因为我买房目标非常明确（锚定了 1 个小区、中间套高层），因此和中介经理沟通时也比较顺畅\n签署居间合同：房子看中了，价格谈好了，就是开始签署居间合同，需要注意的协议内容、补充协议等\n房产中心交易流程（领新产证）：这个流程房产中介经理都非常熟悉，只需要按照他们的指示操作即可，包括网签（付首付、申请按揭贷款）、提前预约过户（签署代缴协议）、提前预约缴税领证（友情提示：缴税可以刷信用卡、且一笔税费金额可分开刷多张卡）\n退税：领证当天可完成办理 拿到新购房产证后，即可办理申请退个税了，因为也在交易中心办理，因此同样的也需要提前 3 天预约（预约在缴税领证当天即可，免得还需要单独跑一趟）。\n下面是《居民一年内换购住房退个人所得税》的官方指南文档，里面列出了退税的材料清单，如果你按照这份指南来准备退税材料的话，那很不幸你可能要白跑一趟了：\n根据我的亲身退税经历，需要的材料清单如下，缺一不可，请务必准备好原件。如果方便的话建议也复印一下，这样就无需在办理时临时复印了，窗口办理也能更省时，当然也可以到拿原件到交易中心去复印（我就是在交易中心由中介经理帮忙临时复印的）。\n卖房相关的材料清单（我前面也重点提到）：\n卖房的网签合同（注意不是居间合同）\n卖房的个税发票：再次建议提取和买家沟通好，买家缴税领证后，就把个税发票及时拿到手保管好\n买房（新购房）相关的材料清单：\n买房的网签合同、新领取的产权证、新购房契税完税证明（注意：下一篇文章，我会介绍住房公积金提取，也需要用到契税完税证明）\n身份材料：夫妻双方身份证、结婚证（注意：若有离婚史，还需要去民政局打印婚史证明材料，在哪个民政局办理就去哪个民政局打印，它们之间信息不互通）、户口本、小孩的出生证明\n银行卡：提供一张产权本人的中资一类借记卡（包括弄清开户行支行名称）\n材料清单原件准备齐全，窗口工作人员检查无误后，收走复印件，填写《居民换购住房个人所得税退税申请表》即可，最终窗口返回一张《退税受理回执》，如下所示：\n在此，我要特别感谢我的购房中介经理王经理（唯宝地产的王和旭经理），退税其实不属于购房交易流程部分，他完全可以不用帮忙办理。王经理不但没有撒手不管，我在办理时忘记带购房的网签合同，王经理帮忙到缴税窗口借了本合同；同时，因材料我均只带了原件，王经理跑了两趟帮忙复印。给王经理点赞 👍🏻\n禅定：总结 恭喜你，在拿到退税窗口的《退税受理回执》后，退税的整个流程就成功办理完成了，按照窗口工作人员提示，在等待半年到一年，资金会打到银行卡上。\n从上面退税流程可以看下，流程的关键是材料齐全，再次强调一下几个重要的材料：\n卖房材料务必保管好，包括卖房网签合同和个税发票，特别是个税发票，务必提前沟通好，买家缴税拿到手\n买房材料务必带齐全，包括交易相关的基本材料、个人身份材料（注意：如果有离婚史，需要去办理的民政局打印婚史材料）、中资一类借记卡\n退税办理好了，接下来就是去办理住房公积金领取手续了，下篇文章在与大家分享。关注本公众号，顺畅办理不白跑。\n我的本博客原地址：https://ntopic.cn/p/2024050501\n","date":"2024-05-05T00:00:00Z","image":"https://ntopic.cn/p/2024050501/00_huf0b22b8b8022cadb1927b450094cc75b_111921_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/2024050501/","title":"上海个人购房退税经历和注意事项（收藏不踩坑）"},{"content":" Python 中有不少的魔法方法，我们在阅读源代码过程中，经常会看到__str__方法和__repr__方法，这 2 个方法有什么联系和区别？它们该如何使用？\n本文源代码：https://gitee.com/obullxl/PythonCS/tree/master/CS-CY2405\n在 Python 中，__str__(self)和__repr__(self) 都是非常重要的魔法方法，它们都用于生成对象的字符串，但它们的设计原则和字符串的使用场景并不完全一致：\n使用场景不同 __str__(self)方法 场景：当对象转换为字符串时，本方法将被调用（如：str(obj)转换为字符串、print(obj)面向用户输出对象等）。\n目的：面向终端用户（如：编程人员、前台展示等），以返回的用户友好的对象的字符串表示内容。该字符串应当易于理解，不一定是对象完整精确描述，它只对终端用户负责。\n异常：类如果没有定义__str__方法时，则 Python 会试图把调用__repr__的字符串结果返回给用户。\n__repr__(self)方法 场景：本方法定义了对象完整的、精确的、机器可读的字符串表示形式（如：repr(obj)方法调用，IDLE、Jupyter Notebook 等交互式环境中直接输入对象名时的输出内容等）。\n目的：面向 Python 内部，是对象合法的表达式字符串，通过该表达式字符串，可通过 eval() 函数执行并重新生成原始对象（即：eval(repr(obj)) == obj应该为True）。\n异常：类如果只定义了__str__而没有定义__repr__方法时，则 Python 会试图调用__str__方法来代替，这通常不是一个好的做法。\n设计原则不同 从上面使用场景分析，这 2 个方法的设计原则如下：\n__str__ 面向终端用户查看阅读，所以应尽可能地清晰和简洁，容易理解。\n__repr__ 面向 Python 内容对象表达，所以应包含足够的信息，并且可以解析为合法的 Python 表达式，且能通过该表达式，通过eval()重新创建对象。\n为了遵循 Python 的明确优于隐晦的原则，建议在自定义类中同时定义__str__和__repr__方法（就像 Java 语言中，同时重写hashCode()和equals()方法一样）。如果确实只需要一个，建议优先实现__repr__方法，因为它提供了创建对象所需的所有信息，且可作为__str__方法的备选方法。\n禅定：__str__和__repr__的应用 如下代码样例：PythonCS定义了__str__和__repr__方法：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 class PythonCS: def __init__(self, name): self.name = name def __str__(self): return \u0026#39;公众号: \u0026#39; + self.name def __repr__(self): return \u0026#34;PythonCS(\u0026#39;{}\u0026#39;)\u0026#34;.format(self.name) object = PythonCS(\u0026#39;Python禅师\u0026#39;) # str调用 print(object) print(str(object)) # repr调用 ex = repr(object) print(ex) # 重新创建对象 object2 = eval(ex) print(object.name == object2.name) 以上 Python 代码执行结果：\n1 2 3 4 公众号: Python禅师 公众号: Python禅师 PythonCS(\u0026#39;Python禅师\u0026#39;) True 我的本博客原地址：https://ntopic.cn/p/2024050402\n","date":"2024-05-04T00:00:00Z","image":"https://ntopic.cn/p/2024050402/00_hua04b28422bbb4c6a12cc5f607adbb1af_476204_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/2024050402/","title":"Python魔法方法__str__和__repr__深入理解和应用"},{"content":" 我走出学校已经 14 年多了，目前除了能粗略阅读英语技术资料之外，像如英语语法等基本功也基本离开 14 年多了。而对于小学四年级的英语，如完型填空和句式转换等基本语法是重中之重了，这些经常难倒了我。但自从有了 AI 工具，我感觉我又回到了学生时代……\n常用的 AI 工具 AI 工具和功能有很多，比如根据文字生成图片、根据文字生成短视频等。我主要是英语基本语法类问答，主要是文字聊天。我常用的有 2 个 AI 工具：\n文心一言：百度出品的 ChatGPT 语言模型工具，免费使用，国内最先推出，也是我的第一个使用的 AI 工具（地址：https://yiyan.baidu.com） 通义千问：阿里云出品的 ChatGPT 语言模型工具，免费使用（地址：https://tongyi.aliyun.com/qianwen）。我有时与文心一言对比一起使用，特别是在文章摘要抽取、文章润色等方面（本文不涉及，后面可发文分享） 网上提到的其他的国内 AI 工具，我没有使用过，读者可以尝试一下，列出几个：讯飞星火、豆包等 AI 工具使用场景 前面提到，我使用 AI 工具源于小学四年级英语作业，下面根据作业内容逐一介绍 AI 工具的使用方法。\n英语句子改写，非常出色 改写内容包括：改写句子为否定句、一般疑问句、一般现在时、现在进行时和对划线部分进行提问。\n题目：Tom speaks Chinese.\n问：Tom speaks Chinese. 改成否定句。\n答：Tom speaks Chinese. 改成否定句是：Tom doesn't speak Chinese.\n问：Tom speaks Chinese. 改成一般疑问句。\n答：Tom speaks Chinese. 改成一般疑问句是：Does Tom speak Chinese?\n问：Tom speaks Chinese. 对Chinese进行提问。\n答：对Tom speaks Chinese中的Chinese进行提问，我们可以问：“Tom说什么语言？”（What language does Tom speak?）\n问：Tom speaks Chinese. 改成现在进行时。\n答：Tom speaks Chinese. 改成现在进行时是：Tom is speaking Chinese.\n题目：The children are playing football.\n问：The children are playing football. 改成否定句。\n答：The children are playing football. 改成否定句是：The children are not playing football. 或者可以缩写为 The children aren't playing football.\n问：The children are playing football. 改成一般疑问句。\n答：The children are playing football. 改成一般疑问句是：Are the children playing football?\n问：The children are playing football. 对playing football进行提问。\n答：对 \u0026quot;The children are playing football.\u0026quot; 中的 \u0026quot;playing football\u0026quot; 进行提问，可以问：“孩子们正在做什么？”（What are the children doing?）\n问：The children are playing football. 改成一般现在时。\n答：\u0026quot;The children are playing football.\u0026quot; 这个句子已经是现在进行时态，要改成一般现在时，我们需要知道主语“the children”是复数，所以动词“play”不需要变形。句子可以改为：The children play football.\n上面的答案，特别是划线提问、现在时改写的注意事项，是不是感觉特别出色？\n英语完型填空（通义千问 比 文心一言 更胜一筹） 问：使用play完型填空：We offen __ in the playground. 1 2 3 4 5 We often __ in the playground. 使用“play”的适当形式填空，这里需要用到一般现在时，并且因为主语“We”是复数，所以动词“play”不需要变化。因此，正确的填空是： We often play in the playground. 问：使用get完型填空：He __ up at six o'clock. 1 2 3 4 5 6 7 He __ up at six o\u0026#39;clock. 使用“get”的适当形式填空，这里描述的是一个人的日常习惯，所以使用一般现在时。主语“He”是第三人称单数，因此“get”需要变成第三人称单数形式“gets”。 正确的填空是： He gets up at six o\u0026#39;clock. 问：使用brush完型填空：__ you __ your teeth every morning? 1 2 3 4 5 6 7 __ you __ your teeth every morning? 使用“brush”的适当形式填空，并且句子是疑问句。首先，这是一个询问日常习惯的问句，所以使用一般现在时。其次，句子需要用到“do”或“does”来帮助构成疑问句，因为“brush”是动词。由于主语“you”是第二人称，所以使用“do”。 正确的填空是： Do you brush your teeth every morning? 问：使用do完型填空：What __ he usually __ after school? 1 2 3 4 5 6 7 What __ he usually __ after school? 使用“do”的适当形式填空，并且句子是询问某人通常放学后做什么的问句。这里需要用到“do”的适当形式来帮助构成疑问句，并且主语“he”是第三人称单数。但在疑问句里，当主语是第三人称单数时，我们通常用“does”而不是“does do”。因此，“does”用来提问，后面接动词原形。 正确的填空是： What does he usually do after school? 问：使用study完型填空：Danny __ English, Chinese, Maths, Science and Art at school. 1 2 3 4 5 6 7 Danny __ English, Chinese, Maths, Science and Art at school. 使用“study”的适当形式填空，这里描述的是Danny在学校学习的科目，所以使用一般现在时。主语“Danny”是第三人称单数，因此“study”需要变成第三人称单数形式“studies”。 正确的填空是： Danny studies English, Chinese, Maths, Science and Art at school. 问：使用go完型填空：Mike sometimes __ to the park with his sister. 1 2 3 4 5 6 7 Mike sometimes __ to the park with his sister. 使用“go”的适当形式填空，描述Mike有时和他的妹妹去公园，所以使用一般现在时。主语“Mike”是第三人称单数，因此“go”需要变成第三人称单数形式“goes”。 正确的填空是： Mike sometimes goes to the park with his sister. 问：使用watch完型填空：Look, she __ TV with his parents. 特别注意：文心一言的回答watches是错误的，通义千问的答案正确：is watching\n问：使用read完型填空：Mike __ English every day? 1 2 3 4 5 6 7 Mike __ English every day? 使用“read”的适当形式填空，描述Mike每天读英语的行为，所以使用一般现在时。主语“Mike”是第三人称单数，因此“read”需要变成第三人称单数形式“reads”。 正确的填空是： Mike reads English every day? （注意，这里句子末尾应该是一个问号，因为这是一个疑问句。） 问：使用have完型填空：How many lessons __ your brother __ on Monday? 1 2 3 4 5 6 7 How many lessons __ your brother __ on Monday? 使用“have”的适当形式填空，描述你兄弟在星期一有多少节课，所以使用一般现在时。主语“your brother”是第三人称单数，因此“have”需要变成第三人称单数形式“has”。 正确的填空是： How many lessons does your brother have on Monday? 问：使用do完型填空：What time __ his mother __ the housework? 1 2 3 4 5 6 7 What time __ his mother __ the housework? 使用“do”的适当形式填空，描述他的母亲什么时间做家务，所以使用一般现在时。主语“his mother”是第三人称单数，因此需要使用“does”来帮助构成疑问句，并且后面接动词原形“do”。 正确的填空是： What time does his mother do the housework? 英语选择填空（四选一，通义千问 比 文心一言 又更胜一筹）） 问：选择填空：__ he do his homework every day? A.Is B.Are C.Do D.Does 1 2 3 选择填空的正确答案是 D. Does。 句子“__ he do his homework every day?” 是一个一般现在时的一般疑问句，询问“他每天都做作业吗？”。因为主语“he”是第三人称单数，所以需要使用助动词“does”来构成疑问句，并将动词“do”变回原形“do”。选项 A（Is）和 B（Are）都是be动词的形式，不适合用来构成这种结构的一般疑问句；选项 C（Do）虽然是一般现在时的助动词，但不适用于第三人称单数主语。因此，正确答案是 D（Does）。 问：选择填空：What __ they doing? A.is B.are C.do D.does 1 2 3 选择填空的正确答案是 B. are。 句子“What __ they doing?”是一个现在进行时的疑问句，询问“他们正在做什么？”。主语“they”是复数，所以需要使用复数形式的be动词“are”来构成现在进行时。选项 A（is）是单数形式的be动词，不适合；选项 C（do）和 D（does）是一般现在时的助动词，不适用于现在进行时。因此，正确答案是 B（are）。 问：选择填空：__ he going to Beijing next Sunday? Yes, he is. A.Is B.Does C.Will D.Do 特别注意：文心一言的回答Will是错误的，通义千问的答案正确：Is\n问：选择填空：He never __ TV at night. A.watch B.watchs C.watching D.watches 1 2 3 4 5 6 7 8 9 句子“He never __ TV at night.”描述的是一个人的日常习惯，所以使用一般现在时。主语“He”是第三人称单数，因此动词“watch”需要变成第三人称单数形式“watches”。 选项分析： A. watch - 动词原形，不适合用于第三人称单数主语。 B. watchs - 这个选项实际上是不正确的，因为“watch”的第三人称单数形式是“watches”，而不是“watchs”。 C. watching - 现在分词形式，用于进行时态，不适合这里。 D. watches - 第三人称单数形式，适合用于主语“He”。 因此，正确答案是 D. watches。 问：选择填空：Look! The children are __ in the sea. A.swiming B.swimming C.swim D.swims 1 2 3 4 5 6 7 8 9 10 11 选择填空的正确答案是 B. swimming。 句子“Look! The children are __ in the sea.”描述的是孩子们正在海里游泳的场景，所以需要使用现在进行时态。现在进行时态的构成是“be动词（am/is/are）+动词-ing形式”。在这里，主语“The children”是复数，所以使用“are”。动词“swim”的现在分词形式是“swimming”。 选项分析： A. swiming - 拼写错误，正确的拼写是“swimming”。 B. swimming - 正确的现在分词形式，符合语境。 C. swim - 动词原形，不适合用于现在进行时。 D. swims - 第三人称单数形式，不适合用于复数主语。 因此，正确答案是 B. swimming。 问：选择填空：How many apples __ he eat every day? A.is B.are B.do D.does 1 2 3 4 5 6 7 8 9 10 11 选择填空的正确答案是 D. does。 句子“How many apples __ he eat every day?”是一个询问数量的疑问句，询问“他每天吃多少个苹果？”。由于主语“he”是第三人称单数，我们需要一个助动词来帮助构成疑问句，并将动词“eat”变为原形。在这里，我们要使用“does”作为第三人称单数的助动词。 选项分析： A. is - be动词的单数形式，不适合在这里使用。 B. are - be动词的复数形式，也不适合。 C. do - 一般现在时的助动词，用于第一人称和第二人称单数及复数，以及第三人称复数。但在这里，主语是第三人称单数，所以不适用。 D. does - 一般现在时的助动词，用于第三人称单数，符合这里的语境。 因此，正确答案是 D. does。 问：选择填空：__ he listening to the music? A.Do B.Does C.Is D.Are 1 2 3 4 5 6 7 8 9 10 11 选择填空的正确答案是 C. Is。 句子“__ he listening to the music?”是一个现在进行时态的疑问句，询问“他正在听音乐吗？”。在现在进行时态的疑问句中，我们需要使用“be动词（am/is/are）+主语+动词-ing形式”的结构。由于主语“he”是第三人称单数，所以应该使用“is”。 选项分析： A. Do - 用于第一人称、第二人称及第三人称复数主语的一般现在时态疑问句中，但不适用于现在进行时态。 B. Does - 用于第三人称单数主语的一般现在时态疑问句中，同样不适用于现在进行时态。 C. Is - 第三人称单数的be动词形式，用于现在进行时态疑问句中，符合这里的语境。 D. Are - 第二人称及第一人称复数、第三人称复数的be动词形式，不适用于这里的主语“he”。 因此，正确答案是 C. Is。 问：选择填空：There __ some pens in the box. A.is B.do C.are D.have 1 2 3 4 5 6 7 8 9 10 11 选择填空的正确答案是 C. are。 句子“There __ some pens in the box.”是一个存在句，描述盒子里有一些钢笔。在存在句中，当主语是可数名词复数时，我们需要使用“be动词”的复数形式“are”。 选项分析： A. is - 用于第三人称单数主语或不可数名词，但这里的主语“pens”是可数名词复数，所以不适用。 B. do - 是助动词，用于构成疑问句或否定句，不适用于存在句。 C. are - 是“be动词”的复数形式，适用于主语为可数名词复数的情况，所以是正确的。 D. have - 是动词，表示“有”，但在存在句中我们通常使用“there be”结构，而不是“have”。 因此，正确答案是 C. are。 问：选择填空：Where __ they always to every Sunday? A.are B.is C.do D.does 1 2 3 4 5 6 7 8 9 10 11 选择填空的正确答案是 C. do。 句子“Where __ they always go every Sunday?”是一个询问他们每周日都去哪里的一般现在时态的疑问句。我们需要一个助动词来帮助构成疑问句，并且动词“go”需要保持原形。 选项分析： A. are - be动词的复数形式，通常用于现在进行时或一般现在时的主语为复数的情况，但在此句中不合适，因为它无法与动词“go”连用构成疑问句。 B. is - be动词的单数形式，同样无法与动词“go”连用构成疑问句。 C. do - 助动词，用于第一人称、第二人称及第三人称复数的疑问句或否定句中，可以与动词“go”连用构成疑问句。 D. does - 助动词的第三人称单数形式，用于第三人称单数的疑问句或否定句中，但在此处主语为“they”（复数），所以不适用。 因此，正确答案是 C. do。完整的句子应该是：“Where do they always go every Sunday?” 问：选择填空：We __ dinner at 6:30 every day. A.eat B.eats C.eating D.eatting 1 2 3 4 5 6 7 8 9 10 11 选择填空的正确答案是 A. eat。 句子“We __ dinner at 6:30 every day.”描述的是“我们每天6:30吃晚饭”这一日常习惯，因此应该使用一般现在时。主语“We”是第一人称复数，所以动词需要使用原形。 选项分析： A. eat - 动词原形，适用于第一人称复数主语的一般现在时，符合语境。 B. eats - 第三人称单数形式，不适用于第一人称复数主语。 C. eating - 现在分词形式，通常用于进行时态，不适用于一般现在时。 D. eatting - 这是错误的拼写，正确的现在分词形式是“eating”，但即使拼写正确，它也不适用于这个句子。 因此，正确答案是 A. eat。 英语：AI 工具其他出色的问答 问：请列出应用中所有的第1人称、第2人称和第3人称 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 当然可以，以下是英语中所有主要的人称代词形式，按照第一人称、第二人称和第三人称进行分类，并包括单数和复数形式以及主格和宾格： ### 第一人称： - 单数： - 主格：I（我） - 宾格：me（我） - 复数： - 主格：we（我们） - 宾格：us（我们） - 物主代词： - 形容词性物主代词：my（我的），our（我们的） - 名词性物主代词：mine（我的东西），ours（我们的东西） ### 第二人称： - 单数： - 主格：you（你） - 宾格：you（你） - 复数： - 主格：you（你们） - 宾格：you（你们） - 物主代词： - 形容词性物主代词：your（你的/你们的） - 名词性物主代词：yours（你的/你们的东西） ### 第三人称： - 单数： - 主格：he（他），she（她），it（它） - 宾格：him（他），her（她），it（它） - 复数： - 主格：they（他们/她们/它们） - 宾格：them（他们/她们/它们） - 物主代词： - 形容词性物主代词：his（他的），her（她的），its（它的） - 名词性物主代词：his（他的东西），hers（她的东西），its（它的东西），theirs（他们的/她们的/它们的东西） 以上涵盖了英语中最基本和常用的人称代词形式。在实际应用中，根据上下文和语法需要选择合适的人称和格的形式。 文生图（通义千问在胜文心一言一筹） 问（通义千问）：请帮忙画一副中国父母辅导10女儿做英语作业的画面 文心一言生成的图片答非所问，不知所云，图片我就不放上来了。\n国内免费 AI 工具总结 我主要使用文心一言和通义千问这 2 个国内免费 AI 工具，从上面的四年级英语作业测试可以看出，他们 2 者在句子改写能力表现一致，结果都符合预期。但：\n在完型填空和选择填空作业方面，通义千问的正确率达到 100%，而文心一言分别出现了 1 次错误（共 2 次错误），通义千问更胜一筹。\n在文生图方面，文心一言生成的图片简直完全不符合预期了，而通义千问至少能理解我的意思，画出的图片也有模有样，因此在胜一筹。\n我的本博客原地址：https://ntopic.cn/p/2024050401\n","date":"2024-05-04T00:00:00Z","image":"https://ntopic.cn/p/2024050401/00_hua04b28422bbb4c6a12cc5f607adbb1af_304554_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/2024050401/","title":"国内免费的AI工具出色地帮我辅导女儿的小学英语作业"},{"content":"本文源代码：https://gitee.com/obullxl/PythonCS/tree/master/CS-CY2405\nPython中*和**很常见 禅师在阅读Python代码过程中，经常看到一个函数或方法的入参是*args和**kwargs（如：def func(*args, **kwargs)等）形式，或者在调用函数或方法时，在入参的元组和列表前面增加1个*号（如：func(*(1, 2, 3))等），或者在字典类型入参前面增加2个*号（如：func(**{'x':1, 'y':2, 'z':3})等）。\n刚开始看到参数前面*号，禅师一度以为是C/C++中的指针，但在万物皆对象的Python种，指针是不会存在的。本文我们一起来看看1个*号和2个**号的含义和用法，也让我们来感受一下Python灵活性有多强大。\n位置参数和命名参数 在了解函数入参*args和**kwargs之前，我们首先要了解一下函数或方法的位置参数和命名参数，以函数定义def func(a, b, c, x=None, y=None, z=None)为例：\n位置参数通俗的讲就是参数的位置的固定的，与函数或方法的参数名无关，比如函数或方法的第1个参数、第2个参数等。样例函数的a/b/c入参就是位置参数，他们分别在函数的第1到第3个参数。Python语法规定，位置参数只能在函数或方法前面的参数（即中间不能有位置参数）。\n命名参数通俗的讲就是参数是有名字，在调用函数或方法时，需要指定参数的名称，命名参数可以有默认值。样例函数的x/y/z入参就是命名参数，x/y/z就是参数的名称，且他们均有None默认值。Python语法规定，位置参数只能在函数或方法后面的参数（即中间不能有位置参数）。\n1个*和2个**的作用 其实从函数入参能看出来，1个*是位置参数，2个**是命名参数，那么它们到底有什么作用呢：\n函数或方法的定义，如def func(*args, **kwargs)：1个*的作用是收集调用本函数的所有的位置参数，收集后的类型是元组；2个**的作用也是参数收集，只是收集的是命名参数，收集后的结果是字典（在Java中称为Map）。\n函数或方法的调用，如func(*(1, 2, 3), **{x=4, y=5, z=6})：*的作用是拆解，与收集相反；其中，1个*为把元组或列表按照顺序拆开，作为位置入参，2个**则把字典按照命名参数拆开。\n例：定义函数或方法（参数收集） 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 def mfunc(*args, **kwargs): \u0026#39;\u0026#39;\u0026#39; 函数定义：收集参数，*把所有的位置参数收集到一个元组中，**则把最后的命名参数组装成Map字典。 \u0026#39;\u0026#39;\u0026#39; print(f\u0026#39;{type(args)} args = {args}\u0026#39;) print(f\u0026#39;{type(kwargs)} kwargs = {kwargs}\u0026#39;) mfunc() mfunc(1, 2, 3, x=\u0026#39;a\u0026#39;, y=\u0026#39;b\u0026#39;) mfunc(1, 2, (\u0026#39;t1\u0026#39;, \u0026#39;t2\u0026#39;), [\u0026#39;t1\u0026#39;, \u0026#39;t2\u0026#39;], {\u0026#39;k1\u0026#39;: \u0026#39;v1\u0026#39;, \u0026#39;k2\u0026#39;: \u0026#39;v2\u0026#39;}, x=\u0026#39;a\u0026#39;, y=\u0026#39;b\u0026#39;) \u0026#39;\u0026#39;\u0026#39; 入参拆解参数，定义收集参数，相互抵消，无实际意义。 \u0026#39;\u0026#39;\u0026#39; mfunc(*(1, 2, 3), **{\u0026#39;k1\u0026#39;: \u0026#39;v1\u0026#39;, \u0026#39;k2\u0026#39;: \u0026#39;v2\u0026#39;}) 例：调用函数或方法（拆解或映射） 1 2 3 4 5 6 7 8 9 10 11 def mprint(x, y): \u0026#39;\u0026#39;\u0026#39; 函数调用：拆解参数，*把元组或列表按照顺序拆开，作为入参，**则把Map字典按照命名参数拆开。 \u0026#39;\u0026#39;\u0026#39; print(f\u0026#39;{type(x)} x = {x}\u0026#39;) print(f\u0026#39;{type(y)} y = {y}\u0026#39;) mprint(*(1, 2)) mprint(*((1, 2), 3)) mprint(**{\u0026#39;x\u0026#39;: \u0026#39;x1\u0026#39;, \u0026#39;y\u0026#39;: \u0026#39;y1\u0026#39;}) 特别注意：拆解位置参数数量需要与函数或方式定义一致，或者拆解命名参数的参数名需要与定义一致！\n禅定：Python为什么需要*和**？ 从前面的分析可以看出*和**的作用和用法，Python中它们是必须的吗？禅师答案：\n函数和方法定义中是必须的：举个反射的简单场景，函数入参的个数、类型是动态的，因此需要收集（如：Java中为Object[]数组） 函数和方法调用是可选的，目的是为了简化编程（Python之禅：Beautifulis better than ugly.），不需要手工逐个参数设置，如调用函数入参func(*my_lsit, **my_dict)就明显比func(my_list[0], my_list[1], my_list[2], my_dict['x'], my_dict['y'], my_dict['z'])优雅太多了。\n我的本博客原地址：https://ntopic.cn/p/2024050201\n","date":"2024-05-02T00:00:00Z","image":"https://ntopic.cn/p/2024050201/00_hu3dfd95962021d2a4cb1da63a3e39075e_612418_120x120_fill_box_smart1_3.png","permalink":"https://ntopic.cn/p/2024050201/","title":"Python深入理解*和**含义和应用"},{"content":"AI 资讯 国资委：加快人工智能等新技术与制造全过程、全要素深度融合 GitHub版 Devin 上线，会打字就能开发应用，微软 CEO：重新定义 IDE 在12个视频理解任务中，Mamba 先打败了 Transformer Sora 会颠覆电影制作吗？3 人团队使用 Sora 制作短片完整技术细节\u0026amp;踩坑分享 小红书让智能体们吵起来了！联合复旦推出大模型专属群聊工具 OpenAI：记忆功能现已对所有ChatGPT Plus用户开放 Phi-3 notebook发布，Phi的调优速度提高了2倍，且VRAM使用减少了50% 使用张量并行技术进行自动驾驶感知模型训练 Meta 宣布在旧金山举办首届 Llama 3 黑客马拉松，与 Cerebral Valley 和 SHACK15sf 合作，提供超过1 0K 美元的现金奖励和合作伙伴积分以启动 AI 项目。这一事件强调了 Meta 在促进 AI 社区创新方面的承诺。 Google 推出了 Transformer 2 模型，该模型将注意力、递归、检索和前馈网络（FFN）集成到单一模块中，与原始 Transformer 的性能相当，但计算效率提高了 20 倍，并能有效处理高达 100M 上下文长度。 国资委：加快人工智能等新技术与制造全过程、全要素深度融合 报道地址\n国务院国资委召开中央企业大规模设备更新工作推进会，深入学习贯彻总书记重要讲话精神和党中央决策部署，落实国务院推动大规模设备更新和消费品以旧换新工作会议精神，对中央企业推进大规模设备更新工作作出部署。\n国务院国资委党委书记、主任张玉卓出席会议并讲话。国家发展改革委党组成员、副主任赵辰昕出席会议并介绍了推动大规模设备更新和消费品以旧换新工作进展和下一步工作考虑。国务院国资委党委委员、副主任苟坪主持会议。\nGitHub版 Devin 上线，会打字就能开发应用，微软CEO：重新定义 IDE 博客地址\n微软的“GitHub 版 Devin” - Copilot WorkSpace，终于上线了！WorkSpace是一种“Copilot 原生”的全新开发环境，目的是让所有开发者都可以用自然语言，把脑海里的创意转化成应用。也就是说，只要有想法，而且会打字，就可以搞软件开发了。\n在12个视频理解任务中，Mamba 先打败了 Transformer 论文链接 GitHub地址\n探索视频理解的新境界，Mamba 模型引领计算机视觉研究新潮流！传统架构的局限已被打破，状态空间模型 Mamba 以其在长序列处理上的独特优势，为视频理解领域带来了革命性的变革。\n来自南京大学、上海人工智能实验室、复旦大学、浙江大学的研究团队发布了一项开创性工作。他们全面审视了 Mamba 在视频建模中的多重角色，提出了针对 14 种模型/模块的 Video Mamba Suite，在 12 项视频理解任务中对其进行了深入评估。\n结果令人振奋：Mamba 在视频专用和视频 - 语言任务中均展现出强劲的潜力，实现了效率与性能的理想平衡。这不仅是技术上的飞跃，更是对未来视频理解研究的有力推动。\nSora 会颠覆电影制作吗？3人团队使用 Sora 制作短片完整技术细节\u0026amp;踩坑分享 原文地址\n今年2月份，OpenAI 发布了人工智能文生视频大模型 Sora，并放出了第一批视频片段，掀起了 AI 生成视频浪潮。目前，Sora 仍未进行公测，只有一些视觉艺术家、设计师、电影制作人等获得了 Sora 的访问权限。他们发布了一些 Sora 生成的视频短片，其连贯、逼真的生成效果令人惊艳。\n最近，被誉为「朋克摇滚皮克斯」的加拿大多媒体制作公司 Shy Kids 发布了一段借助 Sora 制作的视频短片《Air Head》，在社交媒体上迅速引起广泛关注。本周，知名视觉特效总监 Mike Seymour 采访了 Patrick Cederberg，就《Air Head》制作过程、技术难点等信息展开了提问，并在 fxguide 上发布一篇文章介绍了 Sora 在视频实际制作过程中发挥的作用和存在的问题。\n小红书让智能体们吵起来了！联合复旦推出大模型专属群聊工具 原文地址\n语言，不仅仅是文字的堆砌，更是表情包的狂欢，是梗的海洋，是键盘侠的战场。语言如何塑造我们的社会行为？我们的社会结构又是如何在不断的言语交流中演变的？近期，来自复旦大学和小红书的研究者们通过引入一种名为AgentGroupChat的模拟平台，对这些问题进行了深入探讨。\nOpenAI：记忆功能现已对所有 ChatGPT Plus 用户开放 推特地址\n记忆功能现已对所有 ChatGPT Plus 用户开放。使用记忆功能很简单：只需开始新的聊天并告诉 ChatGPT 您希望它记住的任何内容。\n记忆功能可以在设置中开启或关闭，并且目前在欧洲或韩国不可用。团队、企业版和未来的 GPT 产品将陆续推出。\nPhi-3 notebook 发布，Phi 的调优速度提高了 2 倍，且VRAM使用减少了50% 推特地址 模型地址\nPhi-3 notebook 发布了！使用 @UnslothAI，相比于 HF+FA2，Phi 的调优速度提高了 2 倍，且 VRAM 使用减少了 50%！\n由于采用了滑动窗口注意力机制，不得不对其进行 Mistral 化，并修复了 2048/2047 的 SWA（滑动窗口注意力）错误。还解除了注意力机制与 MLP（多层感知机）的融合，因此 QLoRA 损失有所不同，但 16 位的表现相同。\nNVIDIA 与蔚来联合研究：使用张量并行技术进行自动驾驶感知模型训练 原文地址\n文章探讨了 NVIDIA 与蔚来合作，利用张量并行技术优化自动驾驶感知模型的 GPU 内存使用。通过分片处理卷积神经网络（CNN）的输入和中间激活值，将它们均匀分配到多个 GPU 上，显著降低了对单个 GPU 的内存占用。\n此外，文章详述了在 PyTorch 2.0 中使用 DTensor 实现张量并行的细节，包括模型参数和优化器状态的处理方式，以及卷积操作中必要的数据交换和通信。这种方法不仅提高了 GPU 的利用率，还允许使用更深的模型和更高的分辨率输入，从而提升自动驾驶感知模型的精度和训练效率。\n产品投资 「星元AI」获数千万天使轮投资 近日，星元先创网络有限责任公司旗下产品「星元AI」完成了数千万的天使轮融资，此次成功融资为星元先创的发展注入新的活力，本轮资金将重点投向星元先创旗下系统升级和市场推广，为广大服务商提供更多机会。\n对于星元先创而言，意味着有望将领先的数字化营销系统和人工智能领域的成就在现有基础上再上新台阶，增强核心竞争力，迅速扩大新兴市场的市场份额；合作也标志着星元先创进一步强化“人工智能+营销系统”的战略，星元先创将加速产品升级和业务全国布局。\n微软宣布在印尼的云计算和 AI 领域投资 17 亿美元 官方报道\n微软计划在接下来的四年内在印尼投资17亿美元，用于新的云计算和人工智能（AI）基础设施，以及为84万人提供AI技能培训，并支持该国日益增长的开发者社区。这是微软在印尼29年历史中的最大单笔投资。此举旨在帮助实现印尼政府的“2045年黄金印尼愿景”，该愿景旨在将印尼转变为全球经济强国。\n我的本博客原地址：https://ntopic.cn/p/ai20240501\n","date":"2024-05-01T00:00:00Z","image":"https://ntopic.cn/p/ai20240501/20240501-01_huea0d199b913a010c4b2085833d1b487c_2450338_120x120_fill_box_smart1_3.png","permalink":"https://ntopic.cn/p/ai20240501/","title":"[每日AI·0501]GitHub 版 Devin，Transformer的强力挑战者 Mamba，Sora 制作细节与踩坑，OpenAI 记忆功能"},{"content":" 1 2 3 #!/usr/bin/env python import this Python之禅：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 The Zen of Python, by Tim Peters Beautiful is better than ugly. Explicit is better than implicit. Simple is better than complex. Complex is better than complicated. Flat is better than nested. Sparse is better than dense. Readability counts. Special cases aren\u0026#39;t special enough to break the rules. Although practicality beats purity. Errors should never pass silently. Unless explicitly silenced. In the face of ambiguity, refuse the temptation to guess. There should be one-- and preferably only one --obvious way to do it. Although that way may not be obvious at first unless you\u0026#39;re Dutch. Now is better than never. Although never is often better than *right* now. If the implementation is hard to explain, it\u0026#39;s a bad idea. If the implementation is easy to explain, it may be a good idea. Namespaces are one honking great idea -- let\u0026#39;s do more of those! 开宗明义：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 优美胜于丑陋（Python 以编写优美的代码为目标） 明了胜于晦涩（优美的代码应当是明了的，命名规范，风格相似） 简洁胜于复杂（优美的代码应当是简洁的，不要有复杂的内部实现） 复杂胜于凌乱（如果复杂不可避免，那代码间也不能有难懂的关系，要保持接口简洁） 扁平胜于嵌套（优美的代码应当是扁平的，不能有太多的嵌套） 间隔胜于紧凑（优美的代码有适当的间隔，不要奢望一行代码解决问题） 可读性很重要（优美的代码是可读的） 即便假借特例的实用性之名，也不可违背这些规则（这些规则至高无上） 不要包容所有错误，除非你确定需要这样做（精准地捕获异常，不写 except:pass 风格的代码） 当存在多种可能，不要尝试去猜测 而是尽量找一种，最好是唯一一种明显的解决方案（如果不确定，就用穷举法） 虽然这并不容易，因为你不是 Python 之父（这里的 Dutch 是指 Guido ） 做也许好过不做，但不假思索就动手还不如不做（动手之前要细思量） 如果你无法向人描述你的方案，那肯定不是一个好方案；反之亦然（方案测评标准） 命名空间是一种绝妙的理念，我们应当多加利用（倡导与号召） 我的本博客原地址：https://ntopic.cn/p/2024050101\n","date":"2024-05-01T00:00:00Z","image":"https://ntopic.cn/p/2024050101/00_hu3ac2244f57f84ea10383bd11da3dd654_192532_120x120_fill_box_smart1_3.png","permalink":"https://ntopic.cn/p/2024050101/","title":"Python之禅，开宗明义：import this"},{"content":"AI 资讯 AGI 加速降临！人大系多模态模型首次实现自主更新，写真视频生成力压 Sora 支付宝推出的智能助理你用了吗？ 文本一键转 3D 数字人骨骼动画，阿尔伯塔大学提出 MoMask 框架 IDM-VTON：一种创新的虚拟试衣模型 亚马逊研究奖获奖名单出炉：谢赛宁、Mamba 作者 Albert Gu 等入选 马斯克周末突然访华！为世界最大 AI 项目 —— 特斯拉 FSD 以及 Robotaxi 业务做好准备 奥特曼：学会用 30 秒说出大多数人需要 5 分钟才能表达的内容，是一个重大的突破，基本上就是大语言模型的工作原理 本地在 H100 GPU 上运行 VSCode Llama3 Copilot AdvPrompter: 快速自适应生成 LLM 对抗提示词方法 ScrapeGraphAI：爬虫库，使用 LLM 自动爬取 欧盟委员会投资 1.12 亿欧元用于 AI 和量子研究及创新 AGI加速降临！人大系多模态模型首次实现自主更新，写真视频生成力压 Sora 原文地址\nAGI（通用人工智能）是整个 AI 行业的圣杯。AGI 并非终点，而是人类发展史一个新的起点。在通往 AGI 的路上要考虑的事情还有很多，而中国的 AI 行业也是不可忽视的一股力量。\n在4月27日召开的中关村论坛通用人工智能平行论坛上，人大系初创公司智子引擎隆重发布全新的多模态大模型 Awaker 1.0，向 AGI 迈出至关重要的一步。\n相对于智子引擎前代的 ChatImg 序列模型，Awaker 1.0 采用全新的 MOE 架构并具备自主更新能力，是业界首个实现「真正」自主更新的多模态大模型。在视觉生成方面，Awaker 1.0 采用完全自研的视频生成底座 VDT，在写真视频生成上取得好于 Sora 的效果，打破大模型「最后一公里」落地难的困境。\n亚马逊研究奖获奖名单出炉：谢赛宁、Mamba 作者 Albert Gu 等入选 完整名单\n近日，亚马逊研究奖（Amazon Research Awards，ARA）获奖名单公布，今年共有 98 名获得者，来自 15 个国家 51 所大学。亚马逊研究奖 (ARA) 成立于 2015 年，该计划旨在为多个学科研究主题的学术研究人员提供不受限制的资金。获奖者可以公开访问 300 多个亚马逊公共数据集，并可以使用亚马逊的 AI/ML 服务和工具。除此以外，获奖者还将与亚马逊专家建立联系，以获得咨询和建议，还可以参加亚马逊举办的活动、培训课程等。\n获奖者来自以下研究领域：人工智能信息安全、自动推理、AWS 人工智能、AWS 加密和隐私、AWS 数据库服务、可持续发展。值得注意的是，今年的获奖名单中出现了很多华人学者。\n文本一键转 3D 数字人骨骼动画，阿尔伯塔大学提出 MoMask 框架 论文链接 GitHub 地址\n想象一下，你仅需要输入一段简单的文本描述，就可以生成对应的 3D 数字人动画的骨骼动作。而以往，这通常需要昂贵的动作捕捉设备或是专业的动画师逐帧绘制。这些骨骼动作可以进一步的用于游戏开发，影视制作，或者虚拟现实应用。来自阿尔伯塔大学的研究团队提出的新一代 Text2Motion 框架，MoMask，正在让这一切变得可能。\n马斯克周末突然访华！为世界最大 AI 项目 —— 特斯拉 FSD 以及 Robotaxi 业务做好准备 原文地址\n4 月 28 日下午，应中国贸促会邀请，马斯克的私人飞机湾流 G550 飞机抵达北京，并会见了中国总理李强及其他高级官员，包括中国国际贸易促进委员会主任，随行的还包括特斯拉高管朱晓彤。马斯克上次访华是在 2023 年 5 月。在他 44 小时的访问期间，先后与中国外交部、工业和信息化部、商务部、中国国际贸易促进委员会的相关领导会面，还在深夜前往上海特斯拉超级工厂，随后与上海市的相关领导见面，这次是他时隔 11 个月后再次来到中国。\n本地在 H100 GPU 上运行 VSCode Llama3 Copilot 推特地址\n在 VSCode 中引入 Llama3 作为 Copilot，现在使用 @NVIDIAAI 的 TensorRT 和 Triton 推理服务器在本地 GPU 上运行。 在本地和私下运行自己的 Copilot 已经非常棒了，但更令人瞩目的是能够在 H100 GPU 上运行它。现在，通过 CodeGPT 中的“自定义”连接，这种集成已经成为可能。在我们团队撰写的这篇文章中，我们将逐步指导您执行和部署模型，然后将其与 VSCode 连接起来。\n链接：https://medium.com/@vokturz/notes-about-running-a-chat-completion-api-endpoint-with-tensorrt-llm-and-meta-llama-3-8b-instruct-d2ec62a64b9c\n软件开发的未来将看到 AI 代理直接在您的计算机上编写和执行代码……所有这些都由 GPU 的计算能力加速。\n奥特曼：学会用 30 秒说出大多数人需要 5 分钟才能表达的内容，是一个重大的突破，基本上就是大语言模型的工作原理 推特原文\n论文与开源 AdvPrompter: 快速自适应生成 LLM 对抗提示词方法 论文地址\n摘要：最近，大语言模型（LLMs）取得了显著成就，但它们容易受到某些越狱攻击的影响，导致生成不当或有害内容。手动红组测试需要找到导致越狱的敌对提示，例如在给定指令后附加后缀，效率低且耗时。在另一方面，自动敌对提示生成通常导致语义无意义的攻击，可以轻松被感知度过滤器检测到，可能需要来自TargetLLM的梯度信息，或由于耗时的离散优化过程而无法很好地扩展。\n在本文中，我们提出了一种新方法，使用另一个LLM，称为AdvPrompter，仅需几秒钟生成人类可读的敌对提示，比现有基于优化的方法快约800倍。我们使用一种无需访问目标LLM梯度的新算法对AdvPrompter进行训练。此过程交替进行两个步骤：\n（1）通过优化AdvPrompter预测生成高质量的目标敌对后缀，以及\n（2）使用生成的敌对后缀对AdvPrompter进行低秩微调。经过训练的AdvPrompter生成的后缀掩盖了输入指令而不改变其含义，因此TargetLLM被诱导提供有害响应。对流行的开源TargetLLMs的实验结果显示AdvBench数据集上的最新成果，也可转移到闭源的黑盒LLM API。\n此外，我们证明通过对AdvPrompter生成的合成数据集进行微调，可以使LLMs更加抵御越狱攻击而保持性能，即高MMLU得分。\nIDM-VTON：一种创新的虚拟试衣模型 GitHub地址 Hugging Face\n它通过融合服装图像的高低层语义特征以及提供详细的文本提示，生成比之前的方法更加自然和真实的虚拟试衣图像。此外，它还提出了一种基于用户输入的定制方法，进一步提高了生成图像的保真度和真实感，在保留服装细节方面也有出色的表现。\nScrapeGraphAI：爬虫库，使用 LLM 自动爬取 推特原文 GitHub地址\nScrapeGraphAI 是一个网络爬虫 Python 库，它使用大型语言模型(LLM)为网站、文档和 XML 文件创建爬取流水线。只需说明你想提取哪些信息，该库就会为你完成!\n投资 欧盟委员会投资 1.12 亿欧元用于 AI 和量子研究及创新 欧盟委员会宣布将投资 1.12 亿欧元，支持人工智能（AI）和量子技术的研究和创新。这笔资金将通过“地平线欧洲”2023-2024年度的数字、工业和空间工作计划来调配。其中，6500 万欧元将用于AI领域，包括 5000 万欧元专门用于开发新的数据组合方式和扩大大型AI模型的能力，1500 万欧元将投资于开发健壮且透明的 AI 系统。另外，4000万欧元将用于推动尖端量子技术的研究，包括创建全欧洲量子重力仪网络的 2500 万欧元投资，以及 1500 万欧元用于跨国量子技术的下一代研发项目。此外，还有 750 万欧元将用于支持欧洲价值观和加强欧盟在全球ICT标准化中的影响力的项目。\n我的本博客原地址：https://ntopic.cn/p/ai20240430\n","date":"2024-04-30T00:00:00Z","image":"https://ntopic.cn/p/ai20240430/20240430-01_hu14f3ba29a88a821e8bac76c653652693_1759688_120x120_fill_box_smart1_3.png","permalink":"https://ntopic.cn/p/ai20240430/","title":"[每日AI·0430]首个自主更新的多模态大模型，马斯克访华，文本一键转3D数字人骨骼动画，创新的虚拟试衣模型"},{"content":"特别说明：以下仅仅是笔者在通过查阅网络资料后的个人理解和整理，可能无法保证完整性和正确性，仅供参考！\n史蒂芬·沃尔弗拉姆 《这就是ChatGPT》 计算不可约性（Computational irreducibility） 字面理解：“计算”即计算系统，应用程序等。“约”即约束、规定、明确等。整体可理解为：我们给定一个输入，无法明确一个计算系统的输出。\n初步理解感觉很矛盾：程序不是我们人设计的吗，程序不就是一个“输入+函数=输出”吗，我们怎么就不能明确它的输出呢？\n在这里，其实还有个额外信息，那就是这个“计算”是指“复杂的计算系统”，而不是我们为某个产品功能而设计的应用程序。\n那什么又是复杂的计算系统呢？这里就引入了人工智能相关的计算系统或模型了，如ChatGPT机器人的计算系统：我们输入一个问题，经过ChatGPT后端计算系统给出一段文本；如果我们把相同的问题再次输入，ChatGPT输出的文本可能和前面一次不一样了；又或者第二天输入相同问题，得到的文本可能又不一样……\n百度的文心一言：https://yiyan.baidu.com\n阿里的通义千问：https://tongyi.aliyun.com\n我们可能会好奇，就算ChatGPT、文心一言或通义千问的后端计算系统很复杂，但它们毕竟还是我们人设计的，输出怎么就无法明确了呢？从理论上讲，是可以明确的，但实际上，我们很难做到。比如ChatGPT 3.5的计算模型据说有1700多亿个参数，这些参数相互作用相互影响，调整任何一个参数值都有可能影响最终输出；同时，在应用中，通过反馈和自学习，这些参数还在不断的调整，它们本身就一直在自变化中，在如此多的不断变化的参数的作用下，我们怎么能追踪每个参数和它们之间联系和影响，然后确定一个输出呢？\n因此，最终的输出结果实际上无法明确，这就叫计算不可约性。\n计算等价性原理（The Principle of Computational Equivalence） 这里的“计算”和上面一样，也是复杂的计算系统；“等价”即它们都足够复杂，复杂度是等价的，并无高低之分。\n比如ChatGPT、文心一言或通义千问等，它们后端的计算模型都足够复杂，虽然计算模型的参数在数量级上可能有所区别，但它们一直在演进，又怎么能说谁高谁低呢？\n因此，复杂计算系统之间，它们的复杂度都是等价的，不分高低。\n我的本博客原地址：https://ntopic.cn/p/2023120101\n","date":"2023-12-01T00:00:00Z","image":"https://ntopic.cn/p/2023120101/00_hu1dc2816497253c15946b14b4737ac78f_72063_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/2023120101/","title":"AI人工智能·史蒂芬·沃尔弗拉姆的两个思想"},{"content":"Flutter安装文档：\n官方文档：https://docs.flutter.dev/get-started/install 中文文档：https://flutter.cn/docs/get-started/install Dart升级 Dart安装参考前面文章（Dart安装和初体验）：https://ntopic.cn/p/2023092301 后续学习我们采用最新Flutter版本，因此建议升级Dart最新版本（当前：3.2.0）：brew upgrade dart 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 $ brew info dart ==\u0026gt; dart-lang/dart/dart: stable 3.2.0, HEAD SDK https://dart.dev Conflicts with: dart-beta (because dart-beta ships the same binaries) /usr/local/Cellar/dart/3.2.0 (1,022 files, 560.8MB) * Built from source on 2023-11-19 at 10:04:05 From: https://github.com/dart-lang/homebrew-dart/blob/HEAD/Formula/dart.rb ==\u0026gt; Options --HEAD Install HEAD version ==\u0026gt; Caveats Please note the path to the Dart SDK: /usr/local/opt/dart/libexec VS Code安装 VS Code是免费的，支持Flutter的研发、调试和运行，没有理由不使用它作为Flutter的研发IDE（IntelliJ IDEA非常强大，可惜是收费）。\n下载和安装VS Code下载：\n首先通过官网下载VS Code：https://code.visualstudio.com/docs/?dv=win 点击下载之后，发现下载速度很慢：右键复制下载链接，然后把域名换成 “vscode.cdn.azure.cn”，重新通过浏览器打开进行下载，速度杠杠的 VS Code安装成功之后，就进行Flutter安装和配置。\nFlutter SDK安装 我们可以从GitHub原始仓库和Gitee镜像仓库下载，由于中国访问GitHub网速太慢，建议从Gitee镜像仓库下载：\n1 2 3 4 5 6 7 8 # 源代码目录 [/Users/obullxl/FlutterSpace] $ cd /Users/obullxl/FlutterSpace # Gitee下载 [推荐方式] $ git clone -b stable https://gitee.com/mirrors/Flutter.git flutter # GitHub下载 [网速可能较慢] $ git clone -b stable https://github.com/flutter/flutter.git flutter Flutter SDK有多个版本，如beta、master、stable等，其中beta/master为开发分支，stable为稳定分支，建议使用稳定分支：\n查看所有分支：flutter channel 切换本地分支：flutter channel xxx 查看当前版本：flutter --version 升级最新版本：flutter upgrade 检测Flutter配置：flutter doctor 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 # 切换到Flutter源代码目录 $ cd /Users/obullxl/FlutterSpace/flutter # 查看Flutter所有分支 [建议stable分支] $ flutter channel Flutter channels: master main beta * stable # 切换到stable分支 $ flutter channel stable # 查看当前版本，由于我的Flutter是之前安装的版本[3.7.1] $ flutter --version Flutter 3.7.1 • channel stable • https://gitee.com/mirrors/Flutter.git Framework • revision 7048ed95a5 (10 months ago) • 2023-02-01 09:07:31 -0800 Engine • revision 800594f1f4 Tools • Dart 2.19.1 • DevTools 2.20.1 # 升级Flutter版本 $ flutter upgrade # 再次查看Flutter版本，已经是最终版本[3.16.0] $ flutter --version Flutter 3.16.0 • channel stable • https://gitee.com/mirrors/Flutter.git Framework • revision db7ef5bf9f (3 days ago) • 2023-11-15 11:25:44 -0800 Engine • revision 74d16627b9 Tools • Dart 3.2.0 • DevTools 2.28.2 # 检测Flutter配置情况 $ flutter doctor Doctor summary (to see all details, run flutter doctor -v): [!] Flutter (Channel stable, 3.16.0, on macOS 12.6.7 21G651 darwin-x64, locale zh-Hans-CN) ! Warning: `dart` on your path resolves to /usr/local/Cellar/dart/3.1.2/libexec/bin/dart, which is not inside your current Flutter SDK checkout at /Users/obullxl/FlutterSpace/flutter. Consider adding /Users/obullxl/FlutterSpace/flutter/bin to the front of your path. [!] Android toolchain - develop for Android devices (Android SDK version 33.0.1) ✗ cmdline-tools component is missing Run `path/to/sdkmanager --install \u0026#34;cmdline-tools;latest\u0026#34;` See https://developer.android.com/studio/command-line for more details. ✗ Android license status unknown. Run `flutter doctor --android-licenses` to accept the SDK licenses. See https://flutter.dev/docs/get-started/install/macos#android-setup for more details. [✓] Xcode - develop for iOS and macOS (Xcode 14.2) [✓] Chrome - develop for the web [✓] Android Studio (version 2021.3) [✓] IntelliJ IDEA Ultimate Edition (version 2021.1.3) [✓] VS Code (version 1.84.0) [✓] Connected device (2 available) [✓] Network resources ! Doctor found issues in 2 categories. Flutter 仓库配置 和Java编程需要配置Maven国内仓库一样，Flutter也需要配置国内仓库，否则访问国外仓库要么访问不了，要么网速太慢。\n在环境变量中，增加如下配置：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # 建软连接 [便于后面统一管理] $ cd /opt $ sudo ln -s /Users/obullxl/FlutterSpace/flutter ./flutter # 系统变量 $ sudo vi /etc/profile # 增加内容 export PUB_HOSTED_URL=https://pub.flutter-io.cn export FLUTTER_STORAGE_BASE_URL=https://storage.flutter-io.cn export FLUTTER_GIT_URL=https://gitee.com/mirrors/Flutter.git export PATH=$PATH:/opt/flutter/bin # 生效环境变量 $ source /etc/profile 最后 至此，Flutter安装和配置已经完成了，接下来我们就通过VS Code编写Flutter App了！\n我的本博客原地址：https://ntopic.cn/p/2023112001\n","date":"2023-11-19T00:00:00Z","image":"https://ntopic.cn/p/2023112001/00_huc7364261a6c3c41a27a56f1135847b5b_18512_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/2023112001/","title":"FlutterApp实战·第01天：Flutter安装和配置"},{"content":"Dart官方文档：https://dart.dev/language/async\n重要说明：本博客基于Dart官网文档，但并不是简单的对官网进行翻译，在覆盖核心功能情况下，我会根据个人研发经验，加入自己的一些扩展问题和场景验证。\nFuture处理 我们有2种方式编写Future异步代码：\n使用async和wait关键字 使用Future API（https://dart.dev/guides/libraries/library-tour#future） 推荐使用async和wait关键字，让异步代码看起来和同步代码一样。\n如下代码样例：wait关键字等待异步函数返回结果，它必须在async函数中。\n1 2 3 4 5 6 Future\u0026lt;void\u0026gt; checkVersion() async { var version = await lookUpVersion(); var exitCode = await findExitCode(); // ...... await flushThenExit(exitCode); } 可以用try、catch和finally关键字处理错误和清理代码：\n1 2 3 4 5 6 7 try { version = await lookUpVersion(); } catch (e) { // React to inability to look up the version } finally { // Clean code } 申明异步函数 使用async修饰一个函数体，这个函数就是异步函数，它的返回结果是一个Future\u0026lt;T\u0026gt;，当函数无需返回结果时，返回结果为Future\u0026lt;void\u0026gt;：\n1 2 3 4 5 6 7 8 Future\u0026lt;String\u0026gt; lookUpVersion async { // ...... return \u0026#39;1.0.1\u0026#39;; } Future\u0026lt;void\u0026gt; doSomething() { // ...... } Stream处理 同样的，我们也有2种方式编写Stream代码：\n在异步循环使用async异步关键字（await for） 使用Stream API（https://dart.dev/guides/libraries/library-tour#stream） 推荐使用async关键字，异步循环的形式如下：\n1 2 3 await for (varOrType identifier in expression) { // ...... } 在上诉异步循环形式中，expression的值为一个Stream类型，异步循环的执行过程如下：\n等待Stream产出一个值（即触发循环执行） 设置identifier变量值，执行循环体逻辑 重复上面2步，直到Stream关闭（即循环结束） 我们可以通过break或者return语句，退出循环，从而中断监听Stream生产的值。\n同样的，异步循环的函数，必须使用async关键字修饰。如下所示，main()函数体使用了await for异步循环，则函数必须使用async修饰：\n1 2 3 4 5 6 7 void main() async { // ...... await for (final request in requestServer) { handleRequest(request); } // ...... } 最后 第4课对异步编程文档（Future和async/await）：https://ntopic.cn/p/2023092901 其他更多关于异步编程信息，请求参考异步包dart:async（https://dart.dev/guides/libraries/library-tour#dartasync\u0026mdash;asynchronous-programming） 我的本博客原地址：https://ntopic.cn/p/2023111801\n","date":"2023-11-18T00:00:00Z","image":"https://ntopic.cn/p/2023111801/00_hua251d1ea2589f24e4f220f2de00fe773_52076_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/2023111801/","title":"Flutter/Dart第21天：Dart异步编程（Future/Stream）"},{"content":"Dart官方文档：https://dart.dev/language/class-modifiers\n重要说明：本博客基于Dart官网文档，但并不是简单的对官网进行翻译，在覆盖核心功能情况下，我会根据个人研发经验，加入自己的一些扩展问题和场景验证。\n类型修饰符主要是控制类或者Mixin如何被使用，包括在库内部和外部使用。修饰符关键字出现在类型或Mixin申明的前面，如abstract class通过abstract修饰符定义了一个抽象类。\n可用于声明类的修饰符关键字列表如下：\nabstract base final interface sealed mixin 约束：上面的修饰符列表，只有base能用于Mixin类型；同时，上诉修饰符不能用于包括enum、typedef和extension等类型声明。\n实战：当我们决定要使用修饰符时，可能需要考虑一下类的预期用途和类需要提供哪些行为。\n无修饰符 当我们定义类或者Mixin时，不希望对构造函数或者子类进行限制时，我们可以不使用修饰符。\n当类或者Mixin没有修饰符时，默认情况下，可以对这些类或者Mixin进行以下操作：\n通过构造函数创建类实例 通过继承类来创建子类 实现类或者Mixin的接口 混入Mixin或者Mixin类 abstract修饰符（抽象类） 使用场景：当我们定义了一个类（即：抽象类），但又没有完整地实现了它所有的接口时使用（和Java语言一样），请使用abstract修饰符。\n约束：抽象类不能被实例化；一般情况，抽象类都包含抽象方法。\n1 2 3 4 5 6 7 8 9 10 11 12 // 抽象类 abstract class Vehicle { void moveForward(int meters); } // 实现类 class MockVehicle implements Vehicle { @override void moveForward(int meters) { // ... } } base修饰符（基类） 使用场景：当我们用base修饰符定义了一个类或者Mixin时（即：基类），那么这个基类的实现只能基类所在库内。这样做的目的：\n每当创建子类实例时，基类的构造函数被调用 所有已经实现的私有成员都在子类中 在基类中新增加的成员会被所有子类继承（除非：子类中申明了同名的成员但并不兼容的签名。如：子类申明了同名方法，但是方法入参或者返回结果与基类不兼容） 实战：为了保证基类不会被破坏，子类必须使用base，final或者sealed修饰符。\n如下代码样例，基类可以实例化、被继承，但是不能被实现：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 // 基类 base class Vehicle { void moveForward(int meters) { // ... } } // 1. 实例化 Vehicle myVehicle = Vehicle(); // 2. 被继承 base class Car extends Vehicle { int passengers = 4; // ... } // 3. ERROR：不能被实现 base class MockVehicle implements Vehicle { @override void moveForward() { // ... } } interface修饰符（接口类） 使用场景：使用interface修饰符定义一个接口。接口可以被外部库实现，但是不能被继承。这样做的目的：\n当类的一个实例方法使用this调用另一个实例方法时，它总是调用同一个库的实例方法 为了避免不可预期的方法调用，其他库不能重新接口已有的方法 如下代码样例，接口类可以实例化、被实现，但是不能被继承：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 // a.dart 接口类 interface class Vehicle { void moveForward(int meters) { // ... } } // // b.dart // import \u0026#39;a.dart\u0026#39;; // 1. 实例化 Vehicle myVehicle = Vehicle(); // 2. 被实现 class MockVehicle implements Vehicle { @override void moveForward(int meters) { // ... } } // 3. ERROR: 不能被继承 class Car extends Vehicle { int passengers = 4; // ... } abstrace interface（抽象接口类） 一般情况下，我们使用interface来定义纯粹接口。\n当我们使用abstract interface class组合修饰符时，可以定义一个抽象接口类：它即有接口类的功能（可被实现，但不能被继承），也有抽象类的功能（有抽象成员）。\nfinal修饰符（不可变类） 使用场景：当使用final修饰符时，表示该类不能被其他库继承和实现（和Java还有点不一样）。这样做的目的：\n可以安全地进行API变更 该类不会被第三方子类覆盖，因此可以放心调用实例方法 约束：final不可变类可以在本库中被继承和实现，final修饰符包含了base修饰符特性，因此，子类必须使用base，final或者sealed修饰符。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 // a.dart 接口类 final class Vehicle { void moveForward(int meters) { // ... } } // // b.dart // import \u0026#39;a.dart\u0026#39;; // 1. 实例化 Vehicle myVehicle = Vehicle(); // 2. ERROR: 不能被继承 class Car extends Vehicle { int passengers = 4; // ... } class MockVehicle implements Vehicle { // 3. ERROR: 不能被实现 @override void moveForward(int meters) { // ... } } sealed修饰符（密封类） 使用场景：当我们定义了一个类（即：密封类），且明确该类的所有子类集合时，请使用sealed修饰符。这允许我们通过switch穷举所有的子类型。\n约束：sealed修饰的类，禁止被其他库继承或者实现，它隐含abstract修饰符：\n不能被实例化 可以有工厂构造函数 可以定义构造函数，子类可直接使用 子类并不是abstract抽象类 编译器可以知道所有sealed修饰符类的子类（因为他们在同一个库中），这样在switch中，如未穷举，编译器能发出错误警告！\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 // 密封类 sealed class Vehicle {} class Car extends Vehicle {} class Truck implements Vehicle {} class Bicycle extends Vehicle {} // 1. ERROR: 不能被实例化 Vehicle myVehicle = Vehicle(); // 2. 子类可以被实例化 Vehicle myCar = Car(); String getVehicleSound(Vehicle vehicle) { // 3. ERROR: switch中子类未穷举（还有Bicycle子类） return switch (vehicle) { Car() =\u0026gt; \u0026#39;vroom\u0026#39;, Truck() =\u0026gt; \u0026#39;VROOOOMM\u0026#39;, }; } 在switch中，如果我们不想穷举sealed类的子类；又或者以后还会增加子类，但又不想破坏API设计，我也可以使用final修饰符。关于final和sealed修饰符的深入比较，请稍等本博客的下一个博客介绍（请容许我卖个关子！）。\n组合修饰符 通过组合修饰符，可以起到叠加限制效果。我们申明类时，按照顺序，可以叠加的修饰符：\n可选的abstract修饰符：类包含抽象成员，且不能被实例化 可选的base、interface、final和sealed修饰符：限制其他库的子类型 可选的mixin修饰符：类是否可被混入 必选的class类关键字 部分修饰符是不能组合使用，因为他们可能多余或者矛盾互斥：\nabstract修饰符和sealed修饰符：原因是sealed隐含了abstract修饰符 interface、final或sealed修饰符和mixin修饰符：原因是这些修饰符都禁止被混入 完整的有效的修饰符组合列表如下：\n我的本博客原地址：https://ntopic.cn/p/2023110501\n","date":"2023-11-05T00:00:00Z","image":"https://ntopic.cn/p/2023110501/00_hua251d1ea2589f24e4f220f2de00fe773_52076_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/2023110501/","title":"Flutter/Dart第20天：Dart 3.0新特性之类型修饰符"},{"content":"Dart官方文档：https://dart.dev/language/extension-methods\n重要说明：本博客基于Dart官网文档，但并不是简单的对官网进行翻译，在覆盖核心功能情况下，我会根据个人研发经验，加入自己的一些扩展问题和场景验证。\n扩展方法概述 当我们使用了一些被广泛使用的其他库或者自己的库时，我们不太可能去修改这个库API，但是我们又想给库增加一些方法，该怎么办？如：我们想给String类增加一些我自己常用的方法。\nDart作为一门集百家之长的编程语言，也考虑到了这个需求点，它提供了一个扩展方法（Extension methods）来解决问题问题。\n如下代码样例，String类型转换int数字类型，常规的做法如下：\n1 int.parse(\u0026#39;123\u0026#39;); 那如果String类型提供一个转为int数字类型的方法，是不是更好：\n1 \u0026#39;123\u0026#39;.parseInt() 想要实现上诉目的，通过扩展String类型，提供对应方法即可：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 import \u0026#39;./19-ntopic-string-apis.dart\u0026#39;; void main() { print(\u0026#39;123\u0026#39;.parseInt()); // 结果：123 } // // 19-ntopic-string-apis.dart 内容 // extension NumberParsing on String { int parseInt() { return int.parse(this); } } 使用扩展方法 上一章节的最后，其实我们已经展示了如何定义和使用扩展方法。使用扩展方法，和使用类型的其他方法没有任何差异。\n接下来我们来看看，扩展方法在静态类型和动态类型的使用，和如何解决同名扩展方法冲突。\n静态类型和dynamic动态类型 特别注意：dynamic动态类型禁止使用扩展方法！如下代码样例，会抛出NoSuchMethodError运行时异常。\n1 2 3 4 5 6 7 8 9 10 import \u0026#39;./19-ntopic-string-apis.dart\u0026#39;; void main() { print(\u0026#39;123\u0026#39;.parseInt()); // 结果：123 dynamic d = \u0026#39;2\u0026#39;; // NoSuchMethodError: Class \u0026#39;String\u0026#39; has no instance method \u0026#39;parseInt\u0026#39;. print(d.parseInt()); } 但是扩展方法可用于类型推导上，如下代码无任何问题，因为变量v的类型推导成String类型：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 import \u0026#39;./19-ntopic-string-apis.dart\u0026#39;; void main() { print(\u0026#39;123\u0026#39;.parseInt()); // 结果：123 var v = \u0026#39;2\u0026#39;; print(v.parseInt()); // 结果：2 dynamic d = \u0026#39;2\u0026#39;; // NoSuchMethodError: Class \u0026#39;String\u0026#39; has no instance method \u0026#39;parseInt\u0026#39;. print(d.parseInt()); } dynamic动态类型不可用户扩展方法的原因是，扩展方法只能接收静态类型，因此调用扩展方法和调用静态方法一样高效。\n扩展方法冲突 我们在应用中，会引入多个库，如果有多个库都对同类型增加了同名的拓展方法，那么就导出扩展方法冲突了。如：对String类型，库A和库B都有pareInt()扩展方法，那么这个扩展方法就存在冲突。\n一般情况下，有3种方法来解决扩展方法的冲突：\n第一种方法，在引入库时，通过show或者hide关键字限制扩展方法：\n1 2 3 4 5 6 7 8 9 // String扩展方法：parseInt() import \u0026#39;string_apis.dart\u0026#39;; // String扩展方法：parseInt(), `hide`隐藏扩展类型 import \u0026#39;string_apis_2.dart\u0026#39; hide NumberParsing2; // ··· // 使用了 \u0026#39;string_apis.dart\u0026#39; 中定义的扩展方法：parseInt() print(\u0026#39;42\u0026#39;.parseInt()); 第二种方法，显示指定扩展类型的扩展方法：\n1 2 3 4 5 6 7 8 9 // 扩展类型：NumberParsing，扩展方法：parseInt() import \u0026#39;string_apis.dart\u0026#39;; // 扩展类型：NumberParsing2，扩展方法：parseInt() import \u0026#39;string_apis_2.dart\u0026#39;; // 显示使用扩展类型 print(NumberParsing(\u0026#39;42\u0026#39;).parseInt()); print(NumberParsing2(\u0026#39;42\u0026#39;).parseInt()); 第三种方法，假设第二种方法的扩展类型也一样，那么可在引入库增加前缀解决：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 // 扩展类型：NumberParsing，扩展方法：parseInt() import \u0026#39;string_apis.dart\u0026#39;; // 扩展类型：NumberParsing，扩展方法：parseInt()，parseNum() import \u0026#39;string_apis_3.dart\u0026#39; as rad; // \u0026#39;string_apis.dart\u0026#39; 扩展方法：parseInt() print(NumberParsing(\u0026#39;42\u0026#39;).parseInt()); // \u0026#39;string_apis_3.dart\u0026#39; 扩展方法：parseInt() print(rad.NumberParsing(\u0026#39;42\u0026#39;).parseInt()); // \u0026#39;string_apis_3.dart\u0026#39; 扩展方法：parseNum() print(\u0026#39;42\u0026#39;.parseNum()); 因为parseNum()扩展方法不存在冲突，因此可直接使用。仅当存在扩展类型冲突时，才需要增加前缀。\n实现扩展方法 在前面2个章节，其实已经提到了部分扩展方法的实现方法。扩展方法实现语法如下（扩展类型名是可选的）：\n1 2 3 extension \u0026lt;extension name\u0026gt;? on \u0026lt;type\u0026gt; { (\u0026lt;member definition\u0026gt;)* } 如下代码样例，对String类型，增加了2个扩展方法（扩展类型名：NumberParsing）：\n1 2 3 4 5 6 7 8 9 extension NumberParsing on String { int parseInt() { return int.parse(this); } double parseDouble() { return double.parse(this); } } 扩展类型中的成员，可以是方法、Getters、Setters和操作符，同时也可以是静态属性和静态方法，外围可通用普通类型静态属性和静态方法一样使用。\n未命名的扩展类型 我们定义未命名的扩展，它们的可见范围仅在库内容（类似于私有属性和方法）；由于扩展类型未命名，因此无法明确的用于冲突解决，它们的静态属性和静态方法，也只能在扩展内部使用：\n1 2 3 extension on String { bool get isBlank =\u0026gt; trim().isEmpty; } 实现泛型扩展 扩展也运用在泛型参数，如下代码样例，对List\u0026lt;T\u0026gt;增加扩展方法和操作符，类型T在调用时才绑定静态类型：\n1 2 3 4 5 extension MyFancyList\u0026lt;T\u0026gt; on List\u0026lt;T\u0026gt; { int get doubleLength =\u0026gt; length * 2; List\u0026lt;T\u0026gt; operator -() =\u0026gt; reversed.toList(); List\u0026lt;List\u0026lt;T\u0026gt;\u0026gt; split(int at) =\u0026gt; [sublist(0, at), sublist(at)]; } 我的本博客原地址：https://ntopic.cn/p/2023110401\n","date":"2023-11-04T00:00:00Z","image":"https://ntopic.cn/p/2023110401/00_hua251d1ea2589f24e4f220f2de00fe773_52076_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/2023110401/","title":"Flutter/Dart第19天：Dart高级特性之扩展方法（Extension methods）"},{"content":"Dart官方文档：https://dart.dev/language/callable-objects\n重要说明：本博客基于Dart官网文档，但并不是简单的对官网进行翻译，在覆盖核心功能情况下，我会根据个人研发经验，加入自己的一些扩展问题和场景验证。\n可调用对象：Dart实例如果实现了call()方法，那么实例就可以像函数那样被调用，这个对象就被称为可调用对象。call()方法可以定义在任何类中，让类实例像函数一样被调用。这个函数和普通函数没有区别，包括参数和返回值等。\n如下代码样例，WannabeFunction类定义了call()方法，方法入参是3个字符串，方法返回结果是一个字符串：\n1 2 3 4 5 6 7 8 9 class WannabeFunction { String call(String a, String b, String c) =\u0026gt; \u0026#39;$a $b $c!\u0026#39;; } var wf = WannabeFunction(); var out = wf(\u0026#39;Hi\u0026#39;, \u0026#39;NTopic,\u0026#39;, \u0026#39;CN\u0026#39;); void main() =\u0026gt; print(out); // 结果：Hi NTopic, CN! Dart语言的可调用对象的应用场景，我目前还没有涉及到，恳请各位网友帮忙补充，非常感谢 👍🏻\n我的本博客原地址：https://ntopic.cn/p/2023102801\n","date":"2023-10-28T00:00:00Z","image":"https://ntopic.cn/p/2023102801/00_hua251d1ea2589f24e4f220f2de00fe773_52076_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/2023102801/","title":"Flutter/Dart第18天：Dart特性之可调用对象"},{"content":"Dart官方文档：https://dart.dev/language/extend\n重要说明：本博客基于Dart官网文档，但并不是简单的对官网进行翻译，在覆盖核心功能情况下，我会根据个人研发经验，加入自己的一些扩展问题和场景验证。\n类继承（extends/super） Dart语言和Java语言一样，也是通过extends关键字创建子类，通过super关键字引用父类：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 class Television { void turnOn() { _illuminateDisplay(); _activateIrSensor(); } // ··· } // `extends`继承父类 class SmartTelevision extends Television { void turnOn() { // `super`引用父类 super.turnOn(); _bootNetworkInterface(); _initializeMemory(); _upgradeApps(); } // ··· } 成员重写（override） 子类可以重写父类的成员方法，包括操作符、getters和setters等。通过@override注解表明重写父类的成员方法：\n1 2 3 4 5 6 7 8 9 10 class Television { // ··· set contrast(int value) {...} } class SmartTelevision extends Television { @override set contrast(num value) {...} // ··· } 子类重写方法的申明必须与父类被重新的方法相匹配，匹配的方式有以下几种：\n返回类型必须与重写方法的返回类型相同（或子类型）（如：父类方法返回类型是num，那么子类的返回类型必须是num或子类，如int等）。 参数类型必须与重写方法的参数类型相同（或超类型）（如：上诉代码样例，SmartTelevision子类的参数类型num是父类int的超类）。 位置参数的数量必须相同（如：父类接收3个位置参数，则子类必须也是3个位置参数）。 泛型方法不能重写非泛型方法，反之也一样，非泛型方法不能重写泛型方法。 最佳实战：重写方法时，尽量避免缩写参数类型的范围，即尽量避免参数发生向下转换（如父类是num类型，而子类是int类型等），因为这样做可能会引发类型转换错误。当然，如果我们确定不会发生错误，也可以这样做。\n特别注意：当我们重写了相等==操作符，则必须重写hashCode的getter方法：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 class Person { final String firstName, lastName; Person(this.firstName, this.lastName); // 重写 `hashCode` 获取方法 @override int get hashCode =\u0026gt; Object.hash(firstName, lastName); // 重写 `==` 操作符 @override bool operator ==(Object other) { return other is Person \u0026amp;\u0026amp; other.firstName == firstName \u0026amp;\u0026amp; other.lastName == lastName; } } void main() { var p1 = Person(\u0026#39;Bob\u0026#39;, \u0026#39;Smith\u0026#39;); var p2 = Person(\u0026#39;Bob\u0026#39;, \u0026#39;Smith\u0026#39;); var p3 = \u0026#39;not a person\u0026#39;; assert(p1.hashCode == p2.hashCode); assert(p1 == p2); assert(p1 != p3); } noSuchMethod()方法 若需要在访问不存在的方法或实例变量时，我们代码能做出响应（而不是抛出NoSuchMethodError错误），则我们可以重写noSuchMethod()方法：\n1 2 3 4 5 6 7 8 class A { // 重写`noSuchMethod`方法，避免`NoSuchMethodError`错误 @override void noSuchMethod(Invocation invocation) { print(\u0026#39;You tried to use a non-existent member: \u0026#39; \u0026#39;${invocation.memberName}\u0026#39;); } } 在Dart语言中，除了以下几种情况外，我们不可能调用一个不存在的方法（编译就出错）：\n对象是dynamic动态类型，运行时才能确定具体类型。 对象是静态类型，存在未实现的方法，且它实现了noSuchMethod()方法（即它不是继承Object类型的noSuchMethod()方法）。 我的本博客原地址：https://ntopic.cn/p/2023102501\n","date":"2023-10-25T00:00:00Z","image":"https://ntopic.cn/p/2023102501/00_hua251d1ea2589f24e4f220f2de00fe773_52076_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/2023102501/","title":"Flutter/Dart第17天：Dart类继承"},{"content":"Dart官方文档：https://dart.dev/language/methods\n重要说明：本博客基于Dart官网文档，但并不是简单的对官网进行翻译，在覆盖核心功能情况下，我会根据个人研发经验，加入自己的一些扩展问题和场景验证。\n实例方法 实例方法就是在类中定义的函数。对象的实例方法可访问this实例和实例变量。如下代码样例，distanceTo()函数就是一个实例方法：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 import \u0026#39;dart:math\u0026#39;; class Point { final double x; final double y; Point(this.x, this.y); double distanceTo(Point other) { var dx = x - other.x; var dy = y - other.y; return sqrt(dx * dx + dy * dy); } } 操作符 操作符是一种具有特殊名字的实例方法。Dart语言允许我们定义如下操作符：\n特别注意：从上面的操作符列表中可以看到，Dart允许我们定义\u0026lt;= \u0026gt;= ==操作符，但是为啥不能有 != 呢？原因是 !=其实是一个语法糖。比如，表达式object1 != object2其实是 !(object1 == object2) 的语法糖。\n我们通过内置的标识符operator来申明一个操作符。如下代码样例，Vector类申明了加(+)，减(-)和相等(==)三个操作符（或者方法）：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 class Vector { final int x, y; Vector(this.x, this.y); Vector operator +(Vector v) =\u0026gt; Vector(x + v.x, y + v.y); Vector operator -(Vector v) =\u0026gt; Vector(x - v.x, y - v.y); @override bool operator ==(Object other) =\u0026gt; other is Vector \u0026amp;\u0026amp; x == other.x \u0026amp;\u0026amp; y == other.y; @override int get hashCode =\u0026gt; Object.hash(x, y); } void main() { final v = Vector(2, 3); final w = Vector(2, 2); assert(v + w == Vector(4, 5)); assert(v - w == Vector(0, 1)); } Getters和Setters方法 Getters和Setters是读写对象属性的特殊方法，其实每一个实例变量都隐含有一个Getter方法（如：object.x）和一个Setter方法（如：object.x = \u0026lsquo;xxx\u0026rsquo;）。\n如下代码样例，我们可以通过get和set关键字实现Getters和Setters方法，从而能提供额外属性：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 class Rectangle { double left, top, width, height; Rectangle(this.left, this.top, this.width, this.height); // `right` 额外属性 double get right =\u0026gt; left + width; set right(double value) =\u0026gt; left = value - width; // `bottom` 额外属性 double get bottom =\u0026gt; top + height; set bottom(double value) =\u0026gt; top = value - height; } void main() { var rect = Rectangle(3, 4, 20, 15); assert(rect.left == 3); rect.right = 12; assert(rect.left == -8); } 抽象方法 抽象方法仅仅可以在abstract class或者mixin中申明，通过分号;代替方法体即可。抽象方法包括实例方法、getters和setteers等。\n如下代码样例，Doer抽象类申明了doSomething()抽象方法，EffectiveDoer子类实现了该抽象方法：\n1 2 3 4 5 6 7 8 9 10 11 abstract class Doer { // 定义抽象方法 void doSomething(); } class EffectiveDoer extends Doer { // 实现抽象方法 void doSomething() { // ... } } 我的本博客原地址：https://ntopic.cn/p/2023102401\n","date":"2023-10-24T00:00:00Z","image":"https://ntopic.cn/p/2023102401/00_hua251d1ea2589f24e4f220f2de00fe773_52076_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/2023102401/","title":"Flutter/Dart第16天：Dart类方法"},{"content":"Dart官方文档：https://dart.dev/language/constructors\n重要说明：本博客基于Dart官网文档，但并不是简单的对官网进行翻译，在覆盖核心功能情况下，我会根据个人研发经验，加入自己的一些扩展问题和场景验证。\n如下代码样例，和Java类似，最常用的生成式构造函数：\n1 2 3 4 5 6 7 8 9 class Point { double x = 0; double y = 0; Point(double x, double y) { this.x = x; this.y = y; } } 最佳实战：在Dart中，仅当命名冲突时，才使用this关键字，否则一般可以省略this关键字。\n初始化参数列表 如上最常用的构造函数，Dart可以进一步优化如下初始化参数形式。同时，也可以为非空变量设置默认值。\n1 2 3 4 5 6 7 class Point { final double x; final double y; // 在构造函数体执行之前，初始化实例变量 Point(this.x, this.y); } 默认构造函数 和Java类似，类如果没有申明构造函数，那么它会有个默认的构造函数。默认构造函数没有入参，它只会调用父类的没有入参的构造函数。\n构造函数无法继承 子类无法继承父类的构造函数，如果子类没有申明构造函数，那么这个子类就只有默认构造函数（无论父类是否有其他构造函数）。\n命名构造函数 在Dart中，通过命名构造函数，可以为类提供多个构造函数，并且在创建对象时更加清晰。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 const double xOrigin = 0; const double yOrigin = 0; class Point { final double x; final double y; Point(this.x, this.y); // `origin`命名构造函数 Point.origin() : x = xOrigin, y = yOrigin; } 特别注意：如上节提到，子类无法继承父类的构造函数，包括父类的命名构造函数。如果子类想使用父类的某个命名构造函数，那么子类必须实现该命名构造函数。\n调用父类构造函数 默认情况下，子类无入参的非命名构造函数会调用父类的无入参的非命名构造函数。父类的构造函数在构造函数体之前被调用。如果有初始化参数列表，那么初始化参数列表在父类构造函数调用之前被调用。\n构造函数相关的调用顺序如下：\n初始化参数列表 父类无入参的构造函数 子类无入参的构造函数 特别注意：如果父类没有通过无入参且非命名的构造函数，那么我们必须手工调用父类的一个构造函数，通过冒号:后面紧跟父类构造函数。\n如下代码样例，Person是父类，它仅申明了一个命名构造参数。Employee是继承Person的子类，由于父类没有申明无入参非命名的构造函数，因此在它构造函数都必须手工调用父类的某个构造函数。如命名构造函数fromJson后面，通过冒号:调用了父类的命名构造函数。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 // 未申明：无入参、非命名的构造函数 class Person { String? firstName; Person.fromJson(Map data) { print(\u0026#39;in Person\u0026#39;); } } class Employee extends Person { // 手工调用父类的构造函数：super.fromJson() Employee.fromJson(super.data) : super.fromJson() { print(\u0026#39;in Employee\u0026#39;); } } void main() { var employee = Employee.fromJson({}); print(employee); // 结果： // in Person // in Employee // Instance of \u0026#39;Employee\u0026#39; } 特别注意：由于构造函数参数是在调用构造函数之前计算，因此构造函数的参数可以是表达式，如函数调用等。父类的构造函数不能使用this.关键字，因为参数可以是表达式、静态函数等，并不一定是类实例。\n1 2 3 4 class Employee extends Person { Employee() : super.fromJson(fetchDefaultData()); // ··· } 手工调用父类的构造函数，然后逐一设置入参比较繁琐，如果我们想要简化，那么可以父类的初始值构造函数。这个功能不能与重定向构造函数一起使用（因为语法冲突）。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 class Vector2d { final double x; final double y; Vector2d(this.x, this.y); } class Vector3d extends Vector2d { final double z; // 默认情况下，我们的使用方法： // Vector3d(final double x, final double y, this.z) : super(x, y); // 简化版本： Vector3d(super.x, super.y, this.z); } 如下代码样例，父类的初始化构造函数可以通过命名参数调用。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 class Vector2d { // ... Vector2d.named({required this.x, required this.y}); } class Vector3d extends Vector2d { // ... Vector3d.yzPlane({required super.y, required this.z}) : super.named(x: 0); // 等价调用 // Vector3d.yzPlane({required double y, required this.z}) : super.named(x: 0, y: y); } 初始化列表 在构造函数执行之前，我们可以调用父类的构造函数，还可以初始化实例变量。实例变量初始化通过逗号分隔。\n1 2 3 4 5 Point.fromJson(Map\u0026lt;String, double\u0026gt; json) : x = json[\u0026#39;x\u0026#39;]!, y = json[\u0026#39;y\u0026#39;]! { print(\u0026#39;In Point.fromJson(): ($x, $y)\u0026#39;); } 开发阶段，我们可以在初始化列表中增加断言：\n1 2 3 Point.withAssert(this.x, this.y) : assert(x \u0026gt;= 0) { print(\u0026#39;In Point.withAssert(): ($x, $y)\u0026#39;); } 初始化列表在设置final不可变量时非常有用：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 import \u0026#39;dart:math\u0026#39;; class Point { final double x; final double y; final double distanceFromOrigin; Point(double x, double y) : x = x, y = y, distanceFromOrigin = sqrt(x * x + y * y); } void main() { var p = Point(2, 3); print(p.distanceFromOrigin); // 输出：3.605551275463989 } 重定向构造函数 重定向构造函数，就是使用类的其他的构造函数，重定向到的构造函数使用this关键字：\n1 2 3 4 5 6 7 8 9 class Point { double x, y; // 主构造函数 Point(this.x, this.y); // 重定向到主构造函数 Point.alongXAxis(double x) : this(x, 0); } 常量构造函数 如果对象的数据不会改变，这些对象可以作为编译期常量。\n常量构造函数要求：实例变量都是final不可变量，定义一个const修饰符的构造函数。\n特别注意：上一文我们有提到常量构造函数，常量构造函数创建的对象并不一定\u0026lt;?都是常量（当创建的对象没有const修饰符，或者对象不是在const常量上下文中，那么该对象就不是常量）！\n如下代码样例，ImmutablePoint有常量构造函数，它创建的3个对象中，前面2个是常量，后面1个并非常量。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 class ImmutablePoint { static const ImmutablePoint origin = ImmutablePoint(0, 0); final double x, y; const ImmutablePoint(this.x, this.y); } // `a`和`b`对象是常量，且它们属于同一个实例 var a = const ImmutablePoint(1, 2); var b = const ImmutablePoint(1, 2); assert(identical(a, b)); // `c`对象并非常量，它也和`a`或者`b`不是同一个实例 var c = ImmutablePoint(1, 2); assert(!identical(a, b)); 工厂构造函数 在一个不总是创建实例的类中，使用factory关键字实现一个构造函数，即为工厂构造函数。\n工厂构造函数常用使用场景：如通过构造函数，从缓存获取对象，或者创建其子类，或者创建不可变常量但是又不想提供初始化参数列表等。\n如下代码样例，Logger工厂构造函数优先从缓存获取对象，而Logger.fromJson()工厂构造函数则初始化了一个final不可实例变量：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 class Logger { final String name; bool mute = false; static final Map\u0026lt;String, Logger\u0026gt; _cache = \u0026lt;String, Logger\u0026gt;{}; // 优先从缓存获取对象，如果不存在则新增 factory Logger(String name) { print(\u0026#39;默认构造函数：$name\u0026#39;); return _cache.putIfAbsent(name, () =\u0026gt; Logger._internal(name)); } // 命名构造函数，通过工厂构造函数获取对象（缓存，或新增） factory Logger.fromJson(Map\u0026lt;String, Object\u0026gt; json) { print(\u0026#39;命名构造函数：$json\u0026#39;); return Logger(json[\u0026#39;name\u0026#39;].toString()); } Logger._internal(this.name); void log(String msg) { if (!mute) print(msg); } } 工厂构造函数的使用，和普通构造函数无本质区别：\n1 2 3 4 5 6 7 8 9 10 11 12 var logger = Logger(\u0026#39;UI\u0026#39;); logger.log(\u0026#39;Hi NTopicCN.\u0026#39;); // 结果： // 默认构造函数：UI // Hi NTopicCN. var loggerJson = Logger.fromJson({\u0026#39;name\u0026#39;: \u0026#39;UI\u0026#39;}); loggerJson.log(\u0026#39;Hello Logger.\u0026#39;); // 结果： // 命名构造函数：{name: UI} // 默认构造函数：UI // Hello Logger. 我的本博客原地址：https://ntopic.cn/p/2023102101\n","date":"2023-10-21T00:00:00Z","image":"https://ntopic.cn/p/2023102101/00_hua251d1ea2589f24e4f220f2de00fe773_52076_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/2023102101/","title":"Flutter/Dart第15天：Dart类构造函数"},{"content":"Dart官方文档：https://dart.dev/language/classes\n重要说明：本博客基于Dart官网文档，但并不是简单的对官网进行翻译，在覆盖核心功能情况下，我会根据个人研发经验，加入自己的一些扩展问题和场景验证。\nDart类 Dart语言基于类和Mixin继承，是一门面向对象语言。任何对象都是某个类的实例，除Null之外，Object类其他所有类的父类。\nMixin继承：Dart语言和Java语言一样，类只能是单继承。但通过Mixin，一个类的代码可以在多个类层次结构中复用（有关Minxin的详细说明见之前文章：https://ntopic.cn/p/2023093001）。\n方法扩展：在不改变原有类和增加子类的情况之下，通过Dart的方法扩展，可以给类增加功能的一种方法（这个特性在Flutter发布的库特别有用）。\n类修饰符：可以让我们可控制一个库如果定义子类。\n类成员（方法和变量） 对象是由函数和数据组成，分别代码方法和变量。我们通过对象.方法或者对象.变量的方法来访问对象方法和变量。当对象可能为null时，通过对象.?访问方法和变量的方式可防止异常发生。\n1 2 3 4 5 6 7 8 9 10 11 // 定义对象 var p = Point(2, 2); // 获取对象变量`y` assert(p.y == 2); // 调用对象方法：`distanceTo()` double distance = p.distanceTo(Point(4, 4)); // 当对象`p`非空时，`a`值为变量`y`；否则`a`值为null var a = p?.y; 类构造函数 在前面学习中，我们对构造函数有初步认识：https://ntopic.cn/p/2023092401\n如下代码样例，可以通过主构造函数和命名构造函数创建一个对象；构造函数之前，我们也可以增加可选的new关键字：\n1 2 3 4 5 6 var p1 = Point(2, 2); var p2 = Point.fromJson({\u0026#39;x\u0026#39;: 1, \u0026#39;y\u0026#39;: 2}); // 同上等价代码，可选的`new`关键字 var p1 = new Point(2, 2); var p2 = new Point.fromJson({\u0026#39;x\u0026#39;: 1, \u0026#39;y\u0026#39;: 2}); 当类的变量都用final不可变修饰时，我们可以构造常量对象：\n1 2 3 4 5 var a = const ImmutablePoint(1, 1); var b = const ImmutablePoint(1, 1); // 对象`a`和`b`相等 assert(identical(a, b)); 对于一个常量上下文，我们可以去掉构造函数之前的const关键字。如下代码样例，我们定义的是一个常量Map（上下文），那么Map元素的构造器就可以省略const关键字：\n1 2 3 4 5 6 7 8 9 10 const pointAndLine = const { \u0026#39;point\u0026#39;: const [const ImmutablePoint(0, 0)], \u0026#39;line\u0026#39;: const [const ImmutablePoint(1, 10), const ImmutablePoint(-2, 11)], }; // 同上等价代码，可省略`const`关键字 const pointAndLine = { \u0026#39;point\u0026#39;: [ImmutablePoint(0, 0)], \u0026#39;line\u0026#39;: [ImmutablePoint(1, 10), ImmutablePoint(-2, 11)], }; 如果一个对象没有常量上下文，且没有使用const修饰构造器，那么它创建的是一个非常量对象：\n1 2 3 4 5 var a = const ImmutablePoint(1, 1); var b = ImmutablePoint(1, 1); // `a`是常量对象，`b`不是常量对象，因此它们不相等！ assert(!identical(a, b)); 获取对象类型 通过对象.runtimeType属性，返回对象的Type对象。一般情况下，我们通过对象 is Type的方法，检测某个对象是否属于某个类型，而不是使用对象.runtimeType == Type比较方式：\n1 2 3 4 5 print(\u0026#39;The type of a is ${a.runtimeType}\u0026#39;); var a = \u0026#39;Hello NTopicCN\u0026#39;; assert(a.runtimeType == String); assert(a is String); 实例变量 如下代码定义样例，申明实例变量，实例变量的默认值为null：\n1 2 3 4 5 class Point { double? x; // 默认值：null double? y; // 默认值：null double z = 0; // 默认值：0 } 所有的实例变量都隐含有一个getter方法，包括final修饰的变量、未使用final修饰的变量、late final修饰的变量（赋值和未赋值）等，都有getter方法。\n如下代码样例，几种实例变量修饰和访问的方法：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 class Point { double? x; double? y; } void main() { var point = Point(); point.x = 4; // `setter`方法赋值 assert(point.x == 4); // `getter`方法取值 assert(point.y == null); // 默认值为`null` } class ProfileMark { final String name; final DateTime start = DateTime.now(); // 主构造函数 ProfileMark(this.name); // 命名构造函数，同时`name`设置初始值 ProfileMark.unnamed() : name = \u0026#39;\u0026#39;; } 隐性接口 在Dart中，每个类都隐含的定义了一个接口，这个接口包含了该类的所有实例成员和该类实现的所有的其他接口。\n假设我们定义了一个类A，它需要支持类B的API（构造函数不是API），但是类A的定义并不是继承类B，那么类A需要实现B接口。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 // `Person`类，也是`Persion`接口，包含`greet()`方法 class Person { // 属于接口的一部分，但是对外不可见 final String _name; // 构造函数，不属于接口一部分 Person(this._name); // 普通方法，属于接口一部分 String greet(String who) =\u0026gt; \u0026#39;Hello, $who. I am $_name.\u0026#39;; } // 实现`Person`接口 class Impostor implements Person { String get _name =\u0026gt; \u0026#39;\u0026#39;; String greet(String who) =\u0026gt; \u0026#39;Hi $who. Do you know who I am?\u0026#39;; } String greetBob(Person person) =\u0026gt; person.greet(\u0026#39;Bob\u0026#39;); void main() { print(greetBob(Person(\u0026#39;Kathy\u0026#39;))); // Hello, Bob. I am Kathy. print(greetBob(Impostor())); // Hi Bob. Do you know who I am? } 类变量和方法 static关键字，可以定义类变量和方法（Java中成为静态变量和静态方法）。\n如下代码样例，定义和使用一个类变量：\n1 2 3 4 5 6 7 8 class Queue { static const initialCapacity = 16; // ··· } void main() { assert(Queue.initialCapacity == 16); } 如下代码样例，定义和使用类方法：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 import \u0026#39;dart:math\u0026#39;; class Point { double x, y; Point(this.x, this.y); static double distanceBetween(Point a, Point b) { var dx = a.x - b.x; var dy = a.y - b.y; return sqrt(dx * dx + dy * dy); } } void main() { var a = Point(2, 2); var b = Point(4, 4); var distance = Point.distanceBetween(a, b); assert(2.8 \u0026lt; distance \u0026amp;\u0026amp; distance \u0026lt; 2.9); print(distance); } 最佳实践：对于一些常用的工具方法，建议使用顶级方法代替类变量。\n类方法可以用作编译期常量，比如：我们可以把一个类方法当作参数传递给常量构造器。\n我的本博客原地址：https://ntopic.cn/p/2023102001\n","date":"2023-10-20T00:00:00Z","image":"https://ntopic.cn/p/2023102001/00_hua251d1ea2589f24e4f220f2de00fe773_52076_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/2023102001/","title":"Flutter/Dart第14天：Dart类详解"},{"content":"前提：确认占用80端口的程序（sudo lsof -i:80） 我研发用的是Mac电脑，查看80端口被占用的进程命令：sudo lsof -i:80\n我Mac电脑80端口占用进程如下：httpd进程占用了80端口，httpd进程即为Apache服务，下面提供了2种解决办法。\n1 2 3 4 5 6 7 8 9 10 OXL-MacBook:~ obullxl$ sudo lsof -i:80 COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAME httpd 128 root 4u IPv6 0x837e41eb989fc6d1 0t0 TCP *:http (LISTEN) WeChat 390 obullxl 103u IPv4 0x837e41e6ce1a65d9 0t0 TCP 192.168.101.30:49216-\u0026gt;182.50.15.211:http (CLOSE_WAIT) WeChat 390 obullxl 106u IPv4 0x837e41e6ce1a3089 0t0 TCP 192.168.101.30:49217-\u0026gt;182.50.10.149:http (CLOSE_WAIT) WeChat 390 obullxl 342u IPv4 0x837e41e6ce6045d9 0t0 TCP 192.168.101.30:52992-\u0026gt;61.241.138.140:http (CLOSE_WAIT) httpd 616 _www 4u IPv6 0x837e41eb989fc6d1 0t0 TCP *:http (LISTEN) httpd 4646 _www 4u IPv6 0x837e41eb989fc6d1 0t0 TCP *:http (LISTEN) httpd 4647 _www 4u IPv6 0x837e41eb989fc6d1 0t0 TCP *:http (LISTEN) httpd 4648 _www 4u IPv6 0x837e41eb989fc6d1 0t0 TCP *:http (LISTEN) 方案一：暂停Apache系统服务后，启动Web调试 Apache服务启动和暂停，可以使用apachectl命令行：sudo apachectl restart和sudo apachectl stop\n我的电脑暂停了Apache服务之后，再次查看80端口已经没有占用了：\n1 2 3 OXL-MacBook:~ obullxl$ sudo apachectl stop OXL-MacBook:~ obullxl$ sudo lsof -i:80 OXL-MacBook:~ obullxl$ 为了让我们调试Web站点页面更加顺畅，我们可以把Apache暂停命令加到启动Web调试的命令中，如Hugo静态站点的脚本：\n1 2 3 4 5 6 7 #!/bin/sh # 暂停Apache服务，释放80端口 sudo apachectl stop # 在80端口上启动Hugo，调试页面 sudo hugo server -p 80 -D 本方案的唯一不足就是：当我们使用control + C暂停了Hugo服务，释放了80端口后，Apache服务不能自动启动。需要手工执行命令重启Apache服务，优化方案请看方案二。\n方案二：修改Apache系统服务80端口为其他端口 方案一可以解决我们在80端口调试Web页面，但是在调试完成之后，Apache服务需要手工执行命令重启。本方案就是修改Apache服务默认的80端口为其他端口，让出80端口给我们业务页面使用。\n第一步 找到Apache的安装目录：我Mac电脑目录是/etc/apache2 1 2 3 4 5 6 7 OXL-MacBook:~ obullxl$ cd /etc OXL-MacBook:etc obullxl$ ls | grep apache apache2 OXL-MacBook:etc obullxl$ cd apache2/ OXL-MacBook:apache2 obullxl$ ls extra magic other httpd.conf httpd.conf.pre-update mime.types users httpd.conf~previous original 第二步 修改Apache配置文件（httpd.conf）中80端口为其他端口，如8080端口。为了安全起见，修改配置文件之前，先进行备份。 1 2 3 OXL-MacBook:apache2 obullxl$ sudo cp httpd.conf httpd.conf.20231015 Password: OXL-MacBook:apache2 obullxl$ sudo vi httpd.conf 1 2 3 4 5 6 \u0026lt;IfDefine SERVER_APP_HAS_DEFAULT_PORTS\u0026gt; Listen 8080 \u0026lt;/IfDefine\u0026gt; \u0026lt;IfDefine !SERVER_APP_HAS_DEFAULT_PORTS\u0026gt; Listen 8080 \u0026lt;/IfDefine\u0026gt; 第三步 重启Apache服务：sudo apachectl restart，同时可以查看8080端口，已经被Apache服务占用。 1 2 3 4 5 OXL-MacBook:apache2 obullxl$ sudo apachectl restart OXL-MacBook:apache2 obullxl$ sudo lsof -i:8080 COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAME httpd 6942 root 4u IPv6 0x837e41eb989f9cd1 0t0 TCP *:http-alt (LISTEN) httpd 6955 _www 4u IPv6 0x837e41eb989f9cd1 0t0 TCP *:http-alt (LISTEN) 最后 方案一和方案二个人验证均可行~\n我的本博客原地址：https://ntopic.cn/p/2023101501\n","date":"2023-10-15T00:00:00Z","image":"https://ntopic.cn/p/2023101501/00_hud3c48deacf6a5f522c42f9a4136f7dd0_24108_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/2023101501/","title":"Mac电脑80端口被占用的2种解决办法"},{"content":"Dart官方文档：https://dart.dev/language/error-handling\n重要说明：本博客基于Dart官网文档，但并不是简单的对官网进行翻译，在覆盖核心功能情况下，我会根据个人研发经验，加入自己的一些扩展问题和场景验证。\n异常 和Java一样，Dart也可以抛出异常，也可以捕获异常。Dart异常如果未被捕获，容易导致进程挂起和导致程序退出。所以，我们在编写Dart程序时，需要特别关注异常。\n和Java不同的是，Dart中的异常全部都是未检测异常。如果一个方法没有申明任何类型的异常，那么我们就无需捕获该方法的异常。\nDart提供了Exception和Error两种异常的基础类型，同时内置了其他一些它们的子类。我们也可以定义我们自己的异常类型，Dart可以把任何非空的对象当作异常抛出，且这些对象类型不一定是Exception或Error类型的子类（和Java有很大区别）。\nthrow抛出异常 Dart可以把任务非空对象当作异常抛出，但建议抛出的异常为Exception或者Error的子类，下面是几个代码样例：\n1 2 3 4 5 throw FormatException(\u0026#39;Expected at least 1 section\u0026#39;); throw \u0026#39;Out of llamas!\u0026#39;; void distanceTo(Point other) =\u0026gt; throw UnimplementedError(); catch捕获异常 捕获异常，一般有种目的：一是为了阻止异常继续传播，二是我们期望重新抛出异常。\n捕获异常的语法有几种：\non 异常类型 catch(异常对象) catch(异常对象） catch(异常对象, 异常堆栈) 捕获异常后，我们可以进行一些处理。我们可以根据异常类型，捕获多个异常。异常语句中若未定义异常类型，则捕获所有异常，如下代码样例：\n1 2 3 4 5 6 7 8 9 10 11 12 try { breedMoreLlamas(); } on OutOfLlamasException { // `OutOfLlamasException`类型的异常 buyMoreLlamas(); } on Exception catch (e) { // `Exception`类型的异常 print(\u0026#39;Unknown exception: $e\u0026#39;); } catch (e) { // 其他的所有的异常 print(\u0026#39;Something really unknown: $e\u0026#39;); } 异常堆栈类型StackTrace类型，如下代码样例：\n1 2 3 4 5 6 7 8 try { // ··· } on Exception catch (e) { print(\u0026#39;Exception details:\\n $e\u0026#39;); } catch (e, s) { print(\u0026#39;Exception details:\\n $e\u0026#39;); print(\u0026#39;Stack trace:\\n $s\u0026#39;); } rethrow重新抛出异常 如下代码样例，当捕获到异常之后，可以通过rethrow重新抛出异常：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 void misbehave() { try { dynamic foo = true; print(foo++); // 制造异常 } catch (e) { print(\u0026#39;misbehave() partially handled ${e.runtimeType}.\u0026#39;); rethrow; // 重新抛出异常 } } void main() { try { misbehave(); } catch (e) { print(\u0026#39;main() finished handling ${e.runtimeType}.\u0026#39;); } } finally子句 当我们不论是否有异常发生，都行执行某个逻辑时，我们可用使用finally子句。当发生异常时，首先执行catch子句进行捕获，若未被捕获或者重新抛出异常，那么这个异常会在执行完fianlly之后被抛出。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 try { breedMoreLlamas(); } finally { // 不论是否有异常发生，均会执行本逻辑 cleanLlamaStalls(); } try { breedMoreLlamas(); } catch (e) { // 首先：捕获异常 print(\u0026#39;Error: $e\u0026#39;); } finally { // 然后：执行本逻辑 cleanLlamaStalls(); } 断言 在应用的开发阶段，通过结果为false的断言语句（如：assert(\u0026lt;condition\u0026gt;, \u0026lt;optionalMessage\u0026gt;);语句）来阻断程序执行。几种断言语句的代码样例：\n1 2 3 4 5 6 7 8 // `text`不能为null assert(text != null); // `number`必须小于100 assert(number \u0026lt; 100); // `urlString`必须以`https`开头 assert(urlString.startsWith(\u0026#39;https\u0026#39;), \u0026#39;URL ($urlString) should start with \u0026#34;https\u0026#34;.\u0026#39;); 断言失败（表达式结果为false）时，抛出AssertionError类型（Error类型的子类）的异常。\n断言在以下场景下会生效：\nFlutter应用程序在debug模式（开发阶段，开启了断言）。 类似于webdev等开发工具，默认开启了断言。 类似于dart run和dart compile js等工具，且通过命令行增加了--enable-asserts开启断言参数。 而在生产阶段，断言相关代码被忽略，因此断言会失效（不会执行）。\n我的本博客原地址：https://ntopic.cn/p/2023101401\n","date":"2023-10-14T00:00:00Z","image":"https://ntopic.cn/p/2023101401/00_hua251d1ea2589f24e4f220f2de00fe773_52076_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/2023101401/","title":"Flutter/Dart第13天：Dart错误处理"},{"content":"Dart控制流主要由循环和分支组成：\nDart官方文档-循环：https://dart.dev/language/loops Dart官方文档-分支：https://dart.dev/language/branches 重要说明：本博客基于Dart官网文档，但并不是简单的对官网进行翻译，在覆盖核心功能情况下，我会根据个人研发经验，加入自己的一些扩展问题和场景验证。\n循环 Dart可通过循环来控制逻辑流，它支持3种语句：for循环，while和do while循环，break和continue等\n同时，也可以通过非循环来控制逻辑流，包括：分支（如：if和switch）和异常（try、catch和throw）等\nfor循环 for循环可以使用在迭代中，标准用法如下代码样例：\n1 2 3 4 var message = StringBuffer(\u0026#39;Dart is fun\u0026#39;); for (var i = 0; i \u0026lt; 5; i++) { message.write(\u0026#39;!\u0026#39;); } for-in循环常用于无需关注迭代的索引，如下代码样例，包含了闭包列表和迭代：\n1 2 3 4 5 6 7 8 9 var callbacks = []; for (var i = 0; i \u0026lt; 2; i++) { callbacks.add(() =\u0026gt; print(i)); } for (final c in callbacks) { c(); } // 解构：01 同样的，for-in循环还可以使用对象模式（Pattern模式详解：https://ntopic.cn/p/2023100501），解构迭代的元素：\n1 2 3 for (final Candidate(:name, :yearsExperience) in candidates) { print(\u0026#39;$name has $yearsExperience of experience.\u0026#39;); } 对于迭代类型（Dart可迭代集合详解：https://ntopic.cn/p/2023092701），forEach()方法也可以实现循环：\n1 2 var collection = [1, 2, 3]; collection.forEach(print); // 1 2 3 while和do while循环 while循环和do while循环最大的区别：while先检测循环条件，然后执行循环逻辑；do while默认先执行1次循环逻辑，然后检测循环条件，以决定后续是否继续执行循环逻辑。\n1 2 3 4 5 6 7 while (!isDone()) { doSomething(); } do { printLine(); } while (!atEndOfPage()); break和continue循环控制 break和continue在循环中的最大区别：break中断整个循环，而continue中断当前循环的后续逻辑。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 while (true) { if (shutDownRequested()) break; processIncomingRequests(); } for (int i = 0; i \u0026lt; candidates.length; i++) { var candidate = candidates[i]; if (candidate.yearsExperience \u0026lt; 5) { continue; } candidate.interview(); } // 若`candidates`是可迭代列表，则上面的逻辑可改写如下： candidates .where((c) =\u0026gt; c.yearsExperience \u0026gt;= 5) .forEach((c) =\u0026gt; c.interview()); 分支 Dart也可通过分支来控制逻辑流：if语句和元素，if-case语句和元素，switch语句和switch表达式等。\n同样的，我们也可以通过其他方式来控制逻辑流：如循环（就是本文的第1节）和异常（try、catch和throw）等\nif/else分支 if语句可以有else可选项子句，if后面的括号中的表达式计算值必须是boolean类型，如下代码样例：\n1 2 3 4 5 6 7 if (isRaining()) { you.bringRainCoat(); } else if (isSnowing()) { you.wearJacket(); } else { car.putTopDown(); } if-case模式 Dart中if语句后的case子句可以带一个模式（Pattern模式详解：https://ntopic.cn/p/2023100501）：\n1 if (pair case [int x, int y]) return Point(x, y); 上诉代码样例中，列表模式[int x, int y]首先匹配pair值，并且把列表第1和第2个int类型的元素解构并赋值给了x和y变量。\n如果模式未能成功匹配，则需要进入else分支：\n1 2 3 4 5 if (pair case [int x, int y]) { print(\u0026#39;Was coordinate array $x,$y\u0026#39;); } else { throw FormatException(\u0026#39;Invalid coordinates.\u0026#39;); } switch语句和switch表达式 switch语句后面可以有多个case子句，每一个case子句都可以是一个模式（Pattern模式详解：https://ntopic.cn/p/2023100501）。\n当没有case子句匹配时，就会执行default子句或者_通配符子句：\n1 2 3 4 5 6 7 8 9 10 11 12 13 switch (command) { case \u0026#39;OPEN\u0026#39;: executeOpen(); continue newCase; // 继续执行 case \u0026#39;DENIED\u0026#39;: case \u0026#39;CLOSED\u0026#39;: executeClosed(); // `DENIED`和`CLOSED`均会执行 newCase: case \u0026#39;PENDING\u0026#39;: executeNowClosed(); // `OPEN`和`PENDING`均会执行 } 在switch语句中，还可以使用模式，有如下几种用法：\n1 2 3 4 5 var x = switch (y) { ... }; print(switch (x) { ... }); return switch (x) { ... }; switch表达式可以重写switch语句，如下为switch语句的代码样例：\n1 2 3 4 5 6 7 8 9 10 11 // `slash`, `star`, `comma`, `semicolon`等，需要是常量 switch (charCode) { case slash || star || plus || minus: // `逻辑或`模式 token = operator(charCode); case comma || semicolon: // `逻辑或`模式 token = punctuation(charCode); case \u0026gt;= digit0 \u0026amp;\u0026amp; \u0026lt;= digit9: // `关系`和`逻辑与`模式 token = number(); default: throw FormatException(\u0026#39;Invalid\u0026#39;); } 上诉代码样例，可以使用switch表达式重写如下：\n1 2 3 4 5 6 token = switch (charCode) { slash || star || plus || minus =\u0026gt; operator(charCode), comma || semicolon =\u0026gt; punctuation(charCode), \u0026gt;= digit0 \u0026amp;\u0026amp; \u0026lt;= digit9 =\u0026gt; number(), _ =\u0026gt; throw FormatException(\u0026#39;Invalid\u0026#39;) }; switch表达式不同于switch语句的语法，包括以下几个方面：\nCase可选项无需使用case开头。 Case可选项的逻辑是一个表达式，而不是一系列的语句。 Case可选项都必须有逻辑，空可选性不代表隐性失败。 Case可选项模式，逻辑使用=\u0026gt;分割。 多个Case可选项之间，使用,分割。 默认可选项，只能使用_。 Switch还可以使用在穷举检测中，就是所有的值都有对应的可选项被处理，以避免引发编译错误。默认可选项（default或_）涵盖了所有其他情况，因此它可以保障穷举性。\ncase-when子句 case-when子句：在case子句后面增加when子句。case-when子句可以用在if-case、switch语句和switch表达式中。\n1 2 3 4 5 6 switch (pair) { case (int a, int b) when a \u0026gt; b: print(\u0026#39;First element greater\u0026#39;); case (int a, int b): print(\u0026#39;First element not greater\u0026#39;); } case-when子句计算任何一个boolean类型的值，值为true代表可以执行本Case可选项逻辑，为false则继续执行下一个Case可选项，并不会退出整个Switch语句。\n我的本博客原地址：https://ntopic.cn/p/2023101301\n","date":"2023-10-13T00:00:00Z","image":"https://ntopic.cn/p/2023101301/00_hua251d1ea2589f24e4f220f2de00fe773_52076_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/2023101301/","title":"Flutter/Dart第12天：Dart控制流详解"},{"content":"Dart官方文档：https://dart.dev/language/functions\n重要说明：本博客基于Dart官网文档，但并不是简单的对官网进行翻译，在覆盖核心功能情况下，我会根据个人研发经验，加入自己的一些扩展问题和场景验证。\nDart语言是纯面向对象的编程语言，就是是函数也是对象，它的类型就是Function类（https://api.dart.dev/stable/3.1.3/dart-core/Function-class.html）。\n如下代码样例，函数的不同实现。如果函数实现仅仅只有1个表达式，那么函数可以使用箭头语法：=\u0026gt; return expression;\n1 2 3 4 5 6 7 // 函数实现 bool isNoble(int atomicNumber) { return _nobleGases[atomicNumber] != null; } // 箭头语法，上诉函数的简单语法 bool isNoble(int atomicNumber) =\u0026gt; _nobleGases[atomicNumber] != null; 命名参数（必选，默认值） 参数格式：functionName({param1 = value1, param2, ...})\n函数调用：functionName(param1: valueX, param2: value2, ...)\n默认情况下，命名参数是可选的，除非显示增加required标记。在Flutter中，尤其是Widget是构造函数，仅仅使用命名参数，尽管参数是必填参数。\n特别注意：就算了增加required的必选参数，它也可以是可空的。\n位置参数（可选，默认值） 可选位置参数通过[]包裹的参数列表，默认值为null，可以设置默认值：\n参数格式：functionName(param1, param2, [param3 = value3, param4]);\n函数调用：functionName(value1, value2);或functionName(value1, value2, value3);或functionName(value1, value2, value3, value4);\nmain()函数 任何Dart应用，都必须包含一个顶级main()函数，它是应用的唯一入口。它的返回值是void，入参是List\u0026lt;String\u0026gt;类型。\n代码样例：如下代码，我们在执行Dart文件时，指定了参数dart args.dart 1 test\n1 2 3 4 5 6 7 void main(List\u0026lt;String\u0026gt; arguments) { print(arguments); assert(arguments.length == 2); assert(int.parse(arguments[0]) == 1); assert(arguments[1] == \u0026#39;test\u0026#39;); } 函数作为参数 函数可以作为其他函数的入参，也可以赋值给变量。\n1 2 3 4 5 6 7 8 9 10 11 // 1. 函数作为函数入参 void printElement(int element) { print(element); } var list = [1, 2, 3]; list.forEach(printElement); // 2. 函数赋值给变量（匿名函数） var loudify = (msg) =\u0026gt; \u0026#39;!!! ${msg.toUpperCase()} !!!\u0026#39;; assert(loudify(\u0026#39;hello\u0026#39;) == \u0026#39;!!! HELLO !!!\u0026#39;); 匿名函数 如上代码，main()和printElement()函数都是命名函数。我们也可以创建匿名函数（如上代码loudify变量的值），特别是在Lambda表达式或者在闭包函数中，匿名函数使用场景很多。\n代码样例：如下代码，map()和forEach()的入参就是匿名函数。\n1 2 3 4 5 6 void main() { const list = [\u0026#39;apples\u0026#39;, \u0026#39;bananas\u0026#39;, \u0026#39;oranges\u0026#39;]; list .map((item) =\u0026gt; item.toUpperCase()) .forEach((item) =\u0026gt; print(\u0026#39;$item: ${item.length}\u0026#39;)); } 函数相等校验 可以进行相等校验的函数包括：顶级函数，静态函数和实例函数。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 // 1. 顶级函数 void foo() {} class A { // 2. 静态方法 static void bar() {} // 3. 实例方法 void baz() {} } void main() { Function x; // 顶级函数相等校验 x = foo; assert(foo == x); // 静态方法相等校验 x = A.bar; assert(A.bar == x); // 实例方法相等校验 var v = A(); // A实例#1 var w = A(); // A实例#2 var y = w; x = w.baz; // 同是A实例#2方法 assert(y.baz == x); // 不同实例方法 assert(v.baz != w.baz); } 函数返回值 如果没有显示返回值，函数默认返回null，Record记录可以聚合返回多个值。\n1 2 3 (String, int) foo() { return (\u0026#39;something\u0026#39;, 42); } 生成器函数 生成器函数可以延迟产出一系列值，Dart中内置2类生成器函数：\n同步生成器：返回1个Iterable对象。 异步生成器：返回1个Stream对象。 同步生成器函数：使用sync*标记函数体，并且使用yield表达式产生值。\n1 2 3 4 Iterable\u0026lt;int\u0026gt; naturalsTo(int n) sync* { int k = 0; while (k \u0026lt; n) yield k++; } 异步生成器函数：使用async*标记函数体，并且使用yield表达式产生值。\n1 2 3 4 Stream\u0026lt;int\u0026gt; asynchronousNaturalsTo(int n) async* { int k = 0; while (k \u0026lt; n) yield k++; } 如果生成器是递归的，可以通过yield*来提升性能。\n1 2 3 4 5 6 Iterable\u0026lt;int\u0026gt; naturalsDownFrom(int n) sync* { if (n \u0026gt; 0) { yield n; yield* naturalsDownFrom(n - 1); } } 我的本博客原地址：https://ntopic.cn/p/2023100601\n","date":"2023-10-06T00:00:00Z","image":"https://ntopic.cn/p/2023100601/00_hua251d1ea2589f24e4f220f2de00fe773_52076_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/2023100601/","title":"Flutter/Dart第11天：Dart函数方法详解"},{"content":"Dart官方文档：https://dart.dev/language/pattern-types\n重要说明：本博客基于Dart官网文档，但并不是简单的对官网进行翻译，在覆盖核心功能情况下，我会根据个人研发经验，加入自己的一些扩展问题和场景验证。\n和操作符一样，模式运算也遵循一定的优先级规则，我们可以通过增加括号()让低优先级规则的模式优先运算：\n逻辑或模式低于逻辑与模式，逻辑与模式低于关系模式：逻辑或 \u0026lt; 逻辑与 \u0026lt; 关系。 一元模式优先级相同：值转换、空检测、空断言。 其他的模式都具有最高的优先级，集合类型（List列表、Map映射和Record记录）和对象模式包含了其他数据，因此作为外部模式优先运算。 逻辑或模式（Logical-or） 模式语法：子模式1 || 子模式2\n模式规则：逻辑或模式通过||分割子模式，从左到右，任何一个子模式匹配则本模式匹配，且后面的子模式不在运算。\n子模式可以绑定变量，但是每个子模式绑定的变量必须相同，因为任一子模式匹配则后面的子模式不在运算。\n1 2 3 4 var isPrimary = switch (color) { Color.red || Color.yellow || Color.blue =\u0026gt; true, _ =\u0026gt; false }; 逻辑与模式（Logical-and） 模式语法：子模式1 \u0026amp;\u0026amp; 子模式2\n模式规则：逻辑与模式通过\u0026amp;\u0026amp;分隔子模式，从左到右，任何一个子模式未匹配则本模式未匹配，且后面的子模式不在运算。\n子模式可以绑定变量，且每个子模式绑定的变量不能重叠，因为本模式匹配代表每个子模式都必须匹配运算，如果重叠则意味着变量被赋值多次。\n1 2 3 switch ((1, 2)) { case (var a, var b) \u0026amp;\u0026amp; (var c, var d): // ... } 关系模式（Relational） 模式规则：关系模式通过和给定的常量进行比较完成匹配（比较操作符：==，!=，\u0026lt;，\u0026gt;，\u0026lt;=，\u0026gt;=），true代表匹配成功。通常情况下，关系模式和逻辑与模式配合使用。\n1 2 3 4 5 6 7 8 9 10 11 12 13 String asciiCharType(int char) { const space = 32; const zero = 48; const nine = 57; return switch (char) { \u0026lt; space =\u0026gt; \u0026#39;control\u0026#39;, == space =\u0026gt; \u0026#39;space\u0026#39;, \u0026gt; space \u0026amp;\u0026amp; \u0026lt; zero =\u0026gt; \u0026#39;punctuation\u0026#39;, \u0026gt;= zero \u0026amp;\u0026amp; \u0026lt;= nine =\u0026gt; \u0026#39;digit\u0026#39;, _ =\u0026gt; \u0026#39;\u0026#39; }; } 值转换模式（cast） 模式语法：变量 as 类型，如：foo as String\n模式规则：值转换模式允许在对象数据解构过程中进行类型转换，如果类型无法转换，则会产生错误，建议在类型转换之前，进行类型断言。\n1 2 (num, Object) record = (1, \u0026#39;s\u0026#39;); var (i as int, s as String) = record; 空检测模式（Null-check） 模式语法：子模式?\n模式规则：如果检测的值不为NULL，则模式匹配。它允许绑定一个变量，变量的类型是该不可为NULL值类型基类。\n1 2 3 4 5 String? maybeString = \u0026#39;nullable with base type String\u0026#39;; switch (maybeString) { case var s?: // \u0026#39;s\u0026#39; has type non-nullable String here. } 空断言模式（Null-assert） 模式语法：子模式!\n模式规则：首先检测对象不为NULL，然后检测对象数据值。如果匹配的值为NULL，则会抛出错误。它常用于解构并赋值场景，且保证所赋值非NULL。\n1 2 3 4 5 6 7 8 List\u0026lt;String?\u0026gt; row = [\u0026#39;user\u0026#39;, null]; switch (row) { case [\u0026#39;user\u0026#39;, var name!]: // ... // \u0026#39;name\u0026#39; is a non-nullable string here. } (int?, int?) position = (2, 3); var (x!, y!) = position; 常量模式（constant） 当值为常量时，常量模式匹配，常量包括：123, null, string, math.pi, SomeClass.constant, const Thing(1, 2), const (1 + 2)等。\n1 2 3 4 switch (number) { // Matches if 1 == number. case 1: // ... } 我们可以通过字面常量、命名的常量等方式使用常量模式：\n数字字面量：123, 45.56 布尔字面量：true 字符串字面量：string 命名常量：someConstant, math.pi, double.infinity 常量构造器：const Point(0, 0) 常量集合字面量：const [], const {1, 2} 其他更多复杂的常量表达式，可以通过()包裹，并增加const前缀：const (const (1 + 2))\n1 2 3 4 5 // List or map pattern: case [a, b]: // ... // List or map literal: case const [a, b]: // ... 变量模式（variable） 模式规则：变量模式一般在解构和赋值中，它匹配模式、解构对象并完成赋值，如：var bar, String str, final int _。变量的作用域为模式所在的作用域。如果变量指定了类型，那么当对象类型和值均匹配时，模式才被匹配。通配符模式是一个特殊的变量模式。\n1 2 3 4 5 6 7 8 9 10 switch ((1, 2)) { // \u0026#39;var a\u0026#39;和\u0026#39;var b\u0026#39;是变量模式，它们值分别为`1`和`2` case (var a, var b): // ... // \u0026#39;a\u0026#39;和\u0026#39;b\u0026#39;的作用域在case代码块 } switch ((1, 2)) { // `2`是数字类型，与\u0026#39;String b\u0026#39;不匹配，因此本模式为匹配 case (int a, String b): // ... } 标识符模式（identifier） 标识符模式与常量模式或变量模式类似：\n变量申明上下文：给变量申明一个标识符，如：var (a, b) = (1, 2); 变量赋值上下文：给已经存在的标识符赋值，如：(a, b) = (3, 4); 模式匹配上下文：当作一个命名常量模式（除名字是_外） 任意上下文中的通配符标识符：能匹配任何值且忽略该值，如：case [_, var y, _]: print('The middle element is $y'); 括号模式 模式语法：(子模式)\n代码样例：如下代码，和表达式一样，增加括号()目的是提高模式的优先级。\n1 2 3 4 5 6 7 final (x, y, z) = (true, true, false); // 没有括号：true x || y \u0026amp;\u0026amp; z =\u0026gt; \u0026#39;matches true\u0026#39;, // 增加括号：false (x || y) \u0026amp;\u0026amp; z =\u0026gt; \u0026#39;matches false\u0026#39;, 列表模式（List） 模式语法：[子模式1, 子模式2]\nList列表模式首先匹配List类型，然后匹配列表元素值，并进行解构和赋值。List列表模式必须匹配整个列表模式元素，我们可以使用...占位符匹配剩余的列表元素。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 // 全列表元素匹配 const a = \u0026#39;a\u0026#39;; const b = \u0026#39;b\u0026#39;; switch (obj) { case [a, b]: print(\u0026#39;$a, $b\u0026#39;); } // 占位符匹配剩余元素，且忽略 var [a, b, ..., c, d] = [1, 2, 3, 4, 5, 6, 7]; print(\u0026#39;$a $b $c $d\u0026#39;); // 1 2 6 7 // 占位符当作一个子列表 var [a, b, ...rest, c, d] = [1, 2, 3, 4, 5, 6, 7]; print(\u0026#39;$a $b $rest $c $d\u0026#39;); // 1 2 [3, 4, 5] 6 7 Map映射模式 模式语法：{\u0026quot;key\u0026quot;: subpattern1, someConst: subpattern2}\nMap映射模式首先匹配Map类型，然后匹配元素内容，并进行解构和赋值。Map映射模式不需要匹配所有元素，忽略未被匹配到的元素。\nRecord记录模式 模式语法：(subpattern1, subpattern2)或者(x: subpattern1, y: subpattern2)\nRecord记录模式首先匹配记录，然后解构其字段。字段数量、类型和值未匹配，则模式匹配失败。Record记录模式必须匹配所有字段。字段getter可由变量和标识符模式推导得到。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 var (myString: foo, myNumber: bar) = (myString: \u0026#39;string\u0026#39;, myNumber: 1); // Record pattern with variable subpatterns: var (untyped: untyped, typed: int typed) = record; var (:untyped, :int typed) = record; switch (record) { case (untyped: var untyped, typed: int typed): // ... case (:var untyped, :int typed): // ... } // Record pattern wih null-check and null-assert subpatterns: switch (record) { case (checked: var checked?, asserted: var asserted!): // ... case (:var checked?, :var asserted!): // ... } // Record pattern wih cast subpattern: var (untyped: untyped as int, typed: typed as String) = record; var (:untyped as int, :typed as String) = record; Object对象模式 模式语法：SomeClass(x: subpattern1, y: subpattern2)\n对象模式首先对象类型和属性类型，并完成对象属性解构，调用getter方法完成赋值。如果类型不一致，则匹配失败。对象的属性名可以忽略，它可以通过变量模式和标识符模式进行推导。和Map映射模式一样，对象模式不需要匹配所有属性，忽略未被匹配到的属性。\n1 2 3 4 5 6 7 switch (shape) { // Matches if shape is of type Rect, and then against the properties of Rect. case Rect(width: var w, height: var h): // ... } // Binds new variables x and y to the values of Point\u0026#39;s x and y properties. var Point(:x, :y) = Point(1, 2); 通配符模式 模式语法：_ _就是通配符模式，它既是变量模式也是标识符模式，但是它无变量也不赋值。它通常作为一个占位符，目的是匹配解构剩下的位置值。通配符如果带有类型，那么它仅仅进行类型检测，而忽略变量和赋值。\n1 2 3 4 5 6 7 8 9 // 占位符 var list = [1, 2, 3]; var [_, two, _] = list; // 类型检测 switch (record) { case (int _, String _): print(\u0026#39;First field is int and second is String.\u0026#39;); } 我的本博客原地址：https://ntopic.cn/p/2023100501\n","date":"2023-10-05T00:00:00Z","image":"https://ntopic.cn/p/2023100501/00_hua251d1ea2589f24e4f220f2de00fe773_52076_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/2023100501/","title":"Flutter/Dart第10天：Dart高级特性Pattern模式的全部类型（共15种）"},{"content":"Dart官方文档：https://dart.dev/language/patterns\n重要说明：本博客基于Dart官网文档，但并不是简单的对官网进行翻译，在覆盖核心功能情况下，我会根据个人研发经验，加入自己的一些扩展问题和场景验证。\nPattern模式匹配的定义 官网定义：Patterns are a syntactic category in the Dart language, like statements and expressions. A pattern represents the shape of a set of values that it may match against actual values.\n初看定义不太好理解，感觉有点绕，大概意思：模式是Dart语言的一种语法分类，就像声明和表达式一样。模式代表了一组实际值的形状，这个形状可以匹配到实际值。（特别注意：这里的Pattern和正则表达式没有任何关系！）\n有几个重要的概念：语法、形状、匹配\n语法：语法是一个编码语言的基础，可见模式在Dart中的重要程度。 形状：或者说结构，就是一组实际值是如何组织在一起的一种抽象（结构定义）。 匹配：根据一组值的形状，我们匹配到对应的值。 举一个List列表的例子，可能不是完全恰当，但是可以帮忙我们理解模式的这段定义：\n语法：final aList = [1, 2, 3];这个是定义列表的语句，其中aList代表变量名，列表采用[]包裹，元素采用,分隔，最后;结束等等，这些都是Dart中的语法。 形状：列表采用[]包裹，元素采用,分隔，元素类型int由Dart自动推导出来，这些都是这一组值的形状，就是长什么样。 匹配：aList[0] == 1根据列表的语法和形状，可以匹配到实际值。 Pattern模式的用途 Pattern模式主要作用：匹配值、解构值。匹配和解构可以同时作用，需要根据上下文和值的形状或结构具体来看。\n首先，模式可以让我们确定某个值的一些信息，包括：\n有一个明确的形状（或者结构）。 是一个明确的常量。 它和某个值相等（即可用于比较）。 有一个明确的类型。 然后，模式解构可以用一种便利的语法，把这个值进行分解，还可以绑定到某个变量上面。\n匹配 匹配就是校验某个值是否符合我们预期，换句话说，我们是在检测某个值是否符合某种结构且它的值与指定值相等。\n我们在编码过程中，很多逻辑其实都是在进行模式，举例如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 // 常数匹配：1 == number ？ switch (number) { case 1: print(\u0026#39;one\u0026#39;); } // 列表匹配：`obj`是一个2个元素列表 // 元素匹配：`obj`的2个元素值分别为`a`和`b` const a = \u0026#39;a\u0026#39;; const b = \u0026#39;b\u0026#39;; switch (obj) { case [a, b]: print(\u0026#39;$a, $b\u0026#39;); } 解构 当一个对象和一个模式相匹配，那么这个模式可以访问对象的数据，并可以把这个对象拆分成不同部分。换句话说，这个模式解构了这个对象。\n代码样例：如下代码，List列表解构，和解构模式中的嵌套匹配模式。\n1 2 3 4 5 6 7 8 9 10 11 12 // 列表解构：`[a, b, c]`结构`numList`对象 // 1. 匹配：`[a, b, c]`代表了具有3个元素的列表 // 2. 拆分：列表的3个元素，分别赋值给了新的变量`a`、`b`和`cs` var numList = [1, 2, 3]; var [a, b, c] = numList; print(a + b + c); // 列表模式：包含2个元素，且第1个元素是`a`或`b`，第2个元素赋值给变量`c` switch (list) { case [\u0026#39;a\u0026#39; || \u0026#39;b\u0026#39;, var c]: print(c); } 模式的应用场景 在Dart语言总，有几个常见可以使用模式：\n局部变量的申明和赋值。 for和for-in循环语句。 if-case和switch-case语句。 集合相关的控制流。 变量申明 我们可以在Dart允许本地变量声明的任何地方使用模式变量声明，模式变量申明必须由var或者final + 模式组成（这也是Dart的模式变量的语法）。\n代码样例：如下代码，使用模式，我们申明了a，b和c三个变量（并且完成赋值）。\n1 var (a, [b, c]) = (\u0026#39;str\u0026#39;, [1, 2]); 变量赋值 上小节变量申明的代码样例中，其实已经进行了模式变量赋值：首先进行模式匹配，然后解构对象，最终进行遍历赋值。\n代码样例：如下代码，采用变量赋值模式，轻松进行了2个元素值交换，而无需使用第3个变量。\n1 2 3 var (a, b) = (\u0026#39;left\u0026#39;, \u0026#39;right\u0026#39;); (b, a) = (a, b); print(\u0026#39;$a $b\u0026#39;); Switch和表达式模式 本文开头的样例其实已经提到，任何case的语句其实都包含了一个模式。在case中，可以应用任何的模式，变量赋值的作用域仅在Case语句内部。\nCase模式可以匹配失败，它允许控制流：\n匹配并解构switch对象。 匹配失败，则继续执行匹配。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 switch (obj) { // 匹配：1 == obj case 1: print(\u0026#39;one\u0026#39;); // 匹配：[first, last]区间 case \u0026gt;= first \u0026amp;\u0026amp; \u0026lt;= last: print(\u0026#39;in range\u0026#39;); // 匹配：Record记录，包含2个字段 // 赋值：`a`和`b`局部变量（作用域：本Case内部） case (var a, var b): print(\u0026#39;a = $a, b = $b\u0026#39;); default: } // 逻辑或模式：多个case共用 var isPrimary = switch (color) { Color.red || Color.yellow || Color.blue =\u0026gt; true, _ =\u0026gt; false }; switch (shape) { case Square(size: var s) || Circle(size: var s) when s \u0026gt; 0: print(\u0026#39;Non-empty symmetric shape\u0026#39;); } for和for-in循环模式 主要作用：迭代和解构集合。\n代码样例：如下代码，for循环匹配模式，并解构和赋值给变量。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Map\u0026lt;String, int\u0026gt; hist = { \u0026#39;a\u0026#39;: 23, \u0026#39;b\u0026#39;: 100, }; // 匹配：`MapEntry`类型，继续匹配`key`和`value`命名字段子模式 // 赋值：调用`key`和`value`的`getter`并赋值给`key`和`value`变量 for (var MapEntry(key: key, value: count) in hist.entries) { print(\u0026#39;$key occurred $count times\u0026#39;); } // 上诉代码的简写 for (var MapEntry(:key, value: count) in hist.entries) { print(\u0026#39;$key occurred $count times\u0026#39;); } 其他场景模式 本文前面的章节，我们主要是展示Dart类型模式和解构，当然也包括(a, b)内容交换的例子。本章进一步学习其他的场景模式。\n通过本章学习，主要解决我们几个问题：\n什么时候我们需要用到模式，我们为什么需要模式？ 模式主要解决什么类型的问题？ 什么样的模式最适合？ 解构多个返回值 在之前的学习中，Record记录的用途之一就是聚合多个值，并让函数返回多个值。模式能匹配并解构Record记录，并赋值给局部变量。\n代码样例：如下代码，userInfo(json)返回一个位置字段的记录，被解构并把位置值赋值给了name和age局部变量。\n1 2 3 4 5 6 7 // Record记录的使用 var info = userInfo(json); var name = info.$1; var age = info.$2; // Record解构和赋值 var (name, age) = userInfo(json); 解构类实例 对象模式能匹配命名的对象类型，可以解构对象的数据，并调用对象属性的getters方法进行赋值。\n代码样例：如下代码，命名类型Foo实例myFoo被解构并进行赋值给one和two变量。\n1 2 3 final Foo myFoo = Foo(one: \u0026#39;one\u0026#39;, two: 2); var Foo(:one, :two) = myFoo; print(\u0026#39;one $one, two $two\u0026#39;); 代数数据类型 对象解构和Switch模式有助于编写代数数据类型风格代码，它比较适合以下几种场景：\n有一群相关联的类型。 每个类型都有一个相同的操作，但这个操作对每个类型而言又有差异。 我们希望把这个操作能一把实现，而不是把实现散落在每个类型中。 样例代码：如下代码，Shape是一个父类，2个或更多的子类都有计算面积的方法，最终通过calculateArea()函数一把实现了。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 sealed class Shape {} class Square implements Shape { final double length; Square(this.length); } class Circle implements Shape { final double radius; Circle(this.radius); } double calculateArea(Shape shape) =\u0026gt; switch (shape) { Square(length: var l) =\u0026gt; l * l, Circle(radius: var r) =\u0026gt; math.pi * r * r }; 校验JSON格式 前面章节，我们学习了List和Map类型的匹配和解构，它们也适用于JSON的key-value键值对。\n代码样例：如下代码，在已知JSON格式的情况下，我们可以通过List和Map完成JSON的解构和赋值。\n1 2 3 4 5 var json = { \u0026#39;user\u0026#39;: [\u0026#39;Lily\u0026#39;, 13] }; var {\u0026#39;user\u0026#39;: [name, age]} = json; 但是，当JSON格式不明确的情况下，我们可以通过解构来校验JSON的格式。\n代码样例：如下代码，我们通过case模式，完成了JSON数据的校验和赋值。\n1 2 3 if (json case {\u0026#39;user\u0026#39;: [String name, int age]}) { print(\u0026#39;User $name is $age years old.\u0026#39;); } 如上代码，Case模式的匹配和赋值操作如下：\njson是一个非空的map，进一步匹配map模式。 json包含一个名为user的属性，且它是一个包含2个元素的list类型，list中2个元素类型分别为String和int。 最终，list的2个元素分别赋值给了name和age局部变量。 我的本博客原地址：https://ntopic.cn/p/2023100401\n","date":"2023-10-04T00:00:00Z","image":"https://ntopic.cn/p/2023100401/00_hua251d1ea2589f24e4f220f2de00fe773_52076_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/2023100401/","title":"Flutter/Dart第09天：Dart高级特性Pattern模式的概览和用法"},{"content":"Dart内置类型（共10类） Dart官网文档：https://dart.dev/language/built-in-types\nDart内置类型即Dart SDK自带的类型，我们编程过程中可直接使用的类型，主要分为10类：\n数值类型：包括int类、double类等。 字符串类型：即String类。 布尔类型：即bool类。 记录类型：即Record类，Dart 3中开始支持（最新版本的Java 21也支持）。 列表类型：即List类，同时也是数组。 Set类型：即Set类。 映射类型：即Map类。 字符类型：与字符相关处理。 符合类型：即Symbol类。 特殊值：null空值。 Dart中类型的其他一些规则：\nObject类是Dart中除Null之外的所有类的基类。扩展问题：Dart中顶层类是什么呢？ Enum类是Dart中所有枚举类的基类。扩展问题：Enum类是Object的子类吗？答案：是。 dynamic类型Dart的静态检测会失效，容易引发空安全等其他运行时错误，建议使用Object或者Object?代替。 Future和Stream支持异步编程。 Never一般用于总是抛出异常的函数，表明表达式用于无法成功执行。 void表明值不会被再次使用，一般用户函数返回值。 数字类型（int和double） Dart中数字类型类结构如下：\nint和double都是num类的子类。int为不超过64位的整数，double为64位双精度浮点数。\nnum类型支持操作：加+，减-，乘*，除/，abs()绝对值，ceil()向上取整，floor()向下取整操作。特别注意：位操作，如\u0026gt;\u0026gt;右移或者\u0026lt;\u0026lt;左移等位操作，仅int类型支持。\n代码样例：如下代码，数字类型定义。\n1 2 3 4 5 6 7 8 9 10 11 12 // int类型 var x = 1; var hex = 0xDEADBEEF; // double类型 var y = 1.1; var exponents = 1.42e5; double z = 1; // 等同于：double z = 1.0; // num类型：既可以是整数，又可以是浮点数 num x = 1; x += 2.5; 代码样例：如下代码，数字类型和字符串相互转换。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 // String -\u0026gt; int var one = int.parse(\u0026#39;1\u0026#39;); assert(one == 1); // String -\u0026gt; double var onePointOne = double.parse(\u0026#39;1.1\u0026#39;); assert(onePointOne == 1.1); // int -\u0026gt; String String oneAsString = 1.toString(); assert(oneAsString == \u0026#39;1\u0026#39;); // double -\u0026gt; String String piAsString = 3.14159.toStringAsFixed(2); assert(piAsString == \u0026#39;3.14\u0026#39;); 代码样例：如下代码，int类型位操作。\n1 2 3 assert((3 \u0026lt;\u0026lt; 1) == 6); // 0011 \u0026lt;\u0026lt; 1 == 0110 assert((3 | 4) == 7); // 0011 | 0100 == 0111 assert((3 \u0026amp; 4) == 0); // 0011 \u0026amp; 0100 == 0000 代码样例：如下代码，字面量数字属于编译时常量，因此字面量数字表达式也可以作为常量。\n1 2 3 const msPerSecond = 1000; const secondsUntilRetry = 5; const msUntilRetry = secondsUntilRetry * msPerSecond; 字符串类型（String） Dart中字符串是一个UTF-16的序列码，我们可以通过'单引号或者\u0026quot;双引号定义一个字符串、通过${}进行字符串插值、通过相邻字符串连接、使用+符号进行连接，通过3个单引号'''或者3个双引号\u0026quot;\u0026quot;\u0026quot;定义多行字符串，还可以通过增加前缀r定义原始字符串。\n代码样例：如下代码，字符串的各种定义方式。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 // 引号：单引号或双引号 var s1 = \u0026#39;Single quotes work well for string literals.\u0026#39;; var s2 = \u0026#34;Double quotes work just as well.\u0026#34;; var s3 = \u0026#39;It\\\u0026#39;s easy to escape the string delimiter.\u0026#39;; var s4 = \u0026#34;It\u0026#39;s even easier to use the other delimiter.\u0026#34;; // 插值 final name = \u0026#39;Tom\u0026#39;; var s5 = \u0026#39;My name is $name.\u0026#39;; assert(s5 == \u0026#39;My name is Tome.\u0026#39;); // 相邻字符串 var s6 = \u0026#39;String \u0026#39; \u0026#39;concatenation\u0026#39; \u0026#34; works even over line breaks.\u0026#34;; assert(s6 == \u0026#39;String concatenation works even over \u0026#39; \u0026#39;line breaks.\u0026#39;); var s7 = \u0026#39;The + operator \u0026#39; + \u0026#39;works, as well.\u0026#39;; assert(s7 == \u0026#39;The + operator works, as well.\u0026#39;); // 多行字符串：3个单引号或3个双引号 var s8 = \u0026#39;\u0026#39;\u0026#39; My name is Tom. I am a boy. \u0026#39;\u0026#39;\u0026#39;; var s9 = \u0026#34;\u0026#34;\u0026#34; My name is Tom. I am a boy. \u0026#34;\u0026#34;\u0026#34;; // 原始字符串 var s10 = r\u0026#39;My name is Tome.\\nI am a boy.\u0026#39;; 特别注意：==双等于号操作符在Dart中用于检测2个对象是否相等，如果是字符串，则是校验它们的序列码是否相同（这与Java有很多的区别）。\n代码样例：如下代码，字面量字符串是编译时常量，如果字符串插值表达式中的变量是常量，那么插值字符串也是常量。\n1 2 3 4 5 6 7 // 字面量字符串 const aConstString = \u0026#39;a constant string\u0026#39;; // 插值字符串常量 const aConstNum = 0; const aConstBool = true; const validConstString = \u0026#39;$aConstNum $aConstBool $aConstString\u0026#39;; 布尔类型（bool） Dart中只有2个对象是bool类型：true和false，且他们都是常量。\n代码样例：如下代码，在表达式中的布尔类型。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 // Check for an empty string. var fullName = \u0026#39;\u0026#39;; assert(fullName.isEmpty); // Check for zero. var hitPoints = 0; assert(hitPoints \u0026lt;= 0); // Check for null. var unicorn = null; assert(unicorn == null); // Check for NaN. var iMeantToDoThis = 0 / 0; assert(iMeantToDoThis.isNaN); 字符和符号（characters） Dart中字符代表字符串中一个Unicode编码单元。Unicode为世界上的每个字母、数字和符合定义了一个唯一数字值。\n由于Dart的字符串是UTF-16编码，因此Dart中表示Unicode有其对应的语法格式，常用的方式为\\uXXXX代表一个字符，其中XXXX是4位16进制的值。\n举例说明：心形字符♥的值为\\u2665，如果少于或者超过4位16禁止值，则用{}包裹起来，如笑脸字符😁的值为\\u{1f606}\n代码样例：如下所示，一般我们用characters库来操作字符。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 import \u0026#39;package:characters/characters.dart\u0026#39;; void main() { var hi = \u0026#39;Hi 🇩🇰\u0026#39;; print(hi); // 结果：Hi 🇩🇰 print(\u0026#39;The end of the string: ${hi.substring(hi.length - 1)}\u0026#39;); // 结果：The end of the string: ??? final charList = hi.characters; for (int i = 0; i \u0026lt; charList.length; i++) { print(\u0026#39;The character $i = ${charList.elementAt(i)}\u0026#39;); } // 结果： // The character 0 = H // The character 1 = i // The character 2 = // The character 3 = 🇩🇰 print(\u0026#39;The last character: ${hi.characters.last}\u0026#39;); // The last character: 🇩🇰 } 特别注意：如上代码，因为使用了characters库，因此需要在pubspec.yaml文件中增加依赖（如下完整配置）。\n1 2 3 4 5 6 7 name: NTopicDart environment: sdk: ^3.1.2 dependencies: characters: ^1.3.0 符号（Symbol） 有点类似于HTML中锚，#前缀，后面格式标识符。\n记录类型（Record） Dart官网文档：https://dart.dev/language/records\n特别注意：记录类型需要Dart 3.0才开始支持（Java从JDK 21开始支持）。\nRecord记录类型是一个匿名的、不可变的集合类型。有点儿像集合类型，它是多个对象元素的集合，不一样的是，记录大小固定、异构和类型固定的。记录是一个具体的值，因此它可以作为变量、函数入参或结果、也可以嵌套（即记录中的元素是一个记录），也可以作为List/Set/Map等集合类的元素。\n记录语法：\n用()括号包裹的，,逗号分隔的命名字段或者位置字段列表。 记录类型注解是用()括号包裹的，,号分隔的字段类型列表，它可以作为方法入参和结果的类型。 记录如果使用的是命名字段，那么字段名是记录定义的一部分，即2个记录，字段名不一样则是2个不同的记录。 记录的字段内置getters取值方法，但是没有setters设置方法，因为记录是不可变的。其中命名字段的字段名即取值方法，而位置字段则使用$\u0026lt;position\u0026gt;字段位置取值，并且忽略所有的命名字段。 记录比较：如果2个记录相对，那么他们必须有相同的字段、相同的字段类型，已经字段具有相同的值。 记录可作为函数入参和返回值：当作为返回值，其实是一个函数可以返回多个值。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 // 记录定义：位置字段+命名字段 var record = (\u0026#39;first\u0026#39;, a: 2, b: true, \u0026#39;last\u0026#39;); // 记录类型注解 (int, int) swap((int, int) record) { var (a, b) = record; return (b, a); } // 记录定义和初始化 (String, int) record; record = (\u0026#39;A string\u0026#39;, 123); ({int a, bool b}) record; record = (a: 123, b: true); // 命令字段：字段名不一样，属于不同的记录（recordAB和recordXY是不同类型） ({int a, int b}) recordAB = (a: 1, b: 2); ({int x, int y}) recordXY = (x: 3, y: 4); // 位置字段：类型一样即可，参数名不是记录定义的一部分 (int a, int b) recordAB = (1, 2); (int x, int y) recordXY = (3, 4); recordAB = recordXY; // 取值方法：位置字段和命名字段的取值方法 var record = (\u0026#39;first\u0026#39;, a: 2, b: true, \u0026#39;last\u0026#39;); print(record.$1); // 结果：first print(record.a); // 结果：2 print(record.b); // 结果：true print(record.$2); // 结果：last // 记录比较：相同字段、字段类型和字段值 (int x, int y, int z) point = (1, 2, 3); (int r, int g, int b) color = (1, 2, 3); print(point == color); // 结果：true // 记录比较：命名字段名称不同 ({int x, int y, int z}) point = (x: 1, y: 2, z: 3); ({int r, int g, int b}) color = (r: 1, g: 2, b: 3); print(point == color); // 结果：false 集合类型（list/set/map） Dart官方文档：https://dart.dev/language/collections\nlist、set、map的定义和用法：Flutter/Dart第03天：Dart可迭代集合\n代码样例：如下代码，通过...list的方式可以展开一个集合；定义集合时，可增加控制流。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 // 展开非null的集合 var list = [1, 2, 3]; var list2 = [0, ...list]; assert(list2.length == 4); // 展开为null的集合 var list3; var list2 = [0, ...?list3]; assert(list2.length == 1); // 条件判断 var nav = [\u0026#39;Home\u0026#39;, \u0026#39;Furniture\u0026#39;, \u0026#39;Plants\u0026#39;, if (promoActive) \u0026#39;Outlet\u0026#39;]; var nav = [\u0026#39;Home\u0026#39;, \u0026#39;Furniture\u0026#39;, \u0026#39;Plants\u0026#39;, if (login case \u0026#39;Manager\u0026#39;) \u0026#39;Inventory\u0026#39;]; var listOfInts = [1, 2, 3]; var listOfStrings = [\u0026#39;#0\u0026#39;, for (var i in listOfInts) \u0026#39;#$i\u0026#39;]; assert(listOfStrings[1] == \u0026#39;#1\u0026#39;); 泛型类型 Dart官网文档：https://dart.dev/language/generics\n泛型是编码中最常见的编码方式，本文不在赘述。\n类型别名（typedef） Dart官网文档：https://dart.dev/language/typedefs\n类型别名：通过typedef关键字，可以给一个类型增加一个别名类型（目前还没有看到别名存在的必要🐶）。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 // 类型别名 typedef IntList = List\u0026lt;int\u0026gt;; IntList il = [1, 2, 3]; // 类型别名：感觉简短 typedef ListMapper\u0026lt;X\u0026gt; = Map\u0026lt;X, List\u0026lt;X\u0026gt;\u0026gt;; Map\u0026lt;String, List\u0026lt;String\u0026gt;\u0026gt; m1 = {}; ListMapper\u0026lt;String\u0026gt; m2 = {}; // 类型别名：`不建议`把内联函数定义别名 typedef Compare\u0026lt;T\u0026gt; = int Function(T a, T b); int sort(int a, int b) =\u0026gt; a - b; void main() { assert(sort is Compare\u0026lt;int\u0026gt;); // True } 我的本博客原地址：https://ntopic.cn/p/2023100301\n","date":"2023-10-03T00:00:00Z","image":"https://ntopic.cn/p/2023100301/00_hua251d1ea2589f24e4f220f2de00fe773_52076_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/2023100301/","title":"Flutter/Dart第08天：Dart类型（内置类型、记录、集合、泛型和类型别名）"},{"content":"Dart官网文档：https://dart.dev/language/libraries\n重要说明：本博客基于Dart官网文档，但并不是简单的对官网进行翻译，在覆盖核心功能情况下，我会根据个人研发经验，加入自己的一些扩展问题和场景验证。\nDart中的库（Library） Dart语言对代码的复用下了不少功夫，如前面讲到的Mixin高级特性实现类级别代码复用。本文介绍另一种更宽广的代码复用：库。\nDart库可以认为是一组复用类、功能等的集合，它则代表了一个共享的代码模块，任何一个.dart文件都是一个库。\nDart库有一个隐含的规则：凡是以下划线_开头的标识符仅在本库内部可见，其他在库内外部均可见。（Dart没有private/protected/public可见域标识符，那么对应Java语言，下划线_开头为private，其他的为public，Dart没有protected）。\nDart库可以基本可分为3种，建议在使用时严格按照这3类导入库：\nDart中的内置库，即SDK中的库（对于Java语言如java.util.Map），以dart:作为命名空间。 Dart包中心仓库的库（对于Java语言为Maven中心仓库），以package:作为命名空间。 本地文件代码库，以相对路径或者绝对路径作为命名空间。当然，本地代码块也可以使用package作为命名空间，容易和第2中混淆，因此不推荐。 如何使用库（import关键字） 通过import关键字指定在一个库中使用另外一个命名空间的库。\n代码样例：如下代码，我们导入了Dart内置的html库、中心仓库get库，和本地库。\n1 2 3 4 5 6 7 8 // 内置库 import \u0026#39;dart:html\u0026#39;; // 中心仓库 import \u0026#39;package:get/get.dart\u0026#39;; // 本地代码库 import \u0026#39;../01-ntopic-hellodart.dart\u0026#39;; 指定库前缀（as关键字） 研发过程中，我们有时会导入有相同标识符，那么在使用过程中就会产生冲突，这时可以通过as关键字进行重命名库，同时在使用的时候，增加前缀进行区分。\n代码样例：如下代码，我们导入了lib1和lib2两个库，它们都有Element这个类。\n1 2 3 4 5 6 7 8 import \u0026#39;package:lib1/lib1.dart\u0026#39;; import \u0026#39;package:lib2/lib2.dart\u0026#39; as lib2; // `lib1`库中Element类 Element element1 = Element(); // `lib2`库中Element类，通过`lib2`前缀解决冲突 lib2.Element element2 = lib2.Element(); 导入部分库（show/hide关键字） 我们有时候只需要用到某个库的一部分代码或者类，或者需要排除某个库中指定的代码，通过增加show和hide关键字可实现。\n代码样例：如下代码，我们用到了lib1和lib2两个库，lib1通过show仅导入了foo这个标识符（类名、常量名等），而lib2通过hide导入除foo之外的其他所有标识符。\n1 2 3 4 5 // 仅导入`foo` import \u0026#39;package:lib1/lib1.dart\u0026#39; show foo; // 导入除`foo`之外所有 import \u0026#39;package:lib2/lib2.dart\u0026#39; hide foo; 延迟导入库（deferred as关键字） Web应用中通过延迟导入库，可以实现按需加载，如下几种常见的应用场景：\n减少Web应用的启动时间。 A/B测试，比如切流不同的算法库。 不常用的功能，比如可选页面、弹窗等。 代码样例：如下代码，通过deferred as关键字，实现延迟导入库，在实际使用库功能时需要导入库(如：await hello.loadLibrary();），在库加载完成之前，会阻塞代码执行。执行loadLibrary()方法仅实际加载库一次，后续调用该方法不会重复加载。\n1 2 3 4 5 6 7 8 9 // 1. 延迟导入库 import \u0026#39;package:greetings/hello.dart\u0026#39; deferred as hello; // 2. 实际使用时，使用库前缀 Future\u0026lt;void\u0026gt; greet() async { // 加载库，加载成功之前阻塞后续代码执行，多次调用仅加载一次 await hello.loadLibrary(); hello.printGreeting(); } 特别注意：\n延迟导入库中的常量，在该库导入之前并不存在，因此它们并不是导入库（即使用它们的文件）中的常量。 延迟导入库在导入之前并不存在，因此导入库不能使用它们的类型（包括类、枚举等），可以考虑把公共代码（如基础类、接口等）抽取出来作为单独的库，让延迟导入库和导入库共同导入使用。 当使用deferred as XXX延迟导入库时，Dart隐含的给loadLibrary()增加了XXX命名空间。如上代码，loadLibrary()方法是异步的，返回一个Future。 Dart中的关键字（共67个） Dart官方文档：https://dart.dev/language/keywords\nDart关键字一共包含67个，本博客不重复列出这67个关键字，仅对关键字进行说明：\n尽量避免使用这些关键字作为标识符，如有必要，表格中有1，2，3上标的关键字在部分常见可以使用作为标识符。 上标为1的关键字与上下文有关，离开了上下文，这些关键字可以作为标识符。如show和hide关键在只在导入库上下文文中用到，其他地方可以正常作为标识符。 上标为2的关键字是Dart内置的标识符，它们在大部分场景可以使用，但是禁止在类名、类型名和指定库的前缀（其实也是类型的一种）。 上标为3的关键字（共2个：await/yield）限制在异步编程场景使用，在标记为async/async*/sync*同步和异步函数内部，禁止使用。 我的本博客原地址：https://ntopic.cn/p/2023100201\n","date":"2023-10-02T00:00:00Z","image":"https://ntopic.cn/p/2023100201/00_hua251d1ea2589f24e4f220f2de00fe773_52076_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/2023100201/","title":"Flutter/Dart第07天：Dart基础语法详解（库、导入和关键字）"},{"content":"Dart官网文档：https://dart.dev/language/variables\n重要说明：本博客基于Dart官网文档，但并不是简单的对官网进行翻译，在覆盖核心功能情况下，我会根据个人研发经验，加入自己的一些扩展问题和场景验证。\nDart中的变量 变量是一个对象的引用，引用名就是变量的名称；就算引用是null的变量也一样。\n变量有3种定义方式：var关键字，显示类型和Object/dynamic类型。\n1 2 3 4 var varName = \u0026#39;Tom\u0026#39;; String strName = \u0026#39;Tom\u0026#39;; Object objName = \u0026#39;Tom\u0026#39;; dynamic dynName = \u0026#39;Tom\u0026#39;; 最佳实战：对于局部变量，优先使用var关键字，其次为显示类型，再次为Object，不推荐使用dynamic（原因：容易引发运行时错误，后续的学习会讲到）。\n当使用var关键字定义的变量，Dart会根据值内容，推导出变量的实际类型，如上诉代码varName变量最终为String类型。\nNull safety空安全 Dart语言强制健壮空安全。空安全可以防止意外使用null而导致的错误（还记得在Java编程中，在很多对象使用的地方，我们都需要进行null判断，以防止NPE的发生）。Dart在编译期间，就会进行null潜在错误检测分析，从而防止这些错误的发生（注意：并不是所有场景都能检测分析到，后面章节会讲到）。\n代码样例：如下代码，变量strName的默认值为null，在其他编程语言如Java语言中，下面的代码是合法的，但是在运行时，当执行strName.length时会引发NPE异常；但是在Dart语言中，以下代码是非法的（无法编译），因此阻止发生NPE等这些潜在的错误。\n1 2 String strName; print(strName.length) Dart为了强制执行空安全，有3个关键改变：\n如果明确希望某个变量、参数或者其他组件是可以为null的，那么需要在类型后面增加一个?标识： 1 2 String? name // `name`的值可能为`null`, 或者为某个字符串 String name // `name`的值只能是某个字符串，不能为`null` Dart的变量在使用之前，必须被初始化。可空变量（含有?标识）的默认值为null，即默认初始化为null，因此无需显示的赋值初始化；而非空变量因没有默认值，因此必须显示赋值初始化。 禁止直接访问可空类型的属性或者表达式方法，包括访问null对象的hashCode和toString()方法（记住：Dart中一起皆对象，null也是），也会引发错误。 Dart空安全通过以上3个关键改变，保证null潜在错误在代码编写阶段就能被前置发现，而不是等到代码运行时。\n变量默认值 第2章节中，其实已经提到一点：任何变量在使用之前，必须被初始化；可空变量因为默认值为null，因此可无需显示初始化；非空变量必须显示初始化。\n特别注意：非空变量只需要确保它在被使用时已经初始化即可，而不是必须在申明的时候。如下代码，变量lineCount在申明时并未初始化，但是在print被使用时，Dart语言检测到它已经被初始化了，因此如下代码是空安全有效代码。\n1 2 3 4 5 6 7 8 9 10 11 // 申明：未被初始化 int lineCount; if (weLikeToCount) { lineCount = countLines(); } else { lineCount = 0; } // 使用：Dart检测到已经被初始化，因此是可以使用 print(lineCount); late延迟变量 顶级变量和类变量会延迟初始化，当第一次使用到这些变量时，初始化代码的逻辑才会被执行（即：延迟初始化）。\n在大多数情况下，Dart可以检测并分析一个非空变量在使用时已经初始化，但是在有些场景下，Dart无法检测分析或者检测分析会失效。最常见的2种场景：顶级变量和实例变量，Dart无法确定它们在被使用时是否已经被初始化了。\n如果我们明确一个非空变量在被使用之前能完成初始化（但Dart却无法检测到），可通过增加late关键字，告诉Dart该变量为延迟变量，在被使用之前确保能被初始化。当然，对于late延迟变量，在被使用时却并没有初始化，那么使用它同样会导致运行时错误。\n1 2 3 4 5 6 late String description; void main() { description = \u0026#39;Feijoada!\u0026#39;; print(description); } late延迟变量主要处理以下2种场景：\n这些变量并不是必须的，同时初始化它们非常耗时或者浪费资源（如网络交互等）。 在初始化实例变量时，需要用到实例本身。 代码样例：如下代码，当变量temperature在第1次被使用时，才会被调用readThermometer()方法，即该方法仅仅被调用1次。\n1 late String temperature = readThermometer(); final变量和const常量 最佳实践：如果我们明确一个变量在被初始化之后，它的引用值再也不会变化，那么使用final或者const修饰，而不是使用var者显示类型。\nconst变量隐式为final变量 const变量是编译期的常量变量 实例变量可以是final变量但不能是const变量（原因：实例在运行时才能确定，因此其变量无法作为编译期常量） 1 2 3 4 final name = \u0026#39;Bob\u0026#39;; final String nickname = \u0026#39;Bobby\u0026#39;; const bar = 1000000; const double atm = 1.01325 * bar; 特别注意：const不仅可以申明如上代码的常量变量，它也可以用于申明创建常量值，也可以用于申明创建常量值的构造器（还记得第2天学习内容：const构造函数？）；同时，任何变量，都可以有常量值。\n1 2 3 4 5 6 7 // 1. 常量值 var foo = const []; final bar = const []; const baz = []; // 等同：`const []` // 2. 非final/const变量，它的常量值可以更新 foo = const [1, 2, 3]; 特别注意：常量的定义，可以包含类型检测、类型转换、集合过滤和集合展开操作符。\n1 2 3 4 5 6 7 8 9 10 11 // `i`是一个`Object`类型常量，它的值是int数字值 const Object i = 3; // 1. 类型转换 const list = [i as int]; // 2. 类型检测和集合过滤 const map = {if (i is int) i: \u0026#39;int\u0026#39;}; // 3. 类型检测、集合过滤、集合展开 const set = {if (list is List\u0026lt;int\u0026gt;) ...list}; final变量和const常量总结：\nfinal变量不可修改，但是它的值是可以更新。 const常量不可修改，同时它的值也不可以更新，即它的值是不可变的（immutable）。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 void main() { // 1. final变量 final finalList = [1, 2, 3]; print(\u0026#39;1.1 final变量: $finalList\u0026#39;); // 输出：1.1 final变量: [1, 2, 3] finalList ..add(4) ..add(5); print(\u0026#39;1.2 final变量: $finalList\u0026#39;); // 输出：1.2 final变量: [1, 2, 3, 4, 5] // 2. const常量 const constList = [\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;]; print(\u0026#39;2.1 const常量: $constList\u0026#39;); // 输出：2.1 const常量: [a, b, c] constList ..add(\u0026#39;c\u0026#39;) ..add(\u0026#39;d\u0026#39;); print(\u0026#39;2.2 const常量: $constList\u0026#39;); // 错误： // Unhandled exception: // Unsupported operation: Cannot add to an unmodifiable list // #0 UnmodifiableListMixin.add (dart:_internal/list.dart:114:5) } 我的本博客原地址：https://ntopic.cn/p/2023100101\n","date":"2023-10-01T00:00:00Z","image":"https://ntopic.cn/p/2023100101/00_hua251d1ea2589f24e4f220f2de00fe773_52076_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/2023100101/","title":"Flutter/Dart第06天：Dart基础语法详解（变量）"},{"content":"Dart官网文档：https://dart.dev/language/mixins\n重要说明：本博客基于Dart官网文档，但并不是简单的对官网进行翻译，在覆盖核心功能情况下，我会根据个人研发经验，加入自己的一些扩展问题和场景验证。\nMixin目的和使用方法（with） 官网文档：Mixins are a way of defining code that can be reused in multiple class hierarchies. They are intended to provide member implementations en masse.\n大概意思：Mixin是一种定义可在多个类层次结构中复用代码的方法。Mixin的目标是为这些类提供一批成员实现（类属性+类方法）。\n总结起来：使用Mixin可以让代码被其他类所使用（包括属性和方法）。\n使用方法：通过mixin关键字定义一个Mixin类；通过with关键字，一个类可以同时复用多个mixin成员实现。\n代码样例：如下代码，ClassA同时拥有了MixinOne+MixinTwo+MixinThree这3个Mixin的所有成员属性和类方法（感觉有的像多继承？）。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 mixin MixinOne { ...... } mixin MixinTwo { ...... } mixin MixinThree { ...... } class ClassA extends SupperClass with MixinOne, MixinTwo, MixinThree { ...... } Mixin的使用有哪些约束呢？\nMixin不能继承其他Mixin或者抽象类。 Mixin不能有构造方法，也就是Mixin不能被实例化。 Mixin限定/继承其他类型（on） 为了更好的维护Mixin这些可复用的代码，我们有时需要严格限定使用Mixin的类型，通过on关键字达到目的。\n代码样例：如下代码，MixinFine通过on关键字限定使用它的类型是SupperClass，凡是使用MixinFine的类，必须extends继承SupperClass这个Mixin限定的类型。\n1 2 3 4 5 6 7 8 9 10 11 class SupperClass { ...... } mixin MixinFine on SupperClass { ...... } class ClassFine extends SupperClass with MixinFine { ...... } mixin class介绍和使用（类+Mixin） 我们通过mixin定义一个Mixin，通过class定义一个类；那么通过mixin class就可以定义一个mixin和一个类，它们具有相同的名字和相同的类型。\nMixin和类的所有约束，在mixin class同时生效，包括如下：\nMixin不支持extends继承其他类和with复用其他Mixin，因此mixin class也不能有extends继承其他类和with复用其他Mixin 普通类不支持通过on关键字限定可使用类型，因此mixin class也不支持on关键字（但是我们可以通过abstract达到此目的）。 代码样例：如下代码，mixin class可通过with关键字当成Mixin被使用，也可通过extends关键字当成类被继承使用。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 abstract mixin class Musician { // 含有abstract方法，使用它的类必须实现本方法 void playInstrument(String instrumentName); void playPiano() { playInstrument(\u0026#39;Piano\u0026#39;); } void playFlute() { playInstrument(\u0026#39;Flute\u0026#39;); } } class Virtuoso with Musician { // with关键字，Musician作为一个Mixin被使用 void playInstrument(String instrumentName) { print(\u0026#39;Plays the $instrumentName beautifully\u0026#39;); } } class Novice extends Musician { // extends关键字，Musician作为一个类被继承 void playInstrument(String instrumentName) { print(\u0026#39;Plays the $instrumentName poorly\u0026#39;); } } 扩展问题：Mixin如何解决二义性？（覆盖） 通过上面的说明，一个类可以使用多个Mixin的实现，那么有个问题：他们是如何解决二义性的呢？\n样例说明：如下代码，我们有2个Mixin，他们的属性和方法都是相同，同时使用他们时，最终的属性和方法是哪个Mixin的呢？\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 mixin MixinA { String className = \u0026#34;MixinA\u0026#34;; void log() { print(className); } } mixin MixinB { String className = \u0026#34;MixinB\u0026#34;; void log() { print(className); } } class ClassMixinAB with MixinA, MixinB { } class ClassMixinBA with MixinB, MixinA { } void main() { ClassMixinAB mixinAB = ClassMixinAB(); mixinAB.log(); // 结果：MixinB ClassMixinBA mixinBA = ClassMixinBA(); mixinBA.log(); // 结果：MixinA } 通过上面2个代码样例，基本可以判断：Mixin解决二义性的方式非常粗暴，后面Mixin覆盖前面Mixin！！！\nMixin使用场景：打印State生命周期日志 Mixin通过代码复用，可以应用在很多的应用场景。下面代码样例，可以在Flutter组件生命周期逻辑执行之后，打印响应的日志。\n样例代码：LogStateMixin通过on关键字限定/继承了State类型，内部的方法，均通过super代理了State的内容，同时打印相应的日志。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 mixin LogStateMixin\u0026lt;T extends StatefulWidget\u0026gt; on State\u0026lt;T\u0026gt; { @override void initState() { super.initState(); print(\u0026#34;====initState====\u0026#34;); } @override void dispose() { super.dispose(); print(\u0026#34;====dispose====\u0026#34;); } // 其他方法...... } 在凡是想要监听组件的生命周期的组件中，可以使用上面的Mixin即可，无其他侵入代码，特别适合在Flutter应用研发过程中，通过日志观测组件的生命周期：\n1 2 3 class _MinePageState extends State\u0026lt;MinePage\u0026gt; with LogStateMixin\u0026lt;MinePage\u0026gt; // 我的页面逻辑，无需关心日志.... } 这样在页面初始化、销毁的时候，打印响应的日志。\n我的本博客原地址：https://ntopic.cn/p/2023093001\n","date":"2023-09-30T00:00:00Z","image":"https://ntopic.cn/p/2023093001/00_hua251d1ea2589f24e4f220f2de00fe773_52076_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/2023093001/","title":"Flutter/Dart第05天：Dart特殊特性Mixin详解"},{"content":"Dart官网代码实验室：https://dart.dev/codelabs/async-await\n重要说明：本博客基于Dart官网代码实验室，但并不是简单的对官网文章进行翻译，我会根据个人研发经验，在覆盖官网文章核心内容情况下，加入自己的一些扩展问题和问题演示和总结，包括名称解释、使用场景说明、代码样例覆盖、最后完整的场景编程等。\n启蒙：错误的异步编程样例 下面是一个错误的异步编程样例，大概过程：通过模拟网络API获取订单ID，然后组织订单ID文案，最终输出问题。\n我们期望最终输出的是正确的订单ID文案，可结果并不符合我们的期望：订单ID并不是T2023092900001，而是Instance of \u0026lsquo;Future\u0026lt;String\u0026gt;\u0026rsquo;\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 // 1.1 创建订单消息 String createOrderMessage() { var order = fetchOrderID(); return \u0026#39;订单ID: $order\u0026#39;; } // 1.2 获取订单ID内容 Future\u0026lt;String\u0026gt; fetchOrderID() =\u0026gt; // 假设获取订单ID是一次网络交互，处理过程需要2秒钟，因此模拟了2秒钟返回订单ID Future.delayed( const Duration(seconds: 2), () =\u0026gt; \u0026#39;T2023092900001\u0026#39;, ); void main() { // 1. 启蒙：错误的异步编程样例 final message = createOrderMessage(); print(message); // 结果：订单ID: Instance of \u0026#39;Future\u0026lt;String\u0026gt;\u0026#39; } 同步编程和异步编程说明：\n同步编程：按照代码块顺序执行代码块，前面代码块没有执行完成之前，后面代码被阻塞。 异步编程：异步操作代码块完成初始化之后，后面代码块就可以执行了（非阻塞），异步代码块执行完成（如上面样例等待2秒钟），执行完成回调代码块（即回调）。 Future异步结果说明 异步操作的结果都是Future类的实例（https://api.dart.cn/stable/3.1.3/dart-async/Future-class.html），异步操作有2种状态：未完成和完成状态。调用异步代码块（或函数），返回值都是未完成状态的结果。\n未完成状态：调用一个异步函数，返回一个Future\u0026lt;T\u0026gt;结果，在异步操作执行结束或者执行出错之前的状态。 完成状态：异步操作执行结束正常返回结果或者执行出错，都是完成状态。正常完成的返回结果即Futrue\u0026lt;T\u0026gt;的T（如：Future\u0026lt;String\u0026gt;），如果无需返回结果，则为void，即异步函数的返回值为Future\u0026lt;void\u0026gt;。如果异步函数执行出错，则返回结果是一个Error，可以进行捕获。 下面2个代码样例，分别为返回值为void和出错结果：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 // 2.1 异步操作无返回值 Future\u0026lt;void\u0026gt; fetchOrderID2() { // 模拟了2秒钟输出了订单ID return Future.delayed(const Duration(seconds: 2), () =\u0026gt; print(\u0026#39;ID2:T2023092900002\u0026#39;)); } // 2.2 异常操作返回错误 Future\u0026lt;void\u0026gt; fetchOrderID3() { return Future.delayed(const Duration(seconds: 2), () =\u0026gt; throw Exception(\u0026#39;网络异常\u0026#39;)); } void main() { // 2. Future/async/await异步结果说明 fetchOrderID2(); print(\u0026#39;2. fetchOrderID2-Future/async/await异步结果说明...\u0026#39;); // 结果： // 2. fetchOrderID2-Future/async/await异步结果说明... // ID2:T2023092900002 fetchOrderID3(); print(\u0026#39;2. fetchOrderID3-Future/async/await异步结果说明...\u0026#39;); // 结果： // 2. fetchOrderID3-Future/async/await异步结果说明... // Unhandled exception: // Exception: 网络异常 } async/await异步操作定义和使用 async定义一个异步操作，而await则是使用一个异步操作的结果。\n在应用async和await是，有2点需要遵守的基本规则：\n如果需要定义一个异步函数，则在函数体之前增加async关键字 只有在异步函数中（即函数体前有async关键字的函数），await关键字才会生效（也就是await必须配合async使用） 定义一个异步函数方法样例（即增加async关键字）：\n1 2 3 4 5 6 7 8 9 10 11 12 13 // 1. 同步函数转异步函数：无返回结果 // 1.1 同步函数 void funcVoid() {} // 1.2 异步返回 Future\u0026lt;void\u0026gt; funcVoid() async {} // 2. 同步函数转异步函数：有返回结果 // 2.1 同步函数 String funcResult() {} // 2.2 异常函数 Future\u0026lt;String\u0026gt; funcResult() async {} 接下来，我们来重写第1张中，异步函数代码，以使结果符合我们预期：\n获取订单ID函数fetchOrderID()前，增加await关键字。 创建订单消息的函数createOrderMessageV2()，增加async关键字变成异步函数，同返回结果由String变成Future\u0026lt;String\u0026gt;异步结果。 同样的，main()函数调用了异步函数，因此也需要增加async关键字。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 // 3. async/await异步操作定义和使用 Future\u0026lt;String\u0026gt; createOrderMessageV2() async { var order = await fetchOrderID(); return \u0026#39;订单ID: $order\u0026#39;; } void main() async { // 3. async/await异步操作定义和使用 final messageV2 = await createOrderMessageV2(); print(\u0026#39;3. async/await异步操作定义和使用\u0026#39;); print(\u0026#39;$messageV2\u0026#39;); // 结果： // 3. async/await异步操作定义和使用 // 订单ID: T2023092900001 } try/catch异步操作的异常处理 在第2章节中，fetchOrderID3()异步方法会抛出异常，从而中断程序处理。异步操作的异常，我们也可以和同步函数调用一样，通过try-catch的方式进行处理。\n下面我们把fetchOrderID3()使用的地方进行改写，捕获异常从而不中断我们的程序：\n1 2 3 4 5 6 7 8 9 10 11 void main() async { // 4. try/catch异步操作的异常处理 try { await fetchOrderID3(); print(\u0026#39;4. try/catch异步操作的异常处理.\u0026#39;); } catch (e) { print(\u0026#39;4. try/catch异步操作的异常处理: $e\u0026#39;); } // 结果：4. try/catch异步操作的异常处理: Exception: 网络异常 } 场景编程：异步编程的大合唱 应用场景假设：对当前登录的用户打个招呼，同时用户退出登录。因为退出登录操作可被降级，因此退出登录需要捕获所有异常。\n获取当前登录的用户名，因为是网络API操作，因此是异步操作。 退出登录时，需要获取当前缓存的用户名，设计到存储操作，因此也是异步操作。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 // 5.1 组装用户欢迎语 String makeGreeting(String userName) { return \u0026#39;欢迎你 $userName\u0026#39;; } // 5.2 获取用户名，异步操作 Future\u0026lt;String\u0026gt; fetchUserName() async { return Future.delayed(const Duration(seconds: 2), () =\u0026gt; \u0026#39;NTopic.CN\u0026#39;); } // 5.3 用户登录-打声招呼 Future\u0026lt;String\u0026gt; greeting() async { final userName = await fetchUserName(); return makeGreeting(userName); } // 5.4 用户退出-再见 Future\u0026lt;String\u0026gt; goodbye() async { final userName = await fetchUserName(); return \u0026#39;$userName 下次再见！\u0026#39;; } void main() async { // 5. 场景编程：异步编程的大合唱 print(\u0026#39;5. 场景编程：异步编程的大合唱...\u0026#39;); print(await greeting()); try { print(await goodbye()); } catch (e) { print(\u0026#39;5. 场景编程：异步编程的大合唱-Goodbye异常: $e\u0026#39;); } // 结果： // 5. 场景编程：异步编程的大合唱... // 欢迎你 NTopic.CN // NTopic.CN 下次再见！ } 最后-完整的实例代码 本文介绍的完整的实例代码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 // 第04天：异步编程 // 1.1 创建订单消息 String createOrderMessage() { var order = fetchOrderID(); return \u0026#39;订单ID: $order\u0026#39;; } // 1.2 获取订单ID内容 Future\u0026lt;String\u0026gt; fetchOrderID() =\u0026gt; // 假设获取订单ID是一次网络交互，处理过程需要2秒钟，因此模拟了2秒钟返回订单ID Future.delayed( const Duration(seconds: 2), () =\u0026gt; \u0026#39;T2023092900001\u0026#39;, ); // 2.1 异步操作无返回值 Future\u0026lt;void\u0026gt; fetchOrderID2() { // 模拟了2秒钟输出了订单ID return Future.delayed(const Duration(seconds: 2), () =\u0026gt; print(\u0026#39;ID2:T2023092900002\u0026#39;)); } // 2.2 异常操作返回错误 Future\u0026lt;void\u0026gt; fetchOrderID3() { return Future.delayed(const Duration(seconds: 2), () =\u0026gt; throw Exception(\u0026#39;网络异常\u0026#39;)); } // 3. async/await异步操作定义和使用 Future\u0026lt;String\u0026gt; createOrderMessageV2() async { var order = await fetchOrderID(); return \u0026#39;订单ID: $order\u0026#39;; } // 5.1 组装用户欢迎语 String makeGreeting(String userName) { return \u0026#39;欢迎你 $userName\u0026#39;; } // 5.2 获取用户名，异步操作 Future\u0026lt;String\u0026gt; fetchUserName() async { return Future.delayed(const Duration(seconds: 2), () =\u0026gt; \u0026#39;NTopic.CN\u0026#39;); } // 5.3 用户登录-打声招呼 Future\u0026lt;String\u0026gt; greeting() async { final userName = await fetchUserName(); return makeGreeting(userName); } // 5.4 用户退出-再见 Future\u0026lt;String\u0026gt; goodbye() async { final userName = await fetchUserName(); return \u0026#39;$userName 下次再见！\u0026#39;; } void main() async { // 1. 启蒙：错误的异步编程样例 final message = createOrderMessage(); print(message); // 2. Future/async/await异步结果说明 fetchOrderID2(); print(\u0026#39;2. fetchOrderID2-Future/async/await异步结果说明...\u0026#39;); // fetchOrderID3(); print(\u0026#39;2. fetchOrderID3-Future/async/await异步结果说明...\u0026#39;); // 3. async/await异步操作定义和使用 final messageV2 = await createOrderMessageV2(); print(\u0026#39;3. async/await异步操作定义和使用\u0026#39;); print(\u0026#39;$messageV2\u0026#39;); // 4. try/catch异步操作的异常处理 try { await fetchOrderID3(); print(\u0026#39;4. try/catch异步操作的异常处理.\u0026#39;); } catch (e) { print(\u0026#39;4. try/catch异步操作的异常处理: $e\u0026#39;); } // 5. 场景编程：异步编程的大合唱 print(\u0026#39;5. 场景编程：异步编程的大合唱...\u0026#39;); print(await greeting()); try { print(await goodbye()); } catch (e) { print(\u0026#39;5. 场景编程：异步编程的大合唱-Goodbye异常: $e\u0026#39;); } } 我的本博客原地址：https://ntopic.cn/p/2023092901\n","date":"2023-09-29T00:00:00Z","image":"https://ntopic.cn/p/2023092901/00_hua251d1ea2589f24e4f220f2de00fe773_52076_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/2023092901/","title":"Flutter/Dart第04天：Dart异步编程（Future和async/await）"},{"content":"Dart官网代码实验室：https://dart.dev/codelabs/iterables\n重要说明：本博客基于Dart官网代码实验室，但并不是简单的对官网文章进行翻译，我会根据个人研发经验，在覆盖官网文章核心内容情况下，加入自己的一些扩展问题和问题演示和总结，包括名称解释、使用场景说明、代码样例覆盖等。\n可迭代集合说明 什么是集合？集合代表一组对象的组合，集合中的对象一般称为元素，元素的数量可以是0个（即空集合），也可以有多个。\n什么是迭代？迭代即顺序访问，即这个集合中的元素可从头到尾进行顺序访问（一般在循环遍历中使用）。在Java中，我们知道有个Iterable迭代类，在Dart中也有这个类（https://api.dart.dev/stable/3.1.3/dart-core/Iterable-class.html），我们用的最多的就是List和Set接口，他们是迭代集合的基础，也是一个应用程序的基础。\nMap是可迭代集合吗？Map类代表了一组元素，因此它是一个集合。但Map类没有实现Iterable类（https://api.dart.dev/stable/3.1.3/dart-core/Map-class.html），因此它不可迭代，也就是说：Map是不可迭代的集合。但是它的元素集合（Map#entries）、键集合（Map#keys）和值集合（Map#values）都是可迭代集合。\n迭代和集合访问元素的不同方式：迭代通过elementAt(index)方法访问元素，而集合可以通过[index]下标的方法访问元素：\n1 2 3 4 5 6 7 8 void main() { // 1. 迭代和集合访问元素 final List\u0026lt;int\u0026gt; alist = [1, 2, 3]; final Iterable\u0026lt;int\u0026gt; iterable = alist; print(\u0026#39;1. 迭代和集合访问元素: ${iterable.elementAt(2)} \u0026lt;-\u0026gt; ${alist[2]}\u0026#39;); // 结果：1. 迭代和集合访问元素: 3 \u0026lt;-\u0026gt; 3 } 可迭代集合元素访问方法 可迭代集合元素的访问方式有很多种，包括for循环，集合的第1个元素，集合的最后1个元素，寻找符合条件的第1个元素等。\nfor-in循环访问集合元素 这种方式使用最多了，各种编程语言基本类似：\n1 2 3 4 5 6 7 8 9 10 11 12 13 void main() { final List\u0026lt;int\u0026gt; alist = [1, 2, 3]; // 2.1. for循环访问集合元素 for (final element in alist) { print(\u0026#39;2.1. for循环访问集合元素: $element\u0026#39;); } // 结果： // 2.1. for循环访问集合元素: 1 // 2.1. for循环访问集合元素: 2 // 2.1. for循环访问集合元素: 3 } first第一个和last最后一个元素：空集合异常 通过first和last属性，可直接访问集合的第1个和最后1个元素：\n1 2 3 4 5 6 7 8 void main() { final List\u0026lt;int\u0026gt; alist = [1, 2, 3]; // 2.2 first第一个和last最后一个元素 print(\u0026#39;2.2. first第一个和last最后一个元素: first=${alist.first}, last=${alist.last}\u0026#39;); // 结果：2.2. first第一个和last最后一个元素: first=1, last=3 } 扩展问题：如果集合只有1个元素，或者集合是空集合，first和last返回的内容是什么呢？\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 void main() { final List\u0026lt;int\u0026gt; oneList = [1]; print(\u0026#39;2.2. first第一个和last最后一个元素: one.first=${oneList.first}, one.last=${oneList.last}\u0026#39;); // 结果：2.2. first第一个和last最后一个元素: one.first=1, one.last=1 final List\u0026lt;int\u0026gt; emptyList = []; print(\u0026#39;2.2. first第一个和last最后一个元素: empty.first=${emptyList.first}, empty.last=${emptyList.last}\u0026#39;); // 结果：Bad state: No element } // 异常如下： Unhandled exception: Bad state: No element #0 List.first (dart:core-patch/growable_array.dart:343:5) 结论：只有1个元素的集合，first和last返回值相同，均为唯一的那个元素；对于空集合，first或者last均抛出异常！\nfirstWhere()/orElse符合条件的第1个元素 断言：一个返回true/false的表达式、方法或者代码块。firstWhere()的本质就是遍历集合，对每个元素进行断言，然后返回第一个断言为true的元素。\n1 2 3 4 5 6 7 8 9 void main() { final List\u0026lt;int\u0026gt; alist = [1, 2, 3]; // 2.3. firstWhere()符合条件的第1个元素 final int firstWhere = alist.firstWhere((element) =\u0026gt; element \u0026gt; 1); print(\u0026#39;2.3. firstWhere()符合条件的第1个元素: $firstWhere\u0026#39;); // 结果：2.3. firstWhere()符合条件的第1个元素: 2 } 扩展问题：如果过滤条件均不满足（即每个元素断言均返回false），则返回结果是什么呢？\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 void main() { final List\u0026lt;int\u0026gt; alist = [1, 2, 3]; // 2.3. firstWhere()符合条件的第1个元素 final int firstWhere2 = alist.firstWhere((element) =\u0026gt; element \u0026gt; 3); print(\u0026#39;2.3. firstWhere()符合条件的第1个元素2: $firstWhere2\u0026#39;); // 结果：Bad state: No element } // 异常如下： Unhandled exception: Bad state: No element #0 ListBase.firstWhere (dart:collection/list.dart:132:5) 结论：和空集合一样，当firstWhere()无法匹配到任何元素时，会抛出异常！\n那么，当无法匹配到任何元素时，有没有办法不抛出异常，而是返回一个默认值呢？\n答案是有的：firstWhere()断言之后，增加orElse默认值的命名参数，它是一个函数！\n1 2 3 4 5 6 7 8 9 10 11 12 void main() { final List\u0026lt;int\u0026gt; alist = [1, 2, 3]; // 2.3. firstWhere()符合条件的第1个元素 final int firstWhere2 = alist.firstWhere( (element) =\u0026gt; element \u0026gt; 3, orElse: () =\u0026gt; -1, ); print(\u0026#39;2.3. firstWhere()符合条件的第1个元素2: $firstWhere2\u0026#39;); // 结果：2.3. firstWhere()符合条件的第1个元素2: -1 } any()/every()集合检测（有趣的结果和源代码） 当我们需要检测集合中是否存在符合某个条件的元素，或者所有元素是否符合某个条件。在Dart语言中，我们可以使用any()和every()这两个集合条件检测方法，来达到我们的目的。\nany()方法：集合中存在任一一个元素符合条件 every()方法：集合中的所有元素均符合条件 1 2 3 4 5 6 7 8 9 10 11 void main() { final List\u0026lt;int\u0026gt; alist = [1, 2, 3]; // 2.4. any()/every()集合条件检测 final bool anyGtTwo = alist.any((element) =\u0026gt; element \u0026gt; 2); final bool everyGtZero = alist.every((element) =\u0026gt; element \u0026gt; 0); final bool everyGtTwo = alist.every((element) =\u0026gt; element \u0026gt; 2); print(\u0026#39;2.4. any()/every()集合条件检测: anyGtTwo=$anyGtTwo, everyGtZero=$everyGtZero, everyGtTwo=$everyGtTwo\u0026#39;); // 结果：2.4. any()/every()集合条件检测: anyGtTwo=true, everyGtZero=true, everyGtTwo=false } 扩展问题：如果是个空集合，any()和every()的结果如何，会抛出异常吗？\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 void main() { final elist = \u0026lt;int\u0026gt;[]; final bool anyGtZero = elist.any((element) =\u0026gt; element \u0026gt; 0); final bool anyLtEqZero = elist.any((element) =\u0026gt; element \u0026lt;= 0); print(\u0026#39;2.4. any()-空集合条件检测: anyGtZero=$anyGtZero, anyLtEqZero=$anyLtEqZero\u0026#39;); // 结果：2.4. any()-空集合条件检测: anyGtZero=false, anyLtEqZero=false final bool evyGtZero = elist.every((element) =\u0026gt; element \u0026gt; 0); final bool evyLtEqZero = elist.every((element) =\u0026gt; element \u0026lt;= 0); print(\u0026#39;2.4. every()-空集合条件检测: evyGtZero=$evyGtZero, evyLtEqZero=$evyLtEqZero\u0026#39;); // 结果：2.4. every()-空集合条件检测: evyGtZero=true, evyLtEqZero=true } 有趣的结论：\n针对空集合，any()和every()这2个集合条件检测方法，不会抛出异常！ 针对空集合，any()不论断言结果如何，返回值均为false（这个结果比较容易理解） 针对空集合，every()不论断言结果如何，返回值均为true（这个结果有的奇怪！） 我们打开源代码，看看any()和every()的发现：any()的默认返回值为false，every()的默认返回值true\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 abstract mixin class Iterable\u0026lt;E\u0026gt; { // 默认返回值：false bool any(bool test(E element)) { for (E element in this) { if (test(element)) return true; } return false; } // 默认返回值：true bool every(bool test(E element)) { for (E element in this) { if (!test(element)) return false; } return true; } } where()/takeWhile()/skipWhile()筛选子集合 上面的any()/every()只是一个条件判断，本节来看看，通过条件来筛选子集合：\nwhere()方法：遍历集合每个元素，返回所有符合条件的子集合；若所有元素均不符合条件，则返回空集合，而不是抛出异常！ takeWhile()方法：遍历集合元素，构成新子集合元素，直到不符合条件的元素为止。 skipWhile()方法：与takeWhile()相反，遍历集合元素，过滤掉前面所有符合条件元素，取结合后面元素。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 void main() { final List\u0026lt;int\u0026gt; alist = [1, 2, 3]; // 2.5. where()/takeWhile()/skipWhile()筛选子集合 final gtOneList = alist.where((element) =\u0026gt; element \u0026gt; 1); print(\u0026#39;2.5. where()-筛选子集合: gtOneList=$gtOneList\u0026#39;); // 结果：2.5. where()-筛选子集合: gtOneList=(2, 3) final takeLtThreeList = alist.takeWhile((element) =\u0026gt; element \u0026lt; 3); print(\u0026#39;2.5. takeWhile()-筛选子集合: takeLtTwoList=$takeLtThreeList\u0026#39;); // 结果：2.5. takeWhile()-筛选子集合: takeLtTwoList=(1, 2) final skipNeqTwoList = alist.skipWhile((element) =\u0026gt; element != 2); print(\u0026#39;2.5. skipWhile()-筛选子集合: skipNeqTwoList=$skipNeqTwoList\u0026#39;); // 结果：2.5. skipWhile()-筛选子集合: skipNeqTwoList=(2, 3) } map()转换集合元素 对集合的每个元素，通过map()函数进行一次计算，可把一个结合转换为另一个元素的集合。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 void main() { final List\u0026lt;int\u0026gt; alist = [1, 2, 3]; // 2.6. map()转换集合元素 final alistPlus3 = alist.map((e) =\u0026gt; 3 + e); print(\u0026#39;2.6. map()转换集合元素: $alist -\u0026gt; $alistPlus3\u0026#39;); // 结果：2.6. map()转换集合元素: [1, 2, 3] -\u0026gt; (4, 5, 6) final intToStringList = alist.map((e) =\u0026gt; \u0026#39;value:$e\u0026#39;); print(\u0026#39;2.6. map()转换集合元素: $alist -\u0026gt; $intToStringList\u0026#39;); // 结果：2.6. map()转换集合元素: [1, 2, 3] -\u0026gt; (value:1, value:2, value:3) } 最后总结 特别注意：\nfirst/last/firstWhere()这些返回值为单个元素方法，当为空集合或者无法找到元素时，会抛出异常。 where()/takeWhile()/skipWhile()这些返回值为集合的方法，当为空集合或者无法匹配到元素，返回空集合，不会抛出异常。 any()默认返回值为false，every()默认返回值为true 我的本博客原地址：https://ntopic.cn/p/2023092701/\n","date":"2023-09-27T00:00:00Z","image":"https://ntopic.cn/p/2023092701/00_hua251d1ea2589f24e4f220f2de00fe773_52076_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/2023092701/","title":"Flutter/Dart第03天：Dart可迭代集合"},{"content":"Dart官网代码实验室：https://dart.dev/codelabs/dart-cheatsheet\n特别说明：为了更进一步验证Dart代码特性，下面示例的代码并非与官方代码完全一致（为了探究细节，默认比官方代码要复杂一些）。\n字符串插值：$ 基础语法：字符串中，可以通过${}插入上下文中变量和变量运算值。\n1 2 3 4 5 6 7 8 void main() { // 1. 字符串插值 var a = 2; var b = 3; var c = \u0026#39;Hello\u0026#39;; print(\u0026#39;1. 字符串插值: ${c.toUpperCase()} Dart:a is ${a} and b is ${b}, so a\u0026gt;b is ${a \u0026gt; b}, a+b=${a + b}\u0026#39;); // 结果：1. 字符串插值: HELLO Dart:a is 2 and b is 3, so a\u0026gt;b is false, a+b=5 } 变量赋值：?和null 基础语法：Dart是空安全（或者null安全）的语言，也就是说除非显示声明变量是可为null的，否则他们不能为null。默认情况下，变量默认是不能为null的。\n1 2 3 4 5 6 7 8 9 10 void main() { // 2. 变量赋值 // 非法声明：int d = null; String s; int d = 4; int? e; String f = \u0026#39;Hello\u0026#39;; String? g; print(\u0026#39;2. 变量赋值: d: ${d}, e:${e}, f:${f}, g:${g}\u0026#39;); // 结果：2. 变量赋值: d: 4, e:null, f:Hello, g:null } 空运算符：??和??= 基础语法：用于处理可能会为空值的变量，??判断是否为空，??=当为空时才运行赋值。\n特别注意：??和??=这两个运算符中间不能有空格。\n1 2 3 4 5 6 7 8 9 10 void main() { // 3. 空运算符 int? h; h ??= 3; h ??= 5; // 不生效，因为此时h非空 int i = 1 ?? 3; // 1非空，所以3不生效 int j = null ?? 4; print(\u0026#39;3. 空运算符: h=${h}, i=${i}, j=${j}\u0026#39;); // 结果：3. 空运算符: h=3, i=1, j=4 } 访问空对象属性：.? 基础语法：为了正常访问可能为空对象的属性。在Java中，通过if条件判断来访问属性，如：int a = (obj == null) ? 0 : obj.getA();\n1 2 3 4 5 6 7 8 9 void main() { // 4. 访问空对象属性 String? k; String? l = \u0026#39;Hello\u0026#39;; var m = k?.toLowerCase()?.toUpperCase(); var n = l?.toLowerCase()?.toUpperCase(); print(\u0026#39;4. 访问空对象属性: m=${m}, n=${n}\u0026#39;); // 结果：4. 访问空对象属性: m=null, n=HELLO } 集合类型：[]和 基础语法：Dart存在内置的基础集合类型，包括list, set和map等，可以指定元素类型。list元素可以重复，set和map不允许重复。\n特别注意：这里只是简单用例，集合类型的其他用法，我在下次学习并通过博客分享。\n1 2 3 4 5 6 7 8 void main() { // 5. 集合类型 var aList = \u0026lt;String\u0026gt;[\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;b\u0026#39;]; var aSet = \u0026lt;String\u0026gt;{\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;b\u0026#39;}; var aMap = \u0026lt;String, String\u0026gt;{\u0026#39;a\u0026#39;: \u0026#39;Hello\u0026#39;, \u0026#39;b\u0026#39;: \u0026#39;World\u0026#39;, \u0026#39;b\u0026#39;: \u0026#39;Dart\u0026#39;}; print(\u0026#39;5. 集合类型: aList=${aList}, aSet=${aSet}, aMap=${aMap}\u0026#39;); // 结果：5. 集合类型: aList=[a, b, b], aSet={a, b}, aMap={a: Hello, b: Dart} } 箭头语法函数：=\u0026gt; 基础语法：箭头是定义函数的一种简便方法，箭头右边的执行结果作为返回值。\n1 2 3 4 5 6 7 void main() { // 6. 箭头语法函数 String joinWithCommas(List\u0026lt;int\u0026gt; values) =\u0026gt; values.join(\u0026#39;,\u0026#39;); final cList = \u0026lt;int\u0026gt;[2, 5, 7]; print(\u0026#39;6. 箭头语法函数: cList join=${joinWithCommas(cList)}\u0026#39;); // 结果：6. 箭头语法函数: cList join=2,5,7 } 级联和空判断：..和?.. 基础语法：为了简便对同一个对象连续执行多个方法，它每次执行均返回的是操作对象引用，而不是操作结果。\n特别说明：常规情况下myObject.someMethod()返回的是方法的执行结果，但是级联操作myObject..someMethod()返回的是myObject引用本身，那么它就可以连续执行多个方法，如：myObject..someMethod()..otherMethod()等。\n级联联合空判断，可以在连续执行多个方法的时候，无需担心操作对象为null，如以下代码样例：将 BigObject 的 anInt 属性设为 1、aString 属性设为 String!、aList 属性设置为 [3.0]、然后调用 allDone()。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 class BigObject { int anInt = 0; String aString = \u0026#39;\u0026#39;; List\u0026lt;double\u0026gt; aList = []; bool _done = false; void allDone() { _done = true; } } BigObject fillBigObject(BigObject obj) { return obj?..anInt = 1 ..aString = \u0026#39;String!\u0026#39; ..aList = [3.0] ..allDone(); } 对象属性访问器：getters和setters 基础语法：按照类的封装原则，类属性不能直接暴露给外部访问或者设置，应该提供getters和setters方法。Dart提供了简单的实现方式。Dart的类属性没有public/private等可见于修饰符，如果以下划线_开头，则为private，否则为public公共域。\n代码样例：有一个购物车类，其中有一个私有的 List 类型的 prices 属性；一个名为 total 的 getter，用于返回总价格。只要新列表不包含任何负价格， setter 就会用新的列表替换列表（在这种情况下，setter 应该抛出 InvalidPriceException）。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 class InvalidPriceException {} class ShoppingCart { // 下划线开头，私有属性 List\u0026lt;double\u0026gt; _prices = []; // total的getter方法，用户返回总价格值 double get total =\u0026gt; _prices.fold(0, (e,t) =\u0026gt; e+t); // 设置_prices的值，如果存在负数，则抛出异常 set prices(List\u0026lt;double\u0026gt; newPrices) { if(newPrices.any((e) =\u0026gt; e \u0026lt; 0)) { throw InvalidPriceException(); } _prices = newPrices; } } 函数可选位置入参：[] 基础语法：函数的入参列表中，最后面的参数通过[]曝光起来，他们是可选的，即调用函数时可以不传入参数。除非指定了默认值，否则可选入参默的认值均为null。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 void main() { // 8. 函数可选位置入参 int sumUpToFive(int a, [int? b, int? c, int? d, int? e]) { return a + (b ?? 0) + (c ?? 0) + (d ?? 0) + (e ?? 0); } int sumUpToFive2(int a, [int b = 2, int c = 3, int d = 4, int e = 5]) { return a + b + c + d + e; } print(\u0026#39;8. 函数可选位置入参: sumUpToFive(1,2)=${sumUpToFive(1, 2)}, sumUpToFive(1,2,3)=${sumUpToFive(1, 2, 3)}\u0026#39;); print(\u0026#39;8. 函数可选位置入参: sumUpToFive2(1,1)=${sumUpToFive2(1, 1)}, sumUpToFive2(1,1,1)=${sumUpToFive2(1, 1, 1)}\u0026#39;); // 结果： // 8. 函数可选位置入参: sumUpToFive(1,2)=3, sumUpToFive(1,2,3)=6 // 8. 函数可选位置入参: sumUpToFive2(1,1)=14, sumUpToFive2(1,1,1)=12 } 函数命名入参： 基本语法：与位置参数类似，命名参数使用{}包裹，它也是可选的，除非有默认值，否则它的值也是null。在调用命名参数函数时，命名参数必须通过参数名来定位，且它的顺序是可随意（这2点是与位置参数的最大区别）。\n代码样例：有个MyDataObject类，有3个属性和copyWith方法，方法的入参均可能为空，如果为空则使用原对象值，否则使用入参值：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 class MyDataObject { final int anInt; final String aString; final double aDouble; MyDataObject({ this.anInt = 1, this.aString = \u0026#39;OLD\u0026#39;, this.aDouble = 2.0, }); String toString() { return \u0026#39;MyDataObject: {anInt=$anInt, aString=$aString, aDouble=$aDouble}\u0026#39;; } // 本方法的3个入参均为命名参数 MyDataObject copyWith({int? newInt, String? newString, double? newDouble}) { return MyDataObject( anInt: newInt ?? this.anInt, aString: newString ?? this.aString, aDouble: newDouble ?? this.aDouble, ); } } void main() { // 10. 函数命名入参 final myDataObject = MyDataObject(); final newDataObject = myDataObject.copyWith(newInt: 2, newString: \u0026#39;NEW\u0026#39;, newDouble: 4.0); print(\u0026#39;10. 函数命名入参: myDataObject=$myDataObject, newDataObject=$newDataObject\u0026#39;); // 结果：10. 函数命名入参: myDataObject=MyDataObject: {anInt=1, aString=OLD, aDouble=2.0}, newDataObject=MyDataObject: {anInt=2, aString=NEW, aDouble=4.0} } 异常：try,on,catch,rethrow和finally 基础语法：Dart可以抛出和捕获异常，所有异常都是未检测异常。函数或者方法无需声明可能抛出的异常。Dart提供了Exception和Error两种异常类型，但业务逻辑中，可以抛出任意非空的对象（如：throw \u0026lsquo;abc\u0026rsquo;）。通过rethrow关键字，可重新抛出异常。\n代码样例：tryFunction不可靠方法，捕获不同的异常并打印日志。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 typedef VoidFunction = void Function(); class ExceptionWithMessage { final String message; const ExceptionWithMessage(this.message); } // Call logException to log an exception, and doneLogging when finished. abstract class Logger { void logException(Type t, [String? msg]); void doneLogging(); } void tryFunction(VoidFunction untrustworthy, Logger logger) { try { untrustworthy(); } on ExceptionWithMessage catch(e) { logger.logException(e.runtimeType, e.message); } on Exception { logger.logException(Exception); } finally { logger.doneLogging(); } } 构造方法：this,required,位置参数和命名参数 基础语法：在构造函数中，通过this关键字可以为成员变量快速赋值。构造函数的如此可以是位置参数，也可以是命名参数，如果参数是必选参数，则使用required关键字修饰，且该参数不能有默认值。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 // 位置参数 class MyColor1 { int red; int green; int blue; // 主构造函数 MyColor1(this.red, this.green, this.blue); // 命名构造函数：默认值初始化 MyColor1.origin() : red = 0, green = 0, blue = 0; MyColor1.origin2() : this(0, 0, 0); } final color10 = MyColor1(80, 80, 128); final color11 = MyColor1.origin(); final color12 = MyColor1.origin2(); // 命名参数 class MyColor2 { int red; int green; int blue; // 主构造函数 MyColor2({required this.red, required this.green, required this.blue}); // 命名构造函数：默认值初始化 MyColor2.origin() : red = 0, green = 0, blue = 0; MyColor2.origin2() : this(red: 0, green: 0, blue: 0); } final color20 = MyColor2( red: 80, green: 80, blue: 128, ); final color21 = MyColor2.origin(); final color22 = MyColor2.origin2(); 构造方法：:初始化列表 基础语法：在执行构造函数体之前，需要进行一些初始化操作，比如校验参数合法性、初始化参数等。\n代码样例：使用的初始化列表将 word 的前两个字符分配给 letterOne 和 LetterTwo 属性。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 class FirstTwoLetters { final String letterOne; final String letterTwo; // 初始化列表 FirstTwoLetters(String word) : assert(word.length \u0026gt;= 2), letterOne = word[0], letterTwo = word[1]; String toString() { return \u0026#39;FirstTwoLetters: {letterOne=$letterOne, letterTwo=$letterTwo}\u0026#39;; } } void main() { // 13. 构造方法：初始化列表 final firstTwoLetters = FirstTwoLetters(\u0026#39;Dart\u0026#39;); print(\u0026#39;13. 构造方法：初始化列表: firstTwoLetters=$firstTwoLetters\u0026#39;); // 结果：13. 构造方法：初始化列表: firstTwoLetters=FirstTwoLetters: {letterOne=D, letterTwo=a} } 构造方法：factory工厂 基础语法：父类根据入参，返回具体子类。\n代码样例：一般父类方法提供一个无任何参数的构造函数。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 class Square extends Shape {} class Circle extends Shape {} class Shape { Shape(); factory Shape.fromTypeName(String typeName) { if (typeName == \u0026#39;square\u0026#39;) return Square(); if (typeName == \u0026#39;circle\u0026#39;) return Circle(); throw ArgumentError(\u0026#39;Unrecognized $typeName\u0026#39;); } } 构造方法：:重定向 基本语法：构造方法中，通过:引用另外一个构造方法，可以是主构造函数，也可以是命名构造函数。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 class Automobile { String make; String model; int mpg; // 类主构造函数 Automobile(this.make, this.model, this.mpg); // 命名构造函数：重定向主构造函数 Automobile.hybrid(String make, String model) : this(make, model, 60); // 命名构造函数：重定向命名构造函数 Automobile.fancyHybrid() : this.hybrid(\u0026#39;Futurecar\u0026#39;, \u0026#39;Mark 2\u0026#39;); } 构造方法：final,const常量 基础语法：如果类生成的对象永远都不会更改，则可以让这些对象成为编译时常量。为此，请定义 const 构造方法并确保所有实例变量都是 final 的。\n1 2 3 4 5 6 7 8 9 10 class ImmutablePoint { static const ImmutablePoint origin = ImmutablePoint(0, 0); // 类属性必须用final修饰 final int x; final int y; // 构造函数更加const关键字 const ImmutablePoint(this.x, this.y); } 最后 Dart学习第2天，根据官方文档由浅入深学习，更多语法和技巧在后续研发中我在补充。\n完整的测试用的实例代码，部分代码示例在小节中已经提供：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 class MyDataObject { final int anInt; final String aString; final double aDouble; MyDataObject({ this.anInt = 1, this.aString = \u0026#39;OLD\u0026#39;, this.aDouble = 2.0, }); String toString() { return \u0026#39;MyDataObject: {anInt=$anInt, aString=$aString, aDouble=$aDouble}\u0026#39;; } // 本方法的3个入参均为命名参数 MyDataObject copyWith({int? newInt, String? newString, double? newDouble}) { return MyDataObject( anInt: newInt ?? this.anInt, aString: newString ?? this.aString, aDouble: newDouble ?? this.aDouble, ); } } // 位置参数 class MyColor1 { int red; int green; int blue; // 主构造函数 MyColor1(this.red, this.green, this.blue); // 命名构造函数：默认值初始化 MyColor1.origin() : red = 0, green = 0, blue = 0; MyColor1.origin2() : this(0, 0, 0); } final color10 = MyColor1( 80, 80, 128, ); final color11 = MyColor1.origin(); final color12 = MyColor1.origin2(); // 命名参数 class MyColor2 { int red; int green; int blue; // 主构造函数 MyColor2({required this.red, required this.green, required this.blue}); // 命名构造函数：默认值初始化 MyColor2.origin() : red = 0, green = 0, blue = 0; MyColor2.origin2() : this(red: 0, green: 0, blue: 0); } final color20 = MyColor2( red: 80, green: 80, blue: 128, ); final color21 = MyColor2.origin(); final color22 = MyColor2.origin2(); class FirstTwoLetters { final String letterOne; final String letterTwo; // 初始化列表 FirstTwoLetters(String word) : assert(word.length \u0026gt;= 2), letterOne = word[0], letterTwo = word[1]; String toString() { return \u0026#39;FirstTwoLetters: {letterOne=$letterOne, letterTwo=$letterTwo}\u0026#39;; } } class ImmutablePoint { static const ImmutablePoint origin = ImmutablePoint(0, 0); // 类属性必须用final修饰 final int x; final int y; // 构造函数更加const关键字 const ImmutablePoint(this.x, this.y); } void main() { // 1. 字符串插值 var a = 2; var b = 3; var c = \u0026#39;Hello\u0026#39;; print( \u0026#39;1. 字符串插值: ${c.toUpperCase()} Dart:a is ${a} and b is ${b}, so a\u0026gt;b is ${a \u0026gt; b}, a+b=${a + b}\u0026#39;); // 2. 变量赋值 // 非法声明：int d = null; String s; int d = 4; int? e; String f = \u0026#39;Hello\u0026#39;; String? g; print(\u0026#39;2. 变量赋值: d: ${d}, e:${e}, f:${f}, g:${g}\u0026#39;); // 3. 空运算符 int? h; h ??= 3; h ??= 5; // 不生效，因为此时h非空 int i = 1 ?? 3; // 1非空，所以3不生效 int j = null ?? 4; print(\u0026#39;3. 空运算符: h=${h}, i=${i}, j=${j}\u0026#39;); // 4. 访问空对象属性 String? k; String? l = \u0026#39;Hello\u0026#39;; var m = k?.toLowerCase()?.toUpperCase(); var n = l?.toLowerCase()?.toUpperCase(); print(\u0026#39;4. 访问空对象属性: m=${m}, n=${n}\u0026#39;); // 5. 集合类型 var aList = \u0026lt;String\u0026gt;[\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;b\u0026#39;]; var aSet = \u0026lt;String\u0026gt;{\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;b\u0026#39;}; var aMap = \u0026lt;String, String\u0026gt;{\u0026#39;a\u0026#39;: \u0026#39;Hello\u0026#39;, \u0026#39;b\u0026#39;: \u0026#39;World\u0026#39;, \u0026#39;b\u0026#39;: \u0026#39;Dart\u0026#39;}; print(\u0026#39;5. 集合类型: aList=${aList}, aSet=${aSet}, aMap=${aMap}\u0026#39;); // 6. 箭头语法函数 String joinWithCommas(List\u0026lt;int\u0026gt; values) =\u0026gt; values.join(\u0026#39;,\u0026#39;); final cList = \u0026lt;int\u0026gt;[2, 5, 7]; print(\u0026#39;6. 箭头语法函数: cList join=${joinWithCommas(cList)}\u0026#39;); // 8. 函数可选位置入参 int sumUpToFive(int a, [int? b, int? c, int? d, int? e]) { return a + (b ?? 0) + (c ?? 0) + (d ?? 0) + (e ?? 0); } int sumUpToFive2(int a, [int b = 2, int c = 3, int d = 4, int e = 5]) { return a + b + c + d + e; } print( \u0026#39;8. 函数可选位置入参: sumUpToFive(1,2)=${sumUpToFive(1, 2)}, sumUpToFive(1,2,3)=${sumUpToFive(1, 2, 3)}\u0026#39;); print( \u0026#39;8. 函数可选位置入参: sumUpToFive2(1,1)=${sumUpToFive2(1, 1)}, sumUpToFive2(1,1,1)=${sumUpToFive2(1, 1, 1)}\u0026#39;); // 10. 函数命名入参 final myDataObject = MyDataObject(); final newDataObject = myDataObject.copyWith(newInt: 2, newString: \u0026#39;NEW\u0026#39;, newDouble: 4.0); print(\u0026#39;10. 函数命名入参: myDataObject=$myDataObject, newDataObject=$newDataObject\u0026#39;); // 13. 构造方法：初始化列表 final firstTwoLetters = FirstTwoLetters(\u0026#39;Dart\u0026#39;); print(\u0026#39;13. 构造方法：初始化列表: firstTwoLetters=$firstTwoLetters\u0026#39;); } 我的本博客原地址：https://ntopic.cn/p/2023092401/\n","date":"2023-09-24T00:00:00Z","image":"https://ntopic.cn/p/2023092401/00_hua251d1ea2589f24e4f220f2de00fe773_52076_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/2023092401/","title":"Flutter/Dart第02天：Dart基础语法（建议收藏）"},{"content":"Dart的安装方式有几种：一种是下载源代码，然后编译安装；一种是通过包管理工具进行安装。\nDart官方网站分表列出了针对Windows、Linux和MacOS的安装方式：https://dart.dev/get-dart\n我下面在个人MacOS上介绍brew包管理工具安装方法和过程：\n安装HomeBrew包管理工具：brew HomeBrew是MacOS的一个包管理工具，有了它，后面安装、更新、配置等操作就容易多了：https://brew.sh\n获取Dart：brew tap dart-lang/dart 本命令作用是获取Dart的安装包列表。由于需要从GitHub拉取包数据，访问GitHub不稳定容易失败，重试几次即可：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 SZH-MacBook:~ shizihu$ brew tap dart-lang/dart Running `brew update --auto-update`... ==\u0026gt; Downloading https://ghcr.io/v2/homebrew/portable-ruby/portable-ruby/blobs/sha256:61029cec31c68a1fae1fa90fa876adf43d0becff777da793f9b5c5577f00567a ################################################################################### 100.0% ==\u0026gt; Pouring portable-ruby-2.6.10_1.el_capitan.bottle.tar.gz ==\u0026gt; Homebrew collects anonymous analytics. Read the analytics documentation (and how to opt-out) here: https://docs.brew.sh/Analytics No analytics have been recorded yet (nor will be during this `brew` run). Installing from the API is now the default behaviour! You can save space and time by running: brew untap homebrew/core ==\u0026gt; Downloading https://formulae.brew.sh/api/formula.jws.json ################################################################################### 100.0% ==\u0026gt; Downloading https://formulae.brew.sh/api/cask.jws.json ################################################################################### 100.0% ==\u0026gt; Tapping dart-lang/dart Cloning into \u0026#39;/usr/local/Homebrew/Library/Taps/dart-lang/homebrew-dart\u0026#39;... remote: Enumerating objects: 3737, done. remote: Counting objects: 100% (1491/1491), done. remote: Compressing objects: 100% (387/387), done. remote: Total 3737 (delta 1242), reused 1301 (delta 1098), pack-reused 2246 Receiving objects: 100% (3737/3737), 674.85 KiB | 127.00 KiB/s, done. Resolving deltas: 100% (2518/2518), done. Tapped 22 formulae (53 files, 862.2KB). 安装Dart：brew install dart 从安装日志可以看出：\nDart安装版本：3.1.2 Dart安装路径：/usr/local/Cellar/dart/3.1.2 Dart命令路径：/usr/local/opt/dart/libexec 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 SZH-MacBook:~ shizihu$ brew install dart ==\u0026gt; Fetching dart-lang/dart/dart ==\u0026gt; Downloading https://storage.googleapis.com/dart-archive/channels/stable/release/3.1.2/ ################################################################################### 100.0% ==\u0026gt; Installing dart from dart-lang/dart Warning: A newer Command Line Tools release is available. Update them from Software Update in System Preferences. If that doesn\u0026#39;t show you any updates, run: sudo rm -rf /Library/Developer/CommandLineTools sudo xcode-select --install Alternatively, manually download them from: https://developer.apple.com/download/all/. You should download the Command Line Tools for Xcode 14.2. ==\u0026gt; Caveats Please note the path to the Dart SDK: /usr/local/opt/dart/libexec ==\u0026gt; Summary 🍺 /usr/local/Cellar/dart/3.1.2: 987 files, 541.6MB, built in 22 seconds ==\u0026gt; Running `brew cleanup dart`... Disable this behaviour by setting HOMEBREW_NO_INSTALL_CLEANUP. Hide these hints with HOMEBREW_NO_ENV_HINTS (see `man brew`). ==\u0026gt; `brew cleanup` has not been run in the last 30 days, running now... Disable this behaviour by setting HOMEBREW_NO_INSTALL_CLEANUP. Hide these hints with HOMEBREW_NO_ENV_HINTS (see `man brew`). Removing: /Users/shizihu/Library/Caches/Homebrew/gettext--0.21... (8.7MB) Removing: /Users/shizihu/Library/Caches/Homebrew/git--2.37.3... (16.1MB) Removing: /Users/shizihu/Library/Caches/Homebrew/pcre2--10.40... (2MB) Removing: /Users/shizihu/Library/Caches/Homebrew/git_bottle_manifest--2.37.3... (11.4KB) Removing: /Users/shizihu/Library/Caches/Homebrew/gettext_bottle_manifest--0.21... (10.5KB) Removing: /Users/shizihu/Library/Caches/Homebrew/pcre2_bottle_manifest--10.40... (7.5KB) Removing: /Users/shizihu/Library/Logs/Homebrew/icu4c... (64B) Removing: /Users/shizihu/Library/Logs/Homebrew/gettext... (64B) Removing: /Users/shizihu/Library/Logs/Homebrew/pcre2... (64B) Removing: /Users/shizihu/Library/Logs/Homebrew/node... (64B) Removing: /Users/shizihu/Library/Logs/Homebrew/git... (64B) 检测Dart：brew info dart 下面检测Dart安装情况：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 SZH-MacBook:~ shizihu$ brew info dart ==\u0026gt; dart-lang/dart/dart: stable 3.1.2, HEAD SDK https://dart.dev Conflicts with: dart-beta (because dart-beta ships the same binaries) /usr/local/Cellar/dart/3.1.2 (987 files, 541.6MB) * Built from source on 2023-09-22 at 13:10:51 From: https://github.com/dart-lang/homebrew-dart/blob/HEAD/Formula/dart.rb ==\u0026gt; Options --HEAD Install HEAD version ==\u0026gt; Caveats Please note the path to the Dart SDK: /usr/local/opt/dart/libexec 升级Dart：brew upgrade dart 可选操作，因为是刚安装好，肯定是最新版本：\n1 2 3 SZH-MacBook:~ shizihu$ brew upgrade dart Warning: No remote \u0026#39;origin\u0026#39; in /usr/local/Homebrew/Library/Taps/homebrew/homebrew-services, skipping update! Warning: dart-lang/dart/dart 3.1.2 already installed 第一个Dart程序：HelloWorld.dart 编写Dart程序：00-HelloWorld.dart\n1 2 3 4 5 6 7 8 9 10 11 12 void main() { print(\u0026#39;Hello, World!\u0026#39;); final list = []; list.add(1); list.add(\u0026#39;2\u0026#39;); list.add(\u0026#39;a\u0026#39;); printList(list); } void printList(var alist) =\u0026gt; print(alist); 执行Dart程序：dart run 00-HelloWorld.dart 1 2 3 SZH-MacBook:ntopic-dart shizihu$ dart run 00-HelloWorld.dart Hello, World! [1, 2, a] 最后 初试Dart，感觉和Java有点类似：main入口函数，print输出，列表对象和方法。\n接下来的博客中，我会专门通过Dart和Flutter编写一款应用程序，并发布到不同的应用市场！\n我的本博客原地址：https://ntopic.cn/p/2023092301/\n","date":"2023-09-23T00:00:00Z","image":"https://ntopic.cn/p/2023092301/00_hua251d1ea2589f24e4f220f2de00fe773_52076_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/2023092301/","title":"Flutter/Dart第01天：Dart安装和初体验"},{"content":" “啊！多么活泼的海啊，多么热闹的街道啊！”这里就是——青岛。\n青岛的海真是活泼。每片海都是波光粼粼看不到边的。突然，一个大浪打过来，靠在岸边的人都被淋成了落汤鸡。有时，一个小浪打过来，也会淋湿人们的脚丫。海面上时而漂浮着几个游泳圈，从远处望去真像几个五颜六色的小斑点，它时快时慢，真像个顽皮的小孩子。\n青岛的街道真是热闹。墙上挂满了红红的大灯笼，从远处望去好像红红的大番茄。街道旁边都处都是美味诱人的小吃，让人口水直流三千尺，有桂花糕、串串，还有火锅。总之，好吃的东西应有尽有。\n青岛的海鲜真是好吃。说到去青岛，人们首先就会想到青岛的海鲜，青岛的海鲜确实名不虚传啊。首先，海鲜的口味很多；其次，海鲜的品种也有很多，有虾、贝壳等。再搭配上青岛特有的酒，真叫人吃了还想吃。\n这里就是青岛，一个来了还想再来的地方。\n学校： 上海市浦东新区第二中学小学（张江校区） 班级： 四（2）班 作者： 徐韵 指导老师： 史焰礼\n","date":"2023-09-17T00:00:00Z","image":"https://ntopic.cn/p/2023091701/00_hu6e7d3f3aa536a9ff76b5d701cff56539_57778_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/2023091701/","title":"上海·浦东二小·四年级·作文《青岛》"},{"content":"源代码先行：\nGitee本文介绍的完整仓库：https://gitee.com/obullxl/ntopic-boot GitHub本文介绍的完整仓库：https://github.com/obullxl/ntopic-boot 背景介绍 今天走读一个应用程序代码，发现一个有趣的现象：有多个不同的业务Bean中均依赖了一个参数工具类ParamUtils（即：@Autowired ParamUtils paramUtis），ParamUtils依赖了ParamDAO Bean用于从DB中获取参数；为了便于ParamUtils使用，工具类全部都是static静态方法，也就是说，业务Bean仅仅增加Autowired依赖，在实际调用时还是直接使用的ParamUtils类静态方法。那个Autowired注入ParamUtils的依赖看起来是无用代码，但是其实还不能去掉。\n代码业务这么写的目的其实很好理解：因为ParamUtils依赖了DAO Bean，增加依赖是保障ParamUtils的类静态方法在调用时已经被SpringBoot初始化了。那么，有没有更优雅的办法，能让业务代码更优雅更安全的使用ParamUtils工具类呢？\n思路分析 ParamUtils业务Bean，比其他的业务Bean提前初始化，基本思路如下：\n第一思路：采用优先级Ordered注解（类：org.springframework.core.Ordered），但是不可行，因为该注解主要是用于控制Spring自身Bean的初始化顺序，如Listener/Filter等。\n第二思路：采用Bean依赖DependsOn注解（类：org.springframework.context.annotation.DependsOn），该方法可行，它和Autowired注解一致，也是表明Bean之间依赖，但是没有从本质上解决问题。\n第三思路：手工注册Bean让Spring优先初始化，查看SpringApplication类代码，发现里面有个addInitializers(ApplicationContextInitializer\u0026lt;?\u0026gt;... initializers)方法，可以让业务在ApplicationContext初始化时initialize(C applicationContext)基于Context做一些事情。那么可不可以在这个地方，能手工注册业务Bean呢？\n代码实现和验证 代码分为3部分：ParamDAO业务Bean访问DB，ParamUtils参数工具类依赖ParamDAO，RestController测试类使用参数工具类。\n为了阅读方便，以下展示的代码均只有主体部分，完整的代码注释和代码内容，请下载本工程仓库。\nParamDAO业务Bean 为了测试简便，本工程不依赖MySQL数据库，我们还是采用SQLite，源文件就在代码根目录下，clone本仓库后即可执行运行：\nSQLite数据表准备 首先新建一张参数表（nt_param），并且插入一些数据。为了尽快验证我们的思路，其他的数据新增、修改和删除等就不做特别的验证了。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 -- -- 参数表 -- CREATE TABLE nt_param ( id bigint unsigned NOT NULL auto_increment, category varchar(64) NOT NULL, module varchar(64) NOT NULL, name varchar(64) NOT NULL, content varchar(4096) DEFAULT \u0026#39;\u0026#39;, create_time timestamp, modify_time timestamp, PRIMARY KEY (id), UNIQUE (category, module, name) ); -- -- 插入数据 -- INSERT INTO nt_param (category, module, name, content, create_time, modify_time) VALUES (\u0026#39;CONFIG\u0026#39;, \u0026#39;USER\u0026#39;, \u0026#39;minAge\u0026#39;, \u0026#39;18\u0026#39;, strftime(\u0026#39;%Y-%m-%d %H:%M:%f\u0026#39;, \u0026#39;now\u0026#39;), strftime(\u0026#39;%Y-%m-%d %H:%M:%f\u0026#39;, \u0026#39;now\u0026#39;)), (\u0026#39;CONFIG\u0026#39;, \u0026#39;USER\u0026#39;, \u0026#39;maxAge\u0026#39;, \u0026#39;60\u0026#39;, strftime(\u0026#39;%Y-%m-%d %H:%M:%f\u0026#39;, \u0026#39;now\u0026#39;), strftime(\u0026#39;%Y-%m-%d %H:%M:%f\u0026#39;, \u0026#39;now\u0026#39;)); ParamDAO数据查询 NTParamDAO为普通的Spring Bean（ID为：ntParamDAO）\n1 2 3 4 5 6 7 @Repository(\u0026#34;ntParamDAO\u0026#34;) public interface NTParamDAO { @Select(\u0026#34;SELECT * FROM nt_param WHERE category=#{category,jdbcType=VARCHAR} AND module=#{module,jdbcType=VARCHAR}\u0026#34;) List\u0026lt;NTParamDO\u0026gt; selectByModule(@Param(\u0026#34;category\u0026#34;) String category, @Param(\u0026#34;module\u0026#34;) String module); } ParamUtils工具类定义和使用 ParamUtils工具类定义：非Spring Bean ParamUtils是静态工具类，依赖了ParamDAO Spring Bean，并且ParamUtils并不是Spring Bean：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 // @Component(\u0026#34;ntParamUtils\u0026#34;) SpringBoot优先初始化本类，因此无需增加注解 public class NTParamUtils { private static final Logger LOGGER = LoggerFactory.getLogger(LogConstants.DAS); /** * 系统参数DAO */ private static NTParamDAO NT_PARAM_DAO; /** * 依赖注入 */ public NTParamUtils(@Qualifier(\u0026#34;ntParamDAO\u0026#34;) NTParamDAO ntParamDAO) { Assert.notNull(ntParamDAO, \u0026#34;NTParamDAO注入为NULL.\u0026#34;); NT_PARAM_DAO = ntParamDAO; // 打印日志 LOGGER.info(\u0026#34;{}:初始化完成.\u0026#34;, this.getClass().getName()); } public static List\u0026lt;NTParamDO\u0026gt; findList(String category, String module) { Assert.hasText(category, \u0026#34;分类参数为空\u0026#34;); Assert.hasText(module, \u0026#34;模块参数为空\u0026#34;); return NT_PARAM_DAO.selectByModule(category, module); } } ParamUtils工具类使用：普通Spring Bean NTUserServiceImpl是一个普通的Spring Bean，它没有显示依赖ParamUtils，而是直接使用它：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 @Component(\u0026#34;ntUserService\u0026#34;) public final class NTUserServiceImpl implements NTUserService { private static final Logger LOGGER = LoggerFactory.getLogger(LogConstants.BIZ); @Autowired public NTUserServiceImpl() { // 打印日志 LOGGER.info(\u0026#34;{}:初始化完成.\u0026#34;, this.getClass().getName()); } /** * 获取用户模块参数 */ @Override public List\u0026lt;NTParamDO\u0026gt; findUserParamList() { return NTParamUtils.findList(\u0026#34;CONFIG\u0026#34;, \u0026#34;USER\u0026#34;); } } SpringBoot优先初始化设置 两个关键点：\nApplicationContextInitializer类：提供Context初始化入口，业务逻辑可以通过此次注入。 BeanDefinitionRegistryPostProcessor类：Spring Bean收集完成后，但还没有初始化之前入口，我们的关键就在这里定义ParamUtils Bean，并且Bean定义为RootBeanDefinition保障提前初始化。 Context自定义初始化：手工注册ParamUtils Bean 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 public class NTApplicationContextInitializer implements ApplicationContextInitializer\u0026lt;ConfigurableApplicationContext\u0026gt;, BeanDefinitionRegistryPostProcessor { /** * Context初始化，给业务逻辑初始化提供了机会 */ @Override public void initialize(ConfigurableApplicationContext context) { // 注册Bean上下文初始化后处理器，用于手工注册Bean context.addBeanFactoryPostProcessor(this); } /** * 手工注册ParamUtils工具类，并且是RootBean定义，保障优先初始化，下面会详细分析 */ @Override public void postProcessBeanDefinitionRegistry(BeanDefinitionRegistry registry) throws BeansException { // 在ConfigurationClassPostProcessor前手动注册Bean，保障优先于其他Bean初始化 registry.registerBeanDefinition(\u0026#34;ntParamUtils\u0026#34;, new RootBeanDefinition(NTParamUtils.class)); } @Override public void postProcessBeanFactory(ConfigurableListableBeanFactory beanFactory) throws BeansException { } } SpringBoot启动类增加自定义初始化器 原来的方法：SpringApplication.run(NTBootApplication.class, args);\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 @SpringBootApplication(exclude = {SecurityAutoConfiguration.class}) @MapperScan(basePackages = \u0026#34;cn.ntopic.das..**.dao\u0026#34;, sqlSessionFactoryRef = \u0026#34;ntSqlSessionFactory\u0026#34;) public class NTBootApplication { /** * SpringBoot启动 */ public static void main(String[] args) { // 注册自定义处理器 SpringApplication application = new SpringApplication(NTBootApplication.class); application.addInitializers(new NTApplicationContextInitializer()); // SpringBoot启动 application.run(args); } } 至此，业务Bean提前初始化的整个代码完毕，下面进行验证！\nParamUtils初始化验证（符合预期） 我们分表从SpringBoot的启动日志和实际使用2个方面来验证我们的设计思路：\nSpringBoot启动日志：符合预期 从第21行和第22行日志，可以看到，ParamUtils优于其他Bean完成初始化：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 . ____ _ __ _ _ /\\\\ / ___\u0026#39;_ __ _ _(_)_ __ __ _ \\ \\ \\ \\ ( ( )\\___ | \u0026#39;_ | \u0026#39;_| | \u0026#39;_ \\/ _` | \\ \\ \\ \\ \\\\/ ___)| |_)| | | | | || (_| | ) ) ) ) \u0026#39; |____| .__|_| |_|_| |_\\__, | / / / / =========|_|==============|___/=/_/_/_/ :: Spring Boot :: (v2.5.3) 2023-09-09 11:40:55,607 INFO (StartupInfoLogger.java:55)- Starting NTBootApplication using Java 1.8.0_281 on OXL-MacBook.local with PID 1371 (/Users/obullxl/CodeSpace/ntopic-boot/ntopic/target/classes started by obullxl in /Users/obullxl/CodeSpace/ntopic-boot) 2023-09-09 11:40:55,612 INFO (SpringApplication.java:659)- No active profile set, falling back to default profiles: default 2023-09-09 11:40:55,692 INFO (DeferredLog.java:255)- Devtools property defaults active! Set \u0026#39;spring.devtools.add-properties\u0026#39; to \u0026#39;false\u0026#39; to disable 2023-09-09 11:40:55,693 INFO (DeferredLog.java:255)- For additional web related logging consider setting the \u0026#39;logging.level.web\u0026#39; property to \u0026#39;DEBUG\u0026#39; 2023-09-09 11:40:56,834 INFO (TomcatWebServer.java:108)- Tomcat initialized with port(s): 8088 (http) 2023-09-09 11:40:56,842 INFO (DirectJDKLog.java:173)- Initializing ProtocolHandler [\u0026#34;http-nio-8088\u0026#34;] 2023-09-09 11:40:56,842 INFO (DirectJDKLog.java:173)- Starting service [Tomcat] 2023-09-09 11:40:56,842 INFO (DirectJDKLog.java:173)- Starting Servlet engine: [Apache Tomcat/9.0.50] 2023-09-09 11:40:56,901 INFO (DirectJDKLog.java:173)- Initializing Spring embedded WebApplicationContext 2023-09-09 11:40:56,901 INFO (ServletWebServerApplicationContext.java:290)- Root WebApplicationContext: initialization completed in 1208 ms 2023-09-09 11:40:57,043 ERROR (Log4j2Impl.java:58)- testWhileIdle is true, validationQuery not set 2023-09-09 11:40:57,051 INFO (Log4j2Impl.java:106)- {dataSource-1} inited 2023-09-09 11:40:57,127 INFO (NTParamUtils.java:39)- cn.ntopic.NTParamUtils:初始化完成. 2023-09-09 11:40:57,160 INFO (NTUserServiceImpl.java:78)- cn.ntopic.service.impl.NTUserServiceImpl:初始化完成. 2023-09-09 11:40:57,170 INFO (NTExecutorConfig.java:65)- start ntThreadPool 2023-09-09 11:40:57,563 INFO (OptionalLiveReloadServer.java:58)- LiveReload server is running on port 35729 2023-09-09 11:40:57,582 INFO (DirectJDKLog.java:173)- Starting ProtocolHandler [\u0026#34;http-nio-8088\u0026#34;] 2023-09-09 11:40:57,600 INFO (TomcatWebServer.java:220)- Tomcat started on port(s): 8088 (http) with context path \u0026#39;\u0026#39; 2023-09-09 11:40:57,610 INFO (StartupInfoLogger.java:61)- Started NTBootApplication in 2.363 seconds (JVM running for 3.091) RestController验证：符合预期 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 @RestController public class NTParamAct { private final NTUserService ntUserService; public NTParamAct(@Qualifier(\u0026#34;ntUserService\u0026#34;) NTUserService ntUserService) { this.ntUserService = ntUserService; } @RequestMapping(\u0026#34;/param\u0026#34;) public List\u0026lt;NTParamDO\u0026gt; paramList() { return this.ntUserService.findUserParamList(); } } 打开浏览器，访问：http://localhost:8088/param\n可以看到，参数数据被查询并输出：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 [ { \u0026#34;id\u0026#34;: 3, \u0026#34;category\u0026#34;: \u0026#34;CONFIG\u0026#34;, \u0026#34;module\u0026#34;: \u0026#34;USER\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;maxAge\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;60\u0026#34;, \u0026#34;createTime\u0026#34;: \u0026#34;2023-09-08T18:30:20.818+00:00\u0026#34;, \u0026#34;modifyTime\u0026#34;: \u0026#34;2023-09-08T18:30:20.818+00:00\u0026#34; }, { \u0026#34;id\u0026#34;: 2, \u0026#34;category\u0026#34;: \u0026#34;CONFIG\u0026#34;, \u0026#34;module\u0026#34;: \u0026#34;USER\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;minAge\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;18\u0026#34;, \u0026#34;createTime\u0026#34;: \u0026#34;2023-09-08T18:30:20.818+00:00\u0026#34;, \u0026#34;modifyTime\u0026#34;: \u0026#34;2023-09-08T18:30:20.818+00:00\u0026#34; } ] SpringBoot实现分析 SpringBoot启动的代码入口：\n1 2 3 4 5 6 7 8 public static void main(String[] args) { // 注册自定义处理器 SpringApplication application = new SpringApplication(NTBootApplication.class); application.addInitializers(new NTApplicationContextInitializer()); // SpringBoot启动 application.run(args); } 有几个非常核心的点，基本调用链路：\nSpringApplication类：run() -\u0026gt; prepareContext() -\u0026gt; applyInitializers(本方法：调用自定义NTApplicationContextInitializer上下文器) SpringApplication类：run() -\u0026gt; refreshContext() -\u0026gt; refresh(ConfigurableApplicationContext) ConfigurableApplicationContext类：AbstractApplicationContext.refresh() -\u0026gt; finishBeanFactoryInitialization(ConfigurableListableBeanFactory) ConfigurableListableBeanFactory类，关键代码都在这里：preInstantiateSingletons() beanDefinitionNames属性：Spring收集到的所有Bean定义，包括Repository注解、Component注解和我们手工定义的Bean 遍历beanDefinitionNames的时候，优先RootBeanDefinition初始化，手工定义的ParamUtils也是该类型 至此，问题解决，能解决的原因也搞清楚了！\n我的本博客原地址：https://ntopic.cn/p/2023090901/\n","date":"2023-09-09T00:00:00Z","image":"https://ntopic.cn/p/2023090901/00-01_hu375a28468ef84dd47f244fb617076cc1_108802_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/2023090901/","title":"SpringBoot如何让业务Bean优先于其他Bean加载"},{"content":"附：我的青岛旅游计划参见之前的文章：我的2023暑假青岛3天2晚旅游计划\n本文主要分享一下我的旅游体验和个人感受，纯属个人主观感觉，若有失偏颇，请留言或者发邮件给我，非常感谢~\n总体体验感觉：很不错 先上总结，后面在详细介绍，如果还没有去过青岛的网友，看看总结和后面的详细介绍，我期望能有一些帮助：\n3天2晚时间比较充裕：去之前和去之后都有朋友问我，总共3天2晚的时间，在刨去来回路上的时间，最多不超过2天半的旅游时间，时间是不是不够、是不是很赶？本次旅游我带着2个老人和2个小孩，肯定不能太赶；并且根据实际情况（我的住宿地点、我的往返时间和我的旅游路线，下面均会逐一提到），小孩和老人都感觉不累，时间应该是比较充裕，青岛之旅应该是足够了。 青岛服务人员都非常热情有耐心（包括酒店工作人员、地铁工作人员和旅游景点工作人员），他们的服务态度感觉非常舒服：不管是买票、咨询和问路，感觉他们都非常热情有耐心，标准的普通话，称谓里基本没有你，基本都是您或者先生（我是男的），听着舒服，也感觉到不耐烦的情绪。我一直在想，他们的这种良好的服务态度，是不是因为目前处于暑假旅游旺季，接待的都是外地游客，青岛政府部门是不是进行了专门的培训？ 地铁和公交很方便，去旅游景点基本用不着打车：我去之前还担心打车会被黑，还特意在网上找了打车攻略（网上推荐是高德打车，万一被黑有投诉之地），结果到了青岛基本是坐地铁和步行。为啥是基本呢，因为去金沙滩不想等公交车，打了一个黑车，讲好了价钱，根本没有被黑，并且师傅还很热情；其他旅游景点就全部是地铁和步行了。 吃的地方较少，海鲜店并没有想象中的多，价格也并是很便宜：可能是我不擅长觅食，去了青岛后，发现旅游景点吃饭的地特别少（可以说基本没有），我们吃饭基本是点外卖。整个旅程总共吃了一顿海鲜，并不便宜（同样是海边城市，感觉比福建霞浦的海鲜要贵太多了，有小又贵）。 其他的，关于旅游景点和一些注意事项，在这里统一列一下： 个人物品注意事项：带瓶装的液体不要超过100ML，我带了一瓶380ML防蚊药，结果在回来时禁止上高铁；同时带好防晒霜，主要是小朋友穿着泳衣游泳，防止晒黑，建议也不要超过100ML。 旅游景点注意事项：海军博物馆一定要提前一周预约，网上说海军博物馆特别震撼，我也很期待，但是到了门口发现需要预约而没有去处，比较遗憾，微信小程序搜索中国人民解放军海军博物馆小程序中心预约即可；五四广场建议晚上去，确实很美，五四广场和四周的高楼大厦融为一体，感觉比较震撼；海底世界感觉一般，也没有特别之处，和一般的海底公园没有差异；八大关风景区还以为能看到城墙和关隘，其实就是海边的几条街的名字，纯街（没啥商场、商店等），可能是个人见识有限，实在看不出这些街有啥特别之处，特别是带了小朋友或者老人的，走完这几条街，估计会很累，所以个人建议忽略；第二海水浴场真的很小，也可以忽略，厕所特别的脏，进去一股臭味，小朋友憋着尿要去上厕所，打开门后又跑了出来，说太脏了不敢上，我也上了一次，具体情形就不说了，用恶心两个字来形容感觉一点都不为过。 我的旅游路线详细介绍 下面详细介绍我本次旅游的整个行程，如果网友们已经做好了自己的攻略，也可以参考参考，期望能有所帮助。\n往返交通工具：时间比较充裕 去交通：上海火车站（08:41） -\u0026gt; 青岛站（14:25）\n返交通：青岛北站（15:47） -\u0026gt; 上海火车站（21:04）\n早上去青岛时间不赶 下午回上海时间不赶 住宿：可加床，热情，干净整洁 地点：维也纳国际酒店·山东青岛火车站东站广场栈桥店。地理位置特别好，与地铁1号线和3号线仅一墙之隔。\n住宿：豪华双床房，因为带了2个小朋友，还加了一张床。加床是气垫床，100元1晚，送1张早餐券，入住当天早上9点之后打电话到前台告知需要加床即可\n早餐：39元一位，小朋友1.2米以下免费，1.2米~1.4米半价；早餐还算丰富，我比较喜欢爆炒花蛤，早上都需要吃一小盘😊\n日常饮食：店少，海鲜贵，靠外卖 可能是由于我一直在景点和我住宿地方的原因，感觉餐饮店很少，海鲜店更少，不像福建霞浦，海鲜店和大排档还是挺多的。\n第1天到青岛，兴致满满去找海鲜店，想吃顿海鲜，毕竟来了沿海城市青岛了，心想海鲜肯定便宜又实惠，结果我就有点失望了。\n海鲜店：鱻赑海鲜店\n老板推荐海鲜大咖，鉴于我们人数（3大2小），老板推荐258元的海鲜中咖套餐。\n从表面上看，样式还是很多的，有海星、生蚝、鲍鱼、虾、扇贝、花蛤、海螺、螃蟹等，但是都很小（注意：盘子底下全是豆芽）！\n后面由于呆在景点较多，吃饭基本是靠店外卖了，本次旅程饿了么app还真帮我大忙！\n满墙的菜单 端上来的海鲜 第1天：栈桥、第一海水浴场、五四广场 小遗憾：由于不知道海军博物馆需要提前预约，结果没有进不去。想要去的网友们，记得提前一周预约！\n栈桥：免费，达不到优美浪费水准 看网上攻略，栈桥优美又浪漫：碧蓝的海水、白花花的海浪、如棉花糖般的白云、偶尔略过头顶的海鸥、轻微的海风和飘逸的拍婚纱照情侣等。\n栈桥上有很多兜卖海螺、贝壳等纪念品的小摊贩，有小朋友可以买2个。同时还有很多拍纪念照的小摊位，照片从大到小，分别有10元、20元和30元一张，塑封10元一张，可以拍张合影留念，其他景点可没有拍照的地方了。\n栈桥上：人挤人，完全感觉不到优美和浪漫！ 栈桥人多 栈桥人真多 栈桥海水浴场：最好穿上厚底鞋，小心脚底划伤；同时祈祷自己不要摔倒，否则遍体鳞伤！ 栈桥海滩 栈桥海滩 第一海水浴场：免费，人多拥挤 看过了栈桥的海滩，在来看第一海水浴场的沙滩，幸福感一下子就提升上来了。沙子比较细腻，但是沙滩较短，人多且公共设施小，上个厕所都需要排队，还没有免费洗脚洗沙的地方。\n告示牌 海滩 五四广场：免费，优美震撼 我本来以为五四广场就是一个普通的广场、一个圆形花坛、中间一座火炬形雕塑的普通广场而已，但是去之后我发现我错了：广场依海而建，一面是海，另一面是高楼大厦；火炬循环变色，红紫粉等；高楼大厦的墙体全是液晶面板，上面循环播放动画，且不同大厦的面板的动画画面连接在一起。\n可以想象，青岛政府在五四广场花了不少心思，也花费了不菲的资金，为了纪念五四运动和青年们，我认为是值得的！\n特别抱歉：由于我拍的照片大多为家人合影，加上手机拍照效果也不好，因此纯景照片没有拍几张，因此只能截取几张照片了。\n红色火炬 紫色火炬 第2天：海底世界、八大关风景区/第二海水浴场、石老人海水浴场 海底世界：特色不足，价格稍贵，劝退 感觉很一般，没有眼前一亮的特色，虽然有个美人鱼表演，但是完全没有必要去青岛参观，成人170元/位，小孩半价；同时人真多，人挤人，建议劝退。\n1个美人鱼 多个美人鱼 八大关风景区和第二海水浴场：劝退 八大关风景区：听名字感觉此处应该有古城墙和关隘，你要是信了那就要失望了。所谓八大关就是几条街的名字（什么山海关路、宁武关路等），并且这些街还不是购物街，就是纯街道，如果你带着老人或者小孩，我建议还是劝退吧！\n第二海水浴场：第一海水浴场相比栈桥海水浴场，幸福感可以说是爆棚的。但是你到了第二海水浴场，你就会发现，它称第二是非常有道理的，因为它根本就不如第一海水浴场。沙滩长度和宽度均较小，特别是厕所我就不提了，简直有损第二这个名号了！\n劝退的地方，就不拍照留念了，建议想去的网友们慎重考虑！\n石老人海水浴场：免费，儿童设施，大沙滩 小朋友们今天还没有去游泳呢，因此我计划中的备选项石老人海水浴场就被提上来了，去了之后发现远远超出预期：沙滩大，比第一海水浴场大太多了；配套比较齐全，有儿童游乐区域，有洗脚洗沙区域，厕所里一直有2位清洁工驻守打扫。\n第3天：金沙滩海水浴场，不容错过 最后1天，下午3:45火车返程了，想轻松一点，决定就上午去一个地方，下午去火车站。商量下来还是去玩水吧，因此金沙滩海水浴场，也是网友们评价最高的沙滩，这个我计划中的备选项就被提升上来了。\n和石老人海水浴场一样，又一次超出我的预期：沙滩更大更长，沙子感觉更细腻，整体规划更合理（洗脚洗沙区域、厕所更大、遮阳伞也是固定好的）。\n如果网友们去沙滩，金沙滩真的不容错过！\n沙滩左 沙滩右 最后 我的青岛之旅完美收官了，非常期待明年的暑假旅游。明年，厦门我们再见！\n附-青岛旅游计划：我的2023暑假青岛3天2晚旅游计划\n","date":"2023-08-12T00:00:00Z","image":"https://ntopic.cn/p/2023081201/00-01_huc82d7b839fc53b7a5c86bd3cdfedcac6_64788_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/2023081201/","title":"我的2023暑假青岛3天2晚旅游归来"},{"content":"中央新影中学生频道选角：出乎意料 中央新影中学生频道：http://www.zxstv.com.cn\n网上搜索了一翻，是个正规频道：中央新影中学生频道是经国家广电总局批准，面向全国青少年儿童开办的电视频道。内容涵盖教育、科技、动漫、青少年儿童才艺展演类综艺、教育名家访谈、母婴健康等。\n海选选角和试镜邀请 5月份的某一天，我爱人带着2个小朋友，在世纪公园参加她们公司组织的亲子活动。在世纪公园游玩之际，受到了号称是央视《劳动最光荣》栏目组几位老师和摄影师的采访拍摄，他们加了我爱人的微信和联系方式、随机问了我小孩几个问题并让背一首古诗。\n大概一周过后，我爱人收到短信，邀请我小朋友去参见试镜：\n1 2 3 4 5 6 7 8 9 10 11 12 13 邀请函： 家长您好！诚邀您参加《劳动最光荣》栏目本周小演员选拔录制活动。 《劳动最光荣》栏目成片播出频道：中央新影中学生频道 试镜时间：6月3日周六下午1:00 注意事项： 1、着装得体（干净整洁即可） 2、提前准备简单自我介绍。 3、可以准备小才艺展示（朗读一篇古诗、唱歌、跳舞、演绎精彩小片段、演讲等等，内容没有局限性，看小朋友个人意愿 ）现场有工作人员安排流程。 4、至少需要一位家长陪同、最好父母一起陪同 可以在演播厅观看孩子的表现 5、节目录制地点：《中央新影中学生频道上海节目制作中心》杨浦区赤峰路43号知行楼7楼演播厅。 6、如有问题不能到场或时间需要调配请联系：\u0026lt;此处是个电话号码，本文省略\u0026gt;贾老师 7、入场编号：A08 收到此信息请回复“收到”谢谢您的配合！ 同时，栏目组老师用微信给我爱人发了微信消息，包括文字内容、才艺表演需要背诵的古诗词、试镜的地理位置等信息：\n1 2 3 4 5 6 家长晚上好！！！ 温馨提示《劳动最光荣》和您的宝贝在6月3日（周六）有约，请不要缺席！ 1、如果不能到场或者时间需要调配请及时告知，栏目组剧本等等道具需要提前准备，每场小朋友名额也是限定的，感谢家长的理解。 2、爸爸妈妈如果都有时间可以都到场坐在观众席观看小朋友表演，如果只有爸爸或者妈妈一方有时间的也没事。 3、现场有化妆师，小朋友穿着整洁来就可以，现场会帮小朋友画淡妆，上镜效果更佳。 ❤️另外祝家长和小朋友有一个愉快的周末—《劳动最光荣》栏目组 地理位置：上海市杨浦区赤峰路43号知行楼7楼\n说实话，当看到这条消息时，我知道一旦被选上我又要出点钱了，因为如果是免费的，短信和微信的信息里面肯定会特意强调。我和爱人商量了一下，预算控制在1.5万以内，给小朋友一个机会！\n免费的试镜、缴费的聘用 去试镜的地方是一个表演喜剧类的学院，名字有点记不住。\n当天去试镜的小朋友好了好几批，我们那批大概有30多个，小朋友年纪不一，有读中班的小朋友，也有初二的大朋友。\n首先是播音试镜，1个主持人，几位老师打分。因为1个小朋友1首古诗词，大概也就几十秒钟的样子：\n评委老师根据小朋友的表现、仪态等平方，最终我们那批30多个小朋友的试镜结果，只留下了6位小朋友。\n接下来就是面试环节，说白了就是说服家长交钱了。面试过程我就不细讲了，基本思路，大致符合我的预判：激起小孩表演兴致，PUA我要支持她。最终只需要1.4万多元，在我的预算之内，我也就交钱，并在聘书合同上签字画押了：\n演员培训、拍摄注意事项 培训集中在一下午，《劳动最光荣》总导演来给小朋友和家长们上课，具体就不展开了，写几点印象深刻内容。\n行有行规，演员也有规章制度：演戏不容易，安全放第一；人等戏，戏不等人，戏等光线。\n演员也有基本素养：足容重、手容恭、头容直、目容端、口容止、声容静。\n发剧本、背台词准备拍摄 1 2 3 4 5 《劳动最光荣》剧组通知： 1、👆上面剧本里你家小朋友饰演“邱瑛”请小朋友熟背这个角色的台词以及上句其他角色的台词，以便衔接。 2、剧本已发家长，最后一句“注意”请家长仔细阅读。 3、拍摄时间7月29号、7月30号，请家长事前预留时间，如有变化，剧组会及时告知。 收到请回复，谢谢！ 象山影视城的2天2晚之旅 大致行程：\n7.28日周五下午4:30坐大巴（也可自驾）从上海博物馆出发去拍摄酒店；\n7.29日周日早上开机典礼，然后就开始拍摄；7.30日周日拍摄完成，下午坐大巴回上海。\n影视城大酒店：无电梯、无中央空调 象山影视城坐落在象山星光影视小镇上，小镇上周围都是酒店，我们主的是其中一座。光看名字感觉还不错，入住发现很一般。\n这是大堂。特别注意：整座酒店没有中央空调，没有电梯！\n这是服务台的背景墙，上面挂着的是当年入住本酒店的明星，我还能认出几位：陈思诚、黄晓明、孙俪、杨紫、杨幂、迪丽热巴、易烊千玺、雷佳音。\n明星有不少，荣誉墙也挺满。\n这是大堂右手边，看到那个立式空调柜机了吗？真没有中央空调。\n这是酒店门前，挺宽敞的，自驾停车方便。对面远处的大楼，基本都是酒店了。\n酒店房间：极致的简陋 酒店的布局为回字形，中间是个天井，酒店走廊挺宽敞，宽度可以跑汽车，但是很闷热（因为没有空调）：\n房间就比较简陋了：1张写字桌，1把椅子，2张床，1间卫生间，没有多余空间了。连台灯都没有，小孩做作业都不方便，更别说矿泉水、茶叶啥的了。\n空调当然也是挂壁式的，还是对着床头吹的。注意照片是我补排的，因此会看到背包、鞋子等：\n连毛巾都是一次性的：\n影视城一期：神雕广场 酒店就在影视城旁边，影视城分为两期：根据导演讲的，一期主要旅游和参观，拍摄不多了；二期注意是拍戏，我们的微电影会在二期完成拍摄。\n一期神雕广场还是比较气派的，光入口的走道就有好几百米：\n神雕因拍摄《神雕侠侣》这部电视剧而出名：\n影视城二期：各式建筑群 说实话，建筑都比较破旧，有的门都是用订书机订上去的。我们看到的电影，都是经过后续制作的：\n偶遇一个拍戏剧组：演员、拍戏 演员们排队领盒饭：有个管演员的小孩感觉特别嚣张，对演员大吼大叫的：\n虽然是乞丐演员，也可以顺便买瓶汽水喝喝：\n那个站在上面的，应该就是导演了，拿着话筒在指挥拍戏：\n剧组盒饭：相对较好 我们剧组较小，都在酒店吃饭，至少不用蹲在影视城里面。导演说我们伙食相对比好了，剧组一般是1荤1素，荤菜也不怎么好，比如红烧肉，可能都没有瘦肉。\n早餐：2个小笼包，1个鸡蛋，银耳汤或豆浆自助\n午餐/晚餐：基本是1大荤1小荤1素，什么大鸡腿、炸猪排、芹菜炒肉等\n最后-关于影视城周边 其他的一些情况，统一与大家分享一下：\n影视城很偏，白天陆续还能看到去一期游玩的游客，到了晚上基本没有什么人了，空荡荡的大街 本次剧组有几个自驾的家长，有几个他们开车去吃海鲜，单次车程基本需要30分钟，因此想去影视城吃海鲜肯定不是一个好主意 一期影视城参观110元，二期40元。一期参观可以随意走动，但是二期只能坐小火车。导演说一期和二期基本差不多，如果我们去一期游玩不划算，同时如果作为游客，二期只能做小火车兜一圈，中途不停也不能下车 最后的最后，期望小朋友拍摄的微电影《劳动最光荣》能尽快上映，到时在和大家分享~\r","date":"2023-07-31T00:00:00Z","image":"https://ntopic.cn/p/2023073101/00-01_hub6883997a803fbe0e8df057afe44f3fb_312388_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/2023073101/","title":"宁波象山影视城微电影拍摄随班体验"},{"content":"更新：本次旅游行程和个人感受：我的2023暑假青岛3天2晚旅游体验和感受\n看海玩水优选青岛 小朋友们最开心的暑假来了，今年我的2位小朋友最希望去玩的是看海和玩水。这样今年暑假我的出游目标就比较明确了，该计划实施路径了。\n出游目的地的比较和选择（维度：温度适宜、有海有沙滩）：\n上海本地游：有海有沙滩的当属碧海金沙了，可是上海暑假温度一直居高不下，加上小朋友之前已去过2回了。地方较小，平时周末就可以去，不适合作为暑假出游计划，放弃不作考虑目标 上海周边：和上海一样，温度太高了，也放弃了 其他省市：比较几个来回，最后倾向青岛和厦门，查看历史天气30度以内（青岛40天天气预报），同时青岛的海水浴场实在太多，且都聚在一起，实属优选（厦门计划作为明年暑假目标） 青岛3天2晚路线安排 在这里推荐一个非常好用的路线规划工具，高德地图路书：https://ditu.amap.com/plan\n优势：可以根据自己的日程安排，选择想去玩的地方，地图会进行标注且计算每个地方之前的距离 不足：唯一的不足就是只支持在电脑上编辑，手机上只能查看，无法编辑 游玩地点选择 在网上找了很多关于青岛看海玩水攻略，有几个我大概率选择的地方，也是网友们推荐的地方：\n黄岛区：金沙滩。黄岛区其实有2个沙滩，网友们评价都比较好，首先是金沙滩，然后是银沙滩，可惜的是它们俩相隔较远，且交通不是很方便，所以我重点选择了金沙滩 市南区：市南区玩的地方就多了，且可贵的是它们都集中在一起，相隔都非常的近。选择有很多，我选了几个我想去的地方，分别是栈桥、海军博物馆、青岛海底世界、第一海水浴场、五四广场、石老头海水浴场等 第1天：先玩市南区，游玩地点基本都在一起，其中石老人海水浴场距离有点远，作为备选项，到时看情况是否游玩\n第2天：黄岛区，核心目标是金沙滩，其中城市阳台景区沙滩和银沙滩海水浴场作为备选项，到时根据时间情况决定是否游玩\n交通工具选择 游玩地点选择好了，交通工具就比较好选了。\n离市南区游玩地点最近的站点就是青岛站火车站了，因此计划是坐火车去青岛了。上海-青岛站的火车每天有好几趟，其中D2906（08:41~14:25，全程5小时44分钟）车次出发和到达时间点都比较完美：\n出发：早上可不必特意早起，不慌不忙坐地铁去火车站 到达：下午到达，在办理完了入住，还有时间去周边转转 住宿地方选择 游玩地点和交通工具都选择好了，酒店也好选择了：火车站附近，离游玩地方不要太远。\n飞猪上搜索酒店，我选择的是青岛站附近，结果第一家就是维也纳国际酒店·山东青岛火车站东广场栈桥店，距离火车站和游玩地点真是近，真是理想之选：\n最后 期待青岛之旅能完美顺利，到时我在来更新一下整个游玩体验~\n更新：本次旅游行程和个人感受：我的2023暑假青岛3天2晚旅游体验和感受\n","date":"2023-07-23T00:00:00Z","image":"https://ntopic.cn/p/2023072301/01_huc82d7b839fc53b7a5c86bd3cdfedcac6_371739_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/2023072301/","title":"我的2023暑假青岛3天2晚旅游计划"},{"content":"源代码先行：\nGitee多模块项目仓库：https://gitee.com/obullxl/ntopic-boot GitHub多模块项目仓库：https://github.com/obullxl/ntopic-boot 背景介绍 我们项目采用的是Maven多模块架构，我发现项目的部分子模块的pom.xml中重复引用了相同的JAR包。很明显，当初在配置Maven模块的时候，没有考虑清楚各个模块的架构职责，同时也不了解Maven模块依赖的传递性。本文主要介绍一下Maven多模块的配置思路和多模块的配置实操。\nMaven多模块配置 在实操之前，我们先要了解配置概览，配置大致可分为三大步：确定项目需要哪几个模块，项目中的每个模块的依赖关系如何，最后根据依赖关系配置。\n第一步：确定项目的模块划分 模块的划分没有任何强制要求，一般的划分思路如下：\ntest 集成测试模块：该模块除了测试用例代码外，没有实际业务逻辑。项目中我们用junit和Mockito测试框架比较多，Mockito集成测试框架和SpringBoot集成可我之前的博客：https://ntopic.cn/p/2023052001/ facade 对外服务接口模块（如果有的话，一般业务类项目较小，服务类较大）：主要是对外提供服务接口、请求参数和返回结果等JAR包，其他项目需要依赖项目JAR包，因此需要单独一个模块 client 集成外部服务接口模块：和facade对应，如果依赖了外部服务的话 lang 基础公用模块：各个模块都会依赖的公共类模块，比如常用工具类、通用枚举等 das 数据访问模块：接入DB层的代码，包括DO/DAO等 service 领域核心服务模块：各个业务领域的核心服务逻辑 web/biz 业务模块：业务逻辑模块。如果业务比较复杂，可进一步拆分，如web-user/web-order等 根据上面说明，https://gitee.com/obullxl/ntopic-boot的模块划分如下：\nntopic-test 集成测试模块 ntopic 业务逻辑模块，包括SpringBoot启动类、Web模块、业务模块等 ntopic-servcie 领域核心服务 ntopic-client 外部服务器集成 ntopic-das DB操作 ntopic-lang 公共模块 IDEA代码结构 GitHub代码样例 第二步：确定各个模块的依赖关系 Maven模块的依赖具备传递性，即：若模块A依赖了模块B，模块B依赖了模块C，则模块A自动依赖了模块C（有点类似于数学中数值大小的传递性，A\u0026gt;B且B\u0026gt;C，则A\u0026gt;C）。 https://gitee.com/obullxl/ntopic-boot项目的各个模块依赖关系说明：\nntopic-test 集成测试模块：因为集成测试包括了所有的代码逻辑，所以它处于最上层 ntopic 业务模块，业务逻辑依赖service模块提供核心服务 ntopic-service 核心服务模块，它依赖DB数据读写和引入外部服务 ntopic-lang 公共逻辑模块，它进一步依赖其他统一模块，如commons模块等 第三步：Maven多模块实操 多模块的核心在pom.xml文件中，任何一个pom.xml模块，都需要指定五个核心配置元素：\ngroupId 代表大分组，一般都是公司的域名，如 cn.ntopic / com.aliaba 等 artifactId 代表具体的JAR包名，如 sequence-jdbc / fastjson 等 version 代表JAR包版本，如 1.0.1 / 1.2.76 等 packaging 代表模块打包方式，默认都是jar，对于多模块的总模块或者父模块为pom name 代表模块名称，可选配置，建议配置 总pom.xml文件 总pom.xml配置了整个项目的所有信息，包括项目的模块列表、依赖的JAR包、仓库和打包方式等。\n指定父模块：可选的，可以没有父模块。如ntopic-boot是基于SpringBoot框架，所以它的父模块是SpringBoot的。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;project xmlns=\u0026#34;http://maven.apache.org/POM/4.0.0\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xsi:schemaLocation=\u0026#34;http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd\u0026#34;\u0026gt; \u0026lt;modelVersion\u0026gt;4.0.0\u0026lt;/modelVersion\u0026gt; \u0026lt;parent\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-parent\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.5.3\u0026lt;/version\u0026gt; \u0026lt;relativePath/\u0026gt; \u0026lt;/parent\u0026gt; \u0026lt;groupId\u0026gt;ntopic\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;ntopic-parent\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.0.1\u0026lt;/version\u0026gt; \u0026lt;packaging\u0026gt;pom\u0026lt;/packaging\u0026gt; \u0026lt;name\u0026gt;ntopic-parent\u0026lt;/name\u0026gt; 指定模块本项目的模块列表： 1 2 3 4 5 6 7 8 \u0026lt;modules\u0026gt; \u0026lt;module\u0026gt;ntopic-test\u0026lt;/module\u0026gt; \u0026lt;module\u0026gt;ntopic\u0026lt;/module\u0026gt; \u0026lt;module\u0026gt;ntopic-service\u0026lt;/module\u0026gt; \u0026lt;module\u0026gt;ntopic-das\u0026lt;/module\u0026gt; \u0026lt;module\u0026gt;ntopic-client\u0026lt;/module\u0026gt; \u0026lt;module\u0026gt;ntopic-lang\u0026lt;/module\u0026gt; \u0026lt;/modules\u0026gt; 模块依赖管理，各个子模块中，可以直接使用这里配置的依赖： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 \u0026lt;dependencyManagement\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;!-- NTopic Modules --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;ntopic\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;ntopic-lang\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${ntopic.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;ntopic\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;ntopic-client\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${ntopic.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- 其他本项目子模块忽略 --\u0026gt; \u0026lt;!-- SpringBoot Starter --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-jdbc\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${springboot.boot.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- Google Guava --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.google.guava\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;guava\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;30.1.1-jre\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- 其他省略 --\u0026gt; \u0026lt;/dependencies\u0026gt; \u0026lt;/dependencyManagement\u0026gt; 额外的Maven仓库和打包方式配置： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 \u0026lt;repositories\u0026gt; \u0026lt;repository\u0026gt; \u0026lt;id\u0026gt;Gitee-obullxl\u0026lt;/id\u0026gt; \u0026lt;url\u0026gt;https://gitee.com/obullxl/maven-repository/raw/sequence-jdbc\u0026lt;/url\u0026gt; \u0026lt;/repository\u0026gt; \u0026lt;/repositories\u0026gt; \u0026lt;build\u0026gt; \u0026lt;plugins\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${springboot.boot.version}\u0026lt;/version\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;/plugins\u0026gt; \u0026lt;/build\u0026gt; 子模块pom.xml文件 总模块配置好了之后，子模块的配置就简单多了，只需要配置3个信息块：父模块、四元素和依赖其它子模块。其中依赖的子模块按照第二步中的依赖关系配置即可：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;project xmlns=\u0026#34;http://maven.apache.org/POM/4.0.0\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xsi:schemaLocation=\u0026#34;http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd\u0026#34;\u0026gt; \u0026lt;modelVersion\u0026gt;4.0.0\u0026lt;/modelVersion\u0026gt; \u0026lt;parent\u0026gt; \u0026lt;groupId\u0026gt;ntopic\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;ntopic-parent\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.0.1\u0026lt;/version\u0026gt; \u0026lt;/parent\u0026gt; \u0026lt;artifactId\u0026gt;ntopic-service\u0026lt;/artifactId\u0026gt; \u0026lt;packaging\u0026gt;jar\u0026lt;/packaging\u0026gt; \u0026lt;version\u0026gt;${ntopic.version}\u0026lt;/version\u0026gt; \u0026lt;name\u0026gt;ntopic-service\u0026lt;/name\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;!-- NTopic Module --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;ntopic\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;ntopic-das\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;ntopic\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;ntopic-client\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; \u0026lt;/project\u0026gt; 编译项目各个模块 经过上面的配置，项目Maven多模块架构已经配置完成，进行项目编译即可：\n1 mvn compile 编译成功的结果：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 [INFO] ------------------------------------------------------------------------ [INFO] Reactor Summary: [INFO] [INFO] ntopic-parent ...................................... SUCCESS [ 0.022 s] [INFO] ntopic-lang ........................................ SUCCESS [ 0.873 s] [INFO] ntopic-das ......................................... SUCCESS [ 0.121 s] [INFO] ntopic-client ...................................... SUCCESS [ 0.042 s] [INFO] ntopic-service ..................................... SUCCESS [ 0.055 s] [INFO] ntopic ............................................. SUCCESS [ 0.073 s] [INFO] ntopic-test ........................................ SUCCESS [ 0.114 s] [INFO] ------------------------------------------------------------------------ [INFO] BUILD SUCCESS [INFO] ------------------------------------------------------------------------ [INFO] Total time: 2.094 s [INFO] Finished at: 2023-07-15T13:41:57+08:00 [INFO] Final Memory: 35M/448M [INFO] ------------------------------------------------------------------------ 我的本博客原地址：https://ntopic.cn/p/2023071501\n","date":"2023-07-15T00:00:00Z","image":"https://ntopic.cn/p/2023071501/01_huc82d7b839fc53b7a5c86bd3cdfedcac6_126567_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/2023071501/","title":"Maven多模块项目架构配置介绍和实战"},{"content":"源代码先行：\nGitee本文介绍的完整仓库：https://gitee.com/obullxl/ntopic-boot GitHub本文介绍的完整仓库：https://github.com/obullxl/ntopic-boot 背景介绍 在Java众多的ORM框架里面，MyBatis是比较轻量级框架之一，既有数据表和Java对象映射功能，在SQL编写方面又不失原生SQL的特性。SpringBoot提倡使用注解代替XML配置，同样的，在集成MyBatis时也可以做到全注解化，无XML配置。相关的集成方法网上存在大量的教程，本文是个人在实际项目过程的一个备忘，并不是复制和粘贴，同时在本文后面提供了完整的集成测试用例。\nMyBatis集成 涉及到以下4个方面：\n我们Maven工程是一个SpringBoot工程 引入MyBatis的Starter依赖 SpringBoot工程配置中增加MyBatis的配置 Mapper/DAO通过注解实现 SpringBoot工程依赖 1 2 3 4 5 6 7 8 9 10 11 12 13 \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;project xmlns=\u0026#34;http://maven.apache.org/POM/4.0.0\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xsi:schemaLocation=\u0026#34;http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd\u0026#34;\u0026gt; \u0026lt;modelVersion\u0026gt;4.0.0\u0026lt;/modelVersion\u0026gt; \u0026lt;parent\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-parent\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.5.3\u0026lt;/version\u0026gt; \u0026lt;relativePath/\u0026gt; \u0026lt;/parent\u0026gt; \u0026lt;!-- 其他部分省略 --\u0026gt; \u0026lt;/project\u0026gt; MyBatis Starter依赖 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;project xmlns=\u0026#34;http://maven.apache.org/POM/4.0.0\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xsi:schemaLocation=\u0026#34;http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd\u0026#34;\u0026gt; \u0026lt;!-- 其他省略 --\u0026gt; \u0026lt;dependencyManagement\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;!-- MyBatis --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.mybatis.spring.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;mybatis-spring-boot-starter\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.2.0\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- MySQL --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;mysql\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;mysql-connector-java\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;8.0.26\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- SQLite --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.xerial\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;sqlite-jdbc\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.39.2.0\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- Druid --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.alibaba\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;druid\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.1.2\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- 其他省略 --\u0026gt; \u0026lt;/dependencies\u0026gt; \u0026lt;/dependencyManagement\u0026gt; \u0026lt;!-- 其他省略 --\u0026gt; \u0026lt;/project\u0026gt; SpringBoot Main配置 application.properties配置，增加DB数据源： 1 2 3 4 5 6 7 # # 数据库：url值设置成自己的文件路径 # ntopic.datasource.driver-class-name=org.sqlite.JDBC ntopic.datasource.url=jdbc:sqlite:./../NTopicBoot.sqlite ntopic.datasource.userName= ntopic.datasource.password= MapperScan注解：指明MyBatis的Mapper在哪写包中，cn.ntopic.das..**.dao指明，我们的Mapper类所在的包 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 /** * Author: obullxl@163.com * Copyright (c) 2020-2021 All Rights Reserved. */ package cn.ntopic; import com.alibaba.druid.pool.DruidDataSource; import org.apache.ibatis.session.Configuration; import org.mybatis.spring.SqlSessionFactoryBean; import org.mybatis.spring.annotation.MapperScan; import org.springframework.beans.factory.annotation.Qualifier; import org.springframework.beans.factory.annotation.Value; import org.springframework.boot.SpringApplication; import org.springframework.boot.autoconfigure.SpringBootApplication; import org.springframework.context.annotation.Bean; import org.springframework.jdbc.core.JdbcTemplate; import org.springframework.jdbc.datasource.DataSourceTransactionManager; import org.springframework.transaction.TransactionDefinition; import org.springframework.transaction.support.TransactionTemplate; import javax.sql.DataSource; /** * NTopic启动器 * * @author obullxl 2021年06月05日: 新增 */ @SpringBootApplication @MapperScan(basePackages = \u0026#34;cn.ntopic.das..**.dao\u0026#34;, sqlSessionFactoryRef = \u0026#34;ntSqlSessionFactory\u0026#34;) public class NTBootApplication { /** * SpringBoot启动 */ public static void main(String[] args) { SpringApplication.run(NTBootApplication.class, args); } /** * DataSource配置 */ @Bean(name = \u0026#34;ntDataSource\u0026#34;, initMethod = \u0026#34;init\u0026#34;, destroyMethod = \u0026#34;close\u0026#34;) public DruidDataSource ntDataSource(@Value(\u0026#34;${ntopic.datasource.url}\u0026#34;) String url , @Value(\u0026#34;${ntopic.datasource.userName}\u0026#34;) String userName , @Value(\u0026#34;${ntopic.datasource.password}\u0026#34;) String password) { DruidDataSource dataSource = new DruidDataSource(); dataSource.setUrl(url); dataSource.setUsername(userName); dataSource.setPassword(password); dataSource.setInitialSize(0); dataSource.setMinIdle(1); dataSource.setMaxActive(5); dataSource.setMaxWait(3000L); return dataSource; } /** * MyBatis事务配置 */ @Bean(\u0026#34;ntSqlSessionFactory\u0026#34;) public SqlSessionFactoryBean ntSqlSessionFactory(@Qualifier(\u0026#34;ntDataSource\u0026#34;) DataSource dataSource) { SqlSessionFactoryBean sqlSessionFactoryBean = new SqlSessionFactoryBean(); sqlSessionFactoryBean.setDataSource(dataSource); Configuration configuration = new Configuration(); configuration.setMapUnderscoreToCamelCase(true); sqlSessionFactoryBean.setConfiguration(configuration); return sqlSessionFactoryBean; } /** 其他代码省略 */ } MyBatis Mapper类/DAO类 几个核心的注解：\nInsert插入 Select查询 Update更新 Delete删除 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 /** * Author: obullxl@163.com * Copyright (c) 2020-2021 All Rights Reserved. */ package cn.ntopic.das.dao; import cn.ntopic.das.model.UserBaseDO; import org.apache.ibatis.annotations.*; import org.springframework.stereotype.Repository; import java.util.Date; import java.util.List; /** * @author obullxl 2021年10月17日: 新增 */ @Repository(\u0026#34;userBaseDAO\u0026#34;) public interface UserBaseDAO { /** * 新增用户记录 */ @Insert(\u0026#34;INSERT INTO nt_user_base (id, name, password, role_list, ext_map, create_time, modify_time)\u0026#34; + \u0026#34; VALUES (#{id,jdbcType=VARCHAR}, #{name,jdbcType=VARCHAR}, #{password,jdbcType=VARCHAR}, #{roleList,jdbcType=VARCHAR}, #{extMap,jdbcType=VARCHAR}, #{createTime,jdbcType=TIMESTAMP}, #{modifyTime,jdbcType=TIMESTAMP})\u0026#34;) void insert(UserBaseDO userBaseDO); /** * 根据ID查询记录 */ @Select(\u0026#34;SELECT * FROM nt_user_base WHERE id = #{id,jdbcType=VARCHAR}\u0026#34;) UserBaseDO selectById(@Param(\u0026#34;id\u0026#34;) String id); /** * 根据名称查询记录 */ @Select(\u0026#34;SELECT * FROM nt_user_base WHERE name = #{name,jdbcType=VARCHAR}\u0026#34;) UserBaseDO selectByName(@Param(\u0026#34;name\u0026#34;) String name); /** * 查询所有用户 */ @Select(\u0026#34;SELECT * FROM nt_user_base LIMIT 30\u0026#34;) List\u0026lt;UserBaseDO\u0026gt; select(); /** * 更新角色列表 * FIXME: SQLite/MySQL 当前时间函数不一致，本次通过入参传入！ */ @Update(\u0026#34;UPDATE nt_user_base SET modify_time=#{modifyTime,jdbcType=TIMESTAMP}, role_list=#{roleList,jdbcType=VARCHAR} WHERE id=#{id,jdbcType=VARCHAR}\u0026#34;) int updateRoleList(@Param(\u0026#34;id\u0026#34;) String id, @Param(\u0026#34;modifyTime\u0026#34;) Date modifyTime, @Param(\u0026#34;roleList\u0026#34;) String roleList); /** * 删除用户记录 */ @Delete(\u0026#34;DELETE FROM nt_user_base WHERE id = #{id,jdbcType=VARCHAR}\u0026#34;) int deleteById(@Param(\u0026#34;id\u0026#34;) String id); /** * 删除用户记录 */ @Delete(\u0026#34;DELETE FROM nt_user_base WHERE name = #{name,jdbcType=VARCHAR}\u0026#34;) int deleteByName(@Param(\u0026#34;name\u0026#34;) String name); } 集成测试（包括CRUD操作） 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 /** * Author: obullxl@163.com * Copyright (c) 2020-2021 All Rights Reserved. */ package cn.ntopic.das.dao; import cn.ntopic.das.model.UserBaseDO; import org.apache.commons.lang3.RandomUtils; import org.apache.commons.lang3.StringUtils; import org.junit.Assert; import org.junit.Test; import org.junit.runner.RunWith; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.beans.factory.annotation.Qualifier; import org.springframework.boot.test.context.SpringBootTest; import org.springframework.test.context.junit4.AbstractJUnit4SpringContextTests; import org.springframework.test.context.junit4.SpringRunner; import java.util.Date; /** * @author obullxl 2021年10月17日: 新增 */ @SpringBootTest @RunWith(SpringRunner.class) public class UserBaseDAOTest extends AbstractJUnit4SpringContextTests { @Autowired @Qualifier(\u0026#34;userBaseDAO\u0026#34;) private UserBaseDAO userBaseDAO; /** * CRUD简单测试 */ @Test public void test_insert_select_update_delete() { final String id = \u0026#34;ID-\u0026#34; + RandomUtils.nextLong(); final String name = \u0026#34;NAME-\u0026#34; + RandomUtils.nextLong(); // 1. 清理数据 this.userBaseDAO.deleteById(id); this.userBaseDAO.deleteByName(name); // 请求数据 - 验证 UserBaseDO userBaseDO = this.userBaseDAO.selectById(id); Assert.assertNull(userBaseDO); userBaseDO = this.userBaseDAO.selectByName(name); Assert.assertNull(userBaseDO); try { // 2. 新增数据 UserBaseDO newUserDO = new UserBaseDO(); newUserDO.setId(id); newUserDO.setName(name); newUserDO.setCreateTime(new Date()); newUserDO.setModifyTime(new Date()); this.userBaseDAO.insert(newUserDO); // 3. 数据查询 - 新增验证 UserBaseDO checkUserDO = this.userBaseDAO.selectById(id); Assert.assertNotNull(checkUserDO); Assert.assertEquals(name, checkUserDO.getName()); Assert.assertNotNull(checkUserDO.getCreateTime()); Assert.assertNotNull(checkUserDO.getModifyTime()); Assert.assertTrue(StringUtils.isBlank(checkUserDO.getRoleList())); // 4. 更新数据 int update = this.userBaseDAO.updateRoleList(id, new Date(), \u0026#34;ADMIN,DATA\u0026#34;); Assert.assertEquals(1, update); // 更新数据 - 验证 checkUserDO = this.userBaseDAO.selectById(id); Assert.assertNotNull(checkUserDO); Assert.assertEquals(\u0026#34;ADMIN,DATA\u0026#34;, checkUserDO.getRoleList()); // 5. 数据删除 int delete = this.userBaseDAO.deleteById(id); Assert.assertEquals(1, delete); delete = this.userBaseDAO.deleteByName(name); Assert.assertEquals(0, delete); } finally { // 清理数据 this.userBaseDAO.deleteById(id); } } } 关注本公众号，我们共同学习进步 👇🏻👇🏻👇🏻\n我的本博客原地址：https://mp.weixin.qq.com/s/JpvF9PfihYDZffPRKMDWwg\n","date":"2023-07-08T00:00:00Z","image":"https://ntopic.cn/p/2023070801/01_hu3377170802d75c4f54f43c00114013c1_35289_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/2023070801/","title":"SpringBoot与MyBatis零XML配置集成和集成测试"},{"content":"我开源的JAR包的Gitee和GitHub托管的Maven仓库：\nGitee托管仓库：https://gitee.com/obullxl/maven-repository GitHub托管仓库：https://github.com/obullxl/maven-repository 背景说明 在上一篇博客中，我们介绍了开源通用高性能分布式id序列组件（https://ntopic.cn/p/2023062101/）的设计思路，并把源代码托管在了Gitee（https://gitee.com/obullxl/sequence-jdbc）和GitHub（https://github.com/obullxl/sequence-jdbc）。\n我们希望能让更多人便捷的使用本组件，那么把JAR包放到到Maven官方的中心仓库（https://mvnrepository.com）当然是最好的选择。\n然而要把JAR包上传到Maven官方中心仓库，步骤比较繁琐，包括注册、申请、发布配置等一系列操作。其实我们的本意只是想把自己的开源项目打包让大家方便使用，能否有更快捷的方式呢？当然是有的，我们可以使用Gitee或者GitHub作为Maven托管仓库，把我们的组件JAR包存储到托管仓库中。\nGitee/GitHub仓库设置 由于Gitee和GitHub原理完全一致，下面截图说明以Gitee为主（GitHub是我们的主仓库，Gitee只是同步GitHub仓库，但这不妨碍我们的配置）。\n建议在Gitee中单独申请一个仓库，专门用于存放JAR包，比如我的仓库叫maven-repository：https://gitee.com/obullxl/maven-repository\n同时，便于后续多个组件的JAR包能共用一个托管仓库，JAR包统一放到仓库的repository目录中：\n特别注意：仓库请请设置为开源，否则其他人使用Maven托管仓库可能无法访问，从而无法下载组件JAR包：\n打包发布JAR包到仓库 Gitee托管仓库设置好之后，开始设置我们打包并发布JAR包了。为便于后面设置打包命令，我们把托管Maven仓库的目录maven-repository和id序列组件仓库的目录sequence-jdbc放在同一个父目录中：\n1 2 3 OXL-MacBook:CodeSpace obullxl$ ll drwxr-xr-x 7 obullxl staff 224 6 24 10:30 maven-repository drwxr-xr-x 13 obullxl staff 416 6 24 17:42 sequence-jdbc 组件pom.xml打包配置 完整的配置可直接参考分布式id序列的设置：https://gitee.com/obullxl/sequence-jdbc/blob/master/pom.xml\npom.xml文件，一定需要定义groupId/artifactId/version这Maven依赖坐标三要素： 1 2 3 4 \u0026lt;groupId\u0026gt;cn.ntopic\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;sequence-jdbc\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.0.2\u0026lt;/version\u0026gt; \u0026lt;packaging\u0026gt;jar\u0026lt;/packaging\u0026gt; pom.xml文件，配置build节点，指定JAR打包、Deploy发布的配置（发布到Maven仓库的目录：../maven-repository/repository），即以下配置的altDeploymentRepository内容： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 \u0026lt;build\u0026gt; \u0026lt;plugins\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.apache.maven.plugins\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;maven-compiler-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.3.2\u0026lt;/version\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;source\u0026gt;1.8\u0026lt;/source\u0026gt; \u0026lt;target\u0026gt;1.8\u0026lt;/target\u0026gt; \u0026lt;encoding\u0026gt;UTF-8\u0026lt;/encoding\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.apache.maven.plugins\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;maven-jar-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.5\u0026lt;/version\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.apache.maven.plugins\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;maven-source-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.1.2\u0026lt;/version\u0026gt; \u0026lt;executions\u0026gt; \u0026lt;execution\u0026gt; \u0026lt;phase\u0026gt;package\u0026lt;/phase\u0026gt; \u0026lt;goals\u0026gt; \u0026lt;goal\u0026gt;jar\u0026lt;/goal\u0026gt; \u0026lt;/goals\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;/executions\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.codehaus.mojo\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;build-helper-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.9.1\u0026lt;/version\u0026gt; \u0026lt;executions\u0026gt; \u0026lt;execution\u0026gt; \u0026lt;id\u0026gt;timestamp-property\u0026lt;/id\u0026gt; \u0026lt;goals\u0026gt; \u0026lt;goal\u0026gt;timestamp-property\u0026lt;/goal\u0026gt; \u0026lt;/goals\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;/executions\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;name\u0026gt;BuildTime\u0026lt;/name\u0026gt; \u0026lt;pattern\u0026gt;yyyy-MM-dd HH:mm:ss.SSS\u0026lt;/pattern\u0026gt; \u0026lt;timeZone\u0026gt;GMT+8\u0026lt;/timeZone\u0026gt; \u0026lt;regex/\u0026gt; \u0026lt;source/\u0026gt; \u0026lt;value/\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.apache.maven.plugins\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;maven-antrun-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.3\u0026lt;/version\u0026gt; \u0026lt;executions\u0026gt; \u0026lt;execution\u0026gt; \u0026lt;id\u0026gt;generate-release\u0026lt;/id\u0026gt; \u0026lt;phase\u0026gt;compile\u0026lt;/phase\u0026gt; \u0026lt;goals\u0026gt; \u0026lt;goal\u0026gt;run\u0026lt;/goal\u0026gt; \u0026lt;/goals\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;tasks\u0026gt; \u0026lt;!--suppress UnresolvedMavenProperty --\u0026gt; \u0026lt;echo file=\u0026#34;${project.basedir}/target/classes/NTopic.Release\u0026#34; message=\u0026#34;Version=${project.version}${line.separator}BuildTime=${BuildTime}\u0026#34; /\u0026gt; \u0026lt;/tasks\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;/executions\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.apache.maven.plugins\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;maven-deploy-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.7\u0026lt;/version\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;!-- 特别注意的地方：指定打包的目录 --\u0026gt; \u0026lt;altDeploymentRepository\u0026gt;internal.repo::default::file://${project.basedir}/../maven-repository/repository\u0026lt;/altDeploymentRepository\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;/plugins\u0026gt; \u0026lt;/build\u0026gt; 打包并上传到仓库 打包并发布到本地目录命令： 1 2 mvn clean mvn deploy -Dmaven.test.skip=true 上传到远程仓库命令： 1 2 3 4 cd ./../maven-repository git add --all git commit -m \u0026#39;Deploy sequence-jdbc JAR: https://github.com/obullxl/sequence-jdbc\u0026#39; git push origin master 完整的打包命令，请参考分布式id序列源仓库代码：https://gitee.com/obullxl/sequence-jdbc/blob/master/deploy.sh：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 #!/bin/bash # 本地打包 mvn clean \u0026amp;\u0026amp; mvn deploy -Dmaven.test.skip=true # 上传仓库 cd ./../maven-repository git add --all git commit -m \u0026#39;Deploy sequence-jdbc JAR: https://github.com/obullxl/sequence-jdbc\u0026#39; git push origin master # 返回项目 cd ../sequence-jdbc # Gitee刷新：人工刷新仓库，从GitHub同步过来 open -a \u0026#39;/Applications/Microsoft Edge.app\u0026#39; https://gitee.com/obullxl/maven-repository 多个版本完整的Maven托管仓库内容：\n其他项目使用JAR包方法 和Maven官方的中心仓库相比，Gitee托管仓库没有本质区别，只需要在pom.xml中配置Gitee的托管仓库即可，让Maven知道从哪儿去下载JAR包。\npom.xml中增加仓库 pom.xml中增加Gitee托管仓库地址：\n1 2 3 4 5 6 \u0026lt;repositories\u0026gt; \u0026lt;repository\u0026gt; \u0026lt;id\u0026gt;Gitee-obullxl\u0026lt;/id\u0026gt; \u0026lt;url\u0026gt;https://gitee.com/obullxl/maven-repository/raw/master/repository\u0026lt;/url\u0026gt; \u0026lt;/repository\u0026gt; \u0026lt;/repositories\u0026gt; 或者增加GitHub托管仓库地址：\n1 2 3 4 5 6 \u0026lt;repositories\u0026gt; \u0026lt;repository\u0026gt; \u0026lt;id\u0026gt;GitHub-obullxl\u0026lt;/id\u0026gt; \u0026lt;url\u0026gt;https://raw.githubusercontent.com/obullxl/maven-repository/master/repository\u0026lt;/url\u0026gt; \u0026lt;/repository\u0026gt; \u0026lt;/repositories\u0026gt; Maven配置依赖 和其他JAR包一样，pom.xml中增加依赖坐标：\n1 2 3 4 5 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;cn.ntopic\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;sequence-jdbc\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.0.2\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; ","date":"2023-06-22T00:00:00Z","image":"https://ntopic.cn/p/2023062201/01_huf923c7ced42b0cdac23869393834d473_15259_120x120_fill_box_smart1_3.png","permalink":"https://ntopic.cn/p/2023062201/","title":"使用Gitee或GitHub托管Maven仓库JAR包的便捷方法"},{"content":" Gitee源代码仓库：https://gitee.com/obullxl/sequence-jdbc GitHub源代码仓库：https://github.com/obullxl/sequence-jdbc 分布式id序列说明 业务数据的存储，少不了数据记录的id序列。\nid序列（或称序列）的生成方式有很多种，比如当前时间戳、数据库的序列值（Oracle的序列，MySQL的自增ID等）、UUID等方式。\n这些生成方式都有一定的局限性，如时间戳在业务量较大时容易重复、Oracle序列和MySQL的自增ID限定了数据库类型（且MySQL的自增ID只能保证单库唯一，在分库分表的场景下也不适用）、UUID容易重复且无法保证递增等。\n同时，一般业务数据的id序列通常会带上一些业务信息，比如增加业务标识前缀、增加年月日等信息。业务id序列的处理变得多样，则进一步要求业务id序列的生成通用且高效。\n通用分布式id序列组件 为了屏蔽业务获取id序列因数据库类型、分库分表等带来的研发和维护成本，我们把分布式id序列的获取抽取为一个通用组件，对业务统一接口和规范。\n通用分布式id序列实现方式有很多，本文主要介绍一种基于数据表的实现方式，通过一张表记录所有的业务序列名和值，业务根据序列名获取下一个序列值（和Oracle序列类型，但是无需为每个序列创建序列，因此更简单）：\n本方案的设计主要考量点：\n通用性：仅依赖一张序列数据表，JDBC支持的数据库均可使用，包括SQLite、MySQL、Oracle、OceanBase等。 高性能：本地缓存一个序列区间，缓存使用完之前无DB交互；缓存的区间可设置，区间越大，DB访问越少，性能越高。 分布式：受益于集中式的序列数据表，保证了序列全局唯一。 分布式id序列组件设计 组件接口设计 组件接口只有1个，就是获取序列：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 /** * Author: obullxl@163.com * Copyright (c) 2020-2023 All Rights Reserved. */ package cn.ntopic.sequence; /** * 分布式序列服务 * * @author obullxl 2023年06月21日: 新增 */ public interface NTSequence { /** * 默认序列名称 */ String DEFAULT_SEQUENCE_NAME = \u0026#34;DEFAULT\u0026#34;; /** * 序列名称最大长度 */ int MAX_SEQUENCE_NAME_LENGTH = 64; /** * 获取下一个序列值 * * @return 获取默认序列的新的唯一的序列值 {@link #DEFAULT_SEQUENCE_NAME} * @throws IllegalArgumentException 参数非法 */ default long next() { return this.next(DEFAULT_SEQUENCE_NAME); } /** * 获取下一个序列值 * * @param sequenceName 序列名称，非空，1~64字符，业务可随意指定（如：用户模块为`USER`，订单模块为`ORDER`等） * @return 新的唯一的序列值 * @throws IllegalArgumentException 参数非法 */ long next(String sequenceName); } 组件可控参数 在追求通用性和性能的同时，以参数的方式供个性化调控：\nid序列数据表名：默认值为nt_sequence；但对于不同的业务，对数据表名有要求规范（如：表名前置等），因此序列数据表名可设置 id序列起始值：默认值为1，即序列值从1开始递增；但对于存量业务，id值起始值需要比存量最大值要大，否则容易重复 id序列最大值：默认值为99999999，序列值递增到最大值，则循环从起始值开始 序列更新重试次数：默认值为10，当缓存序列用尽，需要查询和更新序列数据表，比较存在网络通讯和DB操作，不可避免存在异常失败，失败后会进行重试 序列缓存大小：默认值为1000，值越大，访问DB越少，性能越高，序列的连续性越差（如：缓存大小为1000，当序列用到400时，服务器重启了，那么401~1000直接的序列就丢失了）；相反，值越小，访问DB越多，性能越低，序列的连续性越好。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 /** * 属性-数据源 */ private final DataSource ntDataSource; /** * 属性-重试次数 */ private int retryTimes = 10; /** * 属性-数据表名 */ private String tableName = \u0026#34;nt_sequence\u0026#34;; /** * 属性-序列获取步长（即序列缓存大小） */ private long step = 1000L; /** * 属性-序列最小值 */ private long minValue = 1L; /** * 属性-序列最大值 */ private long maxValue = 99999999L; 序列组件使用 Gitee源代码仓库：https://gitee.com/obullxl/sequence-jdbc GitHub源代码仓库：https://github.com/obullxl/sequence-jdbc 目前JAR包已经发布，通过Gitee进行仓库托管，也可直接使用，2步即可：\n第一步：在项目源代码的根pom.xml中，设置仓库地址： 1 2 3 4 5 6 \u0026lt;repositories\u0026gt; \u0026lt;repository\u0026gt; \u0026lt;id\u0026gt;Gitee-obullxl\u0026lt;/id\u0026gt; \u0026lt;url\u0026gt;https://gitee.com/obullxl/maven-repository/master/repository\u0026lt;/url\u0026gt; \u0026lt;/repository\u0026gt; \u0026lt;/repositories\u0026gt; 第二步：引用JAR包，仅需要依赖本JAR包，无其他JAR包依赖： 1 2 3 4 5 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;cn.ntopic\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;sequence-jdbc\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.0.1\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 在业务代码中调用序列组件样例：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 // 1. 构建数据源 DruidDataSource dataSource = new DruidDataSource(); dataSource.setUrl(\u0026#34;jdbc:sqlite:/Users/obullxl/CodeSpace/sequence-jdbc/SequenceJDBC.sqlite\u0026#34;); dataSource.setDriverClassName(\u0026#34;org.sqlite.JDBC\u0026#34;); dataSource.setPoolPreparedStatements(false); dataSource.setMaxPoolPreparedStatementPerConnectionSize(-1); dataSource.setTestOnBorrow(true); dataSource.setTestOnReturn(false); dataSource.setTestWhileIdle(true); dataSource.setValidationQuery(\u0026#34;SELECT \u0026#39;1\u0026#39; FROM sqlite_master LIMIT 1\u0026#34;); // 2. 初始化序列组件 NTSequenceImpl ntSequence = new NTSequenceImpl(dataSource); ntSequence.setTableName(tableName); ntSequence.createTable(); ntSequence.init(); // 3. 获取序列值 @Autowire @Qualifier(\u0026#34;ntSequence\u0026#34;) private NTSequence ntSequence; // 获取`DEFAULT`默认序列ID long newId1 = ntSequence.next(); long newId2 = ntSequence.next(); long newId3 = ntSequence.next(); // 获取`USER`用户ID： long newUserId1 = ntSequence.next(\u0026#34;USER\u0026#34;); long newUserId2 = ntSequence.next(\u0026#34;USER\u0026#34;); long newUserId3 = ntSequence.next(\u0026#34;USER\u0026#34;); // 获取`ORDER`订单ID： long newOrderId1 = ntSequence.next(\u0026#34;ORDER\u0026#34;); long newOrderId2 = ntSequence.next(\u0026#34;ORDER\u0026#34;); long newOrderId3 = ntSequence.next(\u0026#34;ORDER\u0026#34;); 完整的使用方法，可参数源代码仓库说明文档（README.md）：\nGitee源代码仓库：https://gitee.com/obullxl/sequence-jdbc GitHub源代码仓库：https://github.com/obullxl/sequence-jdbc 分布式id序列测试用例 因为代码较大，请直接查看源代码：\nGitee测试源代码：https://gitee.com/obullxl/sequence-jdbc/blob/master/src/test/java/cn/ntopic/sequence/NTSequenceTest.java GitHub测试源代码：https://github.com/obullxl/sequence-jdbc/blob/master/src/test/java/cn/ntopic/sequence/NTSequenceTest.java 我的本博客原地址：https://ntopic.cn/p/2023062101\n","date":"2023-06-21T00:00:00Z","image":"https://ntopic.cn/p/2023062101/01_huc82d7b839fc53b7a5c86bd3cdfedcac6_138083_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/2023062101/","title":"开源通用高性能的分布式id序列组件"},{"content":"思路说明 iframe的宽度伸张铺满100% 主页面加载完成之后，注册定时任务：每隔250毫秒 去检测一下iframe的高度是否变化，并重新调整iframe高度（如提交评论，iframe的高度会变高，此时主页面会自动调整） 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width, initial-scale=1.0, minimum-scale=1.0, maximum-scale=1.0,user-scalable=no\u0026#34;\u0026gt; \u0026lt;title\u0026gt;ntopic.cn\u0026lt;/title\u0026gt; \u0026lt;style type=\u0026#34;text/css\u0026#34;\u0026gt; .iframe-wrap { position: relative; box-sizing: border-box; width: 100%; max-width: unset; margin-left: auto; margin-right: auto; } .iframe { color-scheme: light; position: absolute; left: 0; right: 0; width: 1px; min-width: 100%; max-width: 100%; height: 100%; border: 0; } \u0026lt;/style\u0026gt; \u0026lt;script type=\u0026#34;text/javascript\u0026#34;\u0026gt; function onWindowLoad() { window.setInterval(\u0026#34;reInitIframe()\u0026#34;, 250); } function reInitIframe() { const iframe = document.getElementById(\u0026#34;iframe\u0026#34;); try { const newHeight = iframe.contentWindow.document.documentElement.scrollHeight; iframe.style.height = newHeight + \u0026#34;px\u0026#34;; document.getElementById(\u0026#34;iframeWrap\u0026#34;).style.height = newHeight + \u0026#34;px\u0026#34;; } catch (e) { } } \u0026lt;/script\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body onload=\u0026#34;onWindowLoad();\u0026#34;\u0026gt; \u0026lt;div id=\u0026#34;iframeWrap\u0026#34; class=\u0026#34;iframe-wrap\u0026#34; style=\u0026#34;height: 269px;\u0026#34;\u0026gt; \u0026lt;iframe id=\u0026#34;iframe\u0026#34; class=\u0026#34;frame\u0026#34; src=\u0026#34;https://ntopic.cn\u0026#34; loading=\u0026#34;lazy\u0026#34; style=\u0026#34;width:100%;\u0026#34; frameborder=\u0026#34;0\u0026#34; scrolling=\u0026#34;no\u0026#34;\u0026gt;\u0026lt;/iframe\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; ","date":"2023-06-02T00:00:00Z","image":"https://ntopic.cn/p/2023060201/01_hu69e5b7d727305e0c544b0ca1b399e437_123549_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/2023060201/","title":"iframe自适应高度设置方法"},{"content":"问题分析 当我们访问国外网站，特别是在提交注册、登录表单时，经常出现“ReCaptcha is not valid”、“Google recaptcha response is invalid”、“ReCaptcha error”等错误提示文案，导致表单提交失败，如下图所示：\n其原因是该网站采用了Google的人机验证ReCaptcha服务（简单类似于国内的验证码），该服务需要加载的主要JS脚本为：https://www.google.com/recaptcha/api.js ，因国内网络屏蔽了Google服务，导致无法加载本JS脚本\n解决方案 Google网络和服务不可用，但是有可用的服务网站，那就是：https://recaptcha.net/recaptcha/api.js 。我们可以通过浏览器插件，把访问Google的api.js重定向到recaptcha.net的服务。\n安装浏览器插件 插件有很多，我用的是：Header Editor，还有gooreplacer等插件，都可以使用。请自行在浏览器市场搜索该插件，并完成安装和启用。\n配置Header Editor插件（其他插件类似） 配置内容说明：\n名称：随便取一个名称即可（如：Google reCAPTCHA等） 规则类型：选择重定向请求 选项，代表请求Google的服务重定向到可用网络服务 匹配类型：选择正则表达式 选项，通过通配符等匹配，我们本次只有1个网站，所以也不需要通配符 匹配规则：输入www.google.com/recaptcha ，代表需要替换这部分内容 执行类型：选择常规 选项 重定向至：输入recaptcha.net/recaptcha ，代表替换Google的网站内容 配置完成之后，还可以测试一下：\n输入：https://www.google.com/recaptcha/api.js 输出：https://recaptcha.net/recaptcha/api.js 替换成功了。 重新提交表单，就能看到人机校验的弹框了：\n","date":"2023-05-27T00:00:00Z","image":"https://ntopic.cn/p/2023052701/images/01_hu76981e1ce0dd953fec5e99ce803de0c4_59486_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/2023052701/","title":"Google reCAPTCHA无效的解放方案"},{"content":"集成测试为代码质量提供保障 在进行系统迭代升级改造研发过程中，如何保障本次迭代改动的代码逻辑符合预期？如何保障本次迭代改动逻辑对原逻辑的影响面？一种办法是人工把端到端用例全部回归一遍（如通过页面入口、服务接口工具等），这种办法的效率低下且容易遗漏（如人工遗漏用例，页面入口不可见等）；另外一种办法就是通过集成测试用例集，通过精心设计的且运行稳定的集成测试用例集，为迭代研发的质量提供了强有力的保障，特别适用于纯接口类或者平台类系统！\n当一个系统的服务依赖另外一个系统的服务时，集成测试用例集的稳定运行和本系统的鲁棒性存在一定挑战：如在测试环境中依赖系统的服务不稳定可能导致集成测试用例的结果不稳定，如依赖系统服务的各个场景无法全面覆盖（如特定的错误码、数据查询的结果无法预知等）\nMockito框架可保障依赖服务稳定性 为了保障集成测试用例集的运行稳定性和提升用例场景的覆盖面，最常见的做法是模拟依赖系统服务（即集成测试执行并未真正调用依赖系统的服务，而是为达到测试目的对于服务进行模拟），以保障本系统功能逻辑的自闭环（即不依赖其他系统服务），让集成测试更聚焦于本系统的功能逻辑验证。\n在Spring/SpringBoot框架中，Mockito框架为集成测试用例执行模拟依赖服务提供了多种能力，基本覆盖了我们日常集成测试需求：\n正常场景：模拟正常返回结果 异常场景：模拟执行异常 内部逻辑验证： a. 检测被模拟方法的执行次数 b. 检测被模拟方法的执行入参内容 Mockito框架测试使用实战 实战场景： 订单查询服务可根据请求订单ID列表参数，对订单ID进行过滤，并调用另外远程客户端查询数据并返回订单信息，本服务存在以下逻辑点\n订单ID过滤逻辑：属于本系统逻辑，需要测试覆盖 远程订单查询服务：依赖服务，服务不可控，应该进行模拟 SpringBoot测试框架 的注解：\n1 2 3 @SpringBootTest @RunWith(SpringRunner.class) public abstract class NTAbstractTest extends AbstractJUnit4SpringContextTests { Mockito依赖类 的注解：Spring测试框架启动过程中，对于被 @MockBean注解的属性，在Spring上下文中替换原有真实的Bean，即被替换成 Mockito.mock(xx.class)\n1 2 3 4 5 6 7 /** * Mock对象 */ @MockBean @Autowired @Qualifier(\u0026#34;orderQueryClient\u0026#34;) private OrderQueryClient orderQueryClient; 测试目标类 - 订单查询服务接口（OrderQueryService.java）：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 /** * Author: obullxl@163.com * Copyright (c) 2020-2023 All Rights Reserved. */ package cn.ntopic.order; import cn.ntopic.ListResult; import cn.ntopic.order.model.OrderModel; import java.util.List; /** * 订单查询服务 * * @author obullxl 2023年05月20日: 新增 */ public interface OrderQueryService { /** * 根据订单ID查询订单列表，仅返回最近3个月的ID信息 * * @param orderIdList 订单ID列表 * @return 最近3个月的订单信息 */ ListResult\u0026lt;OrderModel\u0026gt; findList(List\u0026lt;String\u0026gt; orderIdList); } 测试目标类 - 订单查询服务逻辑（OrderQueryServiceImpl.java）：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 /** * Author: obullxl@163.com * Copyright (c) 2020-2023 All Rights Reserved. */ package cn.ntopic.order; import cn.ntopic.ListResult; import cn.ntopic.order.dto.OrderDTO; import cn.ntopic.order.model.OrderModel; import org.apache.commons.collections4.CollectionUtils; import org.apache.commons.lang3.StringUtils; import org.springframework.beans.factory.annotation.Qualifier; import org.springframework.stereotype.Component; import java.time.LocalDate; import java.time.format.DateTimeFormatter; import java.time.format.DateTimeParseException; import java.util.List; import java.util.stream.Collectors; /** * 订单查询服务实现 * * @author obullxl 2023年05月20日: 新增 */ @Component(\u0026#34;orderQueryService\u0026#34;) public class OrderQueryServiceImpl implements OrderQueryService { /** * 订单查询客户端，远程服务不可控 */ private final OrderQueryClient orderQueryClient; public OrderQueryServiceImpl(@Qualifier(\u0026#34;orderQueryClient\u0026#34;) OrderQueryClient orderQueryClient) { this.orderQueryClient = orderQueryClient; } @Override public ListResult\u0026lt;OrderModel\u0026gt; findList(List\u0026lt;String\u0026gt; orderIdList) { // 参数校验 if (CollectionUtils.isEmpty(orderIdList)) { return new ListResult\u0026lt;\u0026gt;(true); } // 订单ID过滤：假设仅支持3个月内容的订单 List\u0026lt;String\u0026gt; destOrderIdList = orderIdList.stream().filter(this::filterOrderId).collect(Collectors.toList()); ListResult\u0026lt;OrderModel\u0026gt; orderResult; try { // 查询订单信息 List\u0026lt;OrderDTO\u0026gt; orderList = this.orderQueryClient.queryList(destOrderIdList); // 订单模型转换 List\u0026lt;OrderModel\u0026gt; modelList = orderList.stream().map(OrderModel::from).collect(Collectors.toList()); // 返回成功结果 orderResult = new ListResult\u0026lt;\u0026gt;(true); orderResult.setResultObj(modelList); } catch (Throwable e) { // 测试日志输出 orderResult = new ListResult\u0026lt;\u0026gt;(false); orderResult.setCode(\u0026#34;UNKNOWN_ERROR\u0026#34;); orderResult.setMessage(e.getMessage()); } return orderResult; } /** * yyyyMMdd日期检查：订单ID格式为日期开头，且日期为最近3个月内 */ private boolean filterOrderId(String orderId) { if (StringUtils.length(orderId) \u0026lt; 8) { return false; } // 3个月的日期 LocalDate minOrderDate = LocalDate.now().minusMonths(3L); // 当前订单的日期 LocalDate orderDate; try { String prefix = StringUtils.substring(orderId, 0, 8); orderDate = LocalDate.parse(prefix, DateTimeFormatter.BASIC_ISO_DATE); // 比较是否为3个月之内 return !orderDate.isBefore(minOrderDate); } catch (DateTimeParseException e) { return false; } } } 被模拟对象 - 远程订单查询服务接口（OrderQueryClient.java）：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 /** * Author: obullxl@163.com * Copyright (c) 2020-2023 All Rights Reserved. */ package cn.ntopic.order; import cn.ntopic.order.dto.OrderDTO; import java.util.List; /** * 订单查询客户端 * * @author obullxl 2023年05月20日: 新增 */ public interface OrderQueryClient { /** * 查询订单列表 * * @param orderIdList 订单ID列表 * @return 订单基本信息 */ List\u0026lt;OrderDTO\u0026gt; queryList(List\u0026lt;String\u0026gt; orderIdList); } 其他的依赖类见本文末尾附录！\n集成测试用例（OrderQueryServiceTest.java）：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 /** * Author: obullxl@163.com * Copyright (c) 2020-2023 All Rights Reserved. */ package cn.ntopic.example.mockito; import cn.ntopic.ListResult; import cn.ntopic.NTAbstractTest; import cn.ntopic.order.OrderQueryClient; import cn.ntopic.order.OrderQueryService; import cn.ntopic.order.dto.OrderDTO; import cn.ntopic.order.enums.OrderStatusEnum; import cn.ntopic.order.model.OrderModel; import com.google.common.collect.Lists; import org.junit.Assert; import org.junit.Test; import org.mockito.ArgumentCaptor; import org.mockito.Mockito; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.beans.factory.annotation.Qualifier; import org.springframework.boot.test.mock.mockito.MockBean; import java.time.LocalDate; import java.time.format.DateTimeFormatter; import java.util.List; /** * OrderService-单元测试 * * @author obullxl 2023年05月20日: 新增 */ public class OrderQueryServiceTest extends NTAbstractTest { /** * 测试对象 */ @Autowired @Qualifier(\u0026#34;orderQueryService\u0026#34;) private OrderQueryService orderQueryService; /** * Mock对象 */ @MockBean @Autowired @Qualifier(\u0026#34;orderQueryClient\u0026#34;) private OrderQueryClient orderQueryClient; /** * Case01-正常返回：测试过滤3个月内订单逻辑 */ @Test public void test_queryList_01() { final String orderId1 = LocalDate.now().format(DateTimeFormatter.BASIC_ISO_DATE) + \u0026#34;0001\u0026#34;; final String orderId2 = LocalDate.now().minusMonths(3L).format(DateTimeFormatter.BASIC_ISO_DATE) + \u0026#34;0002\u0026#34;; final String orderId3 = LocalDate.now().minusMonths(3L).minusDays(1L).format(DateTimeFormatter.BASIC_ISO_DATE) + \u0026#34;0003\u0026#34;; // 1. Mock客户端返回结果 // 正常不过滤的订单 OrderDTO orderDTO1 = new OrderDTO(); orderDTO1.setId(orderId1); orderDTO1.setAmount(\u0026#34;3.33\u0026#34;); orderDTO1.setStatus(OrderStatusEnum.WAIT_PAY.getCode()); // 刚好3个月的临界值，不过滤的订单 OrderDTO orderDTO2 = new OrderDTO(); orderDTO2.setId(orderId2); orderDTO2.setAmount(\u0026#34;4.44\u0026#34;); orderDTO2.setStatus(OrderStatusEnum.FINISHED.getCode()); Mockito.when(this.orderQueryClient.queryList(Mockito.anyList())).thenReturn(Lists.newArrayList(orderDTO1, orderDTO2)); // 2. 执行查询逻辑 ListResult\u0026lt;OrderModel\u0026gt; orderResult = this.orderQueryService.findList(Lists.newArrayList(orderId1, orderId2, orderId3)); // 3. 检查执行结果：正常返回，orderId3被过滤 Assert.assertTrue(orderResult.isSuccess()); List\u0026lt;OrderModel\u0026gt; modeList = orderResult.getResultObj(); Assert.assertEquals(2, modeList.size()); ArgumentCaptor\u0026lt;List\u0026lt;String\u0026gt;\u0026gt; captorOrderIdList = ArgumentCaptor.forClass(List.class); Mockito.verify(this.orderQueryClient, Mockito.times(1)).queryList(captorOrderIdList.capture()); List\u0026lt;String\u0026gt; queryOrderIdList = captorOrderIdList.getValue(); Assert.assertEquals(2, queryOrderIdList.size()); Assert.assertTrue(queryOrderIdList.contains(orderId1)); Assert.assertTrue(queryOrderIdList.contains(orderId2)); } /** * Case02-客户端异常：测试订单客户端异常逻辑 */ @Test public void test_queryList_02() { final String orderId1 = LocalDate.now().format(DateTimeFormatter.BASIC_ISO_DATE) + \u0026#34;0001\u0026#34;; final String orderId2 = LocalDate.now().minusMonths(3L).format(DateTimeFormatter.BASIC_ISO_DATE) + \u0026#34;0002\u0026#34;; final String orderId3 = LocalDate.now().minusMonths(3L).minusDays(1L).format(DateTimeFormatter.BASIC_ISO_DATE) + \u0026#34;0003\u0026#34;; final String errorMessage = \u0026#34;Mock异常\u0026#34;; // 1. Mock客户端抛出异常 Mockito.when(this.orderQueryClient.queryList(Mockito.anyList())).thenThrow(new RuntimeException(errorMessage)); // 2. 执行查询逻辑 ListResult\u0026lt;OrderModel\u0026gt; orderResult = this.orderQueryService.findList(Lists.newArrayList(orderId1, orderId2, orderId3)); // 3. 检查执行结果：查询异常，orderId3被过滤 Assert.assertFalse(orderResult.isSuccess()); Assert.assertEquals(\u0026#34;UNKNOWN_ERROR\u0026#34;, orderResult.getCode()); Assert.assertEquals(errorMessage, orderResult.getMessage()); ArgumentCaptor\u0026lt;List\u0026lt;String\u0026gt;\u0026gt; captorOrderIdList = ArgumentCaptor.forClass(List.class); Mockito.verify(this.orderQueryClient, Mockito.atLeast(1)).queryList(captorOrderIdList.capture()); List\u0026lt;String\u0026gt; queryOrderIdList = captorOrderIdList.getValue(); Assert.assertEquals(2, queryOrderIdList.size()); Assert.assertTrue(queryOrderIdList.contains(orderId1)); Assert.assertTrue(queryOrderIdList.contains(orderId2)); } } 附录\u0026ndash;其他代码 NTAbstractTest.java\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 /** * Author: obullxl@163.com * Copyright (c) 2020-2022 All Rights Reserved. */ package cn.ntopic; import cn.ntopic.core.xml.XMLNode; import cn.ntopic.core.xml.XMLUtils; import org.apache.commons.io.IOUtils; import org.junit.runner.RunWith; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.beans.factory.annotation.Qualifier; import org.springframework.boot.test.context.SpringBootTest; import org.springframework.jdbc.core.JdbcTemplate; import org.springframework.test.context.junit4.AbstractJUnit4SpringContextTests; import org.springframework.test.context.junit4.SpringRunner; import java.io.InputStream; /** * @author obullxl 2022年01月09日: 新增 */ @SpringBootTest @RunWith(SpringRunner.class) public abstract class NTAbstractTest extends AbstractJUnit4SpringContextTests { } Result.java\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 /** * Author: obullxl@163.com * Copyright (c) 2020-2023 All Rights Reserved. */ package cn.ntopic; import cn.ntopic.core.builder.ToString; /** * 请求结果包装器 * * @author obullxl 2023年05月20日: 新增 */ public class Result\u0026lt;T\u0026gt; extends ToString { /** * 结果成功标识 */ private final boolean success; /** * 结果返回码（如失败错误码、幂等成功标识等） */ private String code; /** * 结果返回描述 */ private String message; /** * 结果对象 */ private T resultObj; public Result(boolean success) { this.success = success; } public boolean isSuccess() { return success; } public String getCode() { return code; } public void setCode(String code) { this.code = code; } public String getMessage() { return message; } public void setMessage(String message) { this.message = message; } public T getResultObj() { return resultObj; } public void setResultObj(T resultObj) { this.resultObj = resultObj; } } ListResult.java\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 /** * Author: obullxl@163.com * Copyright (c) 2020-2023 All Rights Reserved. */ package cn.ntopic; import java.util.List; /** * 请求结果列表包装器 * * @author obullxl 2023年05月20日: 新增 */ public class ListResult\u0026lt;T\u0026gt; extends Result\u0026lt;List\u0026lt;T\u0026gt;\u0026gt; { public ListResult(boolean success) { super(success); } } OrderDTO.java\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 /** * Author: obullxl@163.com * Copyright (c) 2020-2023 All Rights Reserved. */ package cn.ntopic.order.dto; import cn.ntopic.core.builder.ToString; /** * 订单基本信息 * * @author obullxl 2023年05月20日: 新增 */ public class OrderDTO extends ToString { /** * 订单ID，非空 */ private String id; /** * 订单金额 */ private String amount; /** * 订单状态 */ private int status; public String getId() { return id; } public void setId(String id) { this.id = id; } public String getAmount() { return amount; } public void setAmount(String amount) { this.amount = amount; } public int getStatus() { return status; } public void setStatus(int status) { this.status = status; } } OrderModel.java\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 /** * Author: obullxl@163.com * Copyright (c) 2020-2023 All Rights Reserved. */ package cn.ntopic.order.model; import cn.ntopic.order.dto.OrderDTO; import cn.ntopic.order.enums.OrderStatusEnum; import cn.ntopic.core.builder.ToString; import org.springframework.util.Assert; import java.math.BigDecimal; /** * 订单对象 * * @author obullxl 2023年05月20日: 新增 */ public class OrderModel extends ToString { /** * 订单ID，非空 */ private String id; /** * 订单金额 */ private BigDecimal amount; /** * 订单状态 */ private OrderStatusEnum status; /** * 构建订单模型 */ public static OrderModel from(OrderDTO orderDTO) { Assert.notNull(orderDTO, \u0026#34;OrderDTO入参为NULL.\u0026#34;); OrderModel orderModel = new OrderModel(); orderModel.setId(orderDTO.getId()); orderModel.setAmount(new BigDecimal(orderDTO.getAmount())); orderModel.setStatus(OrderStatusEnum.convert(orderDTO.getStatus()).orElse(OrderStatusEnum.UNKNOWN)); return orderModel; } public String getId() { return id; } public void setId(String id) { this.id = id; } public BigDecimal getAmount() { return amount; } public void setAmount(BigDecimal amount) { this.amount = amount; } public OrderStatusEnum getStatus() { return status; } public void setStatus(OrderStatusEnum status) { this.status = status; } } OrderStatusEnum.java\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 /** * Author: obullxl@163.com * Copyright (c) 2020-2023 All Rights Reserved. */ package cn.ntopic.order.enums; import java.util.Arrays; import java.util.Optional; /** * 订单状态枚举 * * @author obullxl 2023年05月20日: 新增 */ public enum OrderStatusEnum { UNKNOWN(0, \u0026#34;未知\u0026#34;), WAIT_PAY(3, \u0026#34;创建待支付\u0026#34;), CLOSED(5, \u0026#34;支付超时关闭\u0026#34;), FINISHED(9, \u0026#34;订单已完成\u0026#34;), ; /** * 状态代码 */ private final int code; /** * 状态描述 */ private final String message; OrderStatusEnum(int code, String message) { this.code = code; this.message = message; } /** * 枚举转换 */ public static Optional\u0026lt;OrderStatusEnum\u0026gt; convert(int code) { return Arrays.stream(OrderStatusEnum.values()).filter(e -\u0026gt; code == e.getCode()).findFirst(); } public int getCode() { return code; } public String getMessage() { return message; } } ","date":"2023-05-20T00:00:00Z","image":"https://ntopic.cn/p/2023052001/images/01_hu1f2ced91771ffa736a99e93a73a6b369_227088_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/2023052001/","title":"Mockito测试框架在SpringBoot集成测试中的介绍和实战"},{"content":" 可可是个很顽皮的小女孩，常常把自己的小手弄伤。有一天，她的小手突然对她说：“可可，你一点都不喜欢我，我要罢工。”，可可听了，心里越想越害怕。\n“可可，上课呢，你在想什么呢？”老师问到，“没什么，没什么！”可可赶紧回答，拿起笔，准备做笔记。可是，小手一动也不动。可可心想：糟了，小手不会真的罢工了吧？\n老师看到可可根本没有动笔写笔记，于是生气地说：“可可，你怎么还不记笔记！在不记的话，我就要叫家长了！”\n可可赶紧低下头，难过地说：“老师，我马上就记笔记，您先继续上课吧，我错了。”\n老师见可可承认错误了，这才消了气，继续上课去了。\n没办法，可可只好用嘴咬住笔，写起了笔记。\n放学了，可可难过地回到了家。因为这天不是老师骂她，就是同学批评她。妈妈看到这么伤心，于是就问到：“可可，你怎么了？”，可可把今天发生的一切都告诉了妈妈。妈妈安慰她说：“一切都会好起来的，宝贝。”\n神奇的事情发生了，可可的手又能动了。可可对妈妈说：“妈妈，我以后一定要保护我的小手。”\n学校： 上海市浦东新区第二中学小学（张江校区） 班级： 三（2）班 作者： 徐韵 指导老师： 郑卢倩\n","date":"2023-05-14T00:00:00Z","image":"https://ntopic.cn/p/2023051401/images/01_hu1f2ced91771ffa736a99e93a73a6b369_552600_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/2023051401/","title":"上海·浦东二小·三年级·作文《手罢工了》"},{"content":"Fork/Join框架：总分总思路 Java的Doug Lea大神在Java 7的JUC包中，已经实现了Fork/Join框架。\n该框架特别适合总-分-总的使用场景，类似于MapReduce思想：将大任务拆分成若干个小任务，最终汇总每个小任务的结果后得到最终大任务的结果。每个小任务直接相互独立。\nFork/Join框架：2个核心类 Fork/Join框架的核心只有两个：ForkJoinPool和ForkJoinTask\nForkJoinPool主要负责实现工作窃取算法、管理工作线程、提供关于任务的状态以及执行信息。 ForkJoinTask主要提供在任务中执行Fork（拆分任务）和Join（汇总任务）操作的机制。 Fork/Join框架实战：数值累加 任务目标： 累加给定的a~b数字区间\nFork/Join实现思路：\n设定一个阈值，每个任务的计算量超过这个阈值，则进行任务拆分 当拆分了子任务时，当前任务的结果需要汇总子任务的结果 一直递归下去 ForkJoinTask任务实现：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 /** * Author: obullxl@163.com * Copyright (c) 2020-2023 All Rights Reserved. */ package cn.ntopic.example; import java.util.concurrent.RecursiveTask; /** * ForkJoinTask--数据计算样例：各一个数字区间，计算数字的累加值！ * * 基本思路--总体为`总-分-总`思想，类似于MapReduce思路： * 1. 拆分任务：根据当前任务参数，决策是否需要进行任务拆分；如果需要拆分，则本任务的结果为所有拆分任务的汇总 * 2. 汇总任务：根据第1点思路，第1个任务为总任务-拆分子任务-汇总子任务结果，那么第1个任务的值就是最终的值 * * @author obullxl 2023年05月13日: 新增 */ public class CalculateForkJoinTask extends RecursiveTask\u0026lt;Integer\u0026gt; { /** 任务拆分的阈值，超过该值则任务需要拆分*/ public static final int THRESHOLD = 10; /** * 任务参数：数据计算的开始值 */ private final int start; /** * 任务参数：数据计算的结束值 */ private final int finish; public CalculateForkJoinTask(int start, int finish) { this.start = start; this.finish = finish; } @Override public Integer compute() { int sum = 0; // 检测单个任务计算量是否符合阈值，如果超过了的话进行任务拆分 if ((this.finish - this.start) \u0026lt;= THRESHOLD) { for (int i = start; i \u0026lt;= finish; i++) { sum += i; } } else { // 单个任务量超过阈值，则进行任务拆分：这里是拆成了2个任务，可根据业务实际情况拆出多个任务 int middle = (this.start + this.finish) / 2; RecursiveTask\u0026lt;Integer\u0026gt; leftTask = new CalculateForkJoinTask(this.start, middle); RecursiveTask\u0026lt;Integer\u0026gt; rightTask = new CalculateForkJoinTask(middle + 1, this.finish); // 执行每一个子任务：这里只有2个子任务 leftTask.fork(); rightTask.fork(); // 等待并获取每个子任务执行的结束 int leftResult = leftTask.join(); int rightResult = rightTask.join(); // 合并子任务的执行结果 sum = leftResult + rightResult; } // 本任务的结果：可能是最终的子任务，也可能是多个子任务是汇总结果 return sum; } } ForkJoinTask的实战验证：\n验证点： 1~100累加值=5050\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 /** * Author: obullxl@163.com * Copyright (c) 2020-2023 All Rights Reserved. */ package cn.ntopic.example; import org.junit.Assert; import org.junit.Test; import java.util.concurrent.ExecutionException; import java.util.concurrent.ForkJoinPool; import java.util.concurrent.Future; import java.util.concurrent.RecursiveTask; /** * CalculateForkJoinTask--测试验证 * * @author obullxl 2023年05月13日: 新增 */ public class CalculateForkJoinTaskTest { @Test public void test() throws ExecutionException, InterruptedException { // 构建任务：累加1~100值 RecursiveTask\u0026lt;Integer\u0026gt; task = new CalculateForkJoinTask(1, 100); // 执行任务 Future\u0026lt;Integer\u0026gt; result = ForkJoinPool.commonPool().submit(task); // 验证结果 Assert.assertEquals(5050, result.get().intValue()); } } 我的本博客原地址：https://ntopic.cn/p/2023051301\n","date":"2023-05-13T00:00:00Z","image":"https://ntopic.cn/p/2023051301/images/01_hu841d0111d96665be72149f0ea52abb94_55425_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/2023051301/","title":"Java ForkJoin框架分析和实战"},{"content":"2023年5月6日，五一假期刚过，我惠南的房子开始去交易中心过户。聊天过程中，竟然发现买家是和我同行，房产经理和我们都是同龄人，一路都在聊编程的事情。\n业务办完回去后，房产经理给我发消息，他想自学编程，让我推荐给他一些资料： 1 2 3 4 5 6 7 【开发网站，最快速使用范围最广的语言：PHP】https://www.runoob.com/php/php-tutorial.html 【开发服务端语言很多，使用最广：Java】https://www.runoob.com/java/java-tutorial.html Java最好配合SpringBoot研发框架，加速研发效率：https://docs.spring.io/spring-framework/docs/current/reference/html/overview.html#overview 【手机App跨平台的研发框架：Flutter】https://book.flutterchina.club 它只是研发App，APP后端还是需要如Java/C++/Go等 咨询文心一言：如何看待房产中介自学编程这件事情：\n在当今快速变化的科技行业中，学习编程已经成为了越来越多的人的选择。然而，对于一位已经三十岁的房产中介来说，这个决定并不容易。这篇文章将探讨这个决定背后的原因以及可能的优缺点。\n首先，为什么一位三十岁的房产中介开始自学编程？可能是因为他意识到互联网技术在未来的发展趋势中具有重要地位，并且他认为这是实现职业发展的必经之路。此外，学习编程需要付出大量时间和精力，但他相信这是实现自我提升和职业转型的关键步骤。\n然而，这个决定也存在一些潜在的优缺点。首先，学习编程需要投入大量时间和精力，因此这可能会影响到他的日常工作。但是，如果他真正热爱编程并决心将其作为自己的事业来发展，那么这些投入是值得的。其次，学习编程可以帮助他提高自己的技能水平，增强自己在职场中的竞争力。这也可能会使他在未来找到更好的工作机会。\n此外，虽然三十岁对于很多人来说已经是职业发展的晚期，但这并不意味着不能学习编程并取得成功。事实上，很多成功的人都有晚年学习新技能的经历。因此，只要他有热情和决心，就可以实现自己的职业目标。\n总之，对于一位三十岁的房产中介来说，开始自学编程是一个值得考虑的决定。虽然这可能会影响到他的日常工作，但如果他真正热爱编程并决心将其作为自己的事业来发展，那么这些投入是值得的。同时，学习编程可以帮助他提高自己的技能水平，增强自己在职场中的竞争力。只要他有热情和决心，就可以实现自己的职业目标。\n","date":"2023-05-06T00:00:00Z","image":"https://ntopic.cn/p/2023050601/images/00_hu0193db57f06b91c4aa0a6890f0086e09_60269_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/2023050601/","title":"房产中介开始自学编程(AIGC)"},{"content":"我的百度文心一言公测申请通过了，测试一下，以下内容由文心一言生成，总体来说中规中矩，没啥内容也没啥毛病，让我感受一下AIGC的力量：\n读完《思辨与立场》这本书，我深深地感受到了思辨的重要性。在这个信息爆炸的时代，人们需要有自己的思考和判断能力，而思辨就是锻炼这种能力的重要方法。\n在这本书中，作者分享了很多自己的思考和判断过程，让我深受启发。他认为，在表达自己的观点时，要先理清自己的思路，确定自己的立场，然后再进行严密的逻辑推理和论证。只有这样，才能让自己的观点更加有说服力，也更容易被别人接受。\n同时，作者也提到了一些常见的思辨误区，例如过度推理、以偏概全、情感干扰等。这些误区容易让人们产生偏见和误解，因此在表达自己的观点时，需要特别注意避免这些问题。\n除了思辨本身，作者还讲述了一些具体的案例，让我更好地理解了思辨在现实生活中的应用。例如，在处理政治问题时，需要运用逻辑和推理来分析各种观点和利益关系，从而找到最合适的解决方案。在处理道德和伦理问题时，需要运用思辨来分析各种道德原则和行为准则，从而找到最合适的行动方案。\n总之，《思辨与立场》这本书让我认识到了思辨的重要性，也让我更好地理解了思辨在现实生活中的应用。我相信，只有不断地思考和判断，才能让自己成为一个更加有价值的人。\n书籍封面： ","date":"2023-04-30T00:00:00Z","image":"https://ntopic.cn/p/2023043001/images/01_hu6062912998c0a6a562d474e8bba19c78_493815_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/2023043001/","title":"《思辨与立场》读书感想（AIGC）"},{"content":"\n1. 关爱能力和规划能力 2. 管理能力和自我认知 3. 金钱管理和想象能力 4. 抗挫能力和观察总结 5. 关爱能力和规划能力 6. 防范意识和学习价值 ","date":"2023-04-29T00:00:00Z","image":"https://ntopic.cn/p/2023042901/images/01_hu9b6430ee40126011a705e28e56d37910_41379_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/2023042901/","title":"爸爸妈妈们赶紧和孩子聊聊这些话题"},{"content":"\n","date":"2023-03-28T00:00:00Z","image":"https://ntopic.cn/p/2023032801/images/01_huee898996b37f1ad175e3aa44cb140a25_93790_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/2023032801/","title":"一图看懂：阿里巴巴6大业务集团架构"},{"content":" ","date":"2023-03-14T00:00:00Z","image":"https://ntopic.cn/p/2023031401/images/01_hu4c13269df8319b093a1959d4976a5f6c_44838_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/2023031401/","title":"爸爸妈妈睡前应多和孩子聊的话题"},{"content":"最终追溯到2007年（读研期间），我就开始写博客了，主要是记录自己在学习技术过程中总结，当时用百度博客、CSDN博客、博客园博客等。\n最近几年，技术学习时间少了，博客更新也暂停了。当今天想记录点内容时，可惜的发现这些博客基本都打不开了，除了CSDN和博客园，为这2个博客网站的坚持运营点赞👍🏻\n2023年重新起航，同时申请了自己的域名：ntopic.cn\n奔跑的蜗牛 ：ntopic.cn 其他博客空间地址：\nCSDN (2007~2011)：https://blog.csdn.net/obullxl 博客园 (2011~2013)：obullxl.cnblogs.com Gitee Pages ：obullxl.gitee.io GitHub Pages ：obullxl.github.io ","date":"2023-01-01T00:00:00Z","image":"https://ntopic.cn/p/2023010101/images/01_hu974b50fdff02bd4e2320c389d9f6b1fa_61004_120x120_fill_q75_box_smart1.jpg","permalink":"https://ntopic.cn/p/2023010101/","title":"2023我的博客重新起航啦（奔跑的蜗牛）~"}]